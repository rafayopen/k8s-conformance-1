I0731 16:22:49.339726      21 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-776750169
I0731 16:22:49.339742      21 test_context.go:419] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0731 16:22:49.339854      21 e2e.go:109] Starting e2e run "c753a180-79d5-407b-97e2-ac8929ab4a10" on Ginkgo node 1
{"msg":"Test Suite starting","total":280,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1596212567 - Will randomize all specs
Will run 280 of 4843 specs

E0731 16:22:49.351437      21 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post http://localhost:8099/progress: dial tcp 127.0.0.1:8099: connect: connection refused
Jul 31 16:22:49.351: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
Jul 31 16:22:49.353: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jul 31 16:22:49.371: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jul 31 16:22:49.404: INFO: 18 / 18 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul 31 16:22:49.405: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Jul 31 16:22:49.405: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jul 31 16:22:49.414: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jul 31 16:22:49.414: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jul 31 16:22:49.414: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'nvidia-device-plugin-daemonset' (0 seconds elapsed)
Jul 31 16:22:49.414: INFO: e2e test version: v1.17.6
Jul 31 16:22:49.415: INFO: kube-apiserver version: v1.17.6
Jul 31 16:22:49.415: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
Jul 31 16:22:49.420: INFO: Cluster IP family: ipv4
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:22:49.420: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename container-lifecycle-hook
Jul 31 16:22:49.468: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Jul 31 16:22:49.480: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-474
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 31 16:23:01.692: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 31 16:23:01.698: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 31 16:23:03.698: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 31 16:23:03.702: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 31 16:23:05.698: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 31 16:23:05.701: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:23:05.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-474" for this suite.

• [SLOW TEST:16.290 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":280,"completed":1,"skipped":0,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:23:05.711: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1201
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 16:23:06.931: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 31 16:23:08.941: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731809386, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731809386, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731809386, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731809386, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 16:23:11.988: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:23:11.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1201" for this suite.
STEP: Destroying namespace "webhook-1201-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.428 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":280,"completed":2,"skipped":4,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:23:12.150: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3452
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 31 16:23:12.336: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4a6d331a-73a4-4065-8428-49730d6716fd" in namespace "downward-api-3452" to be "success or failure"
Jul 31 16:23:12.355: INFO: Pod "downwardapi-volume-4a6d331a-73a4-4065-8428-49730d6716fd": Phase="Pending", Reason="", readiness=false. Elapsed: 18.91621ms
Jul 31 16:23:14.359: INFO: Pod "downwardapi-volume-4a6d331a-73a4-4065-8428-49730d6716fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023074274s
Jul 31 16:23:16.370: INFO: Pod "downwardapi-volume-4a6d331a-73a4-4065-8428-49730d6716fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034316546s
Jul 31 16:23:18.375: INFO: Pod "downwardapi-volume-4a6d331a-73a4-4065-8428-49730d6716fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038979926s
STEP: Saw pod success
Jul 31 16:23:18.375: INFO: Pod "downwardapi-volume-4a6d331a-73a4-4065-8428-49730d6716fd" satisfied condition "success or failure"
Jul 31 16:23:18.378: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod downwardapi-volume-4a6d331a-73a4-4065-8428-49730d6716fd container client-container: <nil>
STEP: delete the pod
Jul 31 16:23:18.442: INFO: Waiting for pod downwardapi-volume-4a6d331a-73a4-4065-8428-49730d6716fd to disappear
Jul 31 16:23:18.445: INFO: Pod downwardapi-volume-4a6d331a-73a4-4065-8428-49730d6716fd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:23:18.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3452" for this suite.

• [SLOW TEST:6.313 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":3,"skipped":77,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:23:18.464: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7921
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0731 16:23:28.852072      21 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 31 16:23:28.852: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:23:28.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7921" for this suite.

• [SLOW TEST:10.398 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":280,"completed":4,"skipped":87,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:23:28.862: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1559
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Jul 31 16:23:29.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 create -f - --namespace=kubectl-1559'
Jul 31 16:23:30.401: INFO: stderr: ""
Jul 31 16:23:30.401: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 31 16:23:30.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1559'
Jul 31 16:23:30.489: INFO: stderr: ""
Jul 31 16:23:30.489: INFO: stdout: "update-demo-nautilus-cwcsr update-demo-nautilus-jdrnb "
Jul 31 16:23:30.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods update-demo-nautilus-cwcsr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1559'
Jul 31 16:23:30.560: INFO: stderr: ""
Jul 31 16:23:30.560: INFO: stdout: ""
Jul 31 16:23:30.560: INFO: update-demo-nautilus-cwcsr is created but not running
Jul 31 16:23:35.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1559'
Jul 31 16:23:35.695: INFO: stderr: ""
Jul 31 16:23:35.695: INFO: stdout: "update-demo-nautilus-cwcsr update-demo-nautilus-jdrnb "
Jul 31 16:23:35.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods update-demo-nautilus-cwcsr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1559'
Jul 31 16:23:35.778: INFO: stderr: ""
Jul 31 16:23:35.778: INFO: stdout: ""
Jul 31 16:23:35.778: INFO: update-demo-nautilus-cwcsr is created but not running
Jul 31 16:23:40.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1559'
Jul 31 16:23:40.893: INFO: stderr: ""
Jul 31 16:23:40.893: INFO: stdout: "update-demo-nautilus-cwcsr update-demo-nautilus-jdrnb "
Jul 31 16:23:40.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods update-demo-nautilus-cwcsr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1559'
Jul 31 16:23:40.967: INFO: stderr: ""
Jul 31 16:23:40.967: INFO: stdout: "true"
Jul 31 16:23:40.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods update-demo-nautilus-cwcsr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1559'
Jul 31 16:23:41.050: INFO: stderr: ""
Jul 31 16:23:41.050: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 31 16:23:41.050: INFO: validating pod update-demo-nautilus-cwcsr
Jul 31 16:23:41.060: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 31 16:23:41.060: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 31 16:23:41.060: INFO: update-demo-nautilus-cwcsr is verified up and running
Jul 31 16:23:41.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods update-demo-nautilus-jdrnb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1559'
Jul 31 16:23:41.136: INFO: stderr: ""
Jul 31 16:23:41.136: INFO: stdout: "true"
Jul 31 16:23:41.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods update-demo-nautilus-jdrnb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1559'
Jul 31 16:23:41.218: INFO: stderr: ""
Jul 31 16:23:41.218: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 31 16:23:41.218: INFO: validating pod update-demo-nautilus-jdrnb
Jul 31 16:23:41.224: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 31 16:23:41.224: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 31 16:23:41.224: INFO: update-demo-nautilus-jdrnb is verified up and running
STEP: using delete to clean up resources
Jul 31 16:23:41.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 delete --grace-period=0 --force -f - --namespace=kubectl-1559'
Jul 31 16:23:41.338: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 31 16:23:41.338: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 31 16:23:41.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1559'
Jul 31 16:23:41.433: INFO: stderr: "No resources found in kubectl-1559 namespace.\n"
Jul 31 16:23:41.433: INFO: stdout: ""
Jul 31 16:23:41.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods -l name=update-demo --namespace=kubectl-1559 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 31 16:23:41.503: INFO: stderr: ""
Jul 31 16:23:41.503: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:23:41.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1559" for this suite.

• [SLOW TEST:12.662 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":280,"completed":5,"skipped":104,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:23:41.524: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4095
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jul 31 16:23:46.237: INFO: Successfully updated pod "annotationupdateb0659efa-d084-41e7-8eb4-f3cd49da0aa7"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:23:48.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4095" for this suite.

• [SLOW TEST:6.749 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":6,"skipped":106,"failed":0}
SSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:23:48.274: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5442
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod test-webserver-f216ea81-c4fe-4183-bda5-a2644918bc24 in namespace container-probe-5442
Jul 31 16:23:52.447: INFO: Started pod test-webserver-f216ea81-c4fe-4183-bda5-a2644918bc24 in namespace container-probe-5442
STEP: checking the pod's current state and verifying that restartCount is present
Jul 31 16:23:52.449: INFO: Initial restart count of pod test-webserver-f216ea81-c4fe-4183-bda5-a2644918bc24 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:27:52.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5442" for this suite.

• [SLOW TEST:244.648 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":7,"skipped":109,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:27:52.927: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1855
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1855
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1855
STEP: creating replication controller externalsvc in namespace services-1855
I0731 16:27:53.173565      21 runners.go:189] Created replication controller with name: externalsvc, namespace: services-1855, replica count: 2
I0731 16:27:56.223998      21 runners.go:189] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0731 16:27:59.224228      21 runners.go:189] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0731 16:28:02.224443      21 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jul 31 16:28:02.246: INFO: Creating new exec pod
Jul 31 16:28:06.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=services-1855 execpod7wwkh -- /bin/sh -x -c nslookup clusterip-service'
Jul 31 16:28:06.535: INFO: stderr: "+ nslookup clusterip-service\n"
Jul 31 16:28:06.535: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-1855.svc.cluster.local\tcanonical name = externalsvc.services-1855.svc.cluster.local.\nName:\texternalsvc.services-1855.svc.cluster.local\nAddress: 10.109.237.210\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1855, will wait for the garbage collector to delete the pods
Jul 31 16:28:06.601: INFO: Deleting ReplicationController externalsvc took: 13.665135ms
Jul 31 16:28:07.202: INFO: Terminating ReplicationController externalsvc pods took: 600.245274ms
Jul 31 16:28:12.047: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:28:12.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1855" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:19.194 seconds]
[sig-network] Services
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":280,"completed":8,"skipped":151,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:28:12.127: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-338
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1585
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 31 16:28:12.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-338'
Jul 31 16:28:12.373: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 31 16:28:12.373: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Jul 31 16:28:12.387: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jul 31 16:28:12.399: INFO: scanned /root for discovery docs: <nil>
Jul 31 16:28:12.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-338'
Jul 31 16:28:34.211: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul 31 16:28:34.211: INFO: stdout: "Created e2e-test-httpd-rc-7ddb1ea18b731159cd66eba50f3a0bef\nScaling up e2e-test-httpd-rc-7ddb1ea18b731159cd66eba50f3a0bef from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-7ddb1ea18b731159cd66eba50f3a0bef up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-7ddb1ea18b731159cd66eba50f3a0bef to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Jul 31 16:28:34.211: INFO: stdout: "Created e2e-test-httpd-rc-7ddb1ea18b731159cd66eba50f3a0bef\nScaling up e2e-test-httpd-rc-7ddb1ea18b731159cd66eba50f3a0bef from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-7ddb1ea18b731159cd66eba50f3a0bef up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-7ddb1ea18b731159cd66eba50f3a0bef to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Jul 31 16:28:34.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-338'
Jul 31 16:28:34.282: INFO: stderr: ""
Jul 31 16:28:34.282: INFO: stdout: "e2e-test-httpd-rc-7ddb1ea18b731159cd66eba50f3a0bef-hv7n9 "
Jul 31 16:28:34.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods e2e-test-httpd-rc-7ddb1ea18b731159cd66eba50f3a0bef-hv7n9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-338'
Jul 31 16:28:34.366: INFO: stderr: ""
Jul 31 16:28:34.366: INFO: stdout: "true"
Jul 31 16:28:34.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods e2e-test-httpd-rc-7ddb1ea18b731159cd66eba50f3a0bef-hv7n9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-338'
Jul 31 16:28:34.479: INFO: stderr: ""
Jul 31 16:28:34.479: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Jul 31 16:28:34.479: INFO: e2e-test-httpd-rc-7ddb1ea18b731159cd66eba50f3a0bef-hv7n9 is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
Jul 31 16:28:34.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 delete rc e2e-test-httpd-rc --namespace=kubectl-338'
Jul 31 16:28:34.565: INFO: stderr: ""
Jul 31 16:28:34.565: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:28:34.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-338" for this suite.

• [SLOW TEST:22.466 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1580
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl rolling-update should support rolling-update to same image  [Conformance]","total":280,"completed":9,"skipped":219,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:28:34.595: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5183
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 16:28:35.134: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 31 16:28:37.141: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731809714, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731809714, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731809714, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731809714, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 16:28:40.165: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:28:40.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5183" for this suite.
STEP: Destroying namespace "webhook-5183-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.971 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":280,"completed":10,"skipped":251,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:28:40.567: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-375
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 16:28:40.724: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 31 16:28:44.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 --namespace=crd-publish-openapi-375 create -f -'
Jul 31 16:28:45.898: INFO: stderr: ""
Jul 31 16:28:45.898: INFO: stdout: "e2e-test-crd-publish-openapi-7230-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jul 31 16:28:45.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 --namespace=crd-publish-openapi-375 delete e2e-test-crd-publish-openapi-7230-crds test-cr'
Jul 31 16:28:45.975: INFO: stderr: ""
Jul 31 16:28:45.975: INFO: stdout: "e2e-test-crd-publish-openapi-7230-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jul 31 16:28:45.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 --namespace=crd-publish-openapi-375 apply -f -'
Jul 31 16:28:46.177: INFO: stderr: ""
Jul 31 16:28:46.177: INFO: stdout: "e2e-test-crd-publish-openapi-7230-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jul 31 16:28:46.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 --namespace=crd-publish-openapi-375 delete e2e-test-crd-publish-openapi-7230-crds test-cr'
Jul 31 16:28:46.268: INFO: stderr: ""
Jul 31 16:28:46.268: INFO: stdout: "e2e-test-crd-publish-openapi-7230-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jul 31 16:28:46.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 explain e2e-test-crd-publish-openapi-7230-crds'
Jul 31 16:28:46.460: INFO: stderr: ""
Jul 31 16:28:46.460: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7230-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:28:50.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-375" for this suite.

• [SLOW TEST:9.522 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":280,"completed":11,"skipped":262,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:28:50.090: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6412
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating cluster-info
Jul 31 16:28:50.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 cluster-info'
Jul 31 16:28:50.318: INFO: stderr: ""
Jul 31 16:28:50.318: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:28:50.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6412" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":280,"completed":12,"skipped":280,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:28:50.331: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5587
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-5587
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Jul 31 16:28:50.635: INFO: Found 0 stateful pods, waiting for 3
Jul 31 16:29:00.642: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 16:29:00.642: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 16:29:00.642: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jul 31 16:29:10.638: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 16:29:10.638: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 16:29:10.638: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jul 31 16:29:10.682: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jul 31 16:29:20.718: INFO: Updating stateful set ss2
Jul 31 16:29:20.737: INFO: Waiting for Pod statefulset-5587/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 31 16:29:30.741: INFO: Waiting for Pod statefulset-5587/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Jul 31 16:29:40.836: INFO: Found 2 stateful pods, waiting for 3
Jul 31 16:29:50.839: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 16:29:50.839: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 16:29:50.839: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jul 31 16:29:50.863: INFO: Updating stateful set ss2
Jul 31 16:29:50.880: INFO: Waiting for Pod statefulset-5587/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 31 16:30:00.906: INFO: Updating stateful set ss2
Jul 31 16:30:00.920: INFO: Waiting for StatefulSet statefulset-5587/ss2 to complete update
Jul 31 16:30:00.920: INFO: Waiting for Pod statefulset-5587/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 31 16:30:10.928: INFO: Waiting for StatefulSet statefulset-5587/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jul 31 16:30:20.925: INFO: Deleting all statefulset in ns statefulset-5587
Jul 31 16:30:20.926: INFO: Scaling statefulset ss2 to 0
Jul 31 16:30:50.965: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 16:30:50.969: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:30:50.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5587" for this suite.

• [SLOW TEST:120.671 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":280,"completed":13,"skipped":283,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:30:51.002: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5547
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 31 16:30:51.156: INFO: Waiting up to 5m0s for pod "downwardapi-volume-db677565-fc4b-4098-b050-ed60ef3f15e1" in namespace "downward-api-5547" to be "success or failure"
Jul 31 16:30:51.166: INFO: Pod "downwardapi-volume-db677565-fc4b-4098-b050-ed60ef3f15e1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.671946ms
Jul 31 16:30:53.168: INFO: Pod "downwardapi-volume-db677565-fc4b-4098-b050-ed60ef3f15e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012484034s
Jul 31 16:30:55.172: INFO: Pod "downwardapi-volume-db677565-fc4b-4098-b050-ed60ef3f15e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016320595s
STEP: Saw pod success
Jul 31 16:30:55.172: INFO: Pod "downwardapi-volume-db677565-fc4b-4098-b050-ed60ef3f15e1" satisfied condition "success or failure"
Jul 31 16:30:55.176: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod downwardapi-volume-db677565-fc4b-4098-b050-ed60ef3f15e1 container client-container: <nil>
STEP: delete the pod
Jul 31 16:30:55.207: INFO: Waiting for pod downwardapi-volume-db677565-fc4b-4098-b050-ed60ef3f15e1 to disappear
Jul 31 16:30:55.212: INFO: Pod downwardapi-volume-db677565-fc4b-4098-b050-ed60ef3f15e1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:30:55.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5547" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":14,"skipped":287,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:30:55.223: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6791
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jul 31 16:30:55.362: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 31 16:30:55.370: INFO: Waiting for terminating namespaces to be deleted...
Jul 31 16:30:55.372: INFO: 
Logging pods the kubelet thinks is on node test-aruna-123-node-group-687e2c91ce before test
Jul 31 16:30:55.378: INFO: sonobuoy-systemd-logs-daemon-set-21a6c8c28f874b58-rcrfg from sonobuoy started at 2020-07-31 16:22:15 +0000 UTC (2 container statuses recorded)
Jul 31 16:30:55.378: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 16:30:55.378: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 16:30:55.378: INFO: nginx-ingress-controller-bb6bp from ccp started at 2020-07-30 20:35:21 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.378: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jul 31 16:30:55.378: INFO: ccp-monitor-prometheus-pushgateway-7d5b6d448b-4cpkd from ccp started at 2020-07-30 20:37:47 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.378: INFO: 	Container prometheus-pushgateway ready: true, restart count 0
Jul 31 16:30:55.378: INFO: sonobuoy from sonobuoy started at 2020-07-31 16:21:46 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.378: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 31 16:30:55.378: INFO: calico-node-98r4p from kube-system started at 2020-07-30 20:35:01 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.378: INFO: 	Container calico-node ready: true, restart count 0
Jul 31 16:30:55.378: INFO: ccp-helm-operator-749fb6dc7-f9mhf from ccp started at 2020-07-30 20:35:31 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.378: INFO: 	Container ccp-helm-operator ready: true, restart count 0
Jul 31 16:30:55.378: INFO: ccp-monitor-prometheus-pass-job-u7slu-rztnq from ccp started at 2020-07-30 20:37:47 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.378: INFO: 	Container ccp-monitor-prometheus-pass-container ready: false, restart count 0
Jul 31 16:30:55.378: INFO: kube-proxy-9nrgh from kube-system started at 2020-07-30 20:35:01 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.378: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 16:30:55.378: INFO: nvidia-device-plugin-daemonset-tzdxm from kube-system started at 2020-07-30 20:35:21 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.378: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Jul 31 16:30:55.378: INFO: ccp-monitor-prometheus-node-exporter-wmldc from ccp started at 2020-07-30 20:37:46 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.378: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jul 31 16:30:55.378: INFO: ccp-monitor-prometheus-port-update-v83hn-jkj8b from ccp started at 2020-07-30 20:37:47 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.378: INFO: 	Container ccp-monitor-prometheus-port-update ready: false, restart count 0
Jul 31 16:30:55.378: INFO: ccp-monitor-grafana-78fd4cc979-rdwrh from ccp started at 2020-07-30 20:37:51 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.378: INFO: 	Container grafana ready: true, restart count 0
Jul 31 16:30:55.378: INFO: metallb-speaker-lgvxz from ccp started at 2020-07-30 20:35:21 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.378: INFO: 	Container metallb-speaker ready: true, restart count 0
Jul 31 16:30:55.378: INFO: 
Logging pods the kubelet thinks is on node test-aruna-123-node-group-a8f122b701 before test
Jul 31 16:30:55.385: INFO: calico-node-fkmgx from kube-system started at 2020-07-30 20:34:54 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.385: INFO: 	Container calico-node ready: true, restart count 0
Jul 31 16:30:55.385: INFO: cert-manager-7c4fdf69b7-ct9hx from ccp started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.386: INFO: 	Container cert-manager ready: true, restart count 0
Jul 31 16:30:55.386: INFO: nginx-ingress-default-backend-6b546bb848-d7ms8 from ccp started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.386: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jul 31 16:30:55.386: INFO: kube-proxy-rpj2g from kube-system started at 2020-07-30 20:34:54 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.386: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 16:30:55.386: INFO: nginx-ingress-controller-vv7zf from ccp started at 2020-07-30 20:35:04 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.386: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jul 31 16:30:55.386: INFO: metallb-controller-6c8c5fd7fd-9n8gm from ccp started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.386: INFO: 	Container metallb-controller ready: true, restart count 0
Jul 31 16:30:55.386: INFO: ccp-monitor-prometheus-server-67d55955c7-b54b2 from ccp started at 2020-07-30 20:37:51 +0000 UTC (3 container statuses recorded)
Jul 31 16:30:55.386: INFO: 	Container nginx-proxy ready: true, restart count 0
Jul 31 16:30:55.386: INFO: 	Container prometheus-server ready: true, restart count 0
Jul 31 16:30:55.386: INFO: 	Container prometheus-server-configmap-reload ready: true, restart count 0
Jul 31 16:30:55.386: INFO: nvidia-device-plugin-daemonset-b7rb6 from kube-system started at 2020-07-30 20:35:04 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.386: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Jul 31 16:30:55.386: INFO: ccp-monitor-prometheus-node-exporter-82955 from ccp started at 2020-07-30 20:37:46 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.386: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jul 31 16:30:55.386: INFO: sonobuoy-systemd-logs-daemon-set-21a6c8c28f874b58-64p9f from sonobuoy started at 2020-07-31 16:22:16 +0000 UTC (2 container statuses recorded)
Jul 31 16:30:55.386: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 16:30:55.386: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 16:30:55.386: INFO: metallb-speaker-g2jrx from ccp started at 2020-07-30 20:35:04 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.386: INFO: 	Container metallb-speaker ready: true, restart count 0
Jul 31 16:30:55.386: INFO: 
Logging pods the kubelet thinks is on node test-aruna-123-node-group-a92c781fbd before test
Jul 31 16:30:55.397: INFO: metallb-speaker-q6n4f from ccp started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.397: INFO: 	Container metallb-speaker ready: true, restart count 0
Jul 31 16:30:55.397: INFO: ccp-monitor-prometheus-node-exporter-559kz from ccp started at 2020-07-30 20:37:46 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.397: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jul 31 16:30:55.397: INFO: sonobuoy-systemd-logs-daemon-set-21a6c8c28f874b58-5vrgt from sonobuoy started at 2020-07-31 16:22:17 +0000 UTC (2 container statuses recorded)
Jul 31 16:30:55.397: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 16:30:55.397: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 16:30:55.397: INFO: nvidia-device-plugin-daemonset-4pt4d from kube-system started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.397: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Jul 31 16:30:55.397: INFO: kubernetes-dashboard-dbfcd4d-mkklx from ccp started at 2020-07-30 20:37:33 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.397: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jul 31 16:30:55.397: INFO: ccp-monitor-prometheus-kube-state-metrics-7f7c9f986-xzrgl from ccp started at 2020-07-30 20:37:46 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.397: INFO: 	Container prometheus-kube-state-metrics ready: true, restart count 0
Jul 31 16:30:55.397: INFO: ccp-monitor-grafana-set-datasource-qwx6l from ccp started at 2020-07-30 20:37:46 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.397: INFO: 	Container ccp-monitor-grafana-set-datasource ready: false, restart count 0
Jul 31 16:30:55.397: INFO: calico-node-txdhs from kube-system started at 2020-07-30 20:35:00 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.397: INFO: 	Container calico-node ready: true, restart count 0
Jul 31 16:30:55.397: INFO: sonobuoy-e2e-job-e7094f6bfc7c4c8d from sonobuoy started at 2020-07-31 16:22:17 +0000 UTC (2 container statuses recorded)
Jul 31 16:30:55.397: INFO: 	Container e2e ready: true, restart count 0
Jul 31 16:30:55.397: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 16:30:55.397: INFO: kube-proxy-q4q9v from kube-system started at 2020-07-30 20:35:00 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.398: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 16:30:55.398: INFO: nginx-ingress-controller-2447g from ccp started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 16:30:55.398: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jul 31 16:30:55.398: INFO: ccp-monitor-prometheus-alertmanager-57f5689547-wvfrs from ccp started at 2020-07-30 20:37:51 +0000 UTC (2 container statuses recorded)
Jul 31 16:30:55.398: INFO: 	Container prometheus-alertmanager ready: true, restart count 0
Jul 31 16:30:55.398: INFO: 	Container prometheus-alertmanager-configmap-reload ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-78345bee-5766-43ac-8ac8-9b2373aa5891 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-78345bee-5766-43ac-8ac8-9b2373aa5891 off the node test-aruna-123-node-group-687e2c91ce
STEP: verifying the node doesn't have the label kubernetes.io/e2e-78345bee-5766-43ac-8ac8-9b2373aa5891
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:31:11.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6791" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:16.333 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":280,"completed":15,"skipped":337,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:31:11.556: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-705
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-68b8b426-d431-495e-927b-7194e08898b0
STEP: Creating a pod to test consume configMaps
Jul 31 16:31:11.721: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-46406d53-78f9-40a4-9446-c80e559f9d22" in namespace "projected-705" to be "success or failure"
Jul 31 16:31:11.736: INFO: Pod "pod-projected-configmaps-46406d53-78f9-40a4-9446-c80e559f9d22": Phase="Pending", Reason="", readiness=false. Elapsed: 15.395379ms
Jul 31 16:31:13.740: INFO: Pod "pod-projected-configmaps-46406d53-78f9-40a4-9446-c80e559f9d22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018937589s
Jul 31 16:31:15.742: INFO: Pod "pod-projected-configmaps-46406d53-78f9-40a4-9446-c80e559f9d22": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021606242s
Jul 31 16:31:17.744: INFO: Pod "pod-projected-configmaps-46406d53-78f9-40a4-9446-c80e559f9d22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02368572s
STEP: Saw pod success
Jul 31 16:31:17.744: INFO: Pod "pod-projected-configmaps-46406d53-78f9-40a4-9446-c80e559f9d22" satisfied condition "success or failure"
Jul 31 16:31:17.748: INFO: Trying to get logs from node test-aruna-123-node-group-a92c781fbd pod pod-projected-configmaps-46406d53-78f9-40a4-9446-c80e559f9d22 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 16:31:17.774: INFO: Waiting for pod pod-projected-configmaps-46406d53-78f9-40a4-9446-c80e559f9d22 to disappear
Jul 31 16:31:17.777: INFO: Pod pod-projected-configmaps-46406d53-78f9-40a4-9446-c80e559f9d22 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:31:17.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-705" for this suite.

• [SLOW TEST:6.247 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":280,"completed":16,"skipped":341,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:31:17.807: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-6827
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 31 16:31:30.056: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 31 16:31:30.063: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 31 16:31:32.063: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 31 16:31:32.067: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 31 16:31:34.064: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 31 16:31:34.068: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 31 16:31:36.063: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 31 16:31:36.068: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 31 16:31:38.063: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 31 16:31:38.070: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 31 16:31:40.063: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 31 16:31:40.067: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 31 16:31:42.063: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 31 16:31:42.067: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 31 16:31:44.063: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 31 16:31:44.067: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:31:44.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6827" for this suite.

• [SLOW TEST:26.279 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":280,"completed":17,"skipped":392,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:31:44.086: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9336
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:31:55.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9336" for this suite.

• [SLOW TEST:11.196 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":280,"completed":18,"skipped":392,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:31:55.283: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5386
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 16:31:55.443: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jul 31 16:32:00.447: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 31 16:32:00.447: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jul 31 16:32:04.487: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5386 /apis/apps/v1/namespaces/deployment-5386/deployments/test-cleanup-deployment 82b5469f-77da-4583-baad-6ee5584dcde4 212206 1 2020-07-31 16:31:59 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002409bd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-07-31 16:32:00 +0000 UTC,LastTransitionTime:2020-07-31 16:32:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-55ffc6b7b6" has successfully progressed.,LastUpdateTime:2020-07-31 16:32:02 +0000 UTC,LastTransitionTime:2020-07-31 16:32:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 31 16:32:04.489: INFO: New ReplicaSet "test-cleanup-deployment-55ffc6b7b6" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-55ffc6b7b6  deployment-5386 /apis/apps/v1/namespaces/deployment-5386/replicasets/test-cleanup-deployment-55ffc6b7b6 24493bd7-a9ab-4145-92ec-344eed266397 212196 1 2020-07-31 16:32:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 82b5469f-77da-4583-baad-6ee5584dcde4 0xc001fcaaa7 0xc001fcaaa8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55ffc6b7b6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001fcab18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 31 16:32:04.491: INFO: Pod "test-cleanup-deployment-55ffc6b7b6-mzzmn" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-55ffc6b7b6-mzzmn test-cleanup-deployment-55ffc6b7b6- deployment-5386 /api/v1/namespaces/deployment-5386/pods/test-cleanup-deployment-55ffc6b7b6-mzzmn 4fca8728-009f-4142-ada3-ec0c082cadfd 212195 0 2020-07-31 16:32:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[cni.projectcalico.org/podIP:192.168.220.97/32 cni.projectcalico.org/podIPs:192.168.220.97/32] [{apps/v1 ReplicaSet test-cleanup-deployment-55ffc6b7b6 24493bd7-a9ab-4145-92ec-344eed266397 0xc001fcb2c7 0xc001fcb2c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ds2mc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ds2mc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ds2mc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-687e2c91ce,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 16:31:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 16:32:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 16:32:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 16:32:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.92,PodIP:192.168.220.97,StartTime:2020-07-31 16:31:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-31 16:32:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://8e96510b11a80523a26ecc6dbbfa284baaabeee4432e930b89c050eaa32ca0dc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.220.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:32:04.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5386" for this suite.

• [SLOW TEST:9.215 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":280,"completed":19,"skipped":424,"failed":0}
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:32:04.498: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7313
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Jul 31 16:32:04.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 create -f - --namespace=kubectl-7313'
Jul 31 16:32:04.893: INFO: stderr: ""
Jul 31 16:32:04.893: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Jul 31 16:32:05.896: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 31 16:32:05.896: INFO: Found 0 / 1
Jul 31 16:32:06.896: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 31 16:32:06.896: INFO: Found 0 / 1
Jul 31 16:32:07.897: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 31 16:32:07.897: INFO: Found 1 / 1
Jul 31 16:32:07.897: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jul 31 16:32:07.900: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 31 16:32:07.900: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 31 16:32:07.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 patch pod agnhost-master-6wf7g --namespace=kubectl-7313 -p {"metadata":{"annotations":{"x":"y"}}}'
Jul 31 16:32:07.980: INFO: stderr: ""
Jul 31 16:32:07.980: INFO: stdout: "pod/agnhost-master-6wf7g patched\n"
STEP: checking annotations
Jul 31 16:32:07.982: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 31 16:32:07.982: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:32:07.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7313" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":280,"completed":20,"skipped":424,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:32:07.992: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3661
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jul 31 16:32:14.253: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W0731 16:32:14.253527      21 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 31 16:32:14.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3661" for this suite.

• [SLOW TEST:6.297 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":280,"completed":21,"skipped":426,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:32:14.289: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-1856
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 16:32:14.441: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1856
I0731 16:32:14.464114      21 runners.go:189] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1856, replica count: 1
I0731 16:32:15.514641      21 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0731 16:32:16.514816      21 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0731 16:32:17.515204      21 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0731 16:32:18.515496      21 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 31 16:32:18.660: INFO: Created: latency-svc-gtp7j
Jul 31 16:32:18.669: INFO: Got endpoints: latency-svc-gtp7j [53.698159ms]
Jul 31 16:32:18.719: INFO: Created: latency-svc-s9slg
Jul 31 16:32:18.726: INFO: Got endpoints: latency-svc-s9slg [55.11603ms]
Jul 31 16:32:18.746: INFO: Created: latency-svc-7v4pd
Jul 31 16:32:18.759: INFO: Got endpoints: latency-svc-7v4pd [88.097589ms]
Jul 31 16:32:18.790: INFO: Created: latency-svc-s4d4f
Jul 31 16:32:18.797: INFO: Got endpoints: latency-svc-s4d4f [126.484219ms]
Jul 31 16:32:18.843: INFO: Created: latency-svc-bc7m7
Jul 31 16:32:18.854: INFO: Got endpoints: latency-svc-bc7m7 [184.608925ms]
Jul 31 16:32:18.861: INFO: Created: latency-svc-bjs6c
Jul 31 16:32:18.866: INFO: Got endpoints: latency-svc-bjs6c [196.132052ms]
Jul 31 16:32:18.898: INFO: Created: latency-svc-mkpsl
Jul 31 16:32:18.904: INFO: Got endpoints: latency-svc-mkpsl [233.34946ms]
Jul 31 16:32:18.923: INFO: Created: latency-svc-cfqp7
Jul 31 16:32:18.951: INFO: Got endpoints: latency-svc-cfqp7 [281.357698ms]
Jul 31 16:32:18.987: INFO: Created: latency-svc-9t64k
Jul 31 16:32:18.991: INFO: Got endpoints: latency-svc-9t64k [319.95211ms]
Jul 31 16:32:19.036: INFO: Created: latency-svc-8q5nn
Jul 31 16:32:19.041: INFO: Got endpoints: latency-svc-8q5nn [371.010355ms]
Jul 31 16:32:19.080: INFO: Created: latency-svc-8v78j
Jul 31 16:32:19.093: INFO: Got endpoints: latency-svc-8v78j [422.665767ms]
Jul 31 16:32:19.130: INFO: Created: latency-svc-j24n5
Jul 31 16:32:19.140: INFO: Got endpoints: latency-svc-j24n5 [469.051389ms]
Jul 31 16:32:19.147: INFO: Created: latency-svc-fnk5w
Jul 31 16:32:19.159: INFO: Got endpoints: latency-svc-fnk5w [487.954054ms]
Jul 31 16:32:19.210: INFO: Created: latency-svc-hdpfv
Jul 31 16:32:19.224: INFO: Got endpoints: latency-svc-hdpfv [553.624227ms]
Jul 31 16:32:19.241: INFO: Created: latency-svc-9rqg4
Jul 31 16:32:19.281: INFO: Got endpoints: latency-svc-9rqg4 [610.335897ms]
Jul 31 16:32:19.288: INFO: Created: latency-svc-dc5n5
Jul 31 16:32:19.301: INFO: Got endpoints: latency-svc-dc5n5 [629.688476ms]
Jul 31 16:32:19.361: INFO: Created: latency-svc-4htq7
Jul 31 16:32:19.369: INFO: Got endpoints: latency-svc-4htq7 [643.111516ms]
Jul 31 16:32:19.380: INFO: Created: latency-svc-cbccb
Jul 31 16:32:19.415: INFO: Got endpoints: latency-svc-cbccb [655.987474ms]
Jul 31 16:32:19.444: INFO: Created: latency-svc-c87s8
Jul 31 16:32:19.493: INFO: Got endpoints: latency-svc-c87s8 [695.783166ms]
Jul 31 16:32:19.522: INFO: Created: latency-svc-24nxr
Jul 31 16:32:19.535: INFO: Got endpoints: latency-svc-24nxr [680.634972ms]
Jul 31 16:32:19.573: INFO: Created: latency-svc-v26jr
Jul 31 16:32:19.618: INFO: Got endpoints: latency-svc-v26jr [752.207267ms]
Jul 31 16:32:19.652: INFO: Created: latency-svc-knrmk
Jul 31 16:32:19.652: INFO: Got endpoints: latency-svc-knrmk [748.827814ms]
Jul 31 16:32:19.668: INFO: Created: latency-svc-267s6
Jul 31 16:32:19.672: INFO: Got endpoints: latency-svc-267s6 [719.756826ms]
Jul 31 16:32:19.695: INFO: Created: latency-svc-7frzc
Jul 31 16:32:19.708: INFO: Got endpoints: latency-svc-7frzc [716.472332ms]
Jul 31 16:32:19.769: INFO: Created: latency-svc-lmd68
Jul 31 16:32:19.771: INFO: Got endpoints: latency-svc-lmd68 [729.559765ms]
Jul 31 16:32:19.791: INFO: Created: latency-svc-c8zl9
Jul 31 16:32:19.803: INFO: Got endpoints: latency-svc-c8zl9 [710.272177ms]
Jul 31 16:32:19.843: INFO: Created: latency-svc-pjrq4
Jul 31 16:32:19.855: INFO: Got endpoints: latency-svc-pjrq4 [715.300936ms]
Jul 31 16:32:19.902: INFO: Created: latency-svc-46dmv
Jul 31 16:32:19.906: INFO: Got endpoints: latency-svc-46dmv [747.354738ms]
Jul 31 16:32:19.938: INFO: Created: latency-svc-7gj7w
Jul 31 16:32:19.944: INFO: Got endpoints: latency-svc-7gj7w [719.621992ms]
Jul 31 16:32:19.969: INFO: Created: latency-svc-szzf9
Jul 31 16:32:19.981: INFO: Got endpoints: latency-svc-szzf9 [699.516486ms]
Jul 31 16:32:20.036: INFO: Created: latency-svc-6ttfx
Jul 31 16:32:20.044: INFO: Got endpoints: latency-svc-6ttfx [742.72633ms]
Jul 31 16:32:20.045: INFO: Created: latency-svc-tcbt6
Jul 31 16:32:20.073: INFO: Got endpoints: latency-svc-tcbt6 [703.752743ms]
Jul 31 16:32:20.096: INFO: Created: latency-svc-zj2cg
Jul 31 16:32:20.104: INFO: Got endpoints: latency-svc-zj2cg [688.742604ms]
Jul 31 16:32:20.157: INFO: Created: latency-svc-6q2tj
Jul 31 16:32:20.159: INFO: Got endpoints: latency-svc-6q2tj [666.16383ms]
Jul 31 16:32:20.175: INFO: Created: latency-svc-22zlc
Jul 31 16:32:20.185: INFO: Got endpoints: latency-svc-22zlc [649.79625ms]
Jul 31 16:32:20.202: INFO: Created: latency-svc-tf7f2
Jul 31 16:32:20.230: INFO: Got endpoints: latency-svc-tf7f2 [611.985015ms]
Jul 31 16:32:20.249: INFO: Created: latency-svc-wkd5m
Jul 31 16:32:20.278: INFO: Got endpoints: latency-svc-wkd5m [625.569697ms]
Jul 31 16:32:20.286: INFO: Created: latency-svc-m5k4d
Jul 31 16:32:20.288: INFO: Got endpoints: latency-svc-m5k4d [616.304844ms]
Jul 31 16:32:20.317: INFO: Created: latency-svc-gcdzq
Jul 31 16:32:20.329: INFO: Got endpoints: latency-svc-gcdzq [621.114676ms]
Jul 31 16:32:20.347: INFO: Created: latency-svc-89czn
Jul 31 16:32:20.356: INFO: Got endpoints: latency-svc-89czn [584.092932ms]
Jul 31 16:32:20.381: INFO: Created: latency-svc-w222w
Jul 31 16:32:20.411: INFO: Got endpoints: latency-svc-w222w [607.32649ms]
Jul 31 16:32:20.420: INFO: Created: latency-svc-nspxb
Jul 31 16:32:20.426: INFO: Got endpoints: latency-svc-nspxb [570.712917ms]
Jul 31 16:32:20.465: INFO: Created: latency-svc-4g4r8
Jul 31 16:32:20.473: INFO: Got endpoints: latency-svc-4g4r8 [566.336469ms]
Jul 31 16:32:20.498: INFO: Created: latency-svc-vh6kz
Jul 31 16:32:20.543: INFO: Got endpoints: latency-svc-vh6kz [598.761277ms]
Jul 31 16:32:20.554: INFO: Created: latency-svc-jjhsz
Jul 31 16:32:20.583: INFO: Created: latency-svc-r2kkf
Jul 31 16:32:20.583: INFO: Got endpoints: latency-svc-jjhsz [602.652665ms]
Jul 31 16:32:20.593: INFO: Got endpoints: latency-svc-r2kkf [548.913267ms]
Jul 31 16:32:20.617: INFO: Created: latency-svc-jkhck
Jul 31 16:32:20.617: INFO: Got endpoints: latency-svc-jkhck [543.673577ms]
Jul 31 16:32:20.672: INFO: Created: latency-svc-d2h82
Jul 31 16:32:20.688: INFO: Got endpoints: latency-svc-d2h82 [584.242719ms]
Jul 31 16:32:20.708: INFO: Created: latency-svc-kd92q
Jul 31 16:32:20.727: INFO: Got endpoints: latency-svc-kd92q [568.066058ms]
Jul 31 16:32:20.745: INFO: Created: latency-svc-6zn27
Jul 31 16:32:20.759: INFO: Got endpoints: latency-svc-6zn27 [574.244904ms]
Jul 31 16:32:20.767: INFO: Created: latency-svc-2gznh
Jul 31 16:32:20.798: INFO: Got endpoints: latency-svc-2gznh [567.686227ms]
Jul 31 16:32:20.819: INFO: Created: latency-svc-hfnkz
Jul 31 16:32:20.837: INFO: Got endpoints: latency-svc-hfnkz [557.496234ms]
Jul 31 16:32:20.860: INFO: Created: latency-svc-vts56
Jul 31 16:32:20.869: INFO: Got endpoints: latency-svc-vts56 [580.162455ms]
Jul 31 16:32:20.897: INFO: Created: latency-svc-qvqbb
Jul 31 16:32:21.034: INFO: Got endpoints: latency-svc-qvqbb [704.587094ms]
Jul 31 16:32:21.069: INFO: Created: latency-svc-t6mx9
Jul 31 16:32:21.081: INFO: Got endpoints: latency-svc-t6mx9 [725.698995ms]
Jul 31 16:32:21.106: INFO: Created: latency-svc-grjc9
Jul 31 16:32:21.111: INFO: Got endpoints: latency-svc-grjc9 [700.468812ms]
Jul 31 16:32:21.188: INFO: Created: latency-svc-x76cj
Jul 31 16:32:21.197: INFO: Got endpoints: latency-svc-x76cj [770.009625ms]
Jul 31 16:32:21.213: INFO: Created: latency-svc-2swrt
Jul 31 16:32:21.231: INFO: Got endpoints: latency-svc-2swrt [758.740398ms]
Jul 31 16:32:21.250: INFO: Created: latency-svc-dtr6w
Jul 31 16:32:21.257: INFO: Got endpoints: latency-svc-dtr6w [713.898807ms]
Jul 31 16:32:21.303: INFO: Created: latency-svc-g2cq5
Jul 31 16:32:21.326: INFO: Got endpoints: latency-svc-g2cq5 [742.364998ms]
Jul 31 16:32:21.337: INFO: Created: latency-svc-dsq59
Jul 31 16:32:21.347: INFO: Got endpoints: latency-svc-dsq59 [754.335931ms]
Jul 31 16:32:21.359: INFO: Created: latency-svc-4d86w
Jul 31 16:32:21.372: INFO: Got endpoints: latency-svc-4d86w [755.611587ms]
Jul 31 16:32:21.392: INFO: Created: latency-svc-66594
Jul 31 16:32:21.394: INFO: Got endpoints: latency-svc-66594 [706.131322ms]
Jul 31 16:32:21.448: INFO: Created: latency-svc-j58k4
Jul 31 16:32:21.453: INFO: Got endpoints: latency-svc-j58k4 [726.205575ms]
Jul 31 16:32:21.470: INFO: Created: latency-svc-lqv52
Jul 31 16:32:21.482: INFO: Got endpoints: latency-svc-lqv52 [722.876483ms]
Jul 31 16:32:21.509: INFO: Created: latency-svc-dd5wz
Jul 31 16:32:21.513: INFO: Got endpoints: latency-svc-dd5wz [714.864501ms]
Jul 31 16:32:21.533: INFO: Created: latency-svc-57hpb
Jul 31 16:32:21.570: INFO: Got endpoints: latency-svc-57hpb [732.707458ms]
Jul 31 16:32:21.590: INFO: Created: latency-svc-rhbd8
Jul 31 16:32:21.601: INFO: Got endpoints: latency-svc-rhbd8 [732.486116ms]
Jul 31 16:32:21.626: INFO: Created: latency-svc-ld2rn
Jul 31 16:32:21.635: INFO: Got endpoints: latency-svc-ld2rn [600.970856ms]
Jul 31 16:32:21.672: INFO: Created: latency-svc-qzp89
Jul 31 16:32:21.705: INFO: Got endpoints: latency-svc-qzp89 [623.890291ms]
Jul 31 16:32:21.729: INFO: Created: latency-svc-ztsq7
Jul 31 16:32:21.739: INFO: Got endpoints: latency-svc-ztsq7 [627.239629ms]
Jul 31 16:32:21.770: INFO: Created: latency-svc-2cphw
Jul 31 16:32:21.781: INFO: Got endpoints: latency-svc-2cphw [583.710989ms]
Jul 31 16:32:21.796: INFO: Created: latency-svc-hnpcz
Jul 31 16:32:21.803: INFO: Got endpoints: latency-svc-hnpcz [570.916711ms]
Jul 31 16:32:21.834: INFO: Created: latency-svc-428mw
Jul 31 16:32:21.847: INFO: Got endpoints: latency-svc-428mw [589.802834ms]
Jul 31 16:32:21.874: INFO: Created: latency-svc-2bqj6
Jul 31 16:32:21.888: INFO: Got endpoints: latency-svc-2bqj6 [561.348595ms]
Jul 31 16:32:21.900: INFO: Created: latency-svc-2drgf
Jul 31 16:32:21.911: INFO: Got endpoints: latency-svc-2drgf [562.81205ms]
Jul 31 16:32:21.937: INFO: Created: latency-svc-nf229
Jul 31 16:32:21.953: INFO: Got endpoints: latency-svc-nf229 [580.353385ms]
Jul 31 16:32:21.962: INFO: Created: latency-svc-7bt9m
Jul 31 16:32:21.973: INFO: Got endpoints: latency-svc-7bt9m [578.038916ms]
Jul 31 16:32:22.010: INFO: Created: latency-svc-8bxlh
Jul 31 16:32:22.019: INFO: Got endpoints: latency-svc-8bxlh [565.254389ms]
Jul 31 16:32:22.030: INFO: Created: latency-svc-zgqrz
Jul 31 16:32:22.220: INFO: Got endpoints: latency-svc-zgqrz [737.270441ms]
Jul 31 16:32:22.231: INFO: Created: latency-svc-ttb69
Jul 31 16:32:22.249: INFO: Got endpoints: latency-svc-ttb69 [735.458129ms]
Jul 31 16:32:22.269: INFO: Created: latency-svc-z8m5m
Jul 31 16:32:22.269: INFO: Got endpoints: latency-svc-z8m5m [698.15608ms]
Jul 31 16:32:22.292: INFO: Created: latency-svc-l6kmp
Jul 31 16:32:22.303: INFO: Got endpoints: latency-svc-l6kmp [701.280185ms]
Jul 31 16:32:22.351: INFO: Created: latency-svc-hbd24
Jul 31 16:32:22.358: INFO: Got endpoints: latency-svc-hbd24 [721.928151ms]
Jul 31 16:32:22.376: INFO: Created: latency-svc-svpl4
Jul 31 16:32:22.382: INFO: Got endpoints: latency-svc-svpl4 [675.859514ms]
Jul 31 16:32:22.407: INFO: Created: latency-svc-m4wfb
Jul 31 16:32:22.416: INFO: Got endpoints: latency-svc-m4wfb [676.106889ms]
Jul 31 16:32:22.427: INFO: Created: latency-svc-p6xbd
Jul 31 16:32:22.433: INFO: Got endpoints: latency-svc-p6xbd [651.537719ms]
Jul 31 16:32:22.496: INFO: Created: latency-svc-nbfn4
Jul 31 16:32:22.497: INFO: Got endpoints: latency-svc-nbfn4 [694.210856ms]
Jul 31 16:32:22.507: INFO: Created: latency-svc-rwfmv
Jul 31 16:32:22.513: INFO: Got endpoints: latency-svc-rwfmv [664.931391ms]
Jul 31 16:32:22.539: INFO: Created: latency-svc-8wvlz
Jul 31 16:32:22.544: INFO: Got endpoints: latency-svc-8wvlz [655.347398ms]
Jul 31 16:32:22.568: INFO: Created: latency-svc-cslss
Jul 31 16:32:22.571: INFO: Got endpoints: latency-svc-cslss [659.774345ms]
Jul 31 16:32:22.621: INFO: Created: latency-svc-kbjqj
Jul 31 16:32:22.624: INFO: Got endpoints: latency-svc-kbjqj [670.737194ms]
Jul 31 16:32:22.653: INFO: Created: latency-svc-x4p9g
Jul 31 16:32:22.674: INFO: Got endpoints: latency-svc-x4p9g [700.529381ms]
Jul 31 16:32:22.694: INFO: Created: latency-svc-5rzzb
Jul 31 16:32:22.704: INFO: Got endpoints: latency-svc-5rzzb [685.170752ms]
Jul 31 16:32:22.746: INFO: Created: latency-svc-d4kzg
Jul 31 16:32:22.759: INFO: Got endpoints: latency-svc-d4kzg [538.632169ms]
Jul 31 16:32:22.778: INFO: Created: latency-svc-n6rs9
Jul 31 16:32:22.797: INFO: Got endpoints: latency-svc-n6rs9 [548.298547ms]
Jul 31 16:32:22.805: INFO: Created: latency-svc-n2mk9
Jul 31 16:32:22.811: INFO: Got endpoints: latency-svc-n2mk9 [542.110889ms]
Jul 31 16:32:22.887: INFO: Created: latency-svc-ncgwr
Jul 31 16:32:22.891: INFO: Got endpoints: latency-svc-ncgwr [587.403007ms]
Jul 31 16:32:22.906: INFO: Created: latency-svc-gnppf
Jul 31 16:32:22.919: INFO: Got endpoints: latency-svc-gnppf [561.300389ms]
Jul 31 16:32:22.933: INFO: Created: latency-svc-6tsg8
Jul 31 16:32:22.940: INFO: Got endpoints: latency-svc-6tsg8 [556.24056ms]
Jul 31 16:32:22.992: INFO: Created: latency-svc-r726m
Jul 31 16:32:23.005: INFO: Created: latency-svc-8bwht
Jul 31 16:32:23.013: INFO: Got endpoints: latency-svc-r726m [597.148382ms]
Jul 31 16:32:23.033: INFO: Got endpoints: latency-svc-8bwht [600.307513ms]
Jul 31 16:32:23.063: INFO: Created: latency-svc-q4jn5
Jul 31 16:32:23.069: INFO: Got endpoints: latency-svc-q4jn5 [571.986429ms]
Jul 31 16:32:23.096: INFO: Created: latency-svc-gwt6z
Jul 31 16:32:23.122: INFO: Got endpoints: latency-svc-gwt6z [608.778781ms]
Jul 31 16:32:23.145: INFO: Created: latency-svc-w2zs7
Jul 31 16:32:23.157: INFO: Got endpoints: latency-svc-w2zs7 [613.011191ms]
Jul 31 16:32:23.176: INFO: Created: latency-svc-gstht
Jul 31 16:32:23.182: INFO: Got endpoints: latency-svc-gstht [611.26262ms]
Jul 31 16:32:23.202: INFO: Created: latency-svc-pmsdn
Jul 31 16:32:23.208: INFO: Got endpoints: latency-svc-pmsdn [583.118307ms]
Jul 31 16:32:23.250: INFO: Created: latency-svc-vbgft
Jul 31 16:32:23.257: INFO: Got endpoints: latency-svc-vbgft [581.925421ms]
Jul 31 16:32:23.272: INFO: Created: latency-svc-5fx86
Jul 31 16:32:23.281: INFO: Got endpoints: latency-svc-5fx86 [576.148152ms]
Jul 31 16:32:23.288: INFO: Created: latency-svc-l44bj
Jul 31 16:32:23.295: INFO: Got endpoints: latency-svc-l44bj [536.302184ms]
Jul 31 16:32:23.310: INFO: Created: latency-svc-vd4hs
Jul 31 16:32:23.316: INFO: Got endpoints: latency-svc-vd4hs [518.446632ms]
Jul 31 16:32:23.326: INFO: Created: latency-svc-gtp5c
Jul 31 16:32:23.336: INFO: Got endpoints: latency-svc-gtp5c [524.870547ms]
Jul 31 16:32:23.394: INFO: Created: latency-svc-wt5wq
Jul 31 16:32:23.398: INFO: Got endpoints: latency-svc-wt5wq [505.873873ms]
Jul 31 16:32:23.422: INFO: Created: latency-svc-tpsbh
Jul 31 16:32:23.432: INFO: Got endpoints: latency-svc-tpsbh [512.615081ms]
Jul 31 16:32:23.449: INFO: Created: latency-svc-8l7sw
Jul 31 16:32:23.456: INFO: Got endpoints: latency-svc-8l7sw [515.924479ms]
Jul 31 16:32:23.466: INFO: Created: latency-svc-r6lrc
Jul 31 16:32:23.474: INFO: Got endpoints: latency-svc-r6lrc [460.616465ms]
Jul 31 16:32:23.484: INFO: Created: latency-svc-7m4st
Jul 31 16:32:23.511: INFO: Got endpoints: latency-svc-7m4st [477.413465ms]
Jul 31 16:32:23.528: INFO: Created: latency-svc-hhnns
Jul 31 16:32:23.534: INFO: Got endpoints: latency-svc-hhnns [464.519276ms]
Jul 31 16:32:23.547: INFO: Created: latency-svc-wq5nc
Jul 31 16:32:23.555: INFO: Got endpoints: latency-svc-wq5nc [432.673402ms]
Jul 31 16:32:23.567: INFO: Created: latency-svc-g5nbf
Jul 31 16:32:23.576: INFO: Got endpoints: latency-svc-g5nbf [418.358954ms]
Jul 31 16:32:23.589: INFO: Created: latency-svc-58qhm
Jul 31 16:32:23.596: INFO: Got endpoints: latency-svc-58qhm [412.262875ms]
Jul 31 16:32:23.633: INFO: Created: latency-svc-4blfq
Jul 31 16:32:23.639: INFO: Got endpoints: latency-svc-4blfq [431.170058ms]
Jul 31 16:32:23.678: INFO: Created: latency-svc-xss9r
Jul 31 16:32:23.686: INFO: Got endpoints: latency-svc-xss9r [429.644437ms]
Jul 31 16:32:23.708: INFO: Created: latency-svc-9gghn
Jul 31 16:32:23.724: INFO: Got endpoints: latency-svc-9gghn [443.048412ms]
Jul 31 16:32:23.732: INFO: Created: latency-svc-jchc9
Jul 31 16:32:23.740: INFO: Got endpoints: latency-svc-jchc9 [444.931348ms]
Jul 31 16:32:23.781: INFO: Created: latency-svc-lb8st
Jul 31 16:32:23.784: INFO: Got endpoints: latency-svc-lb8st [467.182157ms]
Jul 31 16:32:23.796: INFO: Created: latency-svc-vbqxp
Jul 31 16:32:23.804: INFO: Got endpoints: latency-svc-vbqxp [467.066626ms]
Jul 31 16:32:23.829: INFO: Created: latency-svc-58bt5
Jul 31 16:32:23.840: INFO: Got endpoints: latency-svc-58bt5 [442.234655ms]
Jul 31 16:32:23.862: INFO: Created: latency-svc-pxvds
Jul 31 16:32:23.903: INFO: Got endpoints: latency-svc-pxvds [470.764616ms]
Jul 31 16:32:23.913: INFO: Created: latency-svc-cp9b6
Jul 31 16:32:23.920: INFO: Got endpoints: latency-svc-cp9b6 [463.667882ms]
Jul 31 16:32:23.932: INFO: Created: latency-svc-dcgb9
Jul 31 16:32:23.957: INFO: Got endpoints: latency-svc-dcgb9 [482.722735ms]
Jul 31 16:32:23.964: INFO: Created: latency-svc-kcz65
Jul 31 16:32:23.969: INFO: Got endpoints: latency-svc-kcz65 [457.695077ms]
Jul 31 16:32:23.985: INFO: Created: latency-svc-55klq
Jul 31 16:32:23.991: INFO: Got endpoints: latency-svc-55klq [456.644469ms]
Jul 31 16:32:24.004: INFO: Created: latency-svc-c4zc6
Jul 31 16:32:24.027: INFO: Got endpoints: latency-svc-c4zc6 [472.362461ms]
Jul 31 16:32:24.036: INFO: Created: latency-svc-rhr46
Jul 31 16:32:24.044: INFO: Got endpoints: latency-svc-rhr46 [467.41345ms]
Jul 31 16:32:24.073: INFO: Created: latency-svc-lnpwx
Jul 31 16:32:24.084: INFO: Got endpoints: latency-svc-lnpwx [486.979088ms]
Jul 31 16:32:24.091: INFO: Created: latency-svc-7prfh
Jul 31 16:32:24.097: INFO: Got endpoints: latency-svc-7prfh [458.066704ms]
Jul 31 16:32:24.109: INFO: Created: latency-svc-lkljb
Jul 31 16:32:24.115: INFO: Got endpoints: latency-svc-lkljb [428.803421ms]
Jul 31 16:32:24.126: INFO: Created: latency-svc-jxp2z
Jul 31 16:32:24.150: INFO: Got endpoints: latency-svc-jxp2z [425.095489ms]
Jul 31 16:32:24.168: INFO: Created: latency-svc-qbfw6
Jul 31 16:32:24.178: INFO: Created: latency-svc-c2fxw
Jul 31 16:32:24.188: INFO: Got endpoints: latency-svc-qbfw6 [447.752778ms]
Jul 31 16:32:24.195: INFO: Created: latency-svc-7q6dq
Jul 31 16:32:24.219: INFO: Created: latency-svc-p28wl
Jul 31 16:32:24.232: INFO: Got endpoints: latency-svc-c2fxw [447.915036ms]
Jul 31 16:32:24.235: INFO: Created: latency-svc-gklzc
Jul 31 16:32:24.252: INFO: Created: latency-svc-2tn8l
Jul 31 16:32:24.283: INFO: Got endpoints: latency-svc-7q6dq [478.437144ms]
Jul 31 16:32:24.296: INFO: Created: latency-svc-w65wj
Jul 31 16:32:24.311: INFO: Created: latency-svc-vksw7
Jul 31 16:32:24.327: INFO: Got endpoints: latency-svc-p28wl [486.661125ms]
Jul 31 16:32:24.331: INFO: Created: latency-svc-8g4ns
Jul 31 16:32:24.356: INFO: Created: latency-svc-z2kl7
Jul 31 16:32:24.393: INFO: Got endpoints: latency-svc-gklzc [489.674557ms]
Jul 31 16:32:24.423: INFO: Created: latency-svc-v8r4g
Jul 31 16:32:24.426: INFO: Got endpoints: latency-svc-2tn8l [505.796387ms]
Jul 31 16:32:24.444: INFO: Created: latency-svc-l9ptd
Jul 31 16:32:24.474: INFO: Created: latency-svc-2w9zr
Jul 31 16:32:24.483: INFO: Got endpoints: latency-svc-w65wj [525.377754ms]
Jul 31 16:32:24.488: INFO: Created: latency-svc-gdqt9
Jul 31 16:32:24.592: INFO: Created: latency-svc-782wn
Jul 31 16:32:24.598: INFO: Got endpoints: latency-svc-vksw7 [628.291037ms]
Jul 31 16:32:24.605: INFO: Got endpoints: latency-svc-8g4ns [613.234065ms]
Jul 31 16:32:24.629: INFO: Created: latency-svc-lvkf9
Jul 31 16:32:24.637: INFO: Got endpoints: latency-svc-z2kl7 [609.507393ms]
Jul 31 16:32:24.652: INFO: Created: latency-svc-z7vfk
Jul 31 16:32:24.675: INFO: Created: latency-svc-jzhtb
Jul 31 16:32:24.684: INFO: Got endpoints: latency-svc-v8r4g [638.988101ms]
Jul 31 16:32:24.720: INFO: Created: latency-svc-rnbcm
Jul 31 16:32:24.725: INFO: Got endpoints: latency-svc-l9ptd [638.230629ms]
Jul 31 16:32:24.745: INFO: Created: latency-svc-wht77
Jul 31 16:32:24.769: INFO: Created: latency-svc-2chzj
Jul 31 16:32:24.789: INFO: Got endpoints: latency-svc-2w9zr [691.146066ms]
Jul 31 16:32:24.801: INFO: Created: latency-svc-hrmss
Jul 31 16:32:24.859: INFO: Got endpoints: latency-svc-gdqt9 [743.821047ms]
Jul 31 16:32:24.873: INFO: Created: latency-svc-bkd68
Jul 31 16:32:24.876: INFO: Got endpoints: latency-svc-782wn [725.352383ms]
Jul 31 16:32:24.884: INFO: Created: latency-svc-gmcd4
Jul 31 16:32:24.897: INFO: Created: latency-svc-rw8jr
Jul 31 16:32:24.917: INFO: Created: latency-svc-9slgf
Jul 31 16:32:24.929: INFO: Got endpoints: latency-svc-lvkf9 [740.936793ms]
Jul 31 16:32:24.931: INFO: Created: latency-svc-8h5nj
Jul 31 16:32:24.998: INFO: Got endpoints: latency-svc-z7vfk [764.996859ms]
Jul 31 16:32:24.999: INFO: Created: latency-svc-n27gj
Jul 31 16:32:25.028: INFO: Created: latency-svc-hwgbn
Jul 31 16:32:25.042: INFO: Got endpoints: latency-svc-jzhtb [756.396255ms]
Jul 31 16:32:25.047: INFO: Created: latency-svc-kb8mk
Jul 31 16:32:25.073: INFO: Created: latency-svc-fp94l
Jul 31 16:32:25.079: INFO: Got endpoints: latency-svc-rnbcm [751.322951ms]
Jul 31 16:32:25.137: INFO: Created: latency-svc-rghss
Jul 31 16:32:25.138: INFO: Got endpoints: latency-svc-wht77 [745.396029ms]
Jul 31 16:32:25.166: INFO: Created: latency-svc-jgqht
Jul 31 16:32:25.191: INFO: Created: latency-svc-4l4f6
Jul 31 16:32:25.195: INFO: Got endpoints: latency-svc-2chzj [767.25664ms]
Jul 31 16:32:25.209: INFO: Created: latency-svc-2grv6
Jul 31 16:32:25.227: INFO: Created: latency-svc-xzktt
Jul 31 16:32:25.229: INFO: Got endpoints: latency-svc-hrmss [746.599445ms]
Jul 31 16:32:25.282: INFO: Got endpoints: latency-svc-bkd68 [684.342548ms]
Jul 31 16:32:25.283: INFO: Created: latency-svc-vdbsm
Jul 31 16:32:25.319: INFO: Created: latency-svc-98dxk
Jul 31 16:32:25.329: INFO: Got endpoints: latency-svc-gmcd4 [723.473145ms]
Jul 31 16:32:25.350: INFO: Created: latency-svc-9848w
Jul 31 16:32:25.393: INFO: Got endpoints: latency-svc-rw8jr [755.210077ms]
Jul 31 16:32:25.412: INFO: Created: latency-svc-dkfjb
Jul 31 16:32:25.426: INFO: Got endpoints: latency-svc-9slgf [739.873561ms]
Jul 31 16:32:25.452: INFO: Created: latency-svc-phkfc
Jul 31 16:32:25.475: INFO: Got endpoints: latency-svc-8h5nj [749.690434ms]
Jul 31 16:32:25.528: INFO: Created: latency-svc-4md4r
Jul 31 16:32:25.533: INFO: Got endpoints: latency-svc-n27gj [743.955812ms]
Jul 31 16:32:25.559: INFO: Created: latency-svc-xcmwg
Jul 31 16:32:25.576: INFO: Got endpoints: latency-svc-hwgbn [716.404446ms]
Jul 31 16:32:25.607: INFO: Created: latency-svc-x4dkp
Jul 31 16:32:25.626: INFO: Got endpoints: latency-svc-kb8mk [749.583134ms]
Jul 31 16:32:25.663: INFO: Created: latency-svc-d5q7c
Jul 31 16:32:25.677: INFO: Got endpoints: latency-svc-fp94l [746.737422ms]
Jul 31 16:32:25.699: INFO: Created: latency-svc-8ht86
Jul 31 16:32:25.723: INFO: Got endpoints: latency-svc-rghss [724.582033ms]
Jul 31 16:32:25.746: INFO: Created: latency-svc-zxfvf
Jul 31 16:32:25.779: INFO: Got endpoints: latency-svc-jgqht [737.095374ms]
Jul 31 16:32:25.808: INFO: Created: latency-svc-mckhq
Jul 31 16:32:25.826: INFO: Got endpoints: latency-svc-4l4f6 [746.990723ms]
Jul 31 16:32:25.851: INFO: Created: latency-svc-4hzgh
Jul 31 16:32:25.878: INFO: Got endpoints: latency-svc-2grv6 [739.529767ms]
Jul 31 16:32:25.934: INFO: Created: latency-svc-tx6c4
Jul 31 16:32:25.934: INFO: Got endpoints: latency-svc-xzktt [739.64783ms]
Jul 31 16:32:25.965: INFO: Created: latency-svc-4q47f
Jul 31 16:32:25.973: INFO: Got endpoints: latency-svc-vdbsm [743.685932ms]
Jul 31 16:32:25.992: INFO: Created: latency-svc-vjfnf
Jul 31 16:32:26.031: INFO: Got endpoints: latency-svc-98dxk [748.767664ms]
Jul 31 16:32:26.049: INFO: Created: latency-svc-scbwf
Jul 31 16:32:26.078: INFO: Got endpoints: latency-svc-9848w [749.53839ms]
Jul 31 16:32:26.111: INFO: Created: latency-svc-ndgl5
Jul 31 16:32:26.129: INFO: Got endpoints: latency-svc-dkfjb [735.796072ms]
Jul 31 16:32:26.168: INFO: Created: latency-svc-wj9r9
Jul 31 16:32:26.176: INFO: Got endpoints: latency-svc-phkfc [748.692769ms]
Jul 31 16:32:26.323: INFO: Got endpoints: latency-svc-4md4r [847.811574ms]
Jul 31 16:32:26.390: INFO: Created: latency-svc-2n2kr
Jul 31 16:32:26.393: INFO: Got endpoints: latency-svc-x4dkp [816.505613ms]
Jul 31 16:32:26.393: INFO: Got endpoints: latency-svc-xcmwg [859.759123ms]
Jul 31 16:32:26.405: INFO: Got endpoints: latency-svc-d5q7c [778.561897ms]
Jul 31 16:32:26.450: INFO: Got endpoints: latency-svc-8ht86 [773.370236ms]
Jul 31 16:32:26.458: INFO: Created: latency-svc-p4nbd
Jul 31 16:32:26.479: INFO: Got endpoints: latency-svc-zxfvf [755.962067ms]
Jul 31 16:32:26.486: INFO: Created: latency-svc-4h8h2
Jul 31 16:32:26.505: INFO: Created: latency-svc-2tfsq
Jul 31 16:32:26.529: INFO: Created: latency-svc-xjhqm
Jul 31 16:32:26.530: INFO: Got endpoints: latency-svc-mckhq [750.597114ms]
Jul 31 16:32:26.548: INFO: Created: latency-svc-cpbcb
Jul 31 16:32:26.581: INFO: Got endpoints: latency-svc-4hzgh [754.275336ms]
Jul 31 16:32:26.587: INFO: Created: latency-svc-jk9s5
Jul 31 16:32:26.624: INFO: Got endpoints: latency-svc-tx6c4 [746.032986ms]
Jul 31 16:32:26.680: INFO: Got endpoints: latency-svc-4q47f [745.830165ms]
Jul 31 16:32:26.725: INFO: Got endpoints: latency-svc-vjfnf [751.506934ms]
Jul 31 16:32:26.777: INFO: Got endpoints: latency-svc-scbwf [745.212479ms]
Jul 31 16:32:26.825: INFO: Got endpoints: latency-svc-ndgl5 [746.952756ms]
Jul 31 16:32:26.876: INFO: Got endpoints: latency-svc-wj9r9 [746.754896ms]
Jul 31 16:32:26.926: INFO: Got endpoints: latency-svc-2n2kr [750.006792ms]
Jul 31 16:32:26.976: INFO: Got endpoints: latency-svc-p4nbd [653.010303ms]
Jul 31 16:32:27.031: INFO: Got endpoints: latency-svc-4h8h2 [637.476739ms]
Jul 31 16:32:27.075: INFO: Got endpoints: latency-svc-2tfsq [681.29676ms]
Jul 31 16:32:27.123: INFO: Got endpoints: latency-svc-xjhqm [717.924962ms]
Jul 31 16:32:27.174: INFO: Got endpoints: latency-svc-cpbcb [723.469248ms]
Jul 31 16:32:27.223: INFO: Got endpoints: latency-svc-jk9s5 [743.481421ms]
Jul 31 16:32:27.223: INFO: Latencies: [55.11603ms 88.097589ms 126.484219ms 184.608925ms 196.132052ms 233.34946ms 281.357698ms 319.95211ms 371.010355ms 412.262875ms 418.358954ms 422.665767ms 425.095489ms 428.803421ms 429.644437ms 431.170058ms 432.673402ms 442.234655ms 443.048412ms 444.931348ms 447.752778ms 447.915036ms 456.644469ms 457.695077ms 458.066704ms 460.616465ms 463.667882ms 464.519276ms 467.066626ms 467.182157ms 467.41345ms 469.051389ms 470.764616ms 472.362461ms 477.413465ms 478.437144ms 482.722735ms 486.661125ms 486.979088ms 487.954054ms 489.674557ms 505.796387ms 505.873873ms 512.615081ms 515.924479ms 518.446632ms 524.870547ms 525.377754ms 536.302184ms 538.632169ms 542.110889ms 543.673577ms 548.298547ms 548.913267ms 553.624227ms 556.24056ms 557.496234ms 561.300389ms 561.348595ms 562.81205ms 565.254389ms 566.336469ms 567.686227ms 568.066058ms 570.712917ms 570.916711ms 571.986429ms 574.244904ms 576.148152ms 578.038916ms 580.162455ms 580.353385ms 581.925421ms 583.118307ms 583.710989ms 584.092932ms 584.242719ms 587.403007ms 589.802834ms 597.148382ms 598.761277ms 600.307513ms 600.970856ms 602.652665ms 607.32649ms 608.778781ms 609.507393ms 610.335897ms 611.26262ms 611.985015ms 613.011191ms 613.234065ms 616.304844ms 621.114676ms 623.890291ms 625.569697ms 627.239629ms 628.291037ms 629.688476ms 637.476739ms 638.230629ms 638.988101ms 643.111516ms 649.79625ms 651.537719ms 653.010303ms 655.347398ms 655.987474ms 659.774345ms 664.931391ms 666.16383ms 670.737194ms 675.859514ms 676.106889ms 680.634972ms 681.29676ms 684.342548ms 685.170752ms 688.742604ms 691.146066ms 694.210856ms 695.783166ms 698.15608ms 699.516486ms 700.468812ms 700.529381ms 701.280185ms 703.752743ms 704.587094ms 706.131322ms 710.272177ms 713.898807ms 714.864501ms 715.300936ms 716.404446ms 716.472332ms 717.924962ms 719.621992ms 719.756826ms 721.928151ms 722.876483ms 723.469248ms 723.473145ms 724.582033ms 725.352383ms 725.698995ms 726.205575ms 729.559765ms 732.486116ms 732.707458ms 735.458129ms 735.796072ms 737.095374ms 737.270441ms 739.529767ms 739.64783ms 739.873561ms 740.936793ms 742.364998ms 742.72633ms 743.481421ms 743.685932ms 743.821047ms 743.955812ms 745.212479ms 745.396029ms 745.830165ms 746.032986ms 746.599445ms 746.737422ms 746.754896ms 746.952756ms 746.990723ms 747.354738ms 748.692769ms 748.767664ms 748.827814ms 749.53839ms 749.583134ms 749.690434ms 750.006792ms 750.597114ms 751.322951ms 751.506934ms 752.207267ms 754.275336ms 754.335931ms 755.210077ms 755.611587ms 755.962067ms 756.396255ms 758.740398ms 764.996859ms 767.25664ms 770.009625ms 773.370236ms 778.561897ms 816.505613ms 847.811574ms 859.759123ms]
Jul 31 16:32:27.224: INFO: 50 %ile: 638.230629ms
Jul 31 16:32:27.224: INFO: 90 %ile: 750.006792ms
Jul 31 16:32:27.224: INFO: 99 %ile: 847.811574ms
Jul 31 16:32:27.224: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:32:27.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1856" for this suite.

• [SLOW TEST:12.951 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":280,"completed":22,"skipped":436,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:32:27.241: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9048
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:32:33.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9048" for this suite.

• [SLOW TEST:6.202 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when scheduling a busybox command in a pod
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":280,"completed":23,"skipped":444,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:32:33.444: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9588
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:32:44.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9588" for this suite.

• [SLOW TEST:11.292 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":280,"completed":24,"skipped":444,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:32:44.737: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1922
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 16:32:45.719: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 31 16:32:47.731: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731809965, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731809965, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731809965, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731809965, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 16:32:50.773: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:33:00.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1922" for this suite.
STEP: Destroying namespace "webhook-1922-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.306 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":280,"completed":25,"skipped":452,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:33:01.045: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1932
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 31 16:33:01.217: INFO: Waiting up to 5m0s for pod "pod-65fb563b-ff98-4312-9c02-600d8e73eb40" in namespace "emptydir-1932" to be "success or failure"
Jul 31 16:33:01.225: INFO: Pod "pod-65fb563b-ff98-4312-9c02-600d8e73eb40": Phase="Pending", Reason="", readiness=false. Elapsed: 8.671374ms
Jul 31 16:33:03.232: INFO: Pod "pod-65fb563b-ff98-4312-9c02-600d8e73eb40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015517301s
Jul 31 16:33:05.236: INFO: Pod "pod-65fb563b-ff98-4312-9c02-600d8e73eb40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018940378s
STEP: Saw pod success
Jul 31 16:33:05.236: INFO: Pod "pod-65fb563b-ff98-4312-9c02-600d8e73eb40" satisfied condition "success or failure"
Jul 31 16:33:05.238: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-65fb563b-ff98-4312-9c02-600d8e73eb40 container test-container: <nil>
STEP: delete the pod
Jul 31 16:33:05.267: INFO: Waiting for pod pod-65fb563b-ff98-4312-9c02-600d8e73eb40 to disappear
Jul 31 16:33:05.272: INFO: Pod pod-65fb563b-ff98-4312-9c02-600d8e73eb40 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:33:05.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1932" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":26,"skipped":454,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:33:05.282: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5000
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 31 16:33:05.450: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1876c1e7-8657-4cb8-b169-be1c7f404b0d" in namespace "projected-5000" to be "success or failure"
Jul 31 16:33:05.458: INFO: Pod "downwardapi-volume-1876c1e7-8657-4cb8-b169-be1c7f404b0d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.996806ms
Jul 31 16:33:07.461: INFO: Pod "downwardapi-volume-1876c1e7-8657-4cb8-b169-be1c7f404b0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011207612s
Jul 31 16:33:09.464: INFO: Pod "downwardapi-volume-1876c1e7-8657-4cb8-b169-be1c7f404b0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014562361s
STEP: Saw pod success
Jul 31 16:33:09.465: INFO: Pod "downwardapi-volume-1876c1e7-8657-4cb8-b169-be1c7f404b0d" satisfied condition "success or failure"
Jul 31 16:33:09.468: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod downwardapi-volume-1876c1e7-8657-4cb8-b169-be1c7f404b0d container client-container: <nil>
STEP: delete the pod
Jul 31 16:33:09.490: INFO: Waiting for pod downwardapi-volume-1876c1e7-8657-4cb8-b169-be1c7f404b0d to disappear
Jul 31 16:33:09.496: INFO: Pod downwardapi-volume-1876c1e7-8657-4cb8-b169-be1c7f404b0d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:33:09.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5000" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":280,"completed":27,"skipped":472,"failed":0}
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:33:09.506: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8997
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-8997
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 31 16:33:09.653: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 31 16:33:33.815: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.220.107 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8997 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 16:33:33.815: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
Jul 31 16:33:34.981: INFO: Found all expected endpoints: [netserver-0]
Jul 31 16:33:34.986: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.86.83 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8997 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 16:33:34.986: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
Jul 31 16:33:36.205: INFO: Found all expected endpoints: [netserver-1]
Jul 31 16:33:36.208: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.55.85 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8997 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 16:33:36.208: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
Jul 31 16:33:37.382: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:33:37.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8997" for this suite.

• [SLOW TEST:27.887 seconds]
[sig-network] Networking
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":28,"skipped":476,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:33:37.394: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-143
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 16:33:37.551: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jul 31 16:33:37.564: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 31 16:33:42.566: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 31 16:33:42.567: INFO: Creating deployment "test-rolling-update-deployment"
Jul 31 16:33:42.571: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jul 31 16:33:42.576: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jul 31 16:33:44.581: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jul 31 16:33:44.582: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731810022, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731810022, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731810022, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731810022, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67cf4f6444\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 16:33:46.586: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jul 31 16:33:46.599: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-143 /apis/apps/v1/namespaces/deployment-143/deployments/test-rolling-update-deployment d0bedb82-409b-45e6-a6ee-190937c3bc91 214480 1 2020-07-31 16:33:42 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002de8778 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-07-31 16:33:42 +0000 UTC,LastTransitionTime:2020-07-31 16:33:42 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67cf4f6444" has successfully progressed.,LastUpdateTime:2020-07-31 16:33:44 +0000 UTC,LastTransitionTime:2020-07-31 16:33:42 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 31 16:33:46.601: INFO: New ReplicaSet "test-rolling-update-deployment-67cf4f6444" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67cf4f6444  deployment-143 /apis/apps/v1/namespaces/deployment-143/replicasets/test-rolling-update-deployment-67cf4f6444 79f0c742-fe8b-4e97-9879-aa52c19826e4 214470 1 2020-07-31 16:33:42 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment d0bedb82-409b-45e6-a6ee-190937c3bc91 0xc002de8c17 0xc002de8c18}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67cf4f6444,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002de8c88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 31 16:33:46.601: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jul 31 16:33:46.601: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-143 /apis/apps/v1/namespaces/deployment-143/replicasets/test-rolling-update-controller a66d2b4e-0596-4cb3-a76e-35f0862f1d48 214479 2 2020-07-31 16:33:37 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment d0bedb82-409b-45e6-a6ee-190937c3bc91 0xc002de8b47 0xc002de8b48}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002de8ba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 31 16:33:46.603: INFO: Pod "test-rolling-update-deployment-67cf4f6444-jxn9p" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67cf4f6444-jxn9p test-rolling-update-deployment-67cf4f6444- deployment-143 /api/v1/namespaces/deployment-143/pods/test-rolling-update-deployment-67cf4f6444-jxn9p 6d52aacb-9e29-4b48-9932-1262fb4e2d02 214469 0 2020-07-31 16:33:42 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[cni.projectcalico.org/podIP:192.168.220.109/32 cni.projectcalico.org/podIPs:192.168.220.109/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-67cf4f6444 79f0c742-fe8b-4e97-9879-aa52c19826e4 0xc002de9107 0xc002de9108}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-w5db7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-w5db7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-w5db7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-687e2c91ce,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 16:33:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 16:33:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 16:33:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 16:33:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.92,PodIP:192.168.220.109,StartTime:2020-07-31 16:33:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-31 16:33:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://cda534101c1c51d9d74ac5e825370bea17eb247520b96e9e8f18cafe9759ba4e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.220.109,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:33:46.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-143" for this suite.

• [SLOW TEST:9.217 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":29,"skipped":493,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:33:46.611: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1275
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 31 16:33:46.781: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ddd2cfde-ace4-4b07-8591-17eaedae876b" in namespace "projected-1275" to be "success or failure"
Jul 31 16:33:46.792: INFO: Pod "downwardapi-volume-ddd2cfde-ace4-4b07-8591-17eaedae876b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.110424ms
Jul 31 16:33:48.795: INFO: Pod "downwardapi-volume-ddd2cfde-ace4-4b07-8591-17eaedae876b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013158314s
Jul 31 16:33:50.798: INFO: Pod "downwardapi-volume-ddd2cfde-ace4-4b07-8591-17eaedae876b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016103686s
STEP: Saw pod success
Jul 31 16:33:50.798: INFO: Pod "downwardapi-volume-ddd2cfde-ace4-4b07-8591-17eaedae876b" satisfied condition "success or failure"
Jul 31 16:33:50.800: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod downwardapi-volume-ddd2cfde-ace4-4b07-8591-17eaedae876b container client-container: <nil>
STEP: delete the pod
Jul 31 16:33:50.820: INFO: Waiting for pod downwardapi-volume-ddd2cfde-ace4-4b07-8591-17eaedae876b to disappear
Jul 31 16:33:50.829: INFO: Pod downwardapi-volume-ddd2cfde-ace4-4b07-8591-17eaedae876b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:33:50.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1275" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":30,"skipped":514,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:33:50.845: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4274
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 16:33:51.297: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 31 16:33:53.308: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731810030, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731810030, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731810030, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731810030, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 16:33:56.331: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:34:08.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4274" for this suite.
STEP: Destroying namespace "webhook-4274-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:17.692 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":280,"completed":31,"skipped":539,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:34:08.538: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1606
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 16:34:08.702: INFO: Creating deployment "test-recreate-deployment"
Jul 31 16:34:08.706: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jul 31 16:34:08.724: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jul 31 16:34:10.729: INFO: Waiting deployment "test-recreate-deployment" to complete
Jul 31 16:34:10.731: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731810048, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731810048, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731810048, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731810048, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-799c574856\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 16:34:12.735: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jul 31 16:34:12.743: INFO: Updating deployment test-recreate-deployment
Jul 31 16:34:12.743: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jul 31 16:34:12.883: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-1606 /apis/apps/v1/namespaces/deployment-1606/deployments/test-recreate-deployment 5bdf3447-882f-4273-9daa-c59ac5b5987a 214741 2 2020-07-31 16:34:08 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00678e618 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-07-31 16:34:12 +0000 UTC,LastTransitionTime:2020-07-31 16:34:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-07-31 16:34:12 +0000 UTC,LastTransitionTime:2020-07-31 16:34:08 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jul 31 16:34:12.904: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-1606 /apis/apps/v1/namespaces/deployment-1606/replicasets/test-recreate-deployment-5f94c574ff 2ae072e5-ecd0-48eb-908d-0176cf9265a0 214738 1 2020-07-31 16:34:12 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 5bdf3447-882f-4273-9daa-c59ac5b5987a 0xc00678e9b7 0xc00678e9b8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00678ea18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 31 16:34:12.904: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jul 31 16:34:12.904: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-799c574856  deployment-1606 /apis/apps/v1/namespaces/deployment-1606/replicasets/test-recreate-deployment-799c574856 b96a0a6a-07e7-450c-8e0f-bb8dbb07fedb 214729 2 2020-07-31 16:34:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 5bdf3447-882f-4273-9daa-c59ac5b5987a 0xc00678ea87 0xc00678ea88}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 799c574856,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00678eaf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 31 16:34:12.925: INFO: Pod "test-recreate-deployment-5f94c574ff-tl6pd" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-tl6pd test-recreate-deployment-5f94c574ff- deployment-1606 /api/v1/namespaces/deployment-1606/pods/test-recreate-deployment-5f94c574ff-tl6pd 7fd4eb03-e0f7-481b-b4e5-6ff0895a7317 214742 0 2020-07-31 16:34:12 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 2ae072e5-ecd0-48eb-908d-0176cf9265a0 0xc00678ef67 0xc00678ef68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wx6cj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wx6cj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wx6cj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-687e2c91ce,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 16:34:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 16:34:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 16:34:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 16:34:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.92,PodIP:,StartTime:2020-07-31 16:34:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:34:12.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1606" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":32,"skipped":542,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:34:12.949: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1446
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-1446
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 31 16:34:13.130: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 31 16:34:39.248: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.220.115:8080/dial?request=hostname&protocol=udp&host=192.168.220.114&port=8081&tries=1'] Namespace:pod-network-test-1446 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 16:34:39.249: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
Jul 31 16:34:39.409: INFO: Waiting for responses: map[]
Jul 31 16:34:39.412: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.220.115:8080/dial?request=hostname&protocol=udp&host=192.168.86.85&port=8081&tries=1'] Namespace:pod-network-test-1446 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 16:34:39.412: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
Jul 31 16:34:39.597: INFO: Waiting for responses: map[]
Jul 31 16:34:39.601: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.220.115:8080/dial?request=hostname&protocol=udp&host=192.168.55.86&port=8081&tries=1'] Namespace:pod-network-test-1446 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 16:34:39.601: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
Jul 31 16:34:39.782: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:34:39.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1446" for this suite.

• [SLOW TEST:26.849 seconds]
[sig-network] Networking
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":33,"skipped":573,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:34:39.798: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8477
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1357
STEP: creating an pod
Jul 31 16:34:40.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.8 --namespace=kubectl-8477 -- logs-generator --log-lines-total 100 --run-duration 20s'
Jul 31 16:34:40.084: INFO: stderr: ""
Jul 31 16:34:40.084: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Waiting for log generator to start.
Jul 31 16:34:40.084: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jul 31 16:34:40.084: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8477" to be "running and ready, or succeeded"
Jul 31 16:34:40.087: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.839366ms
Jul 31 16:34:42.090: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006383973s
Jul 31 16:34:44.094: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.010101636s
Jul 31 16:34:44.094: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jul 31 16:34:44.094: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jul 31 16:34:44.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 logs logs-generator logs-generator --namespace=kubectl-8477'
Jul 31 16:34:44.182: INFO: stderr: ""
Jul 31 16:34:44.182: INFO: stdout: "I0731 16:34:40.355626       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/xcn 250\nI0731 16:34:40.555961       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/z9xb 376\nI0731 16:34:40.755989       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/rc5c 460\nI0731 16:34:40.955743       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/vl4 575\nI0731 16:34:41.155744       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/lz8 395\nI0731 16:34:41.355831       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/6x9 411\nI0731 16:34:41.555873       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/5tfw 541\nI0731 16:34:41.755837       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/9rc 204\nI0731 16:34:41.956184       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/6f6 338\nI0731 16:34:42.156585       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/2nc8 321\nI0731 16:34:42.356178       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/2vt7 486\n"
STEP: limiting log lines
Jul 31 16:34:44.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 logs logs-generator logs-generator --namespace=kubectl-8477 --tail=1'
Jul 31 16:34:44.263: INFO: stderr: ""
Jul 31 16:34:44.263: INFO: stdout: "I0731 16:34:42.356178       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/2vt7 486\n"
Jul 31 16:34:44.263: INFO: got output "I0731 16:34:42.356178       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/2vt7 486\n"
STEP: limiting log bytes
Jul 31 16:34:44.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 logs logs-generator logs-generator --namespace=kubectl-8477 --limit-bytes=1'
Jul 31 16:34:44.359: INFO: stderr: ""
Jul 31 16:34:44.359: INFO: stdout: "I"
Jul 31 16:34:44.359: INFO: got output "I"
STEP: exposing timestamps
Jul 31 16:34:44.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 logs logs-generator logs-generator --namespace=kubectl-8477 --tail=1 --timestamps'
Jul 31 16:34:44.485: INFO: stderr: ""
Jul 31 16:34:44.485: INFO: stdout: "2020-07-31T16:34:42.75624698Z I0731 16:34:42.756014       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/mqv 321\n"
Jul 31 16:34:44.485: INFO: got output "2020-07-31T16:34:42.75624698Z I0731 16:34:42.756014       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/mqv 321\n"
STEP: restricting to a time range
Jul 31 16:34:46.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 logs logs-generator logs-generator --namespace=kubectl-8477 --since=1s'
Jul 31 16:34:47.086: INFO: stderr: ""
Jul 31 16:34:47.086: INFO: stdout: "I0731 16:34:44.555893       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/m6l 541\nI0731 16:34:44.755780       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/2f6f 346\nI0731 16:34:44.955799       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/ns/pods/rvqv 254\nI0731 16:34:45.155830       1 logs_generator.go:76] 24 GET /api/v1/namespaces/ns/pods/tcb 331\nI0731 16:34:45.355790       1 logs_generator.go:76] 25 POST /api/v1/namespaces/default/pods/nvk4 503\n"
Jul 31 16:34:47.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 logs logs-generator logs-generator --namespace=kubectl-8477 --since=24h'
Jul 31 16:34:47.178: INFO: stderr: ""
Jul 31 16:34:47.178: INFO: stdout: "I0731 16:34:40.355626       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/xcn 250\nI0731 16:34:40.555961       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/z9xb 376\nI0731 16:34:40.755989       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/rc5c 460\nI0731 16:34:40.955743       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/vl4 575\nI0731 16:34:41.155744       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/lz8 395\nI0731 16:34:41.355831       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/6x9 411\nI0731 16:34:41.555873       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/5tfw 541\nI0731 16:34:41.755837       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/9rc 204\nI0731 16:34:41.956184       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/6f6 338\nI0731 16:34:42.156585       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/2nc8 321\nI0731 16:34:42.356178       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/2vt7 486\nI0731 16:34:42.555833       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/cm4 259\nI0731 16:34:42.756014       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/mqv 321\nI0731 16:34:42.955807       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/mn4 209\nI0731 16:34:43.155943       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/68m 372\nI0731 16:34:43.356219       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/pj7 224\nI0731 16:34:43.556403       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/gms 503\nI0731 16:34:43.756343       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/2wdl 280\nI0731 16:34:43.956450       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/vs7 338\nI0731 16:34:44.155765       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/nbhx 499\nI0731 16:34:44.355939       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/77wk 512\nI0731 16:34:44.555893       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/m6l 541\nI0731 16:34:44.755780       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/2f6f 346\nI0731 16:34:44.955799       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/ns/pods/rvqv 254\nI0731 16:34:45.155830       1 logs_generator.go:76] 24 GET /api/v1/namespaces/ns/pods/tcb 331\nI0731 16:34:45.355790       1 logs_generator.go:76] 25 POST /api/v1/namespaces/default/pods/nvk4 503\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1363
Jul 31 16:34:47.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 delete pod logs-generator --namespace=kubectl-8477'
Jul 31 16:34:53.163: INFO: stderr: ""
Jul 31 16:34:53.163: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:34:53.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8477" for this suite.

• [SLOW TEST:13.382 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1353
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":280,"completed":34,"skipped":576,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:34:53.180: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8583
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-7d8459c3-8ae0-461e-9ea4-7bcf558d9c83
STEP: Creating a pod to test consume configMaps
Jul 31 16:34:53.336: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e3c4d4ab-111e-402f-8b58-6ced394a7e69" in namespace "projected-8583" to be "success or failure"
Jul 31 16:34:53.360: INFO: Pod "pod-projected-configmaps-e3c4d4ab-111e-402f-8b58-6ced394a7e69": Phase="Pending", Reason="", readiness=false. Elapsed: 23.430301ms
Jul 31 16:34:55.363: INFO: Pod "pod-projected-configmaps-e3c4d4ab-111e-402f-8b58-6ced394a7e69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026957078s
Jul 31 16:34:57.366: INFO: Pod "pod-projected-configmaps-e3c4d4ab-111e-402f-8b58-6ced394a7e69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030301135s
STEP: Saw pod success
Jul 31 16:34:57.367: INFO: Pod "pod-projected-configmaps-e3c4d4ab-111e-402f-8b58-6ced394a7e69" satisfied condition "success or failure"
Jul 31 16:34:57.370: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-projected-configmaps-e3c4d4ab-111e-402f-8b58-6ced394a7e69 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 16:34:57.391: INFO: Waiting for pod pod-projected-configmaps-e3c4d4ab-111e-402f-8b58-6ced394a7e69 to disappear
Jul 31 16:34:57.394: INFO: Pod pod-projected-configmaps-e3c4d4ab-111e-402f-8b58-6ced394a7e69 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:34:57.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8583" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":35,"skipped":586,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:34:57.404: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4642
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 16:34:58.128: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 31 16:35:00.140: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731810097, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731810097, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731810097, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731810097, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 16:35:03.160: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:35:03.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4642" for this suite.
STEP: Destroying namespace "webhook-4642-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.963 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":280,"completed":36,"skipped":588,"failed":0}
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:35:03.367: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7073
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7073.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7073.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7073.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7073.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7073.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7073.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7073.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7073.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7073.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7073.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7073.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 222.31.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.31.222_udp@PTR;check="$$(dig +tcp +noall +answer +search 222.31.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.31.222_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7073.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7073.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7073.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7073.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7073.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7073.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7073.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7073.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7073.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7073.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7073.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 222.31.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.31.222_udp@PTR;check="$$(dig +tcp +noall +answer +search 222.31.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.31.222_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 31 16:35:21.612: INFO: Unable to read wheezy_udp@dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:21.616: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:21.620: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:21.624: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:21.644: INFO: Unable to read jessie_udp@dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:21.646: INFO: Unable to read jessie_tcp@dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:21.648: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:21.651: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:21.677: INFO: Lookups using dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972 failed for: [wheezy_udp@dns-test-service.dns-7073.svc.cluster.local wheezy_tcp@dns-test-service.dns-7073.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local jessie_udp@dns-test-service.dns-7073.svc.cluster.local jessie_tcp@dns-test-service.dns-7073.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local]

Jul 31 16:35:26.681: INFO: Unable to read wheezy_udp@dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:26.685: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:26.688: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:26.690: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:26.710: INFO: Unable to read jessie_udp@dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:26.713: INFO: Unable to read jessie_tcp@dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:26.717: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:26.720: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:26.738: INFO: Lookups using dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972 failed for: [wheezy_udp@dns-test-service.dns-7073.svc.cluster.local wheezy_tcp@dns-test-service.dns-7073.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local jessie_udp@dns-test-service.dns-7073.svc.cluster.local jessie_tcp@dns-test-service.dns-7073.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local]

Jul 31 16:35:31.682: INFO: Unable to read wheezy_udp@dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:31.685: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:31.688: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:31.691: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:31.722: INFO: Unable to read jessie_udp@dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:31.724: INFO: Unable to read jessie_tcp@dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:31.727: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:31.730: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:31.744: INFO: Lookups using dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972 failed for: [wheezy_udp@dns-test-service.dns-7073.svc.cluster.local wheezy_tcp@dns-test-service.dns-7073.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local jessie_udp@dns-test-service.dns-7073.svc.cluster.local jessie_tcp@dns-test-service.dns-7073.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local]

Jul 31 16:35:36.680: INFO: Unable to read wheezy_udp@dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:36.684: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:36.688: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:36.692: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:36.716: INFO: Unable to read jessie_udp@dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:36.718: INFO: Unable to read jessie_tcp@dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:36.721: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:36.723: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local from pod dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972: the server could not find the requested resource (get pods dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972)
Jul 31 16:35:36.749: INFO: Lookups using dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972 failed for: [wheezy_udp@dns-test-service.dns-7073.svc.cluster.local wheezy_tcp@dns-test-service.dns-7073.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local jessie_udp@dns-test-service.dns-7073.svc.cluster.local jessie_tcp@dns-test-service.dns-7073.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7073.svc.cluster.local]

Jul 31 16:35:41.758: INFO: DNS probes using dns-7073/dns-test-8b26ec4d-6a4c-47ea-9764-8a8db9a86972 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:35:41.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7073" for this suite.

• [SLOW TEST:38.561 seconds]
[sig-network] DNS
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":280,"completed":37,"skipped":588,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:35:41.928: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4119
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 16:35:42.975: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 31 16:35:44.985: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731810142, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731810142, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731810142, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731810142, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 16:35:48.041: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:35:48.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4119" for this suite.
STEP: Destroying namespace "webhook-4119-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.257 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":280,"completed":38,"skipped":602,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:35:48.186: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-3192
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 16:35:48.353: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Creating first CR 
Jul 31 16:35:48.932: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-07-31T16:35:48Z generation:1 name:name1 resourceVersion:215469 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:ba9de5d4-d91e-4edf-9c2c-dee28fb7660e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jul 31 16:35:58.937: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-07-31T16:35:58Z generation:1 name:name2 resourceVersion:215526 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:2c2985ba-ba12-4fc6-abaf-5e5c9ba5d2be] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jul 31 16:36:08.941: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-07-31T16:35:48Z generation:2 name:name1 resourceVersion:215556 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:ba9de5d4-d91e-4edf-9c2c-dee28fb7660e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jul 31 16:36:18.947: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-07-31T16:35:58Z generation:2 name:name2 resourceVersion:215586 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:2c2985ba-ba12-4fc6-abaf-5e5c9ba5d2be] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jul 31 16:36:28.954: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-07-31T16:35:48Z generation:2 name:name1 resourceVersion:215616 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:ba9de5d4-d91e-4edf-9c2c-dee28fb7660e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jul 31 16:36:38.960: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-07-31T16:35:58Z generation:2 name:name2 resourceVersion:215646 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:2c2985ba-ba12-4fc6-abaf-5e5c9ba5d2be] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:36:49.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-3192" for this suite.

• [SLOW TEST:61.298 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:41
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":280,"completed":39,"skipped":617,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:36:49.484: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7267
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jul 31 16:36:49.649: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7267 /api/v1/namespaces/watch-7267/configmaps/e2e-watch-test-watch-closed c64e7d3b-3f45-4d60-9ddb-792fb99ce5e0 215686 0 2020-07-31 16:36:49 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 31 16:36:49.649: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7267 /api/v1/namespaces/watch-7267/configmaps/e2e-watch-test-watch-closed c64e7d3b-3f45-4d60-9ddb-792fb99ce5e0 215687 0 2020-07-31 16:36:49 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jul 31 16:36:49.685: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7267 /api/v1/namespaces/watch-7267/configmaps/e2e-watch-test-watch-closed c64e7d3b-3f45-4d60-9ddb-792fb99ce5e0 215689 0 2020-07-31 16:36:49 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 31 16:36:49.685: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7267 /api/v1/namespaces/watch-7267/configmaps/e2e-watch-test-watch-closed c64e7d3b-3f45-4d60-9ddb-792fb99ce5e0 215691 0 2020-07-31 16:36:49 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:36:49.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7267" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":280,"completed":40,"skipped":628,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:36:49.697: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3800
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-b7d5198e-5f33-479b-8e51-d99193610c45
STEP: Creating a pod to test consume configMaps
Jul 31 16:36:49.873: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8f22e8e8-9246-46dc-8681-0abdfde87d18" in namespace "projected-3800" to be "success or failure"
Jul 31 16:36:49.882: INFO: Pod "pod-projected-configmaps-8f22e8e8-9246-46dc-8681-0abdfde87d18": Phase="Pending", Reason="", readiness=false. Elapsed: 9.281951ms
Jul 31 16:36:51.885: INFO: Pod "pod-projected-configmaps-8f22e8e8-9246-46dc-8681-0abdfde87d18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012369804s
Jul 31 16:36:53.888: INFO: Pod "pod-projected-configmaps-8f22e8e8-9246-46dc-8681-0abdfde87d18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015728813s
STEP: Saw pod success
Jul 31 16:36:53.888: INFO: Pod "pod-projected-configmaps-8f22e8e8-9246-46dc-8681-0abdfde87d18" satisfied condition "success or failure"
Jul 31 16:36:53.890: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-projected-configmaps-8f22e8e8-9246-46dc-8681-0abdfde87d18 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 16:36:53.911: INFO: Waiting for pod pod-projected-configmaps-8f22e8e8-9246-46dc-8681-0abdfde87d18 to disappear
Jul 31 16:36:53.919: INFO: Pod pod-projected-configmaps-8f22e8e8-9246-46dc-8681-0abdfde87d18 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:36:53.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3800" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":41,"skipped":659,"failed":0}

------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:36:53.934: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7350
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 31 16:37:02.186: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 31 16:37:02.190: INFO: Pod pod-with-prestop-http-hook still exists
Jul 31 16:37:04.191: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 31 16:37:04.194: INFO: Pod pod-with-prestop-http-hook still exists
Jul 31 16:37:06.191: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 31 16:37:06.194: INFO: Pod pod-with-prestop-http-hook still exists
Jul 31 16:37:08.191: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 31 16:37:08.194: INFO: Pod pod-with-prestop-http-hook still exists
Jul 31 16:37:10.191: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 31 16:37:10.195: INFO: Pod pod-with-prestop-http-hook still exists
Jul 31 16:37:12.191: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 31 16:37:12.196: INFO: Pod pod-with-prestop-http-hook still exists
Jul 31 16:37:14.191: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 31 16:37:14.193: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:37:14.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7350" for this suite.

• [SLOW TEST:20.285 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":280,"completed":42,"skipped":659,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:37:14.219: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-117
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jul 31 16:37:54.476: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:37:54.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0731 16:37:54.476470      21 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-117" for this suite.

• [SLOW TEST:40.265 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":280,"completed":43,"skipped":661,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:37:54.485: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2676
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 31 16:37:54.627: INFO: Waiting up to 5m0s for pod "pod-ad6a68bf-d7ad-4d71-8d9c-3fedd106ab00" in namespace "emptydir-2676" to be "success or failure"
Jul 31 16:37:54.647: INFO: Pod "pod-ad6a68bf-d7ad-4d71-8d9c-3fedd106ab00": Phase="Pending", Reason="", readiness=false. Elapsed: 20.802602ms
Jul 31 16:37:56.653: INFO: Pod "pod-ad6a68bf-d7ad-4d71-8d9c-3fedd106ab00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026594718s
Jul 31 16:37:58.657: INFO: Pod "pod-ad6a68bf-d7ad-4d71-8d9c-3fedd106ab00": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030065902s
Jul 31 16:38:00.680: INFO: Pod "pod-ad6a68bf-d7ad-4d71-8d9c-3fedd106ab00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.053646156s
STEP: Saw pod success
Jul 31 16:38:00.680: INFO: Pod "pod-ad6a68bf-d7ad-4d71-8d9c-3fedd106ab00" satisfied condition "success or failure"
Jul 31 16:38:00.683: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-ad6a68bf-d7ad-4d71-8d9c-3fedd106ab00 container test-container: <nil>
STEP: delete the pod
Jul 31 16:38:00.735: INFO: Waiting for pod pod-ad6a68bf-d7ad-4d71-8d9c-3fedd106ab00 to disappear
Jul 31 16:38:00.737: INFO: Pod pod-ad6a68bf-d7ad-4d71-8d9c-3fedd106ab00 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:38:00.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2676" for this suite.

• [SLOW TEST:6.262 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":44,"skipped":672,"failed":0}
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:38:00.746: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1403
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-jfch
STEP: Creating a pod to test atomic-volume-subpath
Jul 31 16:38:00.962: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-jfch" in namespace "subpath-1403" to be "success or failure"
Jul 31 16:38:00.984: INFO: Pod "pod-subpath-test-configmap-jfch": Phase="Pending", Reason="", readiness=false. Elapsed: 20.993494ms
Jul 31 16:38:02.988: INFO: Pod "pod-subpath-test-configmap-jfch": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025398727s
Jul 31 16:38:04.994: INFO: Pod "pod-subpath-test-configmap-jfch": Phase="Running", Reason="", readiness=true. Elapsed: 4.031858024s
Jul 31 16:38:06.998: INFO: Pod "pod-subpath-test-configmap-jfch": Phase="Running", Reason="", readiness=true. Elapsed: 6.035537856s
Jul 31 16:38:09.002: INFO: Pod "pod-subpath-test-configmap-jfch": Phase="Running", Reason="", readiness=true. Elapsed: 8.039363016s
Jul 31 16:38:11.006: INFO: Pod "pod-subpath-test-configmap-jfch": Phase="Running", Reason="", readiness=true. Elapsed: 10.04326158s
Jul 31 16:38:13.011: INFO: Pod "pod-subpath-test-configmap-jfch": Phase="Running", Reason="", readiness=true. Elapsed: 12.04812347s
Jul 31 16:38:15.015: INFO: Pod "pod-subpath-test-configmap-jfch": Phase="Running", Reason="", readiness=true. Elapsed: 14.052211435s
Jul 31 16:38:17.018: INFO: Pod "pod-subpath-test-configmap-jfch": Phase="Running", Reason="", readiness=true. Elapsed: 16.05501029s
Jul 31 16:38:19.022: INFO: Pod "pod-subpath-test-configmap-jfch": Phase="Running", Reason="", readiness=true. Elapsed: 18.059373366s
Jul 31 16:38:21.025: INFO: Pod "pod-subpath-test-configmap-jfch": Phase="Running", Reason="", readiness=true. Elapsed: 20.062884289s
Jul 31 16:38:23.029: INFO: Pod "pod-subpath-test-configmap-jfch": Phase="Running", Reason="", readiness=true. Elapsed: 22.06603307s
Jul 31 16:38:25.032: INFO: Pod "pod-subpath-test-configmap-jfch": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.069689733s
STEP: Saw pod success
Jul 31 16:38:25.032: INFO: Pod "pod-subpath-test-configmap-jfch" satisfied condition "success or failure"
Jul 31 16:38:25.038: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-subpath-test-configmap-jfch container test-container-subpath-configmap-jfch: <nil>
STEP: delete the pod
Jul 31 16:38:25.067: INFO: Waiting for pod pod-subpath-test-configmap-jfch to disappear
Jul 31 16:38:25.069: INFO: Pod pod-subpath-test-configmap-jfch no longer exists
STEP: Deleting pod pod-subpath-test-configmap-jfch
Jul 31 16:38:25.069: INFO: Deleting pod "pod-subpath-test-configmap-jfch" in namespace "subpath-1403"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:38:25.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1403" for this suite.

• [SLOW TEST:24.335 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":280,"completed":45,"skipped":674,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:38:25.083: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9071
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-downwardapi-ts85
STEP: Creating a pod to test atomic-volume-subpath
Jul 31 16:38:25.242: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-ts85" in namespace "subpath-9071" to be "success or failure"
Jul 31 16:38:25.251: INFO: Pod "pod-subpath-test-downwardapi-ts85": Phase="Pending", Reason="", readiness=false. Elapsed: 8.681446ms
Jul 31 16:38:27.254: INFO: Pod "pod-subpath-test-downwardapi-ts85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011704468s
Jul 31 16:38:29.259: INFO: Pod "pod-subpath-test-downwardapi-ts85": Phase="Running", Reason="", readiness=true. Elapsed: 4.016939324s
Jul 31 16:38:31.268: INFO: Pod "pod-subpath-test-downwardapi-ts85": Phase="Running", Reason="", readiness=true. Elapsed: 6.025089241s
Jul 31 16:38:33.271: INFO: Pod "pod-subpath-test-downwardapi-ts85": Phase="Running", Reason="", readiness=true. Elapsed: 8.028012793s
Jul 31 16:38:35.274: INFO: Pod "pod-subpath-test-downwardapi-ts85": Phase="Running", Reason="", readiness=true. Elapsed: 10.031200116s
Jul 31 16:38:37.277: INFO: Pod "pod-subpath-test-downwardapi-ts85": Phase="Running", Reason="", readiness=true. Elapsed: 12.034071115s
Jul 31 16:38:39.280: INFO: Pod "pod-subpath-test-downwardapi-ts85": Phase="Running", Reason="", readiness=true. Elapsed: 14.037324149s
Jul 31 16:38:41.282: INFO: Pod "pod-subpath-test-downwardapi-ts85": Phase="Running", Reason="", readiness=true. Elapsed: 16.03983069s
Jul 31 16:38:43.285: INFO: Pod "pod-subpath-test-downwardapi-ts85": Phase="Running", Reason="", readiness=true. Elapsed: 18.042977354s
Jul 31 16:38:45.290: INFO: Pod "pod-subpath-test-downwardapi-ts85": Phase="Running", Reason="", readiness=true. Elapsed: 20.047440504s
Jul 31 16:38:47.293: INFO: Pod "pod-subpath-test-downwardapi-ts85": Phase="Running", Reason="", readiness=true. Elapsed: 22.05095374s
Jul 31 16:38:49.297: INFO: Pod "pod-subpath-test-downwardapi-ts85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.054346225s
STEP: Saw pod success
Jul 31 16:38:49.297: INFO: Pod "pod-subpath-test-downwardapi-ts85" satisfied condition "success or failure"
Jul 31 16:38:49.299: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-subpath-test-downwardapi-ts85 container test-container-subpath-downwardapi-ts85: <nil>
STEP: delete the pod
Jul 31 16:38:49.332: INFO: Waiting for pod pod-subpath-test-downwardapi-ts85 to disappear
Jul 31 16:38:49.334: INFO: Pod pod-subpath-test-downwardapi-ts85 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-ts85
Jul 31 16:38:49.334: INFO: Deleting pod "pod-subpath-test-downwardapi-ts85" in namespace "subpath-9071"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:38:49.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9071" for this suite.

• [SLOW TEST:24.259 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":280,"completed":46,"skipped":681,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:38:49.344: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7717
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-5d6d2f3e-b1c0-4347-964e-194c8cf2e419
STEP: Creating configMap with name cm-test-opt-upd-0afc4008-494e-4979-af77-07f42cb60169
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-5d6d2f3e-b1c0-4347-964e-194c8cf2e419
STEP: Updating configmap cm-test-opt-upd-0afc4008-494e-4979-af77-07f42cb60169
STEP: Creating configMap with name cm-test-opt-create-0cca4434-107b-48d3-94fc-45b95de36874
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:38:57.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7717" for this suite.

• [SLOW TEST:8.309 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":47,"skipped":691,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:38:57.653: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4830
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jul 31 16:38:57.864: INFO: Waiting up to 5m0s for pod "downward-api-9e5c127a-4ce3-405b-8aef-a203ba551d40" in namespace "downward-api-4830" to be "success or failure"
Jul 31 16:38:57.878: INFO: Pod "downward-api-9e5c127a-4ce3-405b-8aef-a203ba551d40": Phase="Pending", Reason="", readiness=false. Elapsed: 14.117017ms
Jul 31 16:38:59.881: INFO: Pod "downward-api-9e5c127a-4ce3-405b-8aef-a203ba551d40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017201082s
Jul 31 16:39:01.885: INFO: Pod "downward-api-9e5c127a-4ce3-405b-8aef-a203ba551d40": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0204474s
Jul 31 16:39:03.888: INFO: Pod "downward-api-9e5c127a-4ce3-405b-8aef-a203ba551d40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024173414s
STEP: Saw pod success
Jul 31 16:39:03.888: INFO: Pod "downward-api-9e5c127a-4ce3-405b-8aef-a203ba551d40" satisfied condition "success or failure"
Jul 31 16:39:03.891: INFO: Trying to get logs from node test-aruna-123-node-group-a92c781fbd pod downward-api-9e5c127a-4ce3-405b-8aef-a203ba551d40 container dapi-container: <nil>
STEP: delete the pod
Jul 31 16:39:03.927: INFO: Waiting for pod downward-api-9e5c127a-4ce3-405b-8aef-a203ba551d40 to disappear
Jul 31 16:39:03.930: INFO: Pod downward-api-9e5c127a-4ce3-405b-8aef-a203ba551d40 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:39:03.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4830" for this suite.

• [SLOW TEST:6.300 seconds]
[sig-node] Downward API
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:33
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":280,"completed":48,"skipped":718,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:39:03.953: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7702
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7702.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7702.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 31 16:39:16.226: INFO: DNS probes using dns-7702/dns-test-0aa49592-1b01-4744-b9dd-dd8bec986a38 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:39:16.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7702" for this suite.

• [SLOW TEST:12.321 seconds]
[sig-network] DNS
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":280,"completed":49,"skipped":789,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:39:16.276: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1727
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 16:39:16.455: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 31 16:39:20.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 --namespace=crd-publish-openapi-1727 create -f -'
Jul 31 16:39:21.671: INFO: stderr: ""
Jul 31 16:39:21.671: INFO: stdout: "e2e-test-crd-publish-openapi-1237-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jul 31 16:39:21.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 --namespace=crd-publish-openapi-1727 delete e2e-test-crd-publish-openapi-1237-crds test-cr'
Jul 31 16:39:21.752: INFO: stderr: ""
Jul 31 16:39:21.752: INFO: stdout: "e2e-test-crd-publish-openapi-1237-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jul 31 16:39:21.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 --namespace=crd-publish-openapi-1727 apply -f -'
Jul 31 16:39:21.969: INFO: stderr: ""
Jul 31 16:39:21.969: INFO: stdout: "e2e-test-crd-publish-openapi-1237-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jul 31 16:39:21.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 --namespace=crd-publish-openapi-1727 delete e2e-test-crd-publish-openapi-1237-crds test-cr'
Jul 31 16:39:22.049: INFO: stderr: ""
Jul 31 16:39:22.049: INFO: stdout: "e2e-test-crd-publish-openapi-1237-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jul 31 16:39:22.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 explain e2e-test-crd-publish-openapi-1237-crds'
Jul 31 16:39:22.263: INFO: stderr: ""
Jul 31 16:39:22.263: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1237-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:39:25.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1727" for this suite.

• [SLOW TEST:9.538 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":280,"completed":50,"skipped":801,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:39:25.814: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5766
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jul 31 16:39:25.973: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
Jul 31 16:39:29.605: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:39:43.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5766" for this suite.

• [SLOW TEST:17.818 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":280,"completed":51,"skipped":863,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:39:43.633: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3330
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 16:39:44.049: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 31 16:39:46.059: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731810383, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731810383, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731810383, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731810383, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 16:39:49.077: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:39:49.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3330" for this suite.
STEP: Destroying namespace "webhook-3330-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.713 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":280,"completed":52,"skipped":876,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:39:49.347: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7113
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-secret-g9hl
STEP: Creating a pod to test atomic-volume-subpath
Jul 31 16:39:49.557: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-g9hl" in namespace "subpath-7113" to be "success or failure"
Jul 31 16:39:49.567: INFO: Pod "pod-subpath-test-secret-g9hl": Phase="Pending", Reason="", readiness=false. Elapsed: 9.181267ms
Jul 31 16:39:51.578: INFO: Pod "pod-subpath-test-secret-g9hl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020357734s
Jul 31 16:39:53.582: INFO: Pod "pod-subpath-test-secret-g9hl": Phase="Running", Reason="", readiness=true. Elapsed: 4.02385456s
Jul 31 16:39:55.616: INFO: Pod "pod-subpath-test-secret-g9hl": Phase="Running", Reason="", readiness=true. Elapsed: 6.057950409s
Jul 31 16:39:57.619: INFO: Pod "pod-subpath-test-secret-g9hl": Phase="Running", Reason="", readiness=true. Elapsed: 8.060929253s
Jul 31 16:39:59.621: INFO: Pod "pod-subpath-test-secret-g9hl": Phase="Running", Reason="", readiness=true. Elapsed: 10.06355336s
Jul 31 16:40:01.624: INFO: Pod "pod-subpath-test-secret-g9hl": Phase="Running", Reason="", readiness=true. Elapsed: 12.066339653s
Jul 31 16:40:03.627: INFO: Pod "pod-subpath-test-secret-g9hl": Phase="Running", Reason="", readiness=true. Elapsed: 14.068900302s
Jul 31 16:40:05.629: INFO: Pod "pod-subpath-test-secret-g9hl": Phase="Running", Reason="", readiness=true. Elapsed: 16.071760796s
Jul 31 16:40:07.632: INFO: Pod "pod-subpath-test-secret-g9hl": Phase="Running", Reason="", readiness=true. Elapsed: 18.074558252s
Jul 31 16:40:09.638: INFO: Pod "pod-subpath-test-secret-g9hl": Phase="Running", Reason="", readiness=true. Elapsed: 20.079784286s
Jul 31 16:40:11.640: INFO: Pod "pod-subpath-test-secret-g9hl": Phase="Running", Reason="", readiness=true. Elapsed: 22.082775022s
Jul 31 16:40:13.646: INFO: Pod "pod-subpath-test-secret-g9hl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.088517207s
STEP: Saw pod success
Jul 31 16:40:13.646: INFO: Pod "pod-subpath-test-secret-g9hl" satisfied condition "success or failure"
Jul 31 16:40:13.650: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-subpath-test-secret-g9hl container test-container-subpath-secret-g9hl: <nil>
STEP: delete the pod
Jul 31 16:40:13.683: INFO: Waiting for pod pod-subpath-test-secret-g9hl to disappear
Jul 31 16:40:13.694: INFO: Pod pod-subpath-test-secret-g9hl no longer exists
STEP: Deleting pod pod-subpath-test-secret-g9hl
Jul 31 16:40:13.695: INFO: Deleting pod "pod-subpath-test-secret-g9hl" in namespace "subpath-7113"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:40:13.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7113" for this suite.

• [SLOW TEST:24.361 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":280,"completed":53,"skipped":877,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:40:13.711: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7578
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-c57fb8f0-3637-4057-b725-b04d90c45e05
STEP: Creating a pod to test consume configMaps
Jul 31 16:40:13.956: INFO: Waiting up to 5m0s for pod "pod-configmaps-94f90d5d-1c5f-458e-a8a7-9fa49fae54a6" in namespace "configmap-7578" to be "success or failure"
Jul 31 16:40:13.975: INFO: Pod "pod-configmaps-94f90d5d-1c5f-458e-a8a7-9fa49fae54a6": Phase="Pending", Reason="", readiness=false. Elapsed: 18.084631ms
Jul 31 16:40:15.978: INFO: Pod "pod-configmaps-94f90d5d-1c5f-458e-a8a7-9fa49fae54a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020947454s
Jul 31 16:40:17.981: INFO: Pod "pod-configmaps-94f90d5d-1c5f-458e-a8a7-9fa49fae54a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024176279s
STEP: Saw pod success
Jul 31 16:40:17.981: INFO: Pod "pod-configmaps-94f90d5d-1c5f-458e-a8a7-9fa49fae54a6" satisfied condition "success or failure"
Jul 31 16:40:17.983: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-configmaps-94f90d5d-1c5f-458e-a8a7-9fa49fae54a6 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 16:40:18.008: INFO: Waiting for pod pod-configmaps-94f90d5d-1c5f-458e-a8a7-9fa49fae54a6 to disappear
Jul 31 16:40:18.015: INFO: Pod pod-configmaps-94f90d5d-1c5f-458e-a8a7-9fa49fae54a6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:40:18.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7578" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":54,"skipped":884,"failed":0}
SSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:40:18.040: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-9462
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-9462, will wait for the garbage collector to delete the pods
Jul 31 16:40:22.254: INFO: Deleting Job.batch foo took: 5.169368ms
Jul 31 16:40:22.854: INFO: Terminating Job.batch foo pods took: 600.210061ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:41:03.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9462" for this suite.

• [SLOW TEST:45.228 seconds]
[sig-apps] Job
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":280,"completed":55,"skipped":888,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:41:03.269: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7458
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 16:41:03.416: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 31 16:41:07.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 --namespace=crd-publish-openapi-7458 create -f -'
Jul 31 16:41:08.521: INFO: stderr: ""
Jul 31 16:41:08.521: INFO: stdout: "e2e-test-crd-publish-openapi-8613-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jul 31 16:41:08.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 --namespace=crd-publish-openapi-7458 delete e2e-test-crd-publish-openapi-8613-crds test-cr'
Jul 31 16:41:08.600: INFO: stderr: ""
Jul 31 16:41:08.600: INFO: stdout: "e2e-test-crd-publish-openapi-8613-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jul 31 16:41:08.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 --namespace=crd-publish-openapi-7458 apply -f -'
Jul 31 16:41:08.789: INFO: stderr: ""
Jul 31 16:41:08.789: INFO: stdout: "e2e-test-crd-publish-openapi-8613-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jul 31 16:41:08.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 --namespace=crd-publish-openapi-7458 delete e2e-test-crd-publish-openapi-8613-crds test-cr'
Jul 31 16:41:08.865: INFO: stderr: ""
Jul 31 16:41:08.865: INFO: stdout: "e2e-test-crd-publish-openapi-8613-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jul 31 16:41:08.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 explain e2e-test-crd-publish-openapi-8613-crds'
Jul 31 16:41:09.066: INFO: stderr: ""
Jul 31 16:41:09.066: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8613-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:41:12.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7458" for this suite.

• [SLOW TEST:9.423 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":280,"completed":56,"skipped":892,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:41:12.695: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1942
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-1942
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-1942
STEP: Creating statefulset with conflicting port in namespace statefulset-1942
STEP: Waiting until pod test-pod will start running in namespace statefulset-1942
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1942
Jul 31 16:41:16.883: INFO: Observed stateful pod in namespace: statefulset-1942, name: ss-0, uid: 1a39ebfd-1861-4383-81ff-5a9783cd9df0, status phase: Pending. Waiting for statefulset controller to delete.
Jul 31 16:41:17.461: INFO: Observed stateful pod in namespace: statefulset-1942, name: ss-0, uid: 1a39ebfd-1861-4383-81ff-5a9783cd9df0, status phase: Failed. Waiting for statefulset controller to delete.
Jul 31 16:41:17.473: INFO: Observed stateful pod in namespace: statefulset-1942, name: ss-0, uid: 1a39ebfd-1861-4383-81ff-5a9783cd9df0, status phase: Failed. Waiting for statefulset controller to delete.
Jul 31 16:41:17.485: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1942
STEP: Removing pod with conflicting port in namespace statefulset-1942
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1942 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jul 31 16:41:21.550: INFO: Deleting all statefulset in ns statefulset-1942
Jul 31 16:41:21.553: INFO: Scaling statefulset ss to 0
Jul 31 16:41:41.568: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 16:41:41.570: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:41:41.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1942" for this suite.

• [SLOW TEST:28.893 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":280,"completed":57,"skipped":907,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:41:41.589: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6337
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 31 16:41:41.748: INFO: Waiting up to 5m0s for pod "pod-166afd51-462a-4368-9875-479c9e539d42" in namespace "emptydir-6337" to be "success or failure"
Jul 31 16:41:41.754: INFO: Pod "pod-166afd51-462a-4368-9875-479c9e539d42": Phase="Pending", Reason="", readiness=false. Elapsed: 5.57062ms
Jul 31 16:41:43.758: INFO: Pod "pod-166afd51-462a-4368-9875-479c9e539d42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009172672s
Jul 31 16:41:45.761: INFO: Pod "pod-166afd51-462a-4368-9875-479c9e539d42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012506811s
STEP: Saw pod success
Jul 31 16:41:45.761: INFO: Pod "pod-166afd51-462a-4368-9875-479c9e539d42" satisfied condition "success or failure"
Jul 31 16:41:45.763: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-166afd51-462a-4368-9875-479c9e539d42 container test-container: <nil>
STEP: delete the pod
Jul 31 16:41:45.785: INFO: Waiting for pod pod-166afd51-462a-4368-9875-479c9e539d42 to disappear
Jul 31 16:41:45.791: INFO: Pod pod-166afd51-462a-4368-9875-479c9e539d42 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:41:45.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6337" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":58,"skipped":923,"failed":0}
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:41:45.800: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-9004
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 16:41:45.955: INFO: (0) /api/v1/nodes/test-aruna-123-node-group-a8f122b701/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 3.827589ms)
Jul 31 16:41:45.957: INFO: (1) /api/v1/nodes/test-aruna-123-node-group-a8f122b701/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 2.36804ms)
Jul 31 16:41:45.960: INFO: (2) /api/v1/nodes/test-aruna-123-node-group-a8f122b701/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 2.486362ms)
Jul 31 16:41:45.962: INFO: (3) /api/v1/nodes/test-aruna-123-node-group-a8f122b701/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 2.556106ms)
Jul 31 16:41:45.966: INFO: (4) /api/v1/nodes/test-aruna-123-node-group-a8f122b701/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 3.066889ms)
Jul 31 16:41:45.968: INFO: (5) /api/v1/nodes/test-aruna-123-node-group-a8f122b701/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 2.41811ms)
Jul 31 16:41:45.970: INFO: (6) /api/v1/nodes/test-aruna-123-node-group-a8f122b701/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 1.93248ms)
Jul 31 16:41:45.973: INFO: (7) /api/v1/nodes/test-aruna-123-node-group-a8f122b701/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 2.651422ms)
Jul 31 16:41:45.975: INFO: (8) /api/v1/nodes/test-aruna-123-node-group-a8f122b701/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 2.162972ms)
Jul 31 16:41:45.977: INFO: (9) /api/v1/nodes/test-aruna-123-node-group-a8f122b701/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 2.279647ms)
Jul 31 16:41:45.979: INFO: (10) /api/v1/nodes/test-aruna-123-node-group-a8f122b701/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 1.968818ms)
Jul 31 16:41:45.982: INFO: (11) /api/v1/nodes/test-aruna-123-node-group-a8f122b701/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 2.606387ms)
Jul 31 16:41:45.985: INFO: (12) /api/v1/nodes/test-aruna-123-node-group-a8f122b701/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 2.930924ms)
Jul 31 16:41:45.988: INFO: (13) /api/v1/nodes/test-aruna-123-node-group-a8f122b701/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 3.308332ms)
Jul 31 16:41:45.993: INFO: (14) /api/v1/nodes/test-aruna-123-node-group-a8f122b701/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 4.481476ms)
Jul 31 16:41:45.996: INFO: (15) /api/v1/nodes/test-aruna-123-node-group-a8f122b701/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 3.452774ms)
Jul 31 16:41:45.999: INFO: (16) /api/v1/nodes/test-aruna-123-node-group-a8f122b701/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 2.794925ms)
Jul 31 16:41:46.002: INFO: (17) /api/v1/nodes/test-aruna-123-node-group-a8f122b701/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 2.98277ms)
Jul 31 16:41:46.005: INFO: (18) /api/v1/nodes/test-aruna-123-node-group-a8f122b701/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 3.165098ms)
Jul 31 16:41:46.008: INFO: (19) /api/v1/nodes/test-aruna-123-node-group-a8f122b701/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 2.982381ms)
[AfterEach] version v1
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:41:46.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9004" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":280,"completed":59,"skipped":925,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:41:46.015: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-592
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override arguments
Jul 31 16:41:46.161: INFO: Waiting up to 5m0s for pod "client-containers-e5ac11ee-cf48-45cb-8db9-34b6b4470cc0" in namespace "containers-592" to be "success or failure"
Jul 31 16:41:46.176: INFO: Pod "client-containers-e5ac11ee-cf48-45cb-8db9-34b6b4470cc0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.703836ms
Jul 31 16:41:48.179: INFO: Pod "client-containers-e5ac11ee-cf48-45cb-8db9-34b6b4470cc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017923243s
Jul 31 16:41:50.182: INFO: Pod "client-containers-e5ac11ee-cf48-45cb-8db9-34b6b4470cc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020972784s
STEP: Saw pod success
Jul 31 16:41:50.182: INFO: Pod "client-containers-e5ac11ee-cf48-45cb-8db9-34b6b4470cc0" satisfied condition "success or failure"
Jul 31 16:41:50.197: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod client-containers-e5ac11ee-cf48-45cb-8db9-34b6b4470cc0 container test-container: <nil>
STEP: delete the pod
Jul 31 16:41:50.222: INFO: Waiting for pod client-containers-e5ac11ee-cf48-45cb-8db9-34b6b4470cc0 to disappear
Jul 31 16:41:50.227: INFO: Pod client-containers-e5ac11ee-cf48-45cb-8db9-34b6b4470cc0 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:41:50.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-592" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":280,"completed":60,"skipped":967,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:41:50.238: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7187
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on node default medium
Jul 31 16:41:50.447: INFO: Waiting up to 5m0s for pod "pod-1c7dcb2d-fd13-47cd-9639-622297aa6f5b" in namespace "emptydir-7187" to be "success or failure"
Jul 31 16:41:50.453: INFO: Pod "pod-1c7dcb2d-fd13-47cd-9639-622297aa6f5b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.993938ms
Jul 31 16:41:52.457: INFO: Pod "pod-1c7dcb2d-fd13-47cd-9639-622297aa6f5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009647707s
Jul 31 16:41:54.460: INFO: Pod "pod-1c7dcb2d-fd13-47cd-9639-622297aa6f5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012255415s
STEP: Saw pod success
Jul 31 16:41:54.460: INFO: Pod "pod-1c7dcb2d-fd13-47cd-9639-622297aa6f5b" satisfied condition "success or failure"
Jul 31 16:41:54.462: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-1c7dcb2d-fd13-47cd-9639-622297aa6f5b container test-container: <nil>
STEP: delete the pod
Jul 31 16:41:54.488: INFO: Waiting for pod pod-1c7dcb2d-fd13-47cd-9639-622297aa6f5b to disappear
Jul 31 16:41:54.491: INFO: Pod pod-1c7dcb2d-fd13-47cd-9639-622297aa6f5b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:41:54.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7187" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":61,"skipped":1001,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:41:54.503: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-81
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-5e59bb54-b07c-4b88-9345-84949d7c7883
STEP: Creating a pod to test consume secrets
Jul 31 16:41:54.669: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-85b13928-e62c-4491-9ced-2591e884033f" in namespace "projected-81" to be "success or failure"
Jul 31 16:41:54.682: INFO: Pod "pod-projected-secrets-85b13928-e62c-4491-9ced-2591e884033f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.267881ms
Jul 31 16:41:56.685: INFO: Pod "pod-projected-secrets-85b13928-e62c-4491-9ced-2591e884033f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015542121s
Jul 31 16:41:58.689: INFO: Pod "pod-projected-secrets-85b13928-e62c-4491-9ced-2591e884033f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019310376s
STEP: Saw pod success
Jul 31 16:41:58.689: INFO: Pod "pod-projected-secrets-85b13928-e62c-4491-9ced-2591e884033f" satisfied condition "success or failure"
Jul 31 16:41:58.691: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-projected-secrets-85b13928-e62c-4491-9ced-2591e884033f container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 31 16:41:58.727: INFO: Waiting for pod pod-projected-secrets-85b13928-e62c-4491-9ced-2591e884033f to disappear
Jul 31 16:41:58.729: INFO: Pod pod-projected-secrets-85b13928-e62c-4491-9ced-2591e884033f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:41:58.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-81" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":62,"skipped":1006,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:41:58.739: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1888
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-4c4e18ae-7396-4ffb-a7c4-ce28ee00fb3a
STEP: Creating a pod to test consume configMaps
Jul 31 16:41:58.899: INFO: Waiting up to 5m0s for pod "pod-configmaps-40ad567a-8e5d-4933-9e9c-17ef4ed4c0f8" in namespace "configmap-1888" to be "success or failure"
Jul 31 16:41:58.911: INFO: Pod "pod-configmaps-40ad567a-8e5d-4933-9e9c-17ef4ed4c0f8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.234315ms
Jul 31 16:42:00.915: INFO: Pod "pod-configmaps-40ad567a-8e5d-4933-9e9c-17ef4ed4c0f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015872603s
Jul 31 16:42:02.919: INFO: Pod "pod-configmaps-40ad567a-8e5d-4933-9e9c-17ef4ed4c0f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019128363s
STEP: Saw pod success
Jul 31 16:42:02.919: INFO: Pod "pod-configmaps-40ad567a-8e5d-4933-9e9c-17ef4ed4c0f8" satisfied condition "success or failure"
Jul 31 16:42:02.923: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-configmaps-40ad567a-8e5d-4933-9e9c-17ef4ed4c0f8 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 16:42:02.983: INFO: Waiting for pod pod-configmaps-40ad567a-8e5d-4933-9e9c-17ef4ed4c0f8 to disappear
Jul 31 16:42:02.988: INFO: Pod pod-configmaps-40ad567a-8e5d-4933-9e9c-17ef4ed4c0f8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:42:02.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1888" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":63,"skipped":1006,"failed":0}

------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:42:03.011: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9000
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Starting the proxy
Jul 31 16:42:03.162: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-776750169 proxy --unix-socket=/tmp/kubectl-proxy-unix404709968/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:42:03.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9000" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":280,"completed":64,"skipped":1006,"failed":0}

------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:42:03.221: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1784
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 31 16:42:06.412: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:42:06.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1784" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":65,"skipped":1006,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:42:06.449: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9207
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 31 16:42:09.620: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:42:09.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9207" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":66,"skipped":1014,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:42:09.693: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-629
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 16:42:09.904: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:42:11.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-629" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":280,"completed":67,"skipped":1017,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:42:11.165: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1762
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 16:42:11.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 create -f - --namespace=kubectl-1762'
Jul 31 16:42:11.755: INFO: stderr: ""
Jul 31 16:42:11.755: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Jul 31 16:42:11.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 create -f - --namespace=kubectl-1762'
Jul 31 16:42:12.024: INFO: stderr: ""
Jul 31 16:42:12.024: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Jul 31 16:42:13.029: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 31 16:42:13.029: INFO: Found 0 / 1
Jul 31 16:42:14.029: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 31 16:42:14.029: INFO: Found 0 / 1
Jul 31 16:42:15.029: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 31 16:42:15.029: INFO: Found 1 / 1
Jul 31 16:42:15.029: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 31 16:42:15.033: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 31 16:42:15.033: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 31 16:42:15.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 describe pod agnhost-master-fbxvl --namespace=kubectl-1762'
Jul 31 16:42:15.139: INFO: stderr: ""
Jul 31 16:42:15.139: INFO: stdout: "Name:         agnhost-master-fbxvl\nNamespace:    kubectl-1762\nPriority:     0\nNode:         test-aruna-123-node-group-687e2c91ce/10.10.102.92\nStart Time:   Fri, 31 Jul 2020 16:42:10 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 192.168.220.87/32\n              cni.projectcalico.org/podIPs: 192.168.220.87/32\nStatus:       Running\nIP:           192.168.220.87\nIPs:\n  IP:           192.168.220.87\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   docker://eda6ef1d6c8c896e20dd1c7fff168390f1b77648469b8474d048fed9ad2a9661\n    Image:          gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 31 Jul 2020 16:42:12 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-cmbgn (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-cmbgn:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-cmbgn\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                           Message\n  ----    ------     ----  ----                                           -------\n  Normal  Scheduled  4s    default-scheduler                              Successfully assigned kubectl-1762/agnhost-master-fbxvl to test-aruna-123-node-group-687e2c91ce\n  Normal  Pulled     4s    kubelet, test-aruna-123-node-group-687e2c91ce  Container image \"gcr.io/kubernetes-e2e-test-images/agnhost:2.8\" already present on machine\n  Normal  Created    4s    kubelet, test-aruna-123-node-group-687e2c91ce  Created container agnhost-master\n  Normal  Started    3s    kubelet, test-aruna-123-node-group-687e2c91ce  Started container agnhost-master\n"
Jul 31 16:42:15.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 describe rc agnhost-master --namespace=kubectl-1762'
Jul 31 16:42:15.252: INFO: stderr: ""
Jul 31 16:42:15.252: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-1762\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-master-fbxvl\n"
Jul 31 16:42:15.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 describe service agnhost-master --namespace=kubectl-1762'
Jul 31 16:42:15.343: INFO: stderr: ""
Jul 31 16:42:15.343: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-1762\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                10.107.225.105\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.220.87:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jul 31 16:42:15.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 describe node test-aruna-123-master-gro-1eb9ae3485'
Jul 31 16:42:15.462: INFO: stderr: ""
Jul 31 16:42:15.462: INFO: stdout: "Name:               test-aruna-123-master-gro-1eb9ae3485\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=test-aruna-123-master-gro-1eb9ae3485\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.10.102.91/22\n                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.121.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 30 Jul 2020 20:33:19 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  test-aruna-123-master-gro-1eb9ae3485\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 31 Jul 2020 16:42:08 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 30 Jul 2020 20:33:47 +0000   Thu, 30 Jul 2020 20:33:47 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 31 Jul 2020 16:38:13 +0000   Thu, 30 Jul 2020 20:33:15 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 31 Jul 2020 16:38:13 +0000   Thu, 30 Jul 2020 20:33:15 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 31 Jul 2020 16:38:13 +0000   Thu, 30 Jul 2020 20:33:15 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 31 Jul 2020 16:38:13 +0000   Thu, 30 Jul 2020 20:33:56 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  ExternalIP:  10.10.102.91\n  InternalIP:  10.10.102.91\n  Hostname:    test-aruna-123-master-gro-1eb9ae3485\nCapacity:\n  cpu:                2\n  ephemeral-storage:  41218252Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16426000Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  37986740981\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16323600Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 f8b242428bc34b8da0438184263e022b\n  System UUID:                4215DC35-9F2E-99D5-D6EB-8055762E16A5\n  Boot ID:                    90d5779a-ee63-446c-aaef-19c4fc5a0cf0\n  Kernel Version:             4.15.0-101-generic\n  OS Image:                   Ubuntu 18.04.4 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://18.9.9\n  Kubelet Version:            v1.17.6\n  Kube-Proxy Version:         v1.17.6\nPodCIDR:                      192.168.0.0/24\nPodCIDRs:                     192.168.0.0/24\nProviderID:                   vsphere://4215dc35-9f2e-99d5-d6eb-8055762e16a5\nNon-terminated Pods:          (11 in total)\n  Namespace                   Name                                                            CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                            ------------  ----------  ---------------  -------------  ---\n  ccp                         ccp-vip-manager-test-aruna-123-master-gro-1eb9ae3485            0 (0%)        0 (0%)      0 (0%)           0 (0%)         20h\n  kube-system                 calico-kube-controllers-6f48b64697-qsgff                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         20h\n  kube-system                 calico-node-7p7fg                                               250m (12%)    0 (0%)      0 (0%)           0 (0%)         20h\n  kube-system                 coredns-6745dfb89-lm24q                                         100m (5%)     0 (0%)      70Mi (0%)        170Mi (1%)     20h\n  kube-system                 coredns-6745dfb89-tjbjj                                         100m (5%)     0 (0%)      70Mi (0%)        170Mi (1%)     20h\n  kube-system                 etcd-test-aruna-123-master-gro-1eb9ae3485                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         20h\n  kube-system                 kube-apiserver-test-aruna-123-master-gro-1eb9ae3485             250m (12%)    0 (0%)      0 (0%)           0 (0%)         20h\n  kube-system                 kube-controller-manager-test-aruna-123-master-gro-1eb9ae3485    200m (10%)    0 (0%)      0 (0%)           0 (0%)         20h\n  kube-system                 kube-proxy-l7pnz                                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         20h\n  kube-system                 kube-scheduler-test-aruna-123-master-gro-1eb9ae3485             100m (5%)     0 (0%)      0 (0%)           0 (0%)         20h\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-21a6c8c28f874b58-nm2vr         0 (0%)        0 (0%)      0 (0%)           0 (0%)         19m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                1 (50%)     0 (0%)\n  memory             140Mi (0%)  340Mi (2%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:              <none>\n"
Jul 31 16:42:15.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 describe namespace kubectl-1762'
Jul 31 16:42:15.543: INFO: stderr: ""
Jul 31 16:42:15.543: INFO: stdout: "Name:         kubectl-1762\nLabels:       e2e-framework=kubectl\n              e2e-run=c753a180-79d5-407b-97e2-ac8929ab4a10\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:42:15.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1762" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":280,"completed":68,"skipped":1089,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:42:15.556: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3566
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name projected-secret-test-1a0cbacd-67c7-475d-a3ac-3dd3b9454882
STEP: Creating a pod to test consume secrets
Jul 31 16:42:15.721: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c6d4594c-162e-4349-849b-33e2056e6470" in namespace "projected-3566" to be "success or failure"
Jul 31 16:42:15.737: INFO: Pod "pod-projected-secrets-c6d4594c-162e-4349-849b-33e2056e6470": Phase="Pending", Reason="", readiness=false. Elapsed: 15.674287ms
Jul 31 16:42:17.741: INFO: Pod "pod-projected-secrets-c6d4594c-162e-4349-849b-33e2056e6470": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019596658s
Jul 31 16:42:19.744: INFO: Pod "pod-projected-secrets-c6d4594c-162e-4349-849b-33e2056e6470": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023166391s
STEP: Saw pod success
Jul 31 16:42:19.745: INFO: Pod "pod-projected-secrets-c6d4594c-162e-4349-849b-33e2056e6470" satisfied condition "success or failure"
Jul 31 16:42:19.750: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-projected-secrets-c6d4594c-162e-4349-849b-33e2056e6470 container secret-volume-test: <nil>
STEP: delete the pod
Jul 31 16:42:19.794: INFO: Waiting for pod pod-projected-secrets-c6d4594c-162e-4349-849b-33e2056e6470 to disappear
Jul 31 16:42:19.799: INFO: Pod pod-projected-secrets-c6d4594c-162e-4349-849b-33e2056e6470 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:42:19.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3566" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":69,"skipped":1111,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:42:19.821: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1586
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 16:42:20.002: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:42:25.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1586" for this suite.

• [SLOW TEST:5.715 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:47
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":280,"completed":70,"skipped":1152,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:42:25.538: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1974
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-59ddb767-6f32-44d0-a918-1af55dd6828f
STEP: Creating a pod to test consume configMaps
Jul 31 16:42:26.516: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9c3aeb76-5b7f-48e7-869b-dd9137c9e296" in namespace "projected-1974" to be "success or failure"
Jul 31 16:42:26.534: INFO: Pod "pod-projected-configmaps-9c3aeb76-5b7f-48e7-869b-dd9137c9e296": Phase="Pending", Reason="", readiness=false. Elapsed: 18.700069ms
Jul 31 16:42:28.538: INFO: Pod "pod-projected-configmaps-9c3aeb76-5b7f-48e7-869b-dd9137c9e296": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022429314s
Jul 31 16:42:30.553: INFO: Pod "pod-projected-configmaps-9c3aeb76-5b7f-48e7-869b-dd9137c9e296": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037396735s
STEP: Saw pod success
Jul 31 16:42:30.553: INFO: Pod "pod-projected-configmaps-9c3aeb76-5b7f-48e7-869b-dd9137c9e296" satisfied condition "success or failure"
Jul 31 16:42:30.557: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-projected-configmaps-9c3aeb76-5b7f-48e7-869b-dd9137c9e296 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 16:42:30.602: INFO: Waiting for pod pod-projected-configmaps-9c3aeb76-5b7f-48e7-869b-dd9137c9e296 to disappear
Jul 31 16:42:30.607: INFO: Pod pod-projected-configmaps-9c3aeb76-5b7f-48e7-869b-dd9137c9e296 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:42:30.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1974" for this suite.

• [SLOW TEST:5.077 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":71,"skipped":1182,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:42:30.618: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4917
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 16:42:30.810: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:42:31.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4917" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":280,"completed":72,"skipped":1211,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:42:31.488: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-711
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:42:31.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-711" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":280,"completed":73,"skipped":1226,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:42:31.710: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3571
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 31 16:42:31.978: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aa702033-9f34-4479-8017-5c1b9be3b15a" in namespace "projected-3571" to be "success or failure"
Jul 31 16:42:32.016: INFO: Pod "downwardapi-volume-aa702033-9f34-4479-8017-5c1b9be3b15a": Phase="Pending", Reason="", readiness=false. Elapsed: 38.485807ms
Jul 31 16:42:34.019: INFO: Pod "downwardapi-volume-aa702033-9f34-4479-8017-5c1b9be3b15a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041500705s
Jul 31 16:42:36.023: INFO: Pod "downwardapi-volume-aa702033-9f34-4479-8017-5c1b9be3b15a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044884901s
STEP: Saw pod success
Jul 31 16:42:36.023: INFO: Pod "downwardapi-volume-aa702033-9f34-4479-8017-5c1b9be3b15a" satisfied condition "success or failure"
Jul 31 16:42:36.025: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod downwardapi-volume-aa702033-9f34-4479-8017-5c1b9be3b15a container client-container: <nil>
STEP: delete the pod
Jul 31 16:42:36.045: INFO: Waiting for pod downwardapi-volume-aa702033-9f34-4479-8017-5c1b9be3b15a to disappear
Jul 31 16:42:36.049: INFO: Pod downwardapi-volume-aa702033-9f34-4479-8017-5c1b9be3b15a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:42:36.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3571" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":74,"skipped":1226,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:42:36.061: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6295
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 16:42:58.231: INFO: Container started at 2020-07-31 16:42:36 +0000 UTC, pod became ready at 2020-07-31 16:42:56 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:42:58.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6295" for this suite.

• [SLOW TEST:22.178 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":280,"completed":75,"skipped":1228,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:42:58.240: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1828
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-a2f8a3b5-e7f6-495f-903e-28b38df4ef46
STEP: Creating a pod to test consume configMaps
Jul 31 16:42:58.387: INFO: Waiting up to 5m0s for pod "pod-configmaps-2e32f7e4-ddf4-4a76-841d-269dd5582e98" in namespace "configmap-1828" to be "success or failure"
Jul 31 16:42:58.394: INFO: Pod "pod-configmaps-2e32f7e4-ddf4-4a76-841d-269dd5582e98": Phase="Pending", Reason="", readiness=false. Elapsed: 6.982674ms
Jul 31 16:43:00.399: INFO: Pod "pod-configmaps-2e32f7e4-ddf4-4a76-841d-269dd5582e98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011383874s
Jul 31 16:43:02.402: INFO: Pod "pod-configmaps-2e32f7e4-ddf4-4a76-841d-269dd5582e98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014852304s
STEP: Saw pod success
Jul 31 16:43:02.402: INFO: Pod "pod-configmaps-2e32f7e4-ddf4-4a76-841d-269dd5582e98" satisfied condition "success or failure"
Jul 31 16:43:02.405: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-configmaps-2e32f7e4-ddf4-4a76-841d-269dd5582e98 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 16:43:02.437: INFO: Waiting for pod pod-configmaps-2e32f7e4-ddf4-4a76-841d-269dd5582e98 to disappear
Jul 31 16:43:02.441: INFO: Pod pod-configmaps-2e32f7e4-ddf4-4a76-841d-269dd5582e98 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:43:02.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1828" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":76,"skipped":1252,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:43:02.451: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3390
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jul 31 16:43:02.596: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 31 16:43:02.609: INFO: Waiting for terminating namespaces to be deleted...
Jul 31 16:43:02.612: INFO: 
Logging pods the kubelet thinks is on node test-aruna-123-node-group-687e2c91ce before test
Jul 31 16:43:02.620: INFO: ccp-monitor-prometheus-pass-job-u7slu-rztnq from ccp started at 2020-07-30 20:37:47 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.620: INFO: 	Container ccp-monitor-prometheus-pass-container ready: false, restart count 0
Jul 31 16:43:02.620: INFO: kube-proxy-9nrgh from kube-system started at 2020-07-30 20:35:01 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.620: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 16:43:02.620: INFO: ccp-helm-operator-749fb6dc7-f9mhf from ccp started at 2020-07-30 20:35:31 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.620: INFO: 	Container ccp-helm-operator ready: true, restart count 0
Jul 31 16:43:02.621: INFO: ccp-monitor-prometheus-node-exporter-wmldc from ccp started at 2020-07-30 20:37:46 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.621: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jul 31 16:43:02.621: INFO: ccp-monitor-prometheus-port-update-v83hn-jkj8b from ccp started at 2020-07-30 20:37:47 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.621: INFO: 	Container ccp-monitor-prometheus-port-update ready: false, restart count 0
Jul 31 16:43:02.621: INFO: ccp-monitor-grafana-78fd4cc979-rdwrh from ccp started at 2020-07-30 20:37:51 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.621: INFO: 	Container grafana ready: true, restart count 0
Jul 31 16:43:02.621: INFO: test-webserver-c2ace7f3-7c93-4dbe-8c5b-f204f21fec20 from container-probe-6295 started at 2020-07-31 16:42:34 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.621: INFO: 	Container test-webserver ready: true, restart count 0
Jul 31 16:43:02.621: INFO: metallb-speaker-lgvxz from ccp started at 2020-07-30 20:35:21 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.621: INFO: 	Container metallb-speaker ready: true, restart count 0
Jul 31 16:43:02.621: INFO: nvidia-device-plugin-daemonset-tzdxm from kube-system started at 2020-07-30 20:35:21 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.621: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Jul 31 16:43:02.621: INFO: nginx-ingress-controller-bb6bp from ccp started at 2020-07-30 20:35:21 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.621: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jul 31 16:43:02.621: INFO: sonobuoy-systemd-logs-daemon-set-21a6c8c28f874b58-rcrfg from sonobuoy started at 2020-07-31 16:22:15 +0000 UTC (2 container statuses recorded)
Jul 31 16:43:02.621: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 16:43:02.621: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 16:43:02.621: INFO: sonobuoy from sonobuoy started at 2020-07-31 16:21:46 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.621: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 31 16:43:02.621: INFO: calico-node-98r4p from kube-system started at 2020-07-30 20:35:01 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.621: INFO: 	Container calico-node ready: true, restart count 0
Jul 31 16:43:02.621: INFO: ccp-monitor-prometheus-pushgateway-7d5b6d448b-4cpkd from ccp started at 2020-07-30 20:37:47 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.621: INFO: 	Container prometheus-pushgateway ready: true, restart count 0
Jul 31 16:43:02.621: INFO: 
Logging pods the kubelet thinks is on node test-aruna-123-node-group-a8f122b701 before test
Jul 31 16:43:02.629: INFO: calico-node-fkmgx from kube-system started at 2020-07-30 20:34:54 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.629: INFO: 	Container calico-node ready: true, restart count 0
Jul 31 16:43:02.629: INFO: cert-manager-7c4fdf69b7-ct9hx from ccp started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.629: INFO: 	Container cert-manager ready: true, restart count 0
Jul 31 16:43:02.629: INFO: nginx-ingress-default-backend-6b546bb848-d7ms8 from ccp started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.629: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jul 31 16:43:02.629: INFO: kube-proxy-rpj2g from kube-system started at 2020-07-30 20:34:54 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.629: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 16:43:02.629: INFO: nginx-ingress-controller-vv7zf from ccp started at 2020-07-30 20:35:04 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.629: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jul 31 16:43:02.629: INFO: metallb-controller-6c8c5fd7fd-9n8gm from ccp started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.629: INFO: 	Container metallb-controller ready: true, restart count 0
Jul 31 16:43:02.629: INFO: ccp-monitor-prometheus-server-67d55955c7-b54b2 from ccp started at 2020-07-30 20:37:51 +0000 UTC (3 container statuses recorded)
Jul 31 16:43:02.629: INFO: 	Container nginx-proxy ready: true, restart count 0
Jul 31 16:43:02.629: INFO: 	Container prometheus-server ready: true, restart count 0
Jul 31 16:43:02.629: INFO: 	Container prometheus-server-configmap-reload ready: true, restart count 0
Jul 31 16:43:02.629: INFO: nvidia-device-plugin-daemonset-b7rb6 from kube-system started at 2020-07-30 20:35:04 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.629: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Jul 31 16:43:02.629: INFO: ccp-monitor-prometheus-node-exporter-82955 from ccp started at 2020-07-30 20:37:46 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.629: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jul 31 16:43:02.629: INFO: sonobuoy-systemd-logs-daemon-set-21a6c8c28f874b58-64p9f from sonobuoy started at 2020-07-31 16:22:16 +0000 UTC (2 container statuses recorded)
Jul 31 16:43:02.629: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 16:43:02.629: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 16:43:02.629: INFO: metallb-speaker-g2jrx from ccp started at 2020-07-30 20:35:04 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.629: INFO: 	Container metallb-speaker ready: true, restart count 0
Jul 31 16:43:02.629: INFO: 
Logging pods the kubelet thinks is on node test-aruna-123-node-group-a92c781fbd before test
Jul 31 16:43:02.635: INFO: ccp-monitor-prometheus-node-exporter-559kz from ccp started at 2020-07-30 20:37:46 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.635: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jul 31 16:43:02.635: INFO: sonobuoy-systemd-logs-daemon-set-21a6c8c28f874b58-5vrgt from sonobuoy started at 2020-07-31 16:22:17 +0000 UTC (2 container statuses recorded)
Jul 31 16:43:02.635: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 16:43:02.635: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 16:43:02.635: INFO: metallb-speaker-q6n4f from ccp started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.635: INFO: 	Container metallb-speaker ready: true, restart count 0
Jul 31 16:43:02.635: INFO: kubernetes-dashboard-dbfcd4d-mkklx from ccp started at 2020-07-30 20:37:33 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.635: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jul 31 16:43:02.635: INFO: ccp-monitor-prometheus-kube-state-metrics-7f7c9f986-xzrgl from ccp started at 2020-07-30 20:37:46 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.635: INFO: 	Container prometheus-kube-state-metrics ready: true, restart count 0
Jul 31 16:43:02.635: INFO: ccp-monitor-grafana-set-datasource-qwx6l from ccp started at 2020-07-30 20:37:46 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.635: INFO: 	Container ccp-monitor-grafana-set-datasource ready: false, restart count 0
Jul 31 16:43:02.635: INFO: nvidia-device-plugin-daemonset-4pt4d from kube-system started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.635: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Jul 31 16:43:02.635: INFO: sonobuoy-e2e-job-e7094f6bfc7c4c8d from sonobuoy started at 2020-07-31 16:22:17 +0000 UTC (2 container statuses recorded)
Jul 31 16:43:02.635: INFO: 	Container e2e ready: true, restart count 0
Jul 31 16:43:02.635: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 16:43:02.635: INFO: calico-node-txdhs from kube-system started at 2020-07-30 20:35:00 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.635: INFO: 	Container calico-node ready: true, restart count 0
Jul 31 16:43:02.635: INFO: nginx-ingress-controller-2447g from ccp started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.635: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jul 31 16:43:02.635: INFO: ccp-monitor-prometheus-alertmanager-57f5689547-wvfrs from ccp started at 2020-07-30 20:37:51 +0000 UTC (2 container statuses recorded)
Jul 31 16:43:02.635: INFO: 	Container prometheus-alertmanager ready: true, restart count 0
Jul 31 16:43:02.635: INFO: 	Container prometheus-alertmanager-configmap-reload ready: true, restart count 0
Jul 31 16:43:02.635: INFO: kube-proxy-q4q9v from kube-system started at 2020-07-30 20:35:00 +0000 UTC (1 container statuses recorded)
Jul 31 16:43:02.635: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-53e81100-9d2f-40f5-8836-4b5963ad314d 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-53e81100-9d2f-40f5-8836-4b5963ad314d off the node test-aruna-123-node-group-687e2c91ce
STEP: verifying the node doesn't have the label kubernetes.io/e2e-53e81100-9d2f-40f5-8836-4b5963ad314d
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:48:10.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3390" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:308.356 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":280,"completed":77,"skipped":1265,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:48:10.810: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7551
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: executing a command with run --rm and attach with stdin
Jul 31 16:48:10.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 --namespace=kubectl-7551 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jul 31 16:48:13.503: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jul 31 16:48:13.503: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:48:15.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7551" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run --rm job should create a job from an image, then delete the job  [Conformance]","total":280,"completed":78,"skipped":1273,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:48:15.518: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-6303
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jul 31 16:48:19.697: INFO: &Pod{ObjectMeta:{send-events-14c6878a-dc8c-42ae-8c8e-afe2d3b3a8fb  events-6303 /api/v1/namespaces/events-6303/pods/send-events-14c6878a-dc8c-42ae-8c8e-afe2d3b3a8fb 93ffcf16-e86d-444c-baf8-884f5a97bd54 219471 0 2020-07-31 16:48:15 +0000 UTC <nil> <nil> map[name:foo time:672674117] map[cni.projectcalico.org/podIP:192.168.220.96/32 cni.projectcalico.org/podIPs:192.168.220.96/32] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fz7n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fz7n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fz7n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-687e2c91ce,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 16:48:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 16:48:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 16:48:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 16:48:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.92,PodIP:192.168.220.96,StartTime:2020-07-31 16:48:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-31 16:48:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://92529363d022064465f1882f86e70a191ae4821b1a13c15527775ad467d3bb38,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.220.96,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jul 31 16:48:21.701: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jul 31 16:48:23.704: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:48:23.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6303" for this suite.

• [SLOW TEST:8.209 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":280,"completed":79,"skipped":1296,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:48:23.728: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-3813
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:48:33.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3813" for this suite.

• [SLOW TEST:10.169 seconds]
[sig-apps] Job
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":280,"completed":80,"skipped":1332,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:48:33.897: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4634
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-dbd447b4-eaf1-4e01-a7b4-1acf354480cd
STEP: Creating a pod to test consume secrets
Jul 31 16:48:34.150: INFO: Waiting up to 5m0s for pod "pod-secrets-a1b12b4d-d202-42b5-a07c-45013fa1ad76" in namespace "secrets-4634" to be "success or failure"
Jul 31 16:48:34.162: INFO: Pod "pod-secrets-a1b12b4d-d202-42b5-a07c-45013fa1ad76": Phase="Pending", Reason="", readiness=false. Elapsed: 12.463455ms
Jul 31 16:48:36.166: INFO: Pod "pod-secrets-a1b12b4d-d202-42b5-a07c-45013fa1ad76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016016888s
Jul 31 16:48:38.169: INFO: Pod "pod-secrets-a1b12b4d-d202-42b5-a07c-45013fa1ad76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019571749s
STEP: Saw pod success
Jul 31 16:48:38.169: INFO: Pod "pod-secrets-a1b12b4d-d202-42b5-a07c-45013fa1ad76" satisfied condition "success or failure"
Jul 31 16:48:38.172: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-secrets-a1b12b4d-d202-42b5-a07c-45013fa1ad76 container secret-volume-test: <nil>
STEP: delete the pod
Jul 31 16:48:38.198: INFO: Waiting for pod pod-secrets-a1b12b4d-d202-42b5-a07c-45013fa1ad76 to disappear
Jul 31 16:48:38.202: INFO: Pod pod-secrets-a1b12b4d-d202-42b5-a07c-45013fa1ad76 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:48:38.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4634" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":81,"skipped":1333,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:48:38.213: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8280
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jul 31 16:48:38.407: INFO: Waiting up to 5m0s for pod "downward-api-6c442daf-00ea-4b56-a9fa-be0044e4fbeb" in namespace "downward-api-8280" to be "success or failure"
Jul 31 16:48:38.412: INFO: Pod "downward-api-6c442daf-00ea-4b56-a9fa-be0044e4fbeb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.661279ms
Jul 31 16:48:40.415: INFO: Pod "downward-api-6c442daf-00ea-4b56-a9fa-be0044e4fbeb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008069311s
Jul 31 16:48:42.419: INFO: Pod "downward-api-6c442daf-00ea-4b56-a9fa-be0044e4fbeb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012117482s
STEP: Saw pod success
Jul 31 16:48:42.420: INFO: Pod "downward-api-6c442daf-00ea-4b56-a9fa-be0044e4fbeb" satisfied condition "success or failure"
Jul 31 16:48:42.422: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod downward-api-6c442daf-00ea-4b56-a9fa-be0044e4fbeb container dapi-container: <nil>
STEP: delete the pod
Jul 31 16:48:42.448: INFO: Waiting for pod downward-api-6c442daf-00ea-4b56-a9fa-be0044e4fbeb to disappear
Jul 31 16:48:42.450: INFO: Pod downward-api-6c442daf-00ea-4b56-a9fa-be0044e4fbeb no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:48:42.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8280" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":280,"completed":82,"skipped":1367,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:48:42.467: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1805
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:49:42.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1805" for this suite.

• [SLOW TEST:60.173 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":280,"completed":83,"skipped":1396,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:49:42.641: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2447
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Jul 31 16:49:42.791: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:50:01.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2447" for this suite.

• [SLOW TEST:19.240 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":280,"completed":84,"skipped":1419,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:50:01.882: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-5041
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:163
Jul 31 16:50:02.075: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 31 16:51:02.106: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 16:51:02.109: INFO: Starting informer...
STEP: Starting pod...
Jul 31 16:51:02.319: INFO: Pod is running on test-aruna-123-node-group-687e2c91ce. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jul 31 16:51:02.352: INFO: Pod wasn't evicted. Proceeding
Jul 31 16:51:02.352: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jul 31 16:52:17.415: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:52:17.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-5041" for this suite.

• [SLOW TEST:135.545 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":280,"completed":85,"skipped":1452,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:52:17.428: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4505
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating replication controller my-hostname-basic-9b92d99c-3766-4c87-a1d0-7ee9c344de54
Jul 31 16:52:17.579: INFO: Pod name my-hostname-basic-9b92d99c-3766-4c87-a1d0-7ee9c344de54: Found 0 pods out of 1
Jul 31 16:52:22.584: INFO: Pod name my-hostname-basic-9b92d99c-3766-4c87-a1d0-7ee9c344de54: Found 1 pods out of 1
Jul 31 16:52:22.584: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-9b92d99c-3766-4c87-a1d0-7ee9c344de54" are running
Jul 31 16:52:22.588: INFO: Pod "my-hostname-basic-9b92d99c-3766-4c87-a1d0-7ee9c344de54-r6tm2" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-31 16:52:15 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-31 16:52:18 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-31 16:52:18 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-31 16:52:17 +0000 UTC Reason: Message:}])
Jul 31 16:52:22.588: INFO: Trying to dial the pod
Jul 31 16:52:27.598: INFO: Controller my-hostname-basic-9b92d99c-3766-4c87-a1d0-7ee9c344de54: Got expected result from replica 1 [my-hostname-basic-9b92d99c-3766-4c87-a1d0-7ee9c344de54-r6tm2]: "my-hostname-basic-9b92d99c-3766-4c87-a1d0-7ee9c344de54-r6tm2", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:52:27.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4505" for this suite.

• [SLOW TEST:10.181 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":86,"skipped":1470,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:52:27.610: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5369
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jul 31 16:52:27.764: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:52:32.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5369" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":280,"completed":87,"skipped":1519,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:52:32.162: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5647
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-ee6dbe5f-643c-466e-9ea1-3cbbc21111de
STEP: Creating a pod to test consume secrets
Jul 31 16:52:32.329: INFO: Waiting up to 5m0s for pod "pod-secrets-aeffe124-7233-40f9-b3b5-538e134558ea" in namespace "secrets-5647" to be "success or failure"
Jul 31 16:52:32.338: INFO: Pod "pod-secrets-aeffe124-7233-40f9-b3b5-538e134558ea": Phase="Pending", Reason="", readiness=false. Elapsed: 8.648945ms
Jul 31 16:52:34.342: INFO: Pod "pod-secrets-aeffe124-7233-40f9-b3b5-538e134558ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012357965s
Jul 31 16:52:36.345: INFO: Pod "pod-secrets-aeffe124-7233-40f9-b3b5-538e134558ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015041341s
STEP: Saw pod success
Jul 31 16:52:36.345: INFO: Pod "pod-secrets-aeffe124-7233-40f9-b3b5-538e134558ea" satisfied condition "success or failure"
Jul 31 16:52:36.347: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-secrets-aeffe124-7233-40f9-b3b5-538e134558ea container secret-volume-test: <nil>
STEP: delete the pod
Jul 31 16:52:36.373: INFO: Waiting for pod pod-secrets-aeffe124-7233-40f9-b3b5-538e134558ea to disappear
Jul 31 16:52:36.385: INFO: Pod pod-secrets-aeffe124-7233-40f9-b3b5-538e134558ea no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:52:36.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5647" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":88,"skipped":1538,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:52:36.413: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5612
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5612.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5612.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5612.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5612.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 31 16:52:40.600: INFO: DNS probes using dns-test-1f734b72-8322-4bd3-8c9d-99671bab5d8c succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5612.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5612.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5612.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5612.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 31 16:52:44.673: INFO: File wheezy_udp@dns-test-service-3.dns-5612.svc.cluster.local from pod  dns-5612/dns-test-aca5a9c2-69a3-4ac8-9d20-e18c74081dfd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 31 16:52:44.678: INFO: File jessie_udp@dns-test-service-3.dns-5612.svc.cluster.local from pod  dns-5612/dns-test-aca5a9c2-69a3-4ac8-9d20-e18c74081dfd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 31 16:52:44.678: INFO: Lookups using dns-5612/dns-test-aca5a9c2-69a3-4ac8-9d20-e18c74081dfd failed for: [wheezy_udp@dns-test-service-3.dns-5612.svc.cluster.local jessie_udp@dns-test-service-3.dns-5612.svc.cluster.local]

Jul 31 16:52:49.686: INFO: File wheezy_udp@dns-test-service-3.dns-5612.svc.cluster.local from pod  dns-5612/dns-test-aca5a9c2-69a3-4ac8-9d20-e18c74081dfd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 31 16:52:49.692: INFO: File jessie_udp@dns-test-service-3.dns-5612.svc.cluster.local from pod  dns-5612/dns-test-aca5a9c2-69a3-4ac8-9d20-e18c74081dfd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 31 16:52:49.692: INFO: Lookups using dns-5612/dns-test-aca5a9c2-69a3-4ac8-9d20-e18c74081dfd failed for: [wheezy_udp@dns-test-service-3.dns-5612.svc.cluster.local jessie_udp@dns-test-service-3.dns-5612.svc.cluster.local]

Jul 31 16:52:54.684: INFO: File wheezy_udp@dns-test-service-3.dns-5612.svc.cluster.local from pod  dns-5612/dns-test-aca5a9c2-69a3-4ac8-9d20-e18c74081dfd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 31 16:52:54.687: INFO: File jessie_udp@dns-test-service-3.dns-5612.svc.cluster.local from pod  dns-5612/dns-test-aca5a9c2-69a3-4ac8-9d20-e18c74081dfd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 31 16:52:54.687: INFO: Lookups using dns-5612/dns-test-aca5a9c2-69a3-4ac8-9d20-e18c74081dfd failed for: [wheezy_udp@dns-test-service-3.dns-5612.svc.cluster.local jessie_udp@dns-test-service-3.dns-5612.svc.cluster.local]

Jul 31 16:52:59.683: INFO: File wheezy_udp@dns-test-service-3.dns-5612.svc.cluster.local from pod  dns-5612/dns-test-aca5a9c2-69a3-4ac8-9d20-e18c74081dfd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 31 16:52:59.687: INFO: File jessie_udp@dns-test-service-3.dns-5612.svc.cluster.local from pod  dns-5612/dns-test-aca5a9c2-69a3-4ac8-9d20-e18c74081dfd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 31 16:52:59.687: INFO: Lookups using dns-5612/dns-test-aca5a9c2-69a3-4ac8-9d20-e18c74081dfd failed for: [wheezy_udp@dns-test-service-3.dns-5612.svc.cluster.local jessie_udp@dns-test-service-3.dns-5612.svc.cluster.local]

Jul 31 16:53:04.683: INFO: File wheezy_udp@dns-test-service-3.dns-5612.svc.cluster.local from pod  dns-5612/dns-test-aca5a9c2-69a3-4ac8-9d20-e18c74081dfd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 31 16:53:04.686: INFO: File jessie_udp@dns-test-service-3.dns-5612.svc.cluster.local from pod  dns-5612/dns-test-aca5a9c2-69a3-4ac8-9d20-e18c74081dfd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 31 16:53:04.686: INFO: Lookups using dns-5612/dns-test-aca5a9c2-69a3-4ac8-9d20-e18c74081dfd failed for: [wheezy_udp@dns-test-service-3.dns-5612.svc.cluster.local jessie_udp@dns-test-service-3.dns-5612.svc.cluster.local]

Jul 31 16:53:09.688: INFO: DNS probes using dns-test-aca5a9c2-69a3-4ac8-9d20-e18c74081dfd succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5612.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5612.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5612.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5612.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 31 16:53:13.794: INFO: DNS probes using dns-test-33475f0c-2797-47bb-83de-957c243f65cc succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:53:13.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5612" for this suite.

• [SLOW TEST:37.439 seconds]
[sig-network] DNS
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":280,"completed":89,"skipped":1617,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:53:13.855: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7241
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 16:53:14.033: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jul 31 16:53:14.039: INFO: Number of nodes with available pods: 0
Jul 31 16:53:14.039: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jul 31 16:53:14.072: INFO: Number of nodes with available pods: 0
Jul 31 16:53:14.072: INFO: Node test-aruna-123-node-group-a92c781fbd is running more than one daemon pod
Jul 31 16:53:15.075: INFO: Number of nodes with available pods: 0
Jul 31 16:53:15.075: INFO: Node test-aruna-123-node-group-a92c781fbd is running more than one daemon pod
Jul 31 16:53:16.075: INFO: Number of nodes with available pods: 0
Jul 31 16:53:16.075: INFO: Node test-aruna-123-node-group-a92c781fbd is running more than one daemon pod
Jul 31 16:53:17.075: INFO: Number of nodes with available pods: 1
Jul 31 16:53:17.075: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jul 31 16:53:17.093: INFO: Number of nodes with available pods: 1
Jul 31 16:53:17.093: INFO: Number of running nodes: 0, number of available pods: 1
Jul 31 16:53:18.097: INFO: Number of nodes with available pods: 0
Jul 31 16:53:18.097: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jul 31 16:53:18.111: INFO: Number of nodes with available pods: 0
Jul 31 16:53:18.111: INFO: Node test-aruna-123-node-group-a92c781fbd is running more than one daemon pod
Jul 31 16:53:19.115: INFO: Number of nodes with available pods: 0
Jul 31 16:53:19.115: INFO: Node test-aruna-123-node-group-a92c781fbd is running more than one daemon pod
Jul 31 16:53:20.116: INFO: Number of nodes with available pods: 0
Jul 31 16:53:20.116: INFO: Node test-aruna-123-node-group-a92c781fbd is running more than one daemon pod
Jul 31 16:53:21.115: INFO: Number of nodes with available pods: 0
Jul 31 16:53:21.115: INFO: Node test-aruna-123-node-group-a92c781fbd is running more than one daemon pod
Jul 31 16:53:22.115: INFO: Number of nodes with available pods: 0
Jul 31 16:53:22.115: INFO: Node test-aruna-123-node-group-a92c781fbd is running more than one daemon pod
Jul 31 16:53:23.115: INFO: Number of nodes with available pods: 1
Jul 31 16:53:23.116: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7241, will wait for the garbage collector to delete the pods
Jul 31 16:53:23.195: INFO: Deleting DaemonSet.extensions daemon-set took: 20.704601ms
Jul 31 16:53:23.795: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.20514ms
Jul 31 16:53:26.498: INFO: Number of nodes with available pods: 0
Jul 31 16:53:26.499: INFO: Number of running nodes: 0, number of available pods: 0
Jul 31 16:53:26.502: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7241/daemonsets","resourceVersion":"221269"},"items":null}

Jul 31 16:53:26.504: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7241/pods","resourceVersion":"221269"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:53:26.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7241" for this suite.

• [SLOW TEST:12.681 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":280,"completed":90,"skipped":1639,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:53:26.536: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2499
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 31 16:53:34.741: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 31 16:53:34.744: INFO: Pod pod-with-poststart-http-hook still exists
Jul 31 16:53:36.744: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 31 16:53:36.748: INFO: Pod pod-with-poststart-http-hook still exists
Jul 31 16:53:38.744: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 31 16:53:38.747: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:53:38.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2499" for this suite.

• [SLOW TEST:12.219 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":280,"completed":91,"skipped":1649,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:53:38.759: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5018
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-86b0ac19-8b86-4aa1-9ec2-701a58e84428
STEP: Creating configMap with name cm-test-opt-upd-11ad1a04-d37e-471f-bc32-75fbe5355929
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-86b0ac19-8b86-4aa1-9ec2-701a58e84428
STEP: Updating configmap cm-test-opt-upd-11ad1a04-d37e-471f-bc32-75fbe5355929
STEP: Creating configMap with name cm-test-opt-create-240a7d2c-f168-4bad-ac16-485bbbc21c0f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:53:47.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5018" for this suite.

• [SLOW TEST:8.290 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":92,"skipped":1660,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:53:47.049: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7015
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jul 31 16:53:47.191: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 31 16:53:47.200: INFO: Waiting for terminating namespaces to be deleted...
Jul 31 16:53:47.203: INFO: 
Logging pods the kubelet thinks is on node test-aruna-123-node-group-687e2c91ce before test
Jul 31 16:53:47.213: INFO: kube-proxy-9nrgh from kube-system started at 2020-07-30 20:35:01 +0000 UTC (1 container statuses recorded)
Jul 31 16:53:47.213: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 16:53:47.213: INFO: metallb-speaker-bp7cw from ccp started at 2020-07-31 16:51:00 +0000 UTC (1 container statuses recorded)
Jul 31 16:53:47.213: INFO: 	Container metallb-speaker ready: true, restart count 0
Jul 31 16:53:47.213: INFO: ccp-monitor-prometheus-pushgateway-7d5b6d448b-hl5dh from ccp started at 2020-07-31 16:51:00 +0000 UTC (1 container statuses recorded)
Jul 31 16:53:47.213: INFO: 	Container prometheus-pushgateway ready: true, restart count 0
Jul 31 16:53:47.213: INFO: ccp-monitor-prometheus-node-exporter-p4jvn from ccp started at 2020-07-31 16:51:11 +0000 UTC (1 container statuses recorded)
Jul 31 16:53:47.213: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jul 31 16:53:47.213: INFO: nginx-ingress-controller-jt5pp from ccp started at 2020-07-31 16:51:21 +0000 UTC (1 container statuses recorded)
Jul 31 16:53:47.213: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jul 31 16:53:47.213: INFO: pod-projected-configmaps-3c539e01-0f35-4712-a0f3-225eb3af00d6 from projected-5018 started at 2020-07-31 16:53:37 +0000 UTC (3 container statuses recorded)
Jul 31 16:53:47.213: INFO: 	Container createcm-volume-test ready: true, restart count 0
Jul 31 16:53:47.213: INFO: 	Container delcm-volume-test ready: true, restart count 0
Jul 31 16:53:47.213: INFO: 	Container updcm-volume-test ready: true, restart count 0
Jul 31 16:53:47.213: INFO: pod-handle-http-request from container-lifecycle-hook-2499 started at 2020-07-31 16:53:24 +0000 UTC (1 container statuses recorded)
Jul 31 16:53:47.213: INFO: 	Container pod-handle-http-request ready: false, restart count 0
Jul 31 16:53:47.213: INFO: sonobuoy-systemd-logs-daemon-set-21a6c8c28f874b58-rcrfg from sonobuoy started at 2020-07-31 16:22:15 +0000 UTC (2 container statuses recorded)
Jul 31 16:53:47.213: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 16:53:47.213: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 16:53:47.213: INFO: calico-node-98r4p from kube-system started at 2020-07-30 20:35:01 +0000 UTC (1 container statuses recorded)
Jul 31 16:53:47.213: INFO: 	Container calico-node ready: true, restart count 0
Jul 31 16:53:47.213: INFO: nvidia-device-plugin-daemonset-wjtwk from kube-system started at 2020-07-31 16:51:11 +0000 UTC (1 container statuses recorded)
Jul 31 16:53:47.213: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Jul 31 16:53:47.213: INFO: sonobuoy from sonobuoy started at 2020-07-31 16:21:46 +0000 UTC (1 container statuses recorded)
Jul 31 16:53:47.213: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 31 16:53:47.213: INFO: 
Logging pods the kubelet thinks is on node test-aruna-123-node-group-a8f122b701 before test
Jul 31 16:53:47.223: INFO: kube-proxy-rpj2g from kube-system started at 2020-07-30 20:34:54 +0000 UTC (1 container statuses recorded)
Jul 31 16:53:47.223: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 16:53:47.223: INFO: nginx-ingress-controller-vv7zf from ccp started at 2020-07-30 20:35:04 +0000 UTC (1 container statuses recorded)
Jul 31 16:53:47.223: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jul 31 16:53:47.223: INFO: metallb-controller-6c8c5fd7fd-9n8gm from ccp started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 16:53:47.223: INFO: 	Container metallb-controller ready: true, restart count 0
Jul 31 16:53:47.223: INFO: ccp-monitor-prometheus-server-67d55955c7-b54b2 from ccp started at 2020-07-30 20:37:51 +0000 UTC (3 container statuses recorded)
Jul 31 16:53:47.223: INFO: 	Container nginx-proxy ready: true, restart count 0
Jul 31 16:53:47.223: INFO: 	Container prometheus-server ready: true, restart count 0
Jul 31 16:53:47.223: INFO: 	Container prometheus-server-configmap-reload ready: true, restart count 0
Jul 31 16:53:47.223: INFO: nvidia-device-plugin-daemonset-b7rb6 from kube-system started at 2020-07-30 20:35:04 +0000 UTC (1 container statuses recorded)
Jul 31 16:53:47.223: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Jul 31 16:53:47.223: INFO: ccp-monitor-prometheus-node-exporter-82955 from ccp started at 2020-07-30 20:37:46 +0000 UTC (1 container statuses recorded)
Jul 31 16:53:47.223: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jul 31 16:53:47.223: INFO: sonobuoy-systemd-logs-daemon-set-21a6c8c28f874b58-64p9f from sonobuoy started at 2020-07-31 16:22:16 +0000 UTC (2 container statuses recorded)
Jul 31 16:53:47.223: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 16:53:47.223: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 16:53:47.223: INFO: ccp-helm-operator-749fb6dc7-rrdzz from ccp started at 2020-07-31 16:51:01 +0000 UTC (1 container statuses recorded)
Jul 31 16:53:47.223: INFO: 	Container ccp-helm-operator ready: true, restart count 0
Jul 31 16:53:47.223: INFO: metallb-speaker-g2jrx from ccp started at 2020-07-30 20:35:04 +0000 UTC (1 container statuses recorded)
Jul 31 16:53:47.223: INFO: 	Container metallb-speaker ready: true, restart count 0
Jul 31 16:53:47.223: INFO: calico-node-fkmgx from kube-system started at 2020-07-30 20:34:54 +0000 UTC (1 container statuses recorded)
Jul 31 16:53:47.223: INFO: 	Container calico-node ready: true, restart count 0
Jul 31 16:53:47.223: INFO: cert-manager-7c4fdf69b7-ct9hx from ccp started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 16:53:47.223: INFO: 	Container cert-manager ready: true, restart count 0
Jul 31 16:53:47.223: INFO: nginx-ingress-default-backend-6b546bb848-d7ms8 from ccp started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 16:53:47.223: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jul 31 16:53:47.223: INFO: 
Logging pods the kubelet thinks is on node test-aruna-123-node-group-a92c781fbd before test
Jul 31 16:53:47.230: INFO: ccp-monitor-prometheus-alertmanager-57f5689547-wvfrs from ccp started at 2020-07-30 20:37:51 +0000 UTC (2 container statuses recorded)
Jul 31 16:53:47.230: INFO: 	Container prometheus-alertmanager ready: true, restart count 0
Jul 31 16:53:47.230: INFO: 	Container prometheus-alertmanager-configmap-reload ready: true, restart count 0
Jul 31 16:53:47.230: INFO: kube-proxy-q4q9v from kube-system started at 2020-07-30 20:35:00 +0000 UTC (1 container statuses recorded)
Jul 31 16:53:47.230: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 16:53:47.230: INFO: nginx-ingress-controller-2447g from ccp started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 16:53:47.230: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jul 31 16:53:47.230: INFO: sonobuoy-systemd-logs-daemon-set-21a6c8c28f874b58-5vrgt from sonobuoy started at 2020-07-31 16:22:17 +0000 UTC (2 container statuses recorded)
Jul 31 16:53:47.230: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 16:53:47.230: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 16:53:47.230: INFO: metallb-speaker-q6n4f from ccp started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 16:53:47.230: INFO: 	Container metallb-speaker ready: true, restart count 0
Jul 31 16:53:47.230: INFO: ccp-monitor-prometheus-node-exporter-559kz from ccp started at 2020-07-30 20:37:46 +0000 UTC (1 container statuses recorded)
Jul 31 16:53:47.230: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jul 31 16:53:47.230: INFO: ccp-monitor-prometheus-kube-state-metrics-7f7c9f986-xzrgl from ccp started at 2020-07-30 20:37:46 +0000 UTC (1 container statuses recorded)
Jul 31 16:53:47.230: INFO: 	Container prometheus-kube-state-metrics ready: true, restart count 0
Jul 31 16:53:47.230: INFO: ccp-monitor-grafana-set-datasource-qwx6l from ccp started at 2020-07-30 20:37:46 +0000 UTC (1 container statuses recorded)
Jul 31 16:53:47.230: INFO: 	Container ccp-monitor-grafana-set-datasource ready: false, restart count 0
Jul 31 16:53:47.230: INFO: nvidia-device-plugin-daemonset-4pt4d from kube-system started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 16:53:47.230: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Jul 31 16:53:47.230: INFO: kubernetes-dashboard-dbfcd4d-mkklx from ccp started at 2020-07-30 20:37:33 +0000 UTC (1 container statuses recorded)
Jul 31 16:53:47.230: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jul 31 16:53:47.230: INFO: ccp-monitor-grafana-78fd4cc979-qn6mq from ccp started at 2020-07-31 16:51:02 +0000 UTC (1 container statuses recorded)
Jul 31 16:53:47.230: INFO: 	Container grafana ready: true, restart count 0
Jul 31 16:53:47.230: INFO: calico-node-txdhs from kube-system started at 2020-07-30 20:35:00 +0000 UTC (1 container statuses recorded)
Jul 31 16:53:47.230: INFO: 	Container calico-node ready: true, restart count 0
Jul 31 16:53:47.230: INFO: sonobuoy-e2e-job-e7094f6bfc7c4c8d from sonobuoy started at 2020-07-31 16:22:17 +0000 UTC (2 container statuses recorded)
Jul 31 16:53:47.230: INFO: 	Container e2e ready: true, restart count 0
Jul 31 16:53:47.230: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-ee9be16e-5dd7-48b2-9a01-7f5da5cc6600 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-ee9be16e-5dd7-48b2-9a01-7f5da5cc6600 off the node test-aruna-123-node-group-687e2c91ce
STEP: verifying the node doesn't have the label kubernetes.io/e2e-ee9be16e-5dd7-48b2-9a01-7f5da5cc6600
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:53:55.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7015" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:8.276 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":280,"completed":93,"skipped":1687,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:53:55.327: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8395
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:53:59.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8395" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":94,"skipped":1699,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:53:59.512: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5784
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:54:10.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5784" for this suite.

• [SLOW TEST:11.308 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":280,"completed":95,"skipped":1707,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:54:10.820: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4781
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:54:17.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4781" for this suite.

• [SLOW TEST:7.168 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":280,"completed":96,"skipped":1707,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:54:17.989: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5685
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5685.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5685.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5685.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5685.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5685.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5685.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 31 16:54:22.179: INFO: DNS probes using dns-5685/dns-test-2d66b9a2-3fa3-47d2-933a-57f18725820d succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:54:22.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5685" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":280,"completed":97,"skipped":1770,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:54:22.205: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-7561
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 16:54:22.347: INFO: Creating ReplicaSet my-hostname-basic-3caee0d3-48c1-4d58-b626-c35be49a2de5
Jul 31 16:54:22.356: INFO: Pod name my-hostname-basic-3caee0d3-48c1-4d58-b626-c35be49a2de5: Found 0 pods out of 1
Jul 31 16:54:27.358: INFO: Pod name my-hostname-basic-3caee0d3-48c1-4d58-b626-c35be49a2de5: Found 1 pods out of 1
Jul 31 16:54:27.358: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-3caee0d3-48c1-4d58-b626-c35be49a2de5" is running
Jul 31 16:54:27.364: INFO: Pod "my-hostname-basic-3caee0d3-48c1-4d58-b626-c35be49a2de5-rk2xc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-31 16:54:20 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-31 16:54:23 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-31 16:54:23 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-31 16:54:21 +0000 UTC Reason: Message:}])
Jul 31 16:54:27.364: INFO: Trying to dial the pod
Jul 31 16:54:32.374: INFO: Controller my-hostname-basic-3caee0d3-48c1-4d58-b626-c35be49a2de5: Got expected result from replica 1 [my-hostname-basic-3caee0d3-48c1-4d58-b626-c35be49a2de5-rk2xc]: "my-hostname-basic-3caee0d3-48c1-4d58-b626-c35be49a2de5-rk2xc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:54:32.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7561" for this suite.

• [SLOW TEST:10.178 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":98,"skipped":1781,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:54:32.384: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4908
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-4908
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-4908
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4908
Jul 31 16:54:32.555: INFO: Found 0 stateful pods, waiting for 1
Jul 31 16:54:42.558: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jul 31 16:54:42.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=statefulset-4908 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 31 16:54:44.049: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 31 16:54:44.049: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 31 16:54:44.049: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 31 16:54:44.055: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 31 16:54:54.058: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 31 16:54:54.058: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 16:54:54.077: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999491s
Jul 31 16:54:55.084: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994060069s
Jul 31 16:54:56.087: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987103854s
Jul 31 16:54:57.091: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.983433748s
Jul 31 16:54:58.095: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.979422056s
Jul 31 16:54:59.098: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.97624371s
Jul 31 16:55:00.103: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.972823033s
Jul 31 16:55:01.107: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.968224368s
Jul 31 16:55:02.110: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.963989299s
Jul 31 16:55:03.113: INFO: Verifying statefulset ss doesn't scale past 1 for another 960.857143ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4908
Jul 31 16:55:04.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=statefulset-4908 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 31 16:55:04.343: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 31 16:55:04.343: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 31 16:55:04.343: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 31 16:55:04.346: INFO: Found 1 stateful pods, waiting for 3
Jul 31 16:55:14.349: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 16:55:14.349: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 16:55:14.349: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jul 31 16:55:14.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=statefulset-4908 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 31 16:55:14.607: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 31 16:55:14.607: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 31 16:55:14.607: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 31 16:55:14.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=statefulset-4908 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 31 16:55:14.801: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 31 16:55:14.801: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 31 16:55:14.801: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 31 16:55:14.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=statefulset-4908 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 31 16:55:15.011: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 31 16:55:15.011: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 31 16:55:15.011: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 31 16:55:15.011: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 16:55:15.014: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jul 31 16:55:25.020: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 31 16:55:25.020: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 31 16:55:25.020: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 31 16:55:25.038: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999557s
Jul 31 16:55:26.042: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991167384s
Jul 31 16:55:27.046: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987468462s
Jul 31 16:55:28.050: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98333228s
Jul 31 16:55:29.053: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.979629698s
Jul 31 16:55:30.056: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.975969349s
Jul 31 16:55:31.060: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.972960792s
Jul 31 16:55:32.064: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.969206479s
Jul 31 16:55:33.067: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.965396699s
Jul 31 16:55:34.071: INFO: Verifying statefulset ss doesn't scale past 3 for another 962.248539ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4908
Jul 31 16:55:35.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=statefulset-4908 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 31 16:55:35.316: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 31 16:55:35.316: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 31 16:55:35.316: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 31 16:55:35.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=statefulset-4908 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 31 16:55:35.519: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 31 16:55:35.519: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 31 16:55:35.519: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 31 16:55:35.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=statefulset-4908 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 31 16:55:35.727: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 31 16:55:35.727: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 31 16:55:35.727: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 31 16:55:35.727: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jul 31 16:55:55.739: INFO: Deleting all statefulset in ns statefulset-4908
Jul 31 16:55:55.741: INFO: Scaling statefulset ss to 0
Jul 31 16:55:55.747: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 16:55:55.749: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:55:55.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4908" for this suite.

• [SLOW TEST:83.413 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":280,"completed":99,"skipped":1783,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:55:55.798: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4080
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:55:59.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4080" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":100,"skipped":1798,"failed":0}
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:55:59.990: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-4388
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
STEP: reading a file in the container
Jul 31 16:56:04.670: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4388 pod-service-account-7fd96bc2-f670-45d9-bc0a-df7d63e920ad -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jul 31 16:56:04.903: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4388 pod-service-account-7fd96bc2-f670-45d9-bc0a-df7d63e920ad -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jul 31 16:56:05.162: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4388 pod-service-account-7fd96bc2-f670-45d9-bc0a-df7d63e920ad -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:56:05.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4388" for this suite.

• [SLOW TEST:5.406 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":280,"completed":101,"skipped":1801,"failed":0}
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:56:05.397: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-477
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:56:05.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-477" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":280,"completed":102,"skipped":1801,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:56:05.583: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9600
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Jul 31 16:56:05.740: INFO: namespace kubectl-9600
Jul 31 16:56:05.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 create -f - --namespace=kubectl-9600'
Jul 31 16:56:05.988: INFO: stderr: ""
Jul 31 16:56:05.988: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Jul 31 16:56:06.992: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 31 16:56:06.992: INFO: Found 0 / 1
Jul 31 16:56:07.992: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 31 16:56:07.992: INFO: Found 0 / 1
Jul 31 16:56:08.992: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 31 16:56:08.992: INFO: Found 1 / 1
Jul 31 16:56:08.992: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 31 16:56:08.994: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 31 16:56:08.995: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 31 16:56:08.995: INFO: wait on agnhost-master startup in kubectl-9600 
Jul 31 16:56:08.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 logs agnhost-master-2hz98 agnhost-master --namespace=kubectl-9600'
Jul 31 16:56:09.074: INFO: stderr: ""
Jul 31 16:56:09.074: INFO: stdout: "Paused\n"
STEP: exposing RC
Jul 31 16:56:09.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-9600'
Jul 31 16:56:09.168: INFO: stderr: ""
Jul 31 16:56:09.168: INFO: stdout: "service/rm2 exposed\n"
Jul 31 16:56:09.177: INFO: Service rm2 in namespace kubectl-9600 found.
STEP: exposing service
Jul 31 16:56:11.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-9600'
Jul 31 16:56:11.268: INFO: stderr: ""
Jul 31 16:56:11.268: INFO: stdout: "service/rm3 exposed\n"
Jul 31 16:56:11.283: INFO: Service rm3 in namespace kubectl-9600 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:56:13.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9600" for this suite.

• [SLOW TEST:7.724 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1188
    should create services for rc  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":280,"completed":103,"skipped":1807,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:56:13.307: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2424
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 16:56:14.302: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 31 16:56:16.319: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731811373, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731811373, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731811373, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731811373, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 16:56:19.341: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 16:56:19.344: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8925-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:56:20.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2424" for this suite.
STEP: Destroying namespace "webhook-2424-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.683 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":280,"completed":104,"skipped":1821,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:56:20.990: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8474
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 16:56:21.619: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 31 16:56:23.636: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731811381, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731811381, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731811381, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731811381, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 16:56:26.670: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:56:26.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8474" for this suite.
STEP: Destroying namespace "webhook-8474-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.894 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":280,"completed":105,"skipped":1821,"failed":0}
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:56:26.885: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-123
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-123.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-123.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-123.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-123.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-123.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-123.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 31 16:56:31.126: INFO: DNS probes using dns-123/dns-test-5666be97-f35b-4939-885e-7f6f58ae9f3b succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:56:31.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-123" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":280,"completed":106,"skipped":1821,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:56:31.195: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6075
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 31 16:56:35.877: INFO: Successfully updated pod "pod-update-2fc5cf0c-67f2-4e39-9727-f89d2141f9ed"
STEP: verifying the updated pod is in kubernetes
Jul 31 16:56:35.885: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:56:35.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6075" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":280,"completed":107,"skipped":1849,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:56:35.899: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5972
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Jul 31 16:56:36.037: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:56:56.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5972" for this suite.

• [SLOW TEST:20.254 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":280,"completed":108,"skipped":1853,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:56:56.154: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7673
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 31 16:56:56.361: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 16:56:56.368: INFO: Number of nodes with available pods: 0
Jul 31 16:56:56.368: INFO: Node test-aruna-123-node-group-687e2c91ce is running more than one daemon pod
Jul 31 16:56:57.373: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 16:56:57.375: INFO: Number of nodes with available pods: 0
Jul 31 16:56:57.375: INFO: Node test-aruna-123-node-group-687e2c91ce is running more than one daemon pod
Jul 31 16:56:58.373: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 16:56:58.376: INFO: Number of nodes with available pods: 0
Jul 31 16:56:58.376: INFO: Node test-aruna-123-node-group-687e2c91ce is running more than one daemon pod
Jul 31 16:56:59.374: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 16:56:59.377: INFO: Number of nodes with available pods: 3
Jul 31 16:56:59.377: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jul 31 16:56:59.390: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 16:56:59.395: INFO: Number of nodes with available pods: 3
Jul 31 16:56:59.395: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7673, will wait for the garbage collector to delete the pods
Jul 31 16:57:00.464: INFO: Deleting DaemonSet.extensions daemon-set took: 7.393768ms
Jul 31 16:57:01.064: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.506679ms
Jul 31 16:58:35.468: INFO: Number of nodes with available pods: 0
Jul 31 16:58:35.468: INFO: Number of running nodes: 0, number of available pods: 0
Jul 31 16:58:35.471: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7673/daemonsets","resourceVersion":"223274"},"items":null}

Jul 31 16:58:35.474: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7673/pods","resourceVersion":"223274"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:58:35.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7673" for this suite.

• [SLOW TEST:99.349 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":280,"completed":109,"skipped":1897,"failed":0}
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:58:35.503: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4231
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jul 31 16:58:35.642: INFO: PodSpec: initContainers in spec.initContainers
Jul 31 16:59:23.696: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-bda60a8e-b763-4564-abdf-98ca5e8ec8ae", GenerateName:"", Namespace:"init-container-4231", SelfLink:"/api/v1/namespaces/init-container-4231/pods/pod-init-bda60a8e-b763-4564-abdf-98ca5e8ec8ae", UID:"15fe7f71-17ff-4c16-84de-84498683c550", ResourceVersion:"223484", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63731811515, loc:(*time.Location)(0x7925200)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"642970478"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"192.168.220.73/32", "cni.projectcalico.org/podIPs":"192.168.220.73/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-cbkz5", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001be5780), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-cbkz5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-cbkz5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-cbkz5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002ffc068), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"test-aruna-123-node-group-687e2c91ce", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002cf04e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002ffc0f0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002ffc110)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002ffc118), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002ffc11c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731811513, loc:(*time.Location)(0x7925200)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731811513, loc:(*time.Location)(0x7925200)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731811513, loc:(*time.Location)(0x7925200)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731811515, loc:(*time.Location)(0x7925200)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.10.102.92", PodIP:"192.168.220.73", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.220.73"}}, StartTime:(*v1.Time)(0xc00419e060), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc00419e0a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00320e070)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://f906f1424afb181877c496a8792500604dd5b29a1d3d127495be8a601d04d2c6", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00419e0c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00419e080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc002ffc19f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:59:23.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4231" for this suite.

• [SLOW TEST:48.205 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":280,"completed":110,"skipped":1899,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:59:23.709: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-58
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-2824
STEP: Creating secret with name secret-test-6ae4d819-e6ac-41c3-814d-c300f953d710
STEP: Creating a pod to test consume secrets
Jul 31 16:59:24.063: INFO: Waiting up to 5m0s for pod "pod-secrets-c5536d88-7951-4fb2-a2c1-b8828e300f27" in namespace "secrets-58" to be "success or failure"
Jul 31 16:59:24.067: INFO: Pod "pod-secrets-c5536d88-7951-4fb2-a2c1-b8828e300f27": Phase="Pending", Reason="", readiness=false. Elapsed: 3.593987ms
Jul 31 16:59:26.070: INFO: Pod "pod-secrets-c5536d88-7951-4fb2-a2c1-b8828e300f27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007035369s
Jul 31 16:59:28.073: INFO: Pod "pod-secrets-c5536d88-7951-4fb2-a2c1-b8828e300f27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010385566s
STEP: Saw pod success
Jul 31 16:59:28.073: INFO: Pod "pod-secrets-c5536d88-7951-4fb2-a2c1-b8828e300f27" satisfied condition "success or failure"
Jul 31 16:59:28.076: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-secrets-c5536d88-7951-4fb2-a2c1-b8828e300f27 container secret-volume-test: <nil>
STEP: delete the pod
Jul 31 16:59:28.114: INFO: Waiting for pod pod-secrets-c5536d88-7951-4fb2-a2c1-b8828e300f27 to disappear
Jul 31 16:59:28.117: INFO: Pod pod-secrets-c5536d88-7951-4fb2-a2c1-b8828e300f27 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:59:28.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-58" for this suite.
STEP: Destroying namespace "secret-namespace-2824" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":280,"completed":111,"skipped":1921,"failed":0}
SS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:59:28.131: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-5420
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 16:59:28.316: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-f1c3d7fd-21e9-4190-90ce-2ce93309822c" in namespace "security-context-test-5420" to be "success or failure"
Jul 31 16:59:28.327: INFO: Pod "alpine-nnp-false-f1c3d7fd-21e9-4190-90ce-2ce93309822c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.044907ms
Jul 31 16:59:30.330: INFO: Pod "alpine-nnp-false-f1c3d7fd-21e9-4190-90ce-2ce93309822c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014100387s
Jul 31 16:59:32.333: INFO: Pod "alpine-nnp-false-f1c3d7fd-21e9-4190-90ce-2ce93309822c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017246644s
Jul 31 16:59:34.339: INFO: Pod "alpine-nnp-false-f1c3d7fd-21e9-4190-90ce-2ce93309822c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022807884s
Jul 31 16:59:34.339: INFO: Pod "alpine-nnp-false-f1c3d7fd-21e9-4190-90ce-2ce93309822c" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:59:34.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5420" for this suite.

• [SLOW TEST:6.228 seconds]
[k8s.io] Security Context
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:289
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":112,"skipped":1923,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:59:34.359: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6927
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-94f29093-36c1-4f54-bf12-eba0c5388a0e
STEP: Creating a pod to test consume configMaps
Jul 31 16:59:34.527: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-274dacb4-c85f-47b5-a90c-c547de5e3218" in namespace "projected-6927" to be "success or failure"
Jul 31 16:59:34.537: INFO: Pod "pod-projected-configmaps-274dacb4-c85f-47b5-a90c-c547de5e3218": Phase="Pending", Reason="", readiness=false. Elapsed: 9.544327ms
Jul 31 16:59:36.541: INFO: Pod "pod-projected-configmaps-274dacb4-c85f-47b5-a90c-c547de5e3218": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013667964s
Jul 31 16:59:38.544: INFO: Pod "pod-projected-configmaps-274dacb4-c85f-47b5-a90c-c547de5e3218": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016674875s
STEP: Saw pod success
Jul 31 16:59:38.544: INFO: Pod "pod-projected-configmaps-274dacb4-c85f-47b5-a90c-c547de5e3218" satisfied condition "success or failure"
Jul 31 16:59:38.546: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-projected-configmaps-274dacb4-c85f-47b5-a90c-c547de5e3218 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 16:59:38.568: INFO: Waiting for pod pod-projected-configmaps-274dacb4-c85f-47b5-a90c-c547de5e3218 to disappear
Jul 31 16:59:38.580: INFO: Pod pod-projected-configmaps-274dacb4-c85f-47b5-a90c-c547de5e3218 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 16:59:38.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6927" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":280,"completed":113,"skipped":1934,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 16:59:38.593: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7460
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-7460
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Jul 31 16:59:38.784: INFO: Found 0 stateful pods, waiting for 3
Jul 31 16:59:48.787: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 16:59:48.787: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 16:59:48.787: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 16:59:48.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=statefulset-7460 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 31 16:59:49.013: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 31 16:59:49.013: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 31 16:59:49.013: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jul 31 16:59:59.041: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jul 31 17:00:09.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=statefulset-7460 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 31 17:00:09.290: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 31 17:00:09.290: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 31 17:00:09.290: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 31 17:00:29.306: INFO: Waiting for StatefulSet statefulset-7460/ss2 to complete update
Jul 31 17:00:29.306: INFO: Waiting for Pod statefulset-7460/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Jul 31 17:00:39.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=statefulset-7460 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 31 17:00:39.539: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 31 17:00:39.539: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 31 17:00:39.539: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 31 17:00:49.565: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jul 31 17:00:59.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=statefulset-7460 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 31 17:00:59.794: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 31 17:00:59.794: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 31 17:00:59.794: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 31 17:01:09.812: INFO: Waiting for StatefulSet statefulset-7460/ss2 to complete update
Jul 31 17:01:09.812: INFO: Waiting for Pod statefulset-7460/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jul 31 17:01:09.812: INFO: Waiting for Pod statefulset-7460/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jul 31 17:01:09.812: INFO: Waiting for Pod statefulset-7460/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jul 31 17:01:19.819: INFO: Waiting for StatefulSet statefulset-7460/ss2 to complete update
Jul 31 17:01:19.819: INFO: Waiting for Pod statefulset-7460/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jul 31 17:01:19.819: INFO: Waiting for Pod statefulset-7460/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jul 31 17:01:29.823: INFO: Waiting for StatefulSet statefulset-7460/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jul 31 17:01:39.817: INFO: Deleting all statefulset in ns statefulset-7460
Jul 31 17:01:39.819: INFO: Scaling statefulset ss2 to 0
Jul 31 17:01:59.849: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 17:01:59.852: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:01:59.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7460" for this suite.

• [SLOW TEST:141.291 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":280,"completed":114,"skipped":1951,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:01:59.886: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3116
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jul 31 17:02:00.036: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jul 31 17:02:13.570: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
Jul 31 17:02:17.254: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:02:31.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3116" for this suite.

• [SLOW TEST:31.646 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":280,"completed":115,"skipped":1959,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:02:31.532: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2081
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 17:02:31.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 version'
Jul 31 17:02:31.760: INFO: stderr: ""
Jul 31 17:02:31.760: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.6\", GitCommit:\"d32e40e20d167e103faf894261614c5b45c44198\", GitTreeState:\"clean\", BuildDate:\"2020-05-20T13:16:24Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.6\", GitCommit:\"d32e40e20d167e103faf894261614c5b45c44198\", GitTreeState:\"clean\", BuildDate:\"2020-05-20T13:08:34Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:02:31.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2081" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":280,"completed":116,"skipped":1967,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:02:31.770: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-381
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:02:48.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-381" for this suite.

• [SLOW TEST:16.282 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":280,"completed":117,"skipped":1975,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:02:48.052: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1820
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 31 17:02:48.250: INFO: Waiting up to 5m0s for pod "pod-a7fbb4e4-f926-49c5-b1e8-45f7b125e6f6" in namespace "emptydir-1820" to be "success or failure"
Jul 31 17:02:48.263: INFO: Pod "pod-a7fbb4e4-f926-49c5-b1e8-45f7b125e6f6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.743945ms
Jul 31 17:02:50.268: INFO: Pod "pod-a7fbb4e4-f926-49c5-b1e8-45f7b125e6f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017867683s
Jul 31 17:02:52.272: INFO: Pod "pod-a7fbb4e4-f926-49c5-b1e8-45f7b125e6f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021618952s
STEP: Saw pod success
Jul 31 17:02:52.272: INFO: Pod "pod-a7fbb4e4-f926-49c5-b1e8-45f7b125e6f6" satisfied condition "success or failure"
Jul 31 17:02:52.275: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-a7fbb4e4-f926-49c5-b1e8-45f7b125e6f6 container test-container: <nil>
STEP: delete the pod
Jul 31 17:02:52.299: INFO: Waiting for pod pod-a7fbb4e4-f926-49c5-b1e8-45f7b125e6f6 to disappear
Jul 31 17:02:52.305: INFO: Pod pod-a7fbb4e4-f926-49c5-b1e8-45f7b125e6f6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:02:52.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1820" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":118,"skipped":1978,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:02:52.313: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8043
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 17:02:52.457: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jul 31 17:02:54.486: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:02:55.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8043" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":280,"completed":119,"skipped":1990,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:02:55.503: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9922
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service nodeport-test with type=NodePort in namespace services-9922
STEP: creating replication controller nodeport-test in namespace services-9922
I0731 17:02:55.680690      21 runners.go:189] Created replication controller with name: nodeport-test, namespace: services-9922, replica count: 2
Jul 31 17:02:58.731: INFO: Creating new exec pod
I0731 17:02:58.731238      21 runners.go:189] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 31 17:03:03.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=services-9922 execpod9clqx -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Jul 31 17:03:04.005: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jul 31 17:03:04.005: INFO: stdout: ""
Jul 31 17:03:04.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=services-9922 execpod9clqx -- /bin/sh -x -c nc -zv -t -w 2 10.105.191.223 80'
Jul 31 17:03:04.274: INFO: stderr: "+ nc -zv -t -w 2 10.105.191.223 80\nConnection to 10.105.191.223 80 port [tcp/http] succeeded!\n"
Jul 31 17:03:04.274: INFO: stdout: ""
Jul 31 17:03:04.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=services-9922 execpod9clqx -- /bin/sh -x -c nc -zv -t -w 2 10.10.102.93 31974'
Jul 31 17:03:04.559: INFO: stderr: "+ nc -zv -t -w 2 10.10.102.93 31974\nConnection to 10.10.102.93 31974 port [tcp/31974] succeeded!\n"
Jul 31 17:03:04.559: INFO: stdout: ""
Jul 31 17:03:04.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=services-9922 execpod9clqx -- /bin/sh -x -c nc -zv -t -w 2 10.10.102.94 31974'
Jul 31 17:03:04.808: INFO: stderr: "+ nc -zv -t -w 2 10.10.102.94 31974\nConnection to 10.10.102.94 31974 port [tcp/31974] succeeded!\n"
Jul 31 17:03:04.808: INFO: stdout: ""
Jul 31 17:03:04.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=services-9922 execpod9clqx -- /bin/sh -x -c nc -zv -t -w 2 10.10.102.93 31974'
Jul 31 17:03:05.106: INFO: stderr: "+ nc -zv -t -w 2 10.10.102.93 31974\nConnection to 10.10.102.93 31974 port [tcp/31974] succeeded!\n"
Jul 31 17:03:05.106: INFO: stdout: ""
Jul 31 17:03:05.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=services-9922 execpod9clqx -- /bin/sh -x -c nc -zv -t -w 2 10.10.102.94 31974'
Jul 31 17:03:05.343: INFO: stderr: "+ nc -zv -t -w 2 10.10.102.94 31974\nConnection to 10.10.102.94 31974 port [tcp/31974] succeeded!\n"
Jul 31 17:03:05.343: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:03:05.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9922" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:9.852 seconds]
[sig-network] Services
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":280,"completed":120,"skipped":2037,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:03:05.355: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2987
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 17:03:06.086: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 31 17:03:08.093: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731811785, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731811785, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731811785, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731811785, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 17:03:11.121: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:03:11.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2987" for this suite.
STEP: Destroying namespace "webhook-2987-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.905 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":280,"completed":121,"skipped":2040,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:03:11.261: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1182
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 31 17:03:11.417: INFO: Waiting up to 5m0s for pod "downwardapi-volume-21a38c00-c26d-4813-bac4-f3b3a9a72053" in namespace "projected-1182" to be "success or failure"
Jul 31 17:03:11.426: INFO: Pod "downwardapi-volume-21a38c00-c26d-4813-bac4-f3b3a9a72053": Phase="Pending", Reason="", readiness=false. Elapsed: 9.374603ms
Jul 31 17:03:13.430: INFO: Pod "downwardapi-volume-21a38c00-c26d-4813-bac4-f3b3a9a72053": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012615883s
Jul 31 17:03:15.433: INFO: Pod "downwardapi-volume-21a38c00-c26d-4813-bac4-f3b3a9a72053": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016178804s
STEP: Saw pod success
Jul 31 17:03:15.433: INFO: Pod "downwardapi-volume-21a38c00-c26d-4813-bac4-f3b3a9a72053" satisfied condition "success or failure"
Jul 31 17:03:15.436: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod downwardapi-volume-21a38c00-c26d-4813-bac4-f3b3a9a72053 container client-container: <nil>
STEP: delete the pod
Jul 31 17:03:15.463: INFO: Waiting for pod downwardapi-volume-21a38c00-c26d-4813-bac4-f3b3a9a72053 to disappear
Jul 31 17:03:15.467: INFO: Pod downwardapi-volume-21a38c00-c26d-4813-bac4-f3b3a9a72053 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:03:15.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1182" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":122,"skipped":2043,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:03:15.478: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-922
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-922
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating stateful set ss in namespace statefulset-922
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-922
Jul 31 17:03:15.643: INFO: Found 0 stateful pods, waiting for 1
Jul 31 17:03:25.646: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jul 31 17:03:25.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=statefulset-922 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 31 17:03:25.903: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 31 17:03:25.903: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 31 17:03:25.903: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 31 17:03:25.907: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 31 17:03:35.911: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 31 17:03:35.911: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 17:03:35.921: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Jul 31 17:03:35.921: INFO: ss-0  test-aruna-123-node-group-687e2c91ce  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:15 +0000 UTC  }]
Jul 31 17:03:35.921: INFO: 
Jul 31 17:03:35.921: INFO: StatefulSet ss has not reached scale 3, at 1
Jul 31 17:03:36.925: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997743928s
Jul 31 17:03:37.928: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994364768s
Jul 31 17:03:38.932: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.99071171s
Jul 31 17:03:39.936: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986643734s
Jul 31 17:03:40.941: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.982914459s
Jul 31 17:03:41.945: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.978004181s
Jul 31 17:03:42.957: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.973784471s
Jul 31 17:03:43.961: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.962309383s
Jul 31 17:03:44.964: INFO: Verifying statefulset ss doesn't scale past 3 for another 958.487089ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-922
Jul 31 17:03:45.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=statefulset-922 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 31 17:03:46.202: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 31 17:03:46.202: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 31 17:03:46.202: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 31 17:03:46.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=statefulset-922 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 31 17:03:46.398: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 31 17:03:46.399: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 31 17:03:46.399: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 31 17:03:46.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=statefulset-922 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 31 17:03:46.581: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 31 17:03:46.581: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 31 17:03:46.581: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 31 17:03:46.585: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jul 31 17:03:56.589: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 17:03:56.589: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 17:03:56.589: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jul 31 17:03:56.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=statefulset-922 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 31 17:03:56.843: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 31 17:03:56.843: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 31 17:03:56.843: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 31 17:03:56.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=statefulset-922 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 31 17:03:57.041: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 31 17:03:57.041: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 31 17:03:57.041: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 31 17:03:57.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=statefulset-922 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 31 17:03:57.249: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 31 17:03:57.249: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 31 17:03:57.249: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 31 17:03:57.249: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 17:03:57.251: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jul 31 17:04:07.259: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 31 17:04:07.259: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 31 17:04:07.259: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 31 17:04:07.269: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Jul 31 17:04:07.269: INFO: ss-0  test-aruna-123-node-group-687e2c91ce  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:15 +0000 UTC  }]
Jul 31 17:04:07.269: INFO: ss-1  test-aruna-123-node-group-a8f122b701  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:35 +0000 UTC  }]
Jul 31 17:04:07.269: INFO: ss-2  test-aruna-123-node-group-a92c781fbd  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:35 +0000 UTC  }]
Jul 31 17:04:07.269: INFO: 
Jul 31 17:04:07.269: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 31 17:04:08.273: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Jul 31 17:04:08.273: INFO: ss-0  test-aruna-123-node-group-687e2c91ce  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:15 +0000 UTC  }]
Jul 31 17:04:08.273: INFO: ss-1  test-aruna-123-node-group-a8f122b701  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:35 +0000 UTC  }]
Jul 31 17:04:08.273: INFO: ss-2  test-aruna-123-node-group-a92c781fbd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:35 +0000 UTC  }]
Jul 31 17:04:08.273: INFO: 
Jul 31 17:04:08.273: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 31 17:04:09.278: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Jul 31 17:04:09.278: INFO: ss-0  test-aruna-123-node-group-687e2c91ce  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:15 +0000 UTC  }]
Jul 31 17:04:09.278: INFO: ss-1  test-aruna-123-node-group-a8f122b701  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:35 +0000 UTC  }]
Jul 31 17:04:09.278: INFO: ss-2  test-aruna-123-node-group-a92c781fbd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:35 +0000 UTC  }]
Jul 31 17:04:09.278: INFO: 
Jul 31 17:04:09.278: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 31 17:04:10.281: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Jul 31 17:04:10.281: INFO: ss-0  test-aruna-123-node-group-687e2c91ce  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:15 +0000 UTC  }]
Jul 31 17:04:10.281: INFO: ss-1  test-aruna-123-node-group-a8f122b701  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:35 +0000 UTC  }]
Jul 31 17:04:10.281: INFO: ss-2  test-aruna-123-node-group-a92c781fbd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:35 +0000 UTC  }]
Jul 31 17:04:10.281: INFO: 
Jul 31 17:04:10.281: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 31 17:04:11.285: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Jul 31 17:04:11.285: INFO: ss-0  test-aruna-123-node-group-687e2c91ce  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:15 +0000 UTC  }]
Jul 31 17:04:11.285: INFO: ss-1  test-aruna-123-node-group-a8f122b701  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:35 +0000 UTC  }]
Jul 31 17:04:11.285: INFO: ss-2  test-aruna-123-node-group-a92c781fbd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:35 +0000 UTC  }]
Jul 31 17:04:11.285: INFO: 
Jul 31 17:04:11.285: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 31 17:04:12.288: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Jul 31 17:04:12.288: INFO: ss-0  test-aruna-123-node-group-687e2c91ce  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:15 +0000 UTC  }]
Jul 31 17:04:12.289: INFO: ss-1  test-aruna-123-node-group-a8f122b701  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:35 +0000 UTC  }]
Jul 31 17:04:12.289: INFO: 
Jul 31 17:04:12.289: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 31 17:04:13.292: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Jul 31 17:04:13.292: INFO: ss-1  test-aruna-123-node-group-a8f122b701  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:35 +0000 UTC  }]
Jul 31 17:04:13.292: INFO: 
Jul 31 17:04:13.292: INFO: StatefulSet ss has not reached scale 0, at 1
Jul 31 17:04:14.296: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Jul 31 17:04:14.296: INFO: ss-1  test-aruna-123-node-group-a8f122b701  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:35 +0000 UTC  }]
Jul 31 17:04:14.296: INFO: 
Jul 31 17:04:14.296: INFO: StatefulSet ss has not reached scale 0, at 1
Jul 31 17:04:15.300: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Jul 31 17:04:15.300: INFO: ss-1  test-aruna-123-node-group-a8f122b701  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-31 17:03:35 +0000 UTC  }]
Jul 31 17:04:15.300: INFO: 
Jul 31 17:04:15.300: INFO: StatefulSet ss has not reached scale 0, at 1
Jul 31 17:04:16.307: INFO: Verifying statefulset ss doesn't scale past 0 for another 965.304425ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-922
Jul 31 17:04:17.309: INFO: Scaling statefulset ss to 0
Jul 31 17:04:17.314: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jul 31 17:04:17.315: INFO: Deleting all statefulset in ns statefulset-922
Jul 31 17:04:17.316: INFO: Scaling statefulset ss to 0
Jul 31 17:04:17.322: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 17:04:17.323: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:04:17.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-922" for this suite.

• [SLOW TEST:61.862 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":280,"completed":123,"skipped":2060,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:04:17.341: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-144
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 31 17:04:17.567: INFO: Waiting up to 5m0s for pod "pod-7fbaa8c7-3939-4878-a678-0074172f07a7" in namespace "emptydir-144" to be "success or failure"
Jul 31 17:04:17.574: INFO: Pod "pod-7fbaa8c7-3939-4878-a678-0074172f07a7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.976093ms
Jul 31 17:04:19.577: INFO: Pod "pod-7fbaa8c7-3939-4878-a678-0074172f07a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009805893s
Jul 31 17:04:21.580: INFO: Pod "pod-7fbaa8c7-3939-4878-a678-0074172f07a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012888964s
STEP: Saw pod success
Jul 31 17:04:21.581: INFO: Pod "pod-7fbaa8c7-3939-4878-a678-0074172f07a7" satisfied condition "success or failure"
Jul 31 17:04:21.582: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-7fbaa8c7-3939-4878-a678-0074172f07a7 container test-container: <nil>
STEP: delete the pod
Jul 31 17:04:21.603: INFO: Waiting for pod pod-7fbaa8c7-3939-4878-a678-0074172f07a7 to disappear
Jul 31 17:04:21.611: INFO: Pod pod-7fbaa8c7-3939-4878-a678-0074172f07a7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:04:21.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-144" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":124,"skipped":2076,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:04:21.620: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1354
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service multi-endpoint-test in namespace services-1354
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1354 to expose endpoints map[]
Jul 31 17:04:21.786: INFO: successfully validated that service multi-endpoint-test in namespace services-1354 exposes endpoints map[] (7.140761ms elapsed)
STEP: Creating pod pod1 in namespace services-1354
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1354 to expose endpoints map[pod1:[100]]
Jul 31 17:04:24.829: INFO: successfully validated that service multi-endpoint-test in namespace services-1354 exposes endpoints map[pod1:[100]] (3.034967944s elapsed)
STEP: Creating pod pod2 in namespace services-1354
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1354 to expose endpoints map[pod1:[100] pod2:[101]]
Jul 31 17:04:27.867: INFO: successfully validated that service multi-endpoint-test in namespace services-1354 exposes endpoints map[pod1:[100] pod2:[101]] (3.032268006s elapsed)
STEP: Deleting pod pod1 in namespace services-1354
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1354 to expose endpoints map[pod2:[101]]
Jul 31 17:04:28.902: INFO: successfully validated that service multi-endpoint-test in namespace services-1354 exposes endpoints map[pod2:[101]] (1.02839919s elapsed)
STEP: Deleting pod pod2 in namespace services-1354
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1354 to expose endpoints map[]
Jul 31 17:04:28.931: INFO: successfully validated that service multi-endpoint-test in namespace services-1354 exposes endpoints map[] (5.256712ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:04:29.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1354" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:7.448 seconds]
[sig-network] Services
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":280,"completed":125,"skipped":2096,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:04:29.068: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8036
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-7503d06d-01ca-48fe-9f3b-a84e84dab710
STEP: Creating a pod to test consume secrets
Jul 31 17:04:29.277: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3844acc0-4008-4fdd-89d8-8a367a190778" in namespace "projected-8036" to be "success or failure"
Jul 31 17:04:29.290: INFO: Pod "pod-projected-secrets-3844acc0-4008-4fdd-89d8-8a367a190778": Phase="Pending", Reason="", readiness=false. Elapsed: 12.987737ms
Jul 31 17:04:31.293: INFO: Pod "pod-projected-secrets-3844acc0-4008-4fdd-89d8-8a367a190778": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016172266s
Jul 31 17:04:33.297: INFO: Pod "pod-projected-secrets-3844acc0-4008-4fdd-89d8-8a367a190778": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019796742s
STEP: Saw pod success
Jul 31 17:04:33.297: INFO: Pod "pod-projected-secrets-3844acc0-4008-4fdd-89d8-8a367a190778" satisfied condition "success or failure"
Jul 31 17:04:33.298: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-projected-secrets-3844acc0-4008-4fdd-89d8-8a367a190778 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 31 17:04:33.316: INFO: Waiting for pod pod-projected-secrets-3844acc0-4008-4fdd-89d8-8a367a190778 to disappear
Jul 31 17:04:33.321: INFO: Pod pod-projected-secrets-3844acc0-4008-4fdd-89d8-8a367a190778 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:04:33.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8036" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":126,"skipped":2103,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:04:33.337: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1518
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-95acf234-45f3-4fa4-8a50-9263b8c4cb58
STEP: Creating a pod to test consume secrets
Jul 31 17:04:33.507: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-41ce5aa8-c185-4833-a246-d1a9031faaad" in namespace "projected-1518" to be "success or failure"
Jul 31 17:04:33.514: INFO: Pod "pod-projected-secrets-41ce5aa8-c185-4833-a246-d1a9031faaad": Phase="Pending", Reason="", readiness=false. Elapsed: 6.059308ms
Jul 31 17:04:35.518: INFO: Pod "pod-projected-secrets-41ce5aa8-c185-4833-a246-d1a9031faaad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00984791s
Jul 31 17:04:37.521: INFO: Pod "pod-projected-secrets-41ce5aa8-c185-4833-a246-d1a9031faaad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013007967s
STEP: Saw pod success
Jul 31 17:04:37.521: INFO: Pod "pod-projected-secrets-41ce5aa8-c185-4833-a246-d1a9031faaad" satisfied condition "success or failure"
Jul 31 17:04:37.523: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-projected-secrets-41ce5aa8-c185-4833-a246-d1a9031faaad container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 31 17:04:37.540: INFO: Waiting for pod pod-projected-secrets-41ce5aa8-c185-4833-a246-d1a9031faaad to disappear
Jul 31 17:04:37.545: INFO: Pod pod-projected-secrets-41ce5aa8-c185-4833-a246-d1a9031faaad no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:04:37.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1518" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":127,"skipped":2118,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:04:37.557: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1159
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating all guestbook components
Jul 31 17:04:37.717: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Jul 31 17:04:37.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 create -f - --namespace=kubectl-1159'
Jul 31 17:04:37.974: INFO: stderr: ""
Jul 31 17:04:37.974: INFO: stdout: "service/agnhost-slave created\n"
Jul 31 17:04:37.974: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Jul 31 17:04:37.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 create -f - --namespace=kubectl-1159'
Jul 31 17:04:38.218: INFO: stderr: ""
Jul 31 17:04:38.218: INFO: stdout: "service/agnhost-master created\n"
Jul 31 17:04:38.218: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jul 31 17:04:38.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 create -f - --namespace=kubectl-1159'
Jul 31 17:04:38.439: INFO: stderr: ""
Jul 31 17:04:38.439: INFO: stdout: "service/frontend created\n"
Jul 31 17:04:38.439: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jul 31 17:04:38.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 create -f - --namespace=kubectl-1159'
Jul 31 17:04:38.696: INFO: stderr: ""
Jul 31 17:04:38.696: INFO: stdout: "deployment.apps/frontend created\n"
Jul 31 17:04:38.696: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 31 17:04:38.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 create -f - --namespace=kubectl-1159'
Jul 31 17:04:38.908: INFO: stderr: ""
Jul 31 17:04:38.908: INFO: stdout: "deployment.apps/agnhost-master created\n"
Jul 31 17:04:38.908: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 31 17:04:38.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 create -f - --namespace=kubectl-1159'
Jul 31 17:04:39.117: INFO: stderr: ""
Jul 31 17:04:39.117: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Jul 31 17:04:39.117: INFO: Waiting for all frontend pods to be Running.
Jul 31 17:04:44.167: INFO: Waiting for frontend to serve content.
Jul 31 17:04:44.181: INFO: Trying to add a new entry to the guestbook.
Jul 31 17:04:44.192: INFO: Verifying that added entry can be retrieved.
Jul 31 17:04:44.201: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Jul 31 17:04:49.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 delete --grace-period=0 --force -f - --namespace=kubectl-1159'
Jul 31 17:04:50.535: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 31 17:04:50.535: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Jul 31 17:04:50.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 delete --grace-period=0 --force -f - --namespace=kubectl-1159'
Jul 31 17:04:50.645: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 31 17:04:50.645: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Jul 31 17:04:50.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 delete --grace-period=0 --force -f - --namespace=kubectl-1159'
Jul 31 17:04:50.789: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 31 17:04:50.789: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 31 17:04:50.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 delete --grace-period=0 --force -f - --namespace=kubectl-1159'
Jul 31 17:04:50.903: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 31 17:04:50.903: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 31 17:04:50.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 delete --grace-period=0 --force -f - --namespace=kubectl-1159'
Jul 31 17:04:50.982: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 31 17:04:50.982: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Jul 31 17:04:50.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 delete --grace-period=0 --force -f - --namespace=kubectl-1159'
Jul 31 17:04:51.058: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 31 17:04:51.058: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:04:51.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1159" for this suite.

• [SLOW TEST:13.512 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:380
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":280,"completed":128,"skipped":2136,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:04:51.069: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-2252
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jul 31 17:04:55.737: INFO: Successfully updated pod "adopt-release-nvpgt"
STEP: Checking that the Job readopts the Pod
Jul 31 17:04:55.737: INFO: Waiting up to 15m0s for pod "adopt-release-nvpgt" in namespace "job-2252" to be "adopted"
Jul 31 17:04:55.742: INFO: Pod "adopt-release-nvpgt": Phase="Running", Reason="", readiness=true. Elapsed: 5.013206ms
Jul 31 17:04:57.746: INFO: Pod "adopt-release-nvpgt": Phase="Running", Reason="", readiness=true. Elapsed: 2.008884721s
Jul 31 17:04:57.746: INFO: Pod "adopt-release-nvpgt" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jul 31 17:04:58.254: INFO: Successfully updated pod "adopt-release-nvpgt"
STEP: Checking that the Job releases the Pod
Jul 31 17:04:58.254: INFO: Waiting up to 15m0s for pod "adopt-release-nvpgt" in namespace "job-2252" to be "released"
Jul 31 17:04:58.269: INFO: Pod "adopt-release-nvpgt": Phase="Running", Reason="", readiness=true. Elapsed: 14.754865ms
Jul 31 17:04:58.269: INFO: Pod "adopt-release-nvpgt" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:04:58.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2252" for this suite.

• [SLOW TEST:7.251 seconds]
[sig-apps] Job
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":280,"completed":129,"skipped":2149,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:04:58.321: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8336
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating api versions
Jul 31 17:04:58.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 api-versions'
Jul 31 17:04:58.600: INFO: stderr: ""
Jul 31 17:04:58.600: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncertmanager.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nhelm.ccp.cisco.com/v1alpha1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:04:58.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8336" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":280,"completed":130,"skipped":2167,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:04:58.621: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4597
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-c48cd618-ba75-41d0-b61a-771c69462c4e
STEP: Creating secret with name s-test-opt-upd-c7eb1840-e60e-4a37-b0d4-dc9fd81f8229
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-c48cd618-ba75-41d0-b61a-771c69462c4e
STEP: Updating secret s-test-opt-upd-c7eb1840-e60e-4a37-b0d4-dc9fd81f8229
STEP: Creating secret with name s-test-opt-create-7ba6f3e0-84a0-40ac-b440-c1c0a679b4a8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:05:06.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4597" for this suite.

• [SLOW TEST:8.375 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":131,"skipped":2192,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:05:06.999: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4326
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1626
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 31 17:05:07.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-4326'
Jul 31 17:05:07.225: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 31 17:05:07.225: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1631
Jul 31 17:05:11.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 delete deployment e2e-test-httpd-deployment --namespace=kubectl-4326'
Jul 31 17:05:11.308: INFO: stderr: ""
Jul 31 17:05:11.308: INFO: stdout: "deployment.extensions \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:05:11.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4326" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run deployment should create a deployment from an image  [Conformance]","total":280,"completed":132,"skipped":2213,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:05:11.326: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2897
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1525
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 31 17:05:11.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-2897'
Jul 31 17:05:11.559: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 31 17:05:11.559: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Jul 31 17:05:11.588: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-tx6z6]
Jul 31 17:05:11.588: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-tx6z6" in namespace "kubectl-2897" to be "running and ready"
Jul 31 17:05:11.601: INFO: Pod "e2e-test-httpd-rc-tx6z6": Phase="Pending", Reason="", readiness=false. Elapsed: 13.090836ms
Jul 31 17:05:13.605: INFO: Pod "e2e-test-httpd-rc-tx6z6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016343482s
Jul 31 17:05:15.609: INFO: Pod "e2e-test-httpd-rc-tx6z6": Phase="Running", Reason="", readiness=true. Elapsed: 4.021197951s
Jul 31 17:05:15.609: INFO: Pod "e2e-test-httpd-rc-tx6z6" satisfied condition "running and ready"
Jul 31 17:05:15.609: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-tx6z6]
Jul 31 17:05:15.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 logs rc/e2e-test-httpd-rc --namespace=kubectl-2897'
Jul 31 17:05:15.714: INFO: stderr: ""
Jul 31 17:05:15.714: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 192.168.55.103. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 192.168.55.103. Set the 'ServerName' directive globally to suppress this message\n[Fri Jul 31 17:05:13.178495 2020] [mpm_event:notice] [pid 1:tid 140328938109800] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Fri Jul 31 17:05:13.178560 2020] [core:notice] [pid 1:tid 140328938109800] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1530
Jul 31 17:05:15.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 delete rc e2e-test-httpd-rc --namespace=kubectl-2897'
Jul 31 17:05:15.792: INFO: stderr: ""
Jul 31 17:05:15.792: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:05:15.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2897" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run rc should create an rc from an image  [Conformance]","total":280,"completed":133,"skipped":2223,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:05:15.824: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-857
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-1f861f43-17f7-476b-8be0-fe259d8deeee
STEP: Creating a pod to test consume configMaps
Jul 31 17:05:16.068: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-83085b4b-6081-41eb-90e1-fd1e8053a72c" in namespace "projected-857" to be "success or failure"
Jul 31 17:05:16.082: INFO: Pod "pod-projected-configmaps-83085b4b-6081-41eb-90e1-fd1e8053a72c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.304138ms
Jul 31 17:05:18.090: INFO: Pod "pod-projected-configmaps-83085b4b-6081-41eb-90e1-fd1e8053a72c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021975802s
Jul 31 17:05:20.094: INFO: Pod "pod-projected-configmaps-83085b4b-6081-41eb-90e1-fd1e8053a72c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025495696s
STEP: Saw pod success
Jul 31 17:05:20.094: INFO: Pod "pod-projected-configmaps-83085b4b-6081-41eb-90e1-fd1e8053a72c" satisfied condition "success or failure"
Jul 31 17:05:20.096: INFO: Trying to get logs from node test-aruna-123-node-group-a8f122b701 pod pod-projected-configmaps-83085b4b-6081-41eb-90e1-fd1e8053a72c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 17:05:20.123: INFO: Waiting for pod pod-projected-configmaps-83085b4b-6081-41eb-90e1-fd1e8053a72c to disappear
Jul 31 17:05:20.129: INFO: Pod pod-projected-configmaps-83085b4b-6081-41eb-90e1-fd1e8053a72c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:05:20.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-857" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":134,"skipped":2232,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:05:20.147: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9311
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-c00dc5a2-c2aa-4621-a905-93a9e48114c9
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-c00dc5a2-c2aa-4621-a905-93a9e48114c9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:06:46.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9311" for this suite.

• [SLOW TEST:86.593 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":135,"skipped":2237,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:06:46.741: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-9392
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jul 31 17:06:54.980: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9392 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 17:06:54.980: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
Jul 31 17:06:55.136: INFO: Exec stderr: ""
Jul 31 17:06:55.136: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9392 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 17:06:55.136: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
Jul 31 17:06:55.353: INFO: Exec stderr: ""
Jul 31 17:06:55.353: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9392 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 17:06:55.353: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
Jul 31 17:06:55.530: INFO: Exec stderr: ""
Jul 31 17:06:55.530: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9392 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 17:06:55.530: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
Jul 31 17:06:55.745: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jul 31 17:06:55.745: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9392 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 17:06:55.745: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
Jul 31 17:06:55.953: INFO: Exec stderr: ""
Jul 31 17:06:55.953: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9392 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 17:06:55.954: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
Jul 31 17:06:56.158: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jul 31 17:06:56.158: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9392 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 17:06:56.158: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
Jul 31 17:06:56.311: INFO: Exec stderr: ""
Jul 31 17:06:56.311: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9392 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 17:06:56.311: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
Jul 31 17:06:56.498: INFO: Exec stderr: ""
Jul 31 17:06:56.498: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9392 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 17:06:56.498: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
Jul 31 17:06:56.682: INFO: Exec stderr: ""
Jul 31 17:06:56.682: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9392 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 17:06:56.683: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
Jul 31 17:06:56.890: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:06:56.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9392" for this suite.

• [SLOW TEST:10.157 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":136,"skipped":2255,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:06:56.898: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-72
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-116106e1-5d8c-438b-80d4-40f3334725f6
STEP: Creating a pod to test consume configMaps
Jul 31 17:06:57.058: INFO: Waiting up to 5m0s for pod "pod-configmaps-a1e95a04-7d58-4d07-8d7a-adb2d9a9a75b" in namespace "configmap-72" to be "success or failure"
Jul 31 17:06:57.072: INFO: Pod "pod-configmaps-a1e95a04-7d58-4d07-8d7a-adb2d9a9a75b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.971447ms
Jul 31 17:06:59.077: INFO: Pod "pod-configmaps-a1e95a04-7d58-4d07-8d7a-adb2d9a9a75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018867346s
Jul 31 17:07:01.080: INFO: Pod "pod-configmaps-a1e95a04-7d58-4d07-8d7a-adb2d9a9a75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022380333s
STEP: Saw pod success
Jul 31 17:07:01.080: INFO: Pod "pod-configmaps-a1e95a04-7d58-4d07-8d7a-adb2d9a9a75b" satisfied condition "success or failure"
Jul 31 17:07:01.083: INFO: Trying to get logs from node test-aruna-123-node-group-a92c781fbd pod pod-configmaps-a1e95a04-7d58-4d07-8d7a-adb2d9a9a75b container configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 17:07:01.102: INFO: Waiting for pod pod-configmaps-a1e95a04-7d58-4d07-8d7a-adb2d9a9a75b to disappear
Jul 31 17:07:01.110: INFO: Pod pod-configmaps-a1e95a04-7d58-4d07-8d7a-adb2d9a9a75b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:07:01.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-72" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":137,"skipped":2265,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:07:01.119: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8269
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 31 17:07:01.321: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c341ab57-850e-43c7-be29-09c0ea44005a" in namespace "projected-8269" to be "success or failure"
Jul 31 17:07:01.330: INFO: Pod "downwardapi-volume-c341ab57-850e-43c7-be29-09c0ea44005a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.932507ms
Jul 31 17:07:03.334: INFO: Pod "downwardapi-volume-c341ab57-850e-43c7-be29-09c0ea44005a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012087902s
Jul 31 17:07:05.337: INFO: Pod "downwardapi-volume-c341ab57-850e-43c7-be29-09c0ea44005a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015510595s
STEP: Saw pod success
Jul 31 17:07:05.337: INFO: Pod "downwardapi-volume-c341ab57-850e-43c7-be29-09c0ea44005a" satisfied condition "success or failure"
Jul 31 17:07:05.339: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod downwardapi-volume-c341ab57-850e-43c7-be29-09c0ea44005a container client-container: <nil>
STEP: delete the pod
Jul 31 17:07:05.368: INFO: Waiting for pod downwardapi-volume-c341ab57-850e-43c7-be29-09c0ea44005a to disappear
Jul 31 17:07:05.373: INFO: Pod downwardapi-volume-c341ab57-850e-43c7-be29-09c0ea44005a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:07:05.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8269" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":138,"skipped":2269,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:07:05.386: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7544
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jul 31 17:07:10.085: INFO: Successfully updated pod "annotationupdate03fa3aaf-23c1-4305-a1a8-72f87f4a06b4"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:07:12.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7544" for this suite.

• [SLOW TEST:6.733 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":139,"skipped":2300,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:07:12.121: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7817
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-fvqmg in namespace proxy-7817
I0731 17:07:12.310809      21 runners.go:189] Created replication controller with name: proxy-service-fvqmg, namespace: proxy-7817, replica count: 1
I0731 17:07:13.361940      21 runners.go:189] proxy-service-fvqmg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0731 17:07:14.362137      21 runners.go:189] proxy-service-fvqmg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0731 17:07:15.362453      21 runners.go:189] proxy-service-fvqmg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0731 17:07:16.362715      21 runners.go:189] proxy-service-fvqmg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0731 17:07:17.364547      21 runners.go:189] proxy-service-fvqmg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0731 17:07:18.364900      21 runners.go:189] proxy-service-fvqmg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0731 17:07:19.365089      21 runners.go:189] proxy-service-fvqmg Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 31 17:07:19.370: INFO: setup took 7.094501998s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jul 31 17:07:19.399: INFO: (0) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/rewriteme">test</a> (200; 28.678839ms)
Jul 31 17:07:19.399: INFO: (0) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">... (200; 28.873988ms)
Jul 31 17:07:19.410: INFO: (0) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:460/proxy/: tls baz (200; 39.826688ms)
Jul 31 17:07:19.411: INFO: (0) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 39.899665ms)
Jul 31 17:07:19.411: INFO: (0) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:462/proxy/: tls qux (200; 40.794149ms)
Jul 31 17:07:19.411: INFO: (0) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 40.363208ms)
Jul 31 17:07:19.411: INFO: (0) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">test<... (200; 40.295039ms)
Jul 31 17:07:19.415: INFO: (0) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/tlsrewritem... (200; 44.239261ms)
Jul 31 17:07:19.415: INFO: (0) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 44.724054ms)
Jul 31 17:07:19.416: INFO: (0) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 45.710767ms)
Jul 31 17:07:19.417: INFO: (0) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname2/proxy/: bar (200; 46.487842ms)
Jul 31 17:07:19.418: INFO: (0) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname2/proxy/: bar (200; 46.900856ms)
Jul 31 17:07:19.418: INFO: (0) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname1/proxy/: tls baz (200; 47.129241ms)
Jul 31 17:07:19.418: INFO: (0) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname1/proxy/: foo (200; 47.037905ms)
Jul 31 17:07:19.418: INFO: (0) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname2/proxy/: tls qux (200; 47.253019ms)
Jul 31 17:07:19.418: INFO: (0) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname1/proxy/: foo (200; 47.61997ms)
Jul 31 17:07:19.425: INFO: (1) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/rewriteme">test</a> (200; 6.102222ms)
Jul 31 17:07:19.428: INFO: (1) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">... (200; 9.515672ms)
Jul 31 17:07:19.428: INFO: (1) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 9.563166ms)
Jul 31 17:07:19.429: INFO: (1) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 10.010468ms)
Jul 31 17:07:19.429: INFO: (1) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">test<... (200; 10.234078ms)
Jul 31 17:07:19.429: INFO: (1) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:462/proxy/: tls qux (200; 10.589884ms)
Jul 31 17:07:19.429: INFO: (1) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:460/proxy/: tls baz (200; 10.595389ms)
Jul 31 17:07:19.429: INFO: (1) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname2/proxy/: tls qux (200; 10.726298ms)
Jul 31 17:07:19.429: INFO: (1) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 10.68617ms)
Jul 31 17:07:19.429: INFO: (1) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/tlsrewritem... (200; 10.746859ms)
Jul 31 17:07:19.430: INFO: (1) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 10.919318ms)
Jul 31 17:07:19.430: INFO: (1) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname2/proxy/: bar (200; 11.038447ms)
Jul 31 17:07:19.431: INFO: (1) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname1/proxy/: foo (200; 12.265282ms)
Jul 31 17:07:19.431: INFO: (1) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname1/proxy/: tls baz (200; 12.496437ms)
Jul 31 17:07:19.432: INFO: (1) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname2/proxy/: bar (200; 13.034005ms)
Jul 31 17:07:19.432: INFO: (1) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname1/proxy/: foo (200; 13.380081ms)
Jul 31 17:07:19.442: INFO: (2) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">... (200; 9.205712ms)
Jul 31 17:07:19.442: INFO: (2) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/tlsrewritem... (200; 9.776728ms)
Jul 31 17:07:19.442: INFO: (2) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">test<... (200; 10.47839ms)
Jul 31 17:07:19.443: INFO: (2) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 10.155958ms)
Jul 31 17:07:19.443: INFO: (2) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 9.99927ms)
Jul 31 17:07:19.443: INFO: (2) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname1/proxy/: foo (200; 10.299709ms)
Jul 31 17:07:19.443: INFO: (2) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 10.239795ms)
Jul 31 17:07:19.443: INFO: (2) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/rewriteme">test</a> (200; 10.979107ms)
Jul 31 17:07:19.444: INFO: (2) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname1/proxy/: foo (200; 11.544232ms)
Jul 31 17:07:19.444: INFO: (2) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 11.642997ms)
Jul 31 17:07:19.444: INFO: (2) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:462/proxy/: tls qux (200; 11.878141ms)
Jul 31 17:07:19.444: INFO: (2) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname1/proxy/: tls baz (200; 11.64578ms)
Jul 31 17:07:19.444: INFO: (2) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:460/proxy/: tls baz (200; 11.5272ms)
Jul 31 17:07:19.445: INFO: (2) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname2/proxy/: tls qux (200; 11.684302ms)
Jul 31 17:07:19.445: INFO: (2) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname2/proxy/: bar (200; 12.420411ms)
Jul 31 17:07:19.446: INFO: (2) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname2/proxy/: bar (200; 14.069828ms)
Jul 31 17:07:19.454: INFO: (3) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 7.771712ms)
Jul 31 17:07:19.454: INFO: (3) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 8.010906ms)
Jul 31 17:07:19.457: INFO: (3) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname1/proxy/: foo (200; 10.196863ms)
Jul 31 17:07:19.457: INFO: (3) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">... (200; 10.289674ms)
Jul 31 17:07:19.457: INFO: (3) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname1/proxy/: tls baz (200; 10.359266ms)
Jul 31 17:07:19.457: INFO: (3) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname2/proxy/: tls qux (200; 10.694624ms)
Jul 31 17:07:19.457: INFO: (3) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname2/proxy/: bar (200; 10.815258ms)
Jul 31 17:07:19.458: INFO: (3) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname2/proxy/: bar (200; 11.693699ms)
Jul 31 17:07:19.459: INFO: (3) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname1/proxy/: foo (200; 12.220167ms)
Jul 31 17:07:19.459: INFO: (3) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:460/proxy/: tls baz (200; 12.592794ms)
Jul 31 17:07:19.459: INFO: (3) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/tlsrewritem... (200; 12.56989ms)
Jul 31 17:07:19.460: INFO: (3) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">test<... (200; 12.668ms)
Jul 31 17:07:19.460: INFO: (3) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 12.80532ms)
Jul 31 17:07:19.460: INFO: (3) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 12.875315ms)
Jul 31 17:07:19.460: INFO: (3) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:462/proxy/: tls qux (200; 13.145063ms)
Jul 31 17:07:19.460: INFO: (3) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/rewriteme">test</a> (200; 13.191334ms)
Jul 31 17:07:19.466: INFO: (4) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/rewriteme">test</a> (200; 5.628635ms)
Jul 31 17:07:19.468: INFO: (4) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname1/proxy/: foo (200; 7.469726ms)
Jul 31 17:07:19.468: INFO: (4) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:460/proxy/: tls baz (200; 7.160686ms)
Jul 31 17:07:19.468: INFO: (4) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/tlsrewritem... (200; 7.391909ms)
Jul 31 17:07:19.469: INFO: (4) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:462/proxy/: tls qux (200; 8.687124ms)
Jul 31 17:07:19.470: INFO: (4) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">test<... (200; 9.768293ms)
Jul 31 17:07:19.471: INFO: (4) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 10.16893ms)
Jul 31 17:07:19.471: INFO: (4) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">... (200; 10.62406ms)
Jul 31 17:07:19.471: INFO: (4) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname2/proxy/: bar (200; 10.184784ms)
Jul 31 17:07:19.471: INFO: (4) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname1/proxy/: foo (200; 10.301494ms)
Jul 31 17:07:19.471: INFO: (4) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 10.687881ms)
Jul 31 17:07:19.471: INFO: (4) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 10.66121ms)
Jul 31 17:07:19.472: INFO: (4) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname1/proxy/: tls baz (200; 11.167682ms)
Jul 31 17:07:19.472: INFO: (4) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 11.432666ms)
Jul 31 17:07:19.473: INFO: (4) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname2/proxy/: tls qux (200; 12.087549ms)
Jul 31 17:07:19.473: INFO: (4) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname2/proxy/: bar (200; 12.060122ms)
Jul 31 17:07:19.479: INFO: (5) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/tlsrewritem... (200; 6.168418ms)
Jul 31 17:07:19.482: INFO: (5) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 8.649305ms)
Jul 31 17:07:19.482: INFO: (5) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 8.742146ms)
Jul 31 17:07:19.482: INFO: (5) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">... (200; 9.038197ms)
Jul 31 17:07:19.483: INFO: (5) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/rewriteme">test</a> (200; 9.249801ms)
Jul 31 17:07:19.485: INFO: (5) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname2/proxy/: tls qux (200; 11.979449ms)
Jul 31 17:07:19.490: INFO: (5) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname2/proxy/: bar (200; 17.151267ms)
Jul 31 17:07:19.490: INFO: (5) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname1/proxy/: foo (200; 17.252958ms)
Jul 31 17:07:19.491: INFO: (5) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname1/proxy/: foo (200; 17.105ms)
Jul 31 17:07:19.491: INFO: (5) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">test<... (200; 17.938225ms)
Jul 31 17:07:19.491: INFO: (5) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:462/proxy/: tls qux (200; 18.022569ms)
Jul 31 17:07:19.491: INFO: (5) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname1/proxy/: tls baz (200; 17.86062ms)
Jul 31 17:07:19.491: INFO: (5) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:460/proxy/: tls baz (200; 18.398566ms)
Jul 31 17:07:19.492: INFO: (5) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 18.473359ms)
Jul 31 17:07:19.492: INFO: (5) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 18.612706ms)
Jul 31 17:07:19.492: INFO: (5) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname2/proxy/: bar (200; 18.528967ms)
Jul 31 17:07:19.505: INFO: (6) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname2/proxy/: bar (200; 11.872532ms)
Jul 31 17:07:19.505: INFO: (6) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 12.111414ms)
Jul 31 17:07:19.505: INFO: (6) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname2/proxy/: tls qux (200; 12.396102ms)
Jul 31 17:07:19.505: INFO: (6) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">... (200; 12.075851ms)
Jul 31 17:07:19.505: INFO: (6) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:460/proxy/: tls baz (200; 12.577177ms)
Jul 31 17:07:19.505: INFO: (6) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname1/proxy/: foo (200; 13.132541ms)
Jul 31 17:07:19.505: INFO: (6) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 13.312557ms)
Jul 31 17:07:19.506: INFO: (6) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/tlsrewritem... (200; 13.94497ms)
Jul 31 17:07:19.507: INFO: (6) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">test<... (200; 14.226358ms)
Jul 31 17:07:19.507: INFO: (6) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 14.573046ms)
Jul 31 17:07:19.507: INFO: (6) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname1/proxy/: foo (200; 14.521618ms)
Jul 31 17:07:19.507: INFO: (6) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 15.216642ms)
Jul 31 17:07:19.507: INFO: (6) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/rewriteme">test</a> (200; 14.544677ms)
Jul 31 17:07:19.508: INFO: (6) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:462/proxy/: tls qux (200; 14.612878ms)
Jul 31 17:07:19.509: INFO: (6) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname2/proxy/: bar (200; 15.773507ms)
Jul 31 17:07:19.509: INFO: (6) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname1/proxy/: tls baz (200; 16.824683ms)
Jul 31 17:07:19.520: INFO: (7) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 10.132796ms)
Jul 31 17:07:19.520: INFO: (7) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 10.596636ms)
Jul 31 17:07:19.520: INFO: (7) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 10.773769ms)
Jul 31 17:07:19.521: INFO: (7) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:462/proxy/: tls qux (200; 10.915647ms)
Jul 31 17:07:19.524: INFO: (7) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:460/proxy/: tls baz (200; 14.03317ms)
Jul 31 17:07:19.524: INFO: (7) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/rewriteme">test</a> (200; 14.434858ms)
Jul 31 17:07:19.524: INFO: (7) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/tlsrewritem... (200; 14.717923ms)
Jul 31 17:07:19.524: INFO: (7) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname2/proxy/: bar (200; 14.944809ms)
Jul 31 17:07:19.525: INFO: (7) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">... (200; 14.835277ms)
Jul 31 17:07:19.525: INFO: (7) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">test<... (200; 15.011121ms)
Jul 31 17:07:19.525: INFO: (7) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname2/proxy/: tls qux (200; 15.294916ms)
Jul 31 17:07:19.525: INFO: (7) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 15.082669ms)
Jul 31 17:07:19.527: INFO: (7) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname1/proxy/: foo (200; 17.089055ms)
Jul 31 17:07:19.527: INFO: (7) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname1/proxy/: foo (200; 17.205336ms)
Jul 31 17:07:19.529: INFO: (7) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname2/proxy/: bar (200; 19.173256ms)
Jul 31 17:07:19.529: INFO: (7) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname1/proxy/: tls baz (200; 19.719805ms)
Jul 31 17:07:19.541: INFO: (8) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:462/proxy/: tls qux (200; 10.444146ms)
Jul 31 17:07:19.541: INFO: (8) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">... (200; 10.68399ms)
Jul 31 17:07:19.541: INFO: (8) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname1/proxy/: tls baz (200; 10.977393ms)
Jul 31 17:07:19.541: INFO: (8) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 11.146963ms)
Jul 31 17:07:19.542: INFO: (8) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 11.135897ms)
Jul 31 17:07:19.542: INFO: (8) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/rewriteme">test</a> (200; 11.918354ms)
Jul 31 17:07:19.542: INFO: (8) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">test<... (200; 12.407381ms)
Jul 31 17:07:19.544: INFO: (8) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/tlsrewritem... (200; 14.889366ms)
Jul 31 17:07:19.545: INFO: (8) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:460/proxy/: tls baz (200; 15.494528ms)
Jul 31 17:07:19.546: INFO: (8) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 15.822462ms)
Jul 31 17:07:19.547: INFO: (8) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname2/proxy/: bar (200; 17.741712ms)
Jul 31 17:07:19.548: INFO: (8) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname2/proxy/: tls qux (200; 17.876353ms)
Jul 31 17:07:19.549: INFO: (8) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname1/proxy/: foo (200; 18.238047ms)
Jul 31 17:07:19.549: INFO: (8) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 19.565878ms)
Jul 31 17:07:19.550: INFO: (8) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname1/proxy/: foo (200; 19.439127ms)
Jul 31 17:07:19.550: INFO: (8) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname2/proxy/: bar (200; 19.818071ms)
Jul 31 17:07:19.562: INFO: (9) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:460/proxy/: tls baz (200; 11.792663ms)
Jul 31 17:07:19.562: INFO: (9) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname2/proxy/: tls qux (200; 11.288756ms)
Jul 31 17:07:19.562: INFO: (9) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname1/proxy/: foo (200; 11.644893ms)
Jul 31 17:07:19.562: INFO: (9) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname2/proxy/: bar (200; 12.188792ms)
Jul 31 17:07:19.562: INFO: (9) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname1/proxy/: tls baz (200; 11.872399ms)
Jul 31 17:07:19.564: INFO: (9) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname2/proxy/: bar (200; 13.604308ms)
Jul 31 17:07:19.564: INFO: (9) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 13.366251ms)
Jul 31 17:07:19.565: INFO: (9) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname1/proxy/: foo (200; 14.414158ms)
Jul 31 17:07:19.565: INFO: (9) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 14.027662ms)
Jul 31 17:07:19.568: INFO: (9) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">test<... (200; 17.689603ms)
Jul 31 17:07:19.569: INFO: (9) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 18.358693ms)
Jul 31 17:07:19.569: INFO: (9) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">... (200; 18.553921ms)
Jul 31 17:07:19.569: INFO: (9) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/tlsrewritem... (200; 18.413042ms)
Jul 31 17:07:19.569: INFO: (9) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 18.73131ms)
Jul 31 17:07:19.569: INFO: (9) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:462/proxy/: tls qux (200; 18.984976ms)
Jul 31 17:07:19.569: INFO: (9) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/rewriteme">test</a> (200; 19.0633ms)
Jul 31 17:07:19.580: INFO: (10) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">test<... (200; 10.160287ms)
Jul 31 17:07:19.581: INFO: (10) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 11.361379ms)
Jul 31 17:07:19.581: INFO: (10) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 11.671086ms)
Jul 31 17:07:19.581: INFO: (10) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/tlsrewritem... (200; 11.895292ms)
Jul 31 17:07:19.582: INFO: (10) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:462/proxy/: tls qux (200; 11.899383ms)
Jul 31 17:07:19.582: INFO: (10) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/rewriteme">test</a> (200; 11.983219ms)
Jul 31 17:07:19.582: INFO: (10) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 11.99073ms)
Jul 31 17:07:19.582: INFO: (10) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">... (200; 12.259849ms)
Jul 31 17:07:19.582: INFO: (10) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:460/proxy/: tls baz (200; 12.406944ms)
Jul 31 17:07:19.582: INFO: (10) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 12.513697ms)
Jul 31 17:07:19.582: INFO: (10) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname2/proxy/: bar (200; 12.82853ms)
Jul 31 17:07:19.583: INFO: (10) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname2/proxy/: bar (200; 13.142698ms)
Jul 31 17:07:19.583: INFO: (10) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname1/proxy/: foo (200; 13.443307ms)
Jul 31 17:07:19.585: INFO: (10) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname2/proxy/: tls qux (200; 15.5207ms)
Jul 31 17:07:19.586: INFO: (10) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname1/proxy/: foo (200; 15.775173ms)
Jul 31 17:07:19.586: INFO: (10) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname1/proxy/: tls baz (200; 15.836814ms)
Jul 31 17:07:19.596: INFO: (11) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/tlsrewritem... (200; 9.87578ms)
Jul 31 17:07:19.596: INFO: (11) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 10.381741ms)
Jul 31 17:07:19.596: INFO: (11) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">test<... (200; 10.161484ms)
Jul 31 17:07:19.597: INFO: (11) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 11.123094ms)
Jul 31 17:07:19.598: INFO: (11) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/rewriteme">test</a> (200; 11.395091ms)
Jul 31 17:07:19.598: INFO: (11) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">... (200; 11.407907ms)
Jul 31 17:07:19.598: INFO: (11) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 12.165565ms)
Jul 31 17:07:19.598: INFO: (11) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 12.219628ms)
Jul 31 17:07:19.598: INFO: (11) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:462/proxy/: tls qux (200; 12.060185ms)
Jul 31 17:07:19.598: INFO: (11) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname2/proxy/: tls qux (200; 12.10766ms)
Jul 31 17:07:19.599: INFO: (11) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname2/proxy/: bar (200; 12.474053ms)
Jul 31 17:07:19.599: INFO: (11) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname1/proxy/: tls baz (200; 12.894826ms)
Jul 31 17:07:19.599: INFO: (11) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:460/proxy/: tls baz (200; 12.845613ms)
Jul 31 17:07:19.599: INFO: (11) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname1/proxy/: foo (200; 13.029391ms)
Jul 31 17:07:19.599: INFO: (11) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname1/proxy/: foo (200; 13.589604ms)
Jul 31 17:07:19.600: INFO: (11) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname2/proxy/: bar (200; 13.702731ms)
Jul 31 17:07:19.610: INFO: (12) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:460/proxy/: tls baz (200; 8.816855ms)
Jul 31 17:07:19.610: INFO: (12) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname2/proxy/: tls qux (200; 9.346231ms)
Jul 31 17:07:19.610: INFO: (12) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 9.531284ms)
Jul 31 17:07:19.610: INFO: (12) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname2/proxy/: bar (200; 10.224025ms)
Jul 31 17:07:19.611: INFO: (12) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">... (200; 10.548298ms)
Jul 31 17:07:19.613: INFO: (12) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/tlsrewritem... (200; 12.466796ms)
Jul 31 17:07:19.613: INFO: (12) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 12.100985ms)
Jul 31 17:07:19.613: INFO: (12) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 12.425751ms)
Jul 31 17:07:19.613: INFO: (12) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:462/proxy/: tls qux (200; 13.207778ms)
Jul 31 17:07:19.614: INFO: (12) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 13.684182ms)
Jul 31 17:07:19.614: INFO: (12) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/rewriteme">test</a> (200; 13.036218ms)
Jul 31 17:07:19.614: INFO: (12) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">test<... (200; 13.811412ms)
Jul 31 17:07:19.626: INFO: (12) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname1/proxy/: foo (200; 25.322132ms)
Jul 31 17:07:19.626: INFO: (12) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname1/proxy/: tls baz (200; 25.012445ms)
Jul 31 17:07:19.626: INFO: (12) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname2/proxy/: bar (200; 25.304239ms)
Jul 31 17:07:19.626: INFO: (12) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname1/proxy/: foo (200; 25.390727ms)
Jul 31 17:07:19.633: INFO: (13) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:462/proxy/: tls qux (200; 6.305586ms)
Jul 31 17:07:19.633: INFO: (13) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">test<... (200; 6.538392ms)
Jul 31 17:07:19.633: INFO: (13) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 6.607491ms)
Jul 31 17:07:19.633: INFO: (13) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/rewriteme">test</a> (200; 6.824377ms)
Jul 31 17:07:19.635: INFO: (13) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 8.520819ms)
Jul 31 17:07:19.635: INFO: (13) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 8.482795ms)
Jul 31 17:07:19.637: INFO: (13) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">... (200; 10.236274ms)
Jul 31 17:07:19.637: INFO: (13) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 10.538876ms)
Jul 31 17:07:19.638: INFO: (13) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:460/proxy/: tls baz (200; 11.144893ms)
Jul 31 17:07:19.639: INFO: (13) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/tlsrewritem... (200; 12.344829ms)
Jul 31 17:07:19.645: INFO: (13) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname1/proxy/: tls baz (200; 18.302635ms)
Jul 31 17:07:19.645: INFO: (13) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname2/proxy/: bar (200; 18.500885ms)
Jul 31 17:07:19.645: INFO: (13) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname2/proxy/: bar (200; 18.518972ms)
Jul 31 17:07:19.646: INFO: (13) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname2/proxy/: tls qux (200; 19.351833ms)
Jul 31 17:07:19.646: INFO: (13) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname1/proxy/: foo (200; 19.182954ms)
Jul 31 17:07:19.646: INFO: (13) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname1/proxy/: foo (200; 19.262708ms)
Jul 31 17:07:19.650: INFO: (14) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 4.351031ms)
Jul 31 17:07:19.654: INFO: (14) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 7.943109ms)
Jul 31 17:07:19.654: INFO: (14) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/rewriteme">test</a> (200; 8.040333ms)
Jul 31 17:07:19.655: INFO: (14) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/tlsrewritem... (200; 8.279442ms)
Jul 31 17:07:19.657: INFO: (14) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:460/proxy/: tls baz (200; 10.95803ms)
Jul 31 17:07:19.659: INFO: (14) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 13.068577ms)
Jul 31 17:07:19.659: INFO: (14) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 12.973106ms)
Jul 31 17:07:19.660: INFO: (14) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:462/proxy/: tls qux (200; 13.581708ms)
Jul 31 17:07:19.660: INFO: (14) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">test<... (200; 13.601848ms)
Jul 31 17:07:19.660: INFO: (14) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname1/proxy/: tls baz (200; 14.213601ms)
Jul 31 17:07:19.661: INFO: (14) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname2/proxy/: bar (200; 14.546509ms)
Jul 31 17:07:19.667: INFO: (14) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">... (200; 20.926101ms)
Jul 31 17:07:19.667: INFO: (14) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname2/proxy/: bar (200; 21.048061ms)
Jul 31 17:07:19.667: INFO: (14) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname1/proxy/: foo (200; 20.995235ms)
Jul 31 17:07:19.667: INFO: (14) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname2/proxy/: tls qux (200; 21.032195ms)
Jul 31 17:07:19.668: INFO: (14) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname1/proxy/: foo (200; 22.157792ms)
Jul 31 17:07:19.677: INFO: (15) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 8.235701ms)
Jul 31 17:07:19.678: INFO: (15) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:460/proxy/: tls baz (200; 8.967145ms)
Jul 31 17:07:19.678: INFO: (15) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/tlsrewritem... (200; 9.159196ms)
Jul 31 17:07:19.678: INFO: (15) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 9.890312ms)
Jul 31 17:07:19.678: INFO: (15) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 9.951413ms)
Jul 31 17:07:19.678: INFO: (15) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 9.824622ms)
Jul 31 17:07:19.679: INFO: (15) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname2/proxy/: bar (200; 10.282295ms)
Jul 31 17:07:19.679: INFO: (15) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname1/proxy/: tls baz (200; 10.377705ms)
Jul 31 17:07:19.679: INFO: (15) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname1/proxy/: foo (200; 10.569987ms)
Jul 31 17:07:19.680: INFO: (15) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:462/proxy/: tls qux (200; 11.625034ms)
Jul 31 17:07:19.680: INFO: (15) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/rewriteme">test</a> (200; 11.850491ms)
Jul 31 17:07:19.680: INFO: (15) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">... (200; 11.845973ms)
Jul 31 17:07:19.681: INFO: (15) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">test<... (200; 12.136358ms)
Jul 31 17:07:19.681: INFO: (15) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname1/proxy/: foo (200; 12.444494ms)
Jul 31 17:07:19.681: INFO: (15) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname2/proxy/: bar (200; 12.463839ms)
Jul 31 17:07:19.682: INFO: (15) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname2/proxy/: tls qux (200; 13.468145ms)
Jul 31 17:07:19.693: INFO: (16) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 9.988444ms)
Jul 31 17:07:19.693: INFO: (16) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 9.967612ms)
Jul 31 17:07:19.693: INFO: (16) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">test<... (200; 10.523573ms)
Jul 31 17:07:19.693: INFO: (16) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">... (200; 10.705819ms)
Jul 31 17:07:19.693: INFO: (16) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 10.864885ms)
Jul 31 17:07:19.694: INFO: (16) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname1/proxy/: foo (200; 11.0421ms)
Jul 31 17:07:19.694: INFO: (16) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:460/proxy/: tls baz (200; 11.324311ms)
Jul 31 17:07:19.694: INFO: (16) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 11.268477ms)
Jul 31 17:07:19.694: INFO: (16) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:462/proxy/: tls qux (200; 11.218267ms)
Jul 31 17:07:19.694: INFO: (16) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/tlsrewritem... (200; 11.350242ms)
Jul 31 17:07:19.695: INFO: (16) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname1/proxy/: tls baz (200; 12.290539ms)
Jul 31 17:07:19.695: INFO: (16) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname2/proxy/: tls qux (200; 12.244921ms)
Jul 31 17:07:19.695: INFO: (16) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/rewriteme">test</a> (200; 12.170581ms)
Jul 31 17:07:19.696: INFO: (16) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname2/proxy/: bar (200; 13.831029ms)
Jul 31 17:07:19.696: INFO: (16) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname2/proxy/: bar (200; 13.724064ms)
Jul 31 17:07:19.696: INFO: (16) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname1/proxy/: foo (200; 13.823603ms)
Jul 31 17:07:19.708: INFO: (17) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 11.279824ms)
Jul 31 17:07:19.709: INFO: (17) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname1/proxy/: foo (200; 12.337039ms)
Jul 31 17:07:19.709: INFO: (17) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:460/proxy/: tls baz (200; 12.522178ms)
Jul 31 17:07:19.709: INFO: (17) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">... (200; 12.199415ms)
Jul 31 17:07:19.709: INFO: (17) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 12.464121ms)
Jul 31 17:07:19.709: INFO: (17) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 12.571963ms)
Jul 31 17:07:19.709: INFO: (17) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">test<... (200; 12.503055ms)
Jul 31 17:07:19.710: INFO: (17) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:462/proxy/: tls qux (200; 13.355485ms)
Jul 31 17:07:19.711: INFO: (17) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 13.869636ms)
Jul 31 17:07:19.712: INFO: (17) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/rewriteme">test</a> (200; 15.248368ms)
Jul 31 17:07:19.712: INFO: (17) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/tlsrewritem... (200; 15.382777ms)
Jul 31 17:07:19.712: INFO: (17) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname2/proxy/: bar (200; 15.580247ms)
Jul 31 17:07:19.713: INFO: (17) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname2/proxy/: bar (200; 16.143898ms)
Jul 31 17:07:19.713: INFO: (17) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname1/proxy/: tls baz (200; 16.342991ms)
Jul 31 17:07:19.713: INFO: (17) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname2/proxy/: tls qux (200; 16.722441ms)
Jul 31 17:07:19.713: INFO: (17) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname1/proxy/: foo (200; 16.533453ms)
Jul 31 17:07:19.722: INFO: (18) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/tlsrewritem... (200; 8.253315ms)
Jul 31 17:07:19.724: INFO: (18) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">... (200; 9.831534ms)
Jul 31 17:07:19.724: INFO: (18) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 10.566713ms)
Jul 31 17:07:19.725: INFO: (18) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:462/proxy/: tls qux (200; 10.583448ms)
Jul 31 17:07:19.725: INFO: (18) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:460/proxy/: tls baz (200; 10.900534ms)
Jul 31 17:07:19.725: INFO: (18) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 11.188931ms)
Jul 31 17:07:19.725: INFO: (18) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">test<... (200; 11.30436ms)
Jul 31 17:07:19.725: INFO: (18) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 11.102715ms)
Jul 31 17:07:19.731: INFO: (18) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 17.69342ms)
Jul 31 17:07:19.734: INFO: (18) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/rewriteme">test</a> (200; 20.041113ms)
Jul 31 17:07:19.735: INFO: (18) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname1/proxy/: foo (200; 20.667741ms)
Jul 31 17:07:19.735: INFO: (18) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname2/proxy/: tls qux (200; 21.022994ms)
Jul 31 17:07:19.735: INFO: (18) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname2/proxy/: bar (200; 20.735478ms)
Jul 31 17:07:19.735: INFO: (18) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname1/proxy/: foo (200; 20.754258ms)
Jul 31 17:07:19.735: INFO: (18) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname2/proxy/: bar (200; 20.786564ms)
Jul 31 17:07:19.735: INFO: (18) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname1/proxy/: tls baz (200; 20.921743ms)
Jul 31 17:07:19.745: INFO: (19) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname2/proxy/: bar (200; 10.121036ms)
Jul 31 17:07:19.746: INFO: (19) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 9.63717ms)
Jul 31 17:07:19.746: INFO: (19) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">... (200; 10.066174ms)
Jul 31 17:07:19.746: INFO: (19) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:160/proxy/: foo (200; 10.072257ms)
Jul 31 17:07:19.746: INFO: (19) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc/proxy/rewriteme">test</a> (200; 10.246911ms)
Jul 31 17:07:19.749: INFO: (19) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname1/proxy/: tls baz (200; 13.620227ms)
Jul 31 17:07:19.750: INFO: (19) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 14.756866ms)
Jul 31 17:07:19.750: INFO: (19) /api/v1/namespaces/proxy-7817/pods/http:proxy-service-fvqmg-fp6qc:162/proxy/: bar (200; 14.813849ms)
Jul 31 17:07:19.750: INFO: (19) /api/v1/namespaces/proxy-7817/services/http:proxy-service-fvqmg:portname1/proxy/: foo (200; 14.369412ms)
Jul 31 17:07:19.750: INFO: (19) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:443/proxy/tlsrewritem... (200; 15.476396ms)
Jul 31 17:07:19.751: INFO: (19) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname2/proxy/: bar (200; 15.12736ms)
Jul 31 17:07:19.751: INFO: (19) /api/v1/namespaces/proxy-7817/services/https:proxy-service-fvqmg:tlsportname2/proxy/: tls qux (200; 16.280734ms)
Jul 31 17:07:19.752: INFO: (19) /api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/: <a href="/api/v1/namespaces/proxy-7817/pods/proxy-service-fvqmg-fp6qc:1080/proxy/rewriteme">test<... (200; 16.577769ms)
Jul 31 17:07:19.752: INFO: (19) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:462/proxy/: tls qux (200; 16.578198ms)
Jul 31 17:07:19.752: INFO: (19) /api/v1/namespaces/proxy-7817/pods/https:proxy-service-fvqmg-fp6qc:460/proxy/: tls baz (200; 16.618549ms)
Jul 31 17:07:19.752: INFO: (19) /api/v1/namespaces/proxy-7817/services/proxy-service-fvqmg:portname1/proxy/: foo (200; 16.577866ms)
STEP: deleting ReplicationController proxy-service-fvqmg in namespace proxy-7817, will wait for the garbage collector to delete the pods
Jul 31 17:07:19.815: INFO: Deleting ReplicationController proxy-service-fvqmg took: 10.454645ms
Jul 31 17:07:20.415: INFO: Terminating ReplicationController proxy-service-fvqmg pods took: 600.272778ms
[AfterEach] version v1
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:07:33.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7817" for this suite.

• [SLOW TEST:21.107 seconds]
[sig-network] Proxy
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":280,"completed":140,"skipped":2308,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:07:33.229: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-602
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:07:50.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-602" for this suite.

• [SLOW TEST:17.204 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":280,"completed":141,"skipped":2319,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:07:50.434: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5572
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 31 17:07:50.574: INFO: Waiting up to 5m0s for pod "downwardapi-volume-50d7c460-b720-4001-910a-285f5fa5e5d3" in namespace "downward-api-5572" to be "success or failure"
Jul 31 17:07:50.577: INFO: Pod "downwardapi-volume-50d7c460-b720-4001-910a-285f5fa5e5d3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.285049ms
Jul 31 17:07:52.581: INFO: Pod "downwardapi-volume-50d7c460-b720-4001-910a-285f5fa5e5d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006597051s
Jul 31 17:07:54.584: INFO: Pod "downwardapi-volume-50d7c460-b720-4001-910a-285f5fa5e5d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010414074s
STEP: Saw pod success
Jul 31 17:07:54.585: INFO: Pod "downwardapi-volume-50d7c460-b720-4001-910a-285f5fa5e5d3" satisfied condition "success or failure"
Jul 31 17:07:54.587: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod downwardapi-volume-50d7c460-b720-4001-910a-285f5fa5e5d3 container client-container: <nil>
STEP: delete the pod
Jul 31 17:07:54.622: INFO: Waiting for pod downwardapi-volume-50d7c460-b720-4001-910a-285f5fa5e5d3 to disappear
Jul 31 17:07:54.625: INFO: Pod downwardapi-volume-50d7c460-b720-4001-910a-285f5fa5e5d3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:07:54.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5572" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":280,"completed":142,"skipped":2321,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:07:54.633: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4038
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 31 17:07:59.335: INFO: Successfully updated pod "pod-update-activedeadlineseconds-2d1bd6de-a79c-4555-ab2b-52e3cb4a1e6f"
Jul 31 17:07:59.335: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-2d1bd6de-a79c-4555-ab2b-52e3cb4a1e6f" in namespace "pods-4038" to be "terminated due to deadline exceeded"
Jul 31 17:07:59.337: INFO: Pod "pod-update-activedeadlineseconds-2d1bd6de-a79c-4555-ab2b-52e3cb4a1e6f": Phase="Running", Reason="", readiness=true. Elapsed: 1.870691ms
Jul 31 17:08:01.340: INFO: Pod "pod-update-activedeadlineseconds-2d1bd6de-a79c-4555-ab2b-52e3cb4a1e6f": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.005104325s
Jul 31 17:08:01.340: INFO: Pod "pod-update-activedeadlineseconds-2d1bd6de-a79c-4555-ab2b-52e3cb4a1e6f" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:08:01.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4038" for this suite.

• [SLOW TEST:6.716 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":280,"completed":143,"skipped":2350,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:08:01.349: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5484
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 17:08:01.502: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jul 31 17:08:01.515: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:08:01.526: INFO: Number of nodes with available pods: 0
Jul 31 17:08:01.526: INFO: Node test-aruna-123-node-group-687e2c91ce is running more than one daemon pod
Jul 31 17:08:02.532: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:08:02.534: INFO: Number of nodes with available pods: 0
Jul 31 17:08:02.534: INFO: Node test-aruna-123-node-group-687e2c91ce is running more than one daemon pod
Jul 31 17:08:03.531: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:08:03.536: INFO: Number of nodes with available pods: 0
Jul 31 17:08:03.536: INFO: Node test-aruna-123-node-group-687e2c91ce is running more than one daemon pod
Jul 31 17:08:04.531: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:08:04.540: INFO: Number of nodes with available pods: 3
Jul 31 17:08:04.540: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jul 31 17:08:04.580: INFO: Wrong image for pod: daemon-set-76v88. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:04.580: INFO: Wrong image for pod: daemon-set-g7qvp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:04.580: INFO: Wrong image for pod: daemon-set-tqntq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:04.587: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:08:05.591: INFO: Wrong image for pod: daemon-set-76v88. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:05.591: INFO: Wrong image for pod: daemon-set-g7qvp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:05.591: INFO: Wrong image for pod: daemon-set-tqntq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:05.595: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:08:06.594: INFO: Wrong image for pod: daemon-set-76v88. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:06.594: INFO: Wrong image for pod: daemon-set-g7qvp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:06.594: INFO: Wrong image for pod: daemon-set-tqntq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:06.600: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:08:07.591: INFO: Wrong image for pod: daemon-set-76v88. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:07.591: INFO: Pod daemon-set-76v88 is not available
Jul 31 17:08:07.591: INFO: Wrong image for pod: daemon-set-g7qvp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:07.591: INFO: Wrong image for pod: daemon-set-tqntq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:07.594: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:08:08.592: INFO: Wrong image for pod: daemon-set-g7qvp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:08.592: INFO: Wrong image for pod: daemon-set-tqntq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:08.592: INFO: Pod daemon-set-txt45 is not available
Jul 31 17:08:08.596: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:08:09.591: INFO: Wrong image for pod: daemon-set-g7qvp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:09.591: INFO: Wrong image for pod: daemon-set-tqntq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:09.591: INFO: Pod daemon-set-txt45 is not available
Jul 31 17:08:09.596: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:08:10.591: INFO: Wrong image for pod: daemon-set-g7qvp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:10.591: INFO: Wrong image for pod: daemon-set-tqntq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:10.594: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:08:11.590: INFO: Wrong image for pod: daemon-set-g7qvp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:11.590: INFO: Wrong image for pod: daemon-set-tqntq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:11.594: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:08:12.591: INFO: Wrong image for pod: daemon-set-g7qvp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:12.591: INFO: Wrong image for pod: daemon-set-tqntq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:12.591: INFO: Pod daemon-set-tqntq is not available
Jul 31 17:08:12.595: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:08:13.593: INFO: Wrong image for pod: daemon-set-g7qvp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:13.593: INFO: Wrong image for pod: daemon-set-tqntq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:13.593: INFO: Pod daemon-set-tqntq is not available
Jul 31 17:08:13.601: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:08:14.591: INFO: Wrong image for pod: daemon-set-g7qvp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:14.591: INFO: Wrong image for pod: daemon-set-tqntq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:14.591: INFO: Pod daemon-set-tqntq is not available
Jul 31 17:08:14.594: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:08:15.591: INFO: Wrong image for pod: daemon-set-g7qvp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:15.592: INFO: Pod daemon-set-thrdz is not available
Jul 31 17:08:15.597: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:08:16.591: INFO: Wrong image for pod: daemon-set-g7qvp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:16.591: INFO: Pod daemon-set-thrdz is not available
Jul 31 17:08:16.597: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:08:17.591: INFO: Wrong image for pod: daemon-set-g7qvp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:17.591: INFO: Pod daemon-set-thrdz is not available
Jul 31 17:08:17.594: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:08:18.592: INFO: Wrong image for pod: daemon-set-g7qvp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:18.595: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:08:19.592: INFO: Wrong image for pod: daemon-set-g7qvp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 31 17:08:19.592: INFO: Pod daemon-set-g7qvp is not available
Jul 31 17:08:19.595: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:08:20.591: INFO: Pod daemon-set-n97p4 is not available
Jul 31 17:08:20.598: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jul 31 17:08:20.603: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:08:20.610: INFO: Number of nodes with available pods: 2
Jul 31 17:08:20.610: INFO: Node test-aruna-123-node-group-a92c781fbd is running more than one daemon pod
Jul 31 17:08:21.614: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:08:21.617: INFO: Number of nodes with available pods: 2
Jul 31 17:08:21.617: INFO: Node test-aruna-123-node-group-a92c781fbd is running more than one daemon pod
Jul 31 17:08:22.615: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:08:22.618: INFO: Number of nodes with available pods: 2
Jul 31 17:08:22.618: INFO: Node test-aruna-123-node-group-a92c781fbd is running more than one daemon pod
Jul 31 17:08:23.620: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:08:23.623: INFO: Number of nodes with available pods: 3
Jul 31 17:08:23.623: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5484, will wait for the garbage collector to delete the pods
Jul 31 17:08:23.701: INFO: Deleting DaemonSet.extensions daemon-set took: 14.217546ms
Jul 31 17:08:24.301: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.221333ms
Jul 31 17:08:35.504: INFO: Number of nodes with available pods: 0
Jul 31 17:08:35.504: INFO: Number of running nodes: 0, number of available pods: 0
Jul 31 17:08:35.506: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5484/daemonsets","resourceVersion":"227405"},"items":null}

Jul 31 17:08:35.508: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5484/pods","resourceVersion":"227405"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:08:35.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5484" for this suite.

• [SLOW TEST:34.179 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":280,"completed":144,"skipped":2351,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:08:35.528: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-8001
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 17:08:35.738: INFO: Waiting up to 5m0s for pod "busybox-user-65534-4f69949b-a6d8-4686-9e13-1f97a9ea4770" in namespace "security-context-test-8001" to be "success or failure"
Jul 31 17:08:35.749: INFO: Pod "busybox-user-65534-4f69949b-a6d8-4686-9e13-1f97a9ea4770": Phase="Pending", Reason="", readiness=false. Elapsed: 10.276747ms
Jul 31 17:08:37.752: INFO: Pod "busybox-user-65534-4f69949b-a6d8-4686-9e13-1f97a9ea4770": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013764488s
Jul 31 17:08:39.756: INFO: Pod "busybox-user-65534-4f69949b-a6d8-4686-9e13-1f97a9ea4770": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017264949s
Jul 31 17:08:39.756: INFO: Pod "busybox-user-65534-4f69949b-a6d8-4686-9e13-1f97a9ea4770" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:08:39.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8001" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":145,"skipped":2372,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:08:39.770: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6800
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 17:08:39.939: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jul 31 17:08:44.943: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 31 17:08:44.943: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jul 31 17:08:46.946: INFO: Creating deployment "test-rollover-deployment"
Jul 31 17:08:46.956: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jul 31 17:08:48.961: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jul 31 17:08:48.966: INFO: Ensure that both replica sets have 1 created replica
Jul 31 17:08:48.971: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jul 31 17:08:48.977: INFO: Updating deployment test-rollover-deployment
Jul 31 17:08:48.977: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jul 31 17:08:50.986: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jul 31 17:08:50.991: INFO: Make sure deployment "test-rollover-deployment" is complete
Jul 31 17:08:50.996: INFO: all replica sets need to contain the pod-template-hash label
Jul 31 17:08:50.997: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812126, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812126, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812128, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812126, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 17:08:53.002: INFO: all replica sets need to contain the pod-template-hash label
Jul 31 17:08:53.002: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812126, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812126, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812130, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812126, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 17:08:55.004: INFO: all replica sets need to contain the pod-template-hash label
Jul 31 17:08:55.004: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812126, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812126, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812130, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812126, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 17:08:57.002: INFO: all replica sets need to contain the pod-template-hash label
Jul 31 17:08:57.002: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812126, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812126, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812130, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812126, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 17:08:59.002: INFO: all replica sets need to contain the pod-template-hash label
Jul 31 17:08:59.002: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812126, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812126, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812130, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812126, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 17:09:01.002: INFO: all replica sets need to contain the pod-template-hash label
Jul 31 17:09:01.002: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812126, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812126, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812130, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812126, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 17:09:03.007: INFO: 
Jul 31 17:09:03.007: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jul 31 17:09:03.014: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6800 /apis/apps/v1/namespaces/deployment-6800/deployments/test-rollover-deployment 03be330d-e208-4a3f-b44c-05623c06646a 227647 2 2020-07-31 17:08:46 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006b51008 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-07-31 17:08:46 +0000 UTC,LastTransitionTime:2020-07-31 17:08:46 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-574d6dfbff" has successfully progressed.,LastUpdateTime:2020-07-31 17:09:01 +0000 UTC,LastTransitionTime:2020-07-31 17:08:46 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 31 17:09:03.016: INFO: New ReplicaSet "test-rollover-deployment-574d6dfbff" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-574d6dfbff  deployment-6800 /apis/apps/v1/namespaces/deployment-6800/replicasets/test-rollover-deployment-574d6dfbff cf9d1807-6d5b-4c66-9519-9b22ca0cccda 227636 2 2020-07-31 17:08:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 03be330d-e208-4a3f-b44c-05623c06646a 0xc006b51497 0xc006b51498}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 574d6dfbff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006b51508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 31 17:09:03.016: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jul 31 17:09:03.016: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6800 /apis/apps/v1/namespaces/deployment-6800/replicasets/test-rollover-controller b8c81116-337b-4737-a521-759ef021760d 227646 2 2020-07-31 17:08:39 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 03be330d-e208-4a3f-b44c-05623c06646a 0xc006b513c7 0xc006b513c8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006b51428 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 31 17:09:03.016: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-6800 /apis/apps/v1/namespaces/deployment-6800/replicasets/test-rollover-deployment-f6c94f66c 3d589049-3040-48b6-86ef-d5651ec8e601 227585 2 2020-07-31 17:08:46 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 03be330d-e208-4a3f-b44c-05623c06646a 0xc006b51570 0xc006b51571}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006b515e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 31 17:09:03.028: INFO: Pod "test-rollover-deployment-574d6dfbff-j75xf" is available:
&Pod{ObjectMeta:{test-rollover-deployment-574d6dfbff-j75xf test-rollover-deployment-574d6dfbff- deployment-6800 /api/v1/namespaces/deployment-6800/pods/test-rollover-deployment-574d6dfbff-j75xf 9c88cd04-6536-4dae-b336-4be7f3dc99ed 227601 0 2020-07-31 17:08:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[cni.projectcalico.org/podIP:192.168.220.107/32 cni.projectcalico.org/podIPs:192.168.220.107/32] [{apps/v1 ReplicaSet test-rollover-deployment-574d6dfbff cf9d1807-6d5b-4c66-9519-9b22ca0cccda 0xc006b51b57 0xc006b51b58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hw8ts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hw8ts,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hw8ts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-687e2c91ce,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:08:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:08:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:08:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:08:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.92,PodIP:192.168.220.107,StartTime:2020-07-31 17:08:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-31 17:08:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://1c61ca88e6b030a04af89747c830fc931e8a921995d990c06559b2c80e295e92,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.220.107,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:09:03.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6800" for this suite.

• [SLOW TEST:23.266 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":280,"completed":146,"skipped":2373,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:09:03.037: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7941
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jul 31 17:09:03.200: INFO: Waiting up to 5m0s for pod "downward-api-65a5dc63-44c4-40be-9970-de560551dd74" in namespace "downward-api-7941" to be "success or failure"
Jul 31 17:09:03.210: INFO: Pod "downward-api-65a5dc63-44c4-40be-9970-de560551dd74": Phase="Pending", Reason="", readiness=false. Elapsed: 9.859616ms
Jul 31 17:09:05.213: INFO: Pod "downward-api-65a5dc63-44c4-40be-9970-de560551dd74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01269313s
Jul 31 17:09:07.216: INFO: Pod "downward-api-65a5dc63-44c4-40be-9970-de560551dd74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015700269s
STEP: Saw pod success
Jul 31 17:09:07.216: INFO: Pod "downward-api-65a5dc63-44c4-40be-9970-de560551dd74" satisfied condition "success or failure"
Jul 31 17:09:07.219: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod downward-api-65a5dc63-44c4-40be-9970-de560551dd74 container dapi-container: <nil>
STEP: delete the pod
Jul 31 17:09:07.252: INFO: Waiting for pod downward-api-65a5dc63-44c4-40be-9970-de560551dd74 to disappear
Jul 31 17:09:07.254: INFO: Pod downward-api-65a5dc63-44c4-40be-9970-de560551dd74 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:09:07.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7941" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":280,"completed":147,"skipped":2374,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:09:07.263: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6199
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-72603141-490b-45c3-a98b-f31e81a2386c
STEP: Creating a pod to test consume secrets
Jul 31 17:09:07.425: INFO: Waiting up to 5m0s for pod "pod-secrets-323ef90a-9105-41aa-b8ed-e4067eeb0d88" in namespace "secrets-6199" to be "success or failure"
Jul 31 17:09:07.431: INFO: Pod "pod-secrets-323ef90a-9105-41aa-b8ed-e4067eeb0d88": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005051ms
Jul 31 17:09:09.433: INFO: Pod "pod-secrets-323ef90a-9105-41aa-b8ed-e4067eeb0d88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008783117s
Jul 31 17:09:11.437: INFO: Pod "pod-secrets-323ef90a-9105-41aa-b8ed-e4067eeb0d88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011871215s
STEP: Saw pod success
Jul 31 17:09:11.437: INFO: Pod "pod-secrets-323ef90a-9105-41aa-b8ed-e4067eeb0d88" satisfied condition "success or failure"
Jul 31 17:09:11.441: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-secrets-323ef90a-9105-41aa-b8ed-e4067eeb0d88 container secret-volume-test: <nil>
STEP: delete the pod
Jul 31 17:09:11.482: INFO: Waiting for pod pod-secrets-323ef90a-9105-41aa-b8ed-e4067eeb0d88 to disappear
Jul 31 17:09:11.488: INFO: Pod pod-secrets-323ef90a-9105-41aa-b8ed-e4067eeb0d88 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:09:11.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6199" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":148,"skipped":2394,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:09:11.500: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1118
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1118
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-1118
I0731 17:09:11.698723      21 runners.go:189] Created replication controller with name: externalname-service, namespace: services-1118, replica count: 2
I0731 17:09:14.749060      21 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 31 17:09:14.749: INFO: Creating new exec pod
Jul 31 17:09:19.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=services-1118 execpod6thbj -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jul 31 17:09:20.051: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 31 17:09:20.051: INFO: stdout: ""
Jul 31 17:09:20.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=services-1118 execpod6thbj -- /bin/sh -x -c nc -zv -t -w 2 10.99.156.181 80'
Jul 31 17:09:20.396: INFO: stderr: "+ nc -zv -t -w 2 10.99.156.181 80\nConnection to 10.99.156.181 80 port [tcp/http] succeeded!\n"
Jul 31 17:09:20.396: INFO: stdout: ""
Jul 31 17:09:20.396: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:09:20.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1118" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:8.946 seconds]
[sig-network] Services
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":280,"completed":149,"skipped":2509,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:09:20.446: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5438
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5438 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5438;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5438 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5438;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5438.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5438.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5438.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5438.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5438.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5438.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5438.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5438.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5438.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5438.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5438.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5438.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5438.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 72.73.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.73.72_udp@PTR;check="$$(dig +tcp +noall +answer +search 72.73.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.73.72_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5438 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5438;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5438 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5438;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5438.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5438.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5438.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5438.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5438.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5438.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5438.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5438.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5438.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5438.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5438.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5438.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5438.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 72.73.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.73.72_udp@PTR;check="$$(dig +tcp +noall +answer +search 72.73.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.73.72_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 31 17:09:24.692: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:24.696: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:24.699: INFO: Unable to read wheezy_udp@dns-test-service.dns-5438 from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:24.702: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5438 from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:24.705: INFO: Unable to read wheezy_udp@dns-test-service.dns-5438.svc from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:24.707: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5438.svc from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:24.712: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5438.svc from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:24.735: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:24.739: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:24.742: INFO: Unable to read jessie_udp@dns-test-service.dns-5438 from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:24.745: INFO: Unable to read jessie_tcp@dns-test-service.dns-5438 from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:24.748: INFO: Unable to read jessie_udp@dns-test-service.dns-5438.svc from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:24.749: INFO: Unable to read jessie_tcp@dns-test-service.dns-5438.svc from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:24.752: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5438.svc from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:24.755: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5438.svc from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:24.776: INFO: Lookups using dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5438 wheezy_tcp@dns-test-service.dns-5438 wheezy_udp@dns-test-service.dns-5438.svc wheezy_tcp@dns-test-service.dns-5438.svc wheezy_udp@_http._tcp.dns-test-service.dns-5438.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5438 jessie_tcp@dns-test-service.dns-5438 jessie_udp@dns-test-service.dns-5438.svc jessie_tcp@dns-test-service.dns-5438.svc jessie_udp@_http._tcp.dns-test-service.dns-5438.svc jessie_tcp@_http._tcp.dns-test-service.dns-5438.svc]

Jul 31 17:09:29.781: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:29.787: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:29.792: INFO: Unable to read wheezy_udp@dns-test-service.dns-5438 from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:29.795: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5438 from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:29.798: INFO: Unable to read wheezy_udp@dns-test-service.dns-5438.svc from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:29.801: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5438.svc from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:29.838: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:29.842: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:29.852: INFO: Unable to read jessie_udp@dns-test-service.dns-5438 from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:29.866: INFO: Unable to read jessie_tcp@dns-test-service.dns-5438 from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:29.877: INFO: Unable to read jessie_udp@dns-test-service.dns-5438.svc from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:29.881: INFO: Unable to read jessie_tcp@dns-test-service.dns-5438.svc from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:29.924: INFO: Lookups using dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5438 wheezy_tcp@dns-test-service.dns-5438 wheezy_udp@dns-test-service.dns-5438.svc wheezy_tcp@dns-test-service.dns-5438.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5438 jessie_tcp@dns-test-service.dns-5438 jessie_udp@dns-test-service.dns-5438.svc jessie_tcp@dns-test-service.dns-5438.svc]

Jul 31 17:09:34.791: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:34.800: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:34.805: INFO: Unable to read wheezy_udp@dns-test-service.dns-5438 from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:34.809: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5438 from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:34.812: INFO: Unable to read wheezy_udp@dns-test-service.dns-5438.svc from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:34.816: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5438.svc from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:34.882: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:34.885: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:34.888: INFO: Unable to read jessie_udp@dns-test-service.dns-5438 from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:34.892: INFO: Unable to read jessie_tcp@dns-test-service.dns-5438 from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:34.896: INFO: Unable to read jessie_udp@dns-test-service.dns-5438.svc from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:34.899: INFO: Unable to read jessie_tcp@dns-test-service.dns-5438.svc from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:34.929: INFO: Lookups using dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5438 wheezy_tcp@dns-test-service.dns-5438 wheezy_udp@dns-test-service.dns-5438.svc wheezy_tcp@dns-test-service.dns-5438.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5438 jessie_tcp@dns-test-service.dns-5438 jessie_udp@dns-test-service.dns-5438.svc jessie_tcp@dns-test-service.dns-5438.svc]

Jul 31 17:09:39.784: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:39.794: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:39.803: INFO: Unable to read wheezy_udp@dns-test-service.dns-5438 from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:39.809: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5438 from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:39.812: INFO: Unable to read wheezy_udp@dns-test-service.dns-5438.svc from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:39.816: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5438.svc from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:39.852: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:39.858: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:39.865: INFO: Unable to read jessie_udp@dns-test-service.dns-5438 from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:39.869: INFO: Unable to read jessie_tcp@dns-test-service.dns-5438 from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:39.874: INFO: Unable to read jessie_udp@dns-test-service.dns-5438.svc from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:39.879: INFO: Unable to read jessie_tcp@dns-test-service.dns-5438.svc from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:39.912: INFO: Lookups using dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5438 wheezy_tcp@dns-test-service.dns-5438 wheezy_udp@dns-test-service.dns-5438.svc wheezy_tcp@dns-test-service.dns-5438.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5438 jessie_tcp@dns-test-service.dns-5438 jessie_udp@dns-test-service.dns-5438.svc jessie_tcp@dns-test-service.dns-5438.svc]

Jul 31 17:09:44.783: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:44.789: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:44.793: INFO: Unable to read wheezy_udp@dns-test-service.dns-5438 from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:44.796: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5438 from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:44.800: INFO: Unable to read wheezy_udp@dns-test-service.dns-5438.svc from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:44.803: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5438.svc from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:44.848: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:44.852: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:44.855: INFO: Unable to read jessie_udp@dns-test-service.dns-5438 from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:44.858: INFO: Unable to read jessie_tcp@dns-test-service.dns-5438 from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:44.860: INFO: Unable to read jessie_udp@dns-test-service.dns-5438.svc from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:44.862: INFO: Unable to read jessie_tcp@dns-test-service.dns-5438.svc from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:44.898: INFO: Lookups using dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5438 wheezy_tcp@dns-test-service.dns-5438 wheezy_udp@dns-test-service.dns-5438.svc wheezy_tcp@dns-test-service.dns-5438.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5438 jessie_tcp@dns-test-service.dns-5438 jessie_udp@dns-test-service.dns-5438.svc jessie_tcp@dns-test-service.dns-5438.svc]

Jul 31 17:09:49.780: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:49.790: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:49.796: INFO: Unable to read wheezy_udp@dns-test-service.dns-5438 from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:49.800: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5438 from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:49.808: INFO: Unable to read wheezy_udp@dns-test-service.dns-5438.svc from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:49.814: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5438.svc from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:49.886: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:49.892: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:49.897: INFO: Unable to read jessie_udp@dns-test-service.dns-5438 from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:49.901: INFO: Unable to read jessie_tcp@dns-test-service.dns-5438 from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:49.906: INFO: Unable to read jessie_udp@dns-test-service.dns-5438.svc from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:49.909: INFO: Unable to read jessie_tcp@dns-test-service.dns-5438.svc from pod dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32: the server could not find the requested resource (get pods dns-test-619f7922-117a-4e68-a8c7-f41547339b32)
Jul 31 17:09:49.935: INFO: Lookups using dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5438 wheezy_tcp@dns-test-service.dns-5438 wheezy_udp@dns-test-service.dns-5438.svc wheezy_tcp@dns-test-service.dns-5438.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5438 jessie_tcp@dns-test-service.dns-5438 jessie_udp@dns-test-service.dns-5438.svc jessie_tcp@dns-test-service.dns-5438.svc]

Jul 31 17:09:54.866: INFO: DNS probes using dns-5438/dns-test-619f7922-117a-4e68-a8c7-f41547339b32 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:09:55.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5438" for this suite.

• [SLOW TEST:34.604 seconds]
[sig-network] DNS
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":280,"completed":150,"skipped":2525,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:09:55.050: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4952
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jul 31 17:09:55.209: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 31 17:09:55.227: INFO: Waiting for terminating namespaces to be deleted...
Jul 31 17:09:55.231: INFO: 
Logging pods the kubelet thinks is on node test-aruna-123-node-group-687e2c91ce before test
Jul 31 17:09:55.242: INFO: calico-node-98r4p from kube-system started at 2020-07-30 20:35:01 +0000 UTC (1 container statuses recorded)
Jul 31 17:09:55.242: INFO: 	Container calico-node ready: true, restart count 0
Jul 31 17:09:55.242: INFO: nvidia-device-plugin-daemonset-wjtwk from kube-system started at 2020-07-31 16:51:11 +0000 UTC (1 container statuses recorded)
Jul 31 17:09:55.242: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Jul 31 17:09:55.242: INFO: sonobuoy from sonobuoy started at 2020-07-31 16:21:46 +0000 UTC (1 container statuses recorded)
Jul 31 17:09:55.242: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 31 17:09:55.242: INFO: kube-proxy-9nrgh from kube-system started at 2020-07-30 20:35:01 +0000 UTC (1 container statuses recorded)
Jul 31 17:09:55.242: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 17:09:55.242: INFO: metallb-speaker-bp7cw from ccp started at 2020-07-31 16:51:00 +0000 UTC (1 container statuses recorded)
Jul 31 17:09:55.242: INFO: 	Container metallb-speaker ready: true, restart count 0
Jul 31 17:09:55.242: INFO: ccp-monitor-prometheus-pushgateway-7d5b6d448b-hl5dh from ccp started at 2020-07-31 16:51:00 +0000 UTC (1 container statuses recorded)
Jul 31 17:09:55.242: INFO: 	Container prometheus-pushgateway ready: true, restart count 0
Jul 31 17:09:55.242: INFO: ccp-monitor-prometheus-node-exporter-p4jvn from ccp started at 2020-07-31 16:51:11 +0000 UTC (1 container statuses recorded)
Jul 31 17:09:55.242: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jul 31 17:09:55.242: INFO: nginx-ingress-controller-jt5pp from ccp started at 2020-07-31 16:51:21 +0000 UTC (1 container statuses recorded)
Jul 31 17:09:55.242: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jul 31 17:09:55.242: INFO: sonobuoy-systemd-logs-daemon-set-21a6c8c28f874b58-rcrfg from sonobuoy started at 2020-07-31 16:22:15 +0000 UTC (2 container statuses recorded)
Jul 31 17:09:55.242: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 17:09:55.242: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 17:09:55.242: INFO: 
Logging pods the kubelet thinks is on node test-aruna-123-node-group-a8f122b701 before test
Jul 31 17:09:55.250: INFO: ccp-monitor-prometheus-node-exporter-82955 from ccp started at 2020-07-30 20:37:46 +0000 UTC (1 container statuses recorded)
Jul 31 17:09:55.250: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jul 31 17:09:55.250: INFO: sonobuoy-systemd-logs-daemon-set-21a6c8c28f874b58-64p9f from sonobuoy started at 2020-07-31 16:22:16 +0000 UTC (2 container statuses recorded)
Jul 31 17:09:55.250: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 17:09:55.250: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 17:09:55.250: INFO: ccp-helm-operator-749fb6dc7-rrdzz from ccp started at 2020-07-31 16:51:01 +0000 UTC (1 container statuses recorded)
Jul 31 17:09:55.250: INFO: 	Container ccp-helm-operator ready: true, restart count 0
Jul 31 17:09:55.250: INFO: nvidia-device-plugin-daemonset-b7rb6 from kube-system started at 2020-07-30 20:35:04 +0000 UTC (1 container statuses recorded)
Jul 31 17:09:55.250: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Jul 31 17:09:55.251: INFO: metallb-speaker-g2jrx from ccp started at 2020-07-30 20:35:04 +0000 UTC (1 container statuses recorded)
Jul 31 17:09:55.251: INFO: 	Container metallb-speaker ready: true, restart count 0
Jul 31 17:09:55.251: INFO: cert-manager-7c4fdf69b7-ct9hx from ccp started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 17:09:55.251: INFO: 	Container cert-manager ready: true, restart count 0
Jul 31 17:09:55.251: INFO: nginx-ingress-default-backend-6b546bb848-d7ms8 from ccp started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 17:09:55.251: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jul 31 17:09:55.251: INFO: calico-node-fkmgx from kube-system started at 2020-07-30 20:34:54 +0000 UTC (1 container statuses recorded)
Jul 31 17:09:55.251: INFO: 	Container calico-node ready: true, restart count 0
Jul 31 17:09:55.251: INFO: nginx-ingress-controller-vv7zf from ccp started at 2020-07-30 20:35:04 +0000 UTC (1 container statuses recorded)
Jul 31 17:09:55.251: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jul 31 17:09:55.251: INFO: metallb-controller-6c8c5fd7fd-9n8gm from ccp started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 17:09:55.251: INFO: 	Container metallb-controller ready: true, restart count 0
Jul 31 17:09:55.251: INFO: ccp-monitor-prometheus-server-67d55955c7-b54b2 from ccp started at 2020-07-30 20:37:51 +0000 UTC (3 container statuses recorded)
Jul 31 17:09:55.251: INFO: 	Container nginx-proxy ready: true, restart count 0
Jul 31 17:09:55.251: INFO: 	Container prometheus-server ready: true, restart count 0
Jul 31 17:09:55.251: INFO: 	Container prometheus-server-configmap-reload ready: true, restart count 0
Jul 31 17:09:55.251: INFO: kube-proxy-rpj2g from kube-system started at 2020-07-30 20:34:54 +0000 UTC (1 container statuses recorded)
Jul 31 17:09:55.251: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 17:09:55.251: INFO: 
Logging pods the kubelet thinks is on node test-aruna-123-node-group-a92c781fbd before test
Jul 31 17:09:55.260: INFO: kube-proxy-q4q9v from kube-system started at 2020-07-30 20:35:00 +0000 UTC (1 container statuses recorded)
Jul 31 17:09:55.260: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 17:09:55.260: INFO: nginx-ingress-controller-2447g from ccp started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 17:09:55.260: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jul 31 17:09:55.260: INFO: ccp-monitor-prometheus-alertmanager-57f5689547-wvfrs from ccp started at 2020-07-30 20:37:51 +0000 UTC (2 container statuses recorded)
Jul 31 17:09:55.260: INFO: 	Container prometheus-alertmanager ready: true, restart count 0
Jul 31 17:09:55.260: INFO: 	Container prometheus-alertmanager-configmap-reload ready: true, restart count 0
Jul 31 17:09:55.260: INFO: metallb-speaker-q6n4f from ccp started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 17:09:55.260: INFO: 	Container metallb-speaker ready: true, restart count 0
Jul 31 17:09:55.260: INFO: ccp-monitor-prometheus-node-exporter-559kz from ccp started at 2020-07-30 20:37:46 +0000 UTC (1 container statuses recorded)
Jul 31 17:09:55.260: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jul 31 17:09:55.260: INFO: sonobuoy-systemd-logs-daemon-set-21a6c8c28f874b58-5vrgt from sonobuoy started at 2020-07-31 16:22:17 +0000 UTC (2 container statuses recorded)
Jul 31 17:09:55.260: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 17:09:55.260: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 17:09:55.260: INFO: nvidia-device-plugin-daemonset-4pt4d from kube-system started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 17:09:55.260: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Jul 31 17:09:55.260: INFO: kubernetes-dashboard-dbfcd4d-mkklx from ccp started at 2020-07-30 20:37:33 +0000 UTC (1 container statuses recorded)
Jul 31 17:09:55.260: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jul 31 17:09:55.260: INFO: ccp-monitor-prometheus-kube-state-metrics-7f7c9f986-xzrgl from ccp started at 2020-07-30 20:37:46 +0000 UTC (1 container statuses recorded)
Jul 31 17:09:55.260: INFO: 	Container prometheus-kube-state-metrics ready: true, restart count 0
Jul 31 17:09:55.260: INFO: ccp-monitor-grafana-set-datasource-qwx6l from ccp started at 2020-07-30 20:37:46 +0000 UTC (1 container statuses recorded)
Jul 31 17:09:55.260: INFO: 	Container ccp-monitor-grafana-set-datasource ready: false, restart count 0
Jul 31 17:09:55.260: INFO: calico-node-txdhs from kube-system started at 2020-07-30 20:35:00 +0000 UTC (1 container statuses recorded)
Jul 31 17:09:55.260: INFO: 	Container calico-node ready: true, restart count 0
Jul 31 17:09:55.260: INFO: sonobuoy-e2e-job-e7094f6bfc7c4c8d from sonobuoy started at 2020-07-31 16:22:17 +0000 UTC (2 container statuses recorded)
Jul 31 17:09:55.260: INFO: 	Container e2e ready: true, restart count 0
Jul 31 17:09:55.260: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 17:09:55.260: INFO: ccp-monitor-grafana-78fd4cc979-qn6mq from ccp started at 2020-07-31 16:51:02 +0000 UTC (1 container statuses recorded)
Jul 31 17:09:55.260: INFO: 	Container grafana ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1626e571c8bf6be0], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:09:56.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4952" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":280,"completed":151,"skipped":2549,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:09:56.301: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9515
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:10:12.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9515" for this suite.

• [SLOW TEST:16.271 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":280,"completed":152,"skipped":2561,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:10:12.573: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9230
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1790
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 31 17:10:12.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-9230'
Jul 31 17:10:12.804: INFO: stderr: ""
Jul 31 17:10:12.804: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jul 31 17:10:17.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pod e2e-test-httpd-pod --namespace=kubectl-9230 -o json'
Jul 31 17:10:17.918: INFO: stderr: ""
Jul 31 17:10:17.918: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"192.168.220.116/32\",\n            \"cni.projectcalico.org/podIPs\": \"192.168.220.116/32\"\n        },\n        \"creationTimestamp\": \"2020-07-31T17:10:12Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9230\",\n        \"resourceVersion\": \"228217\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-9230/pods/e2e-test-httpd-pod\",\n        \"uid\": \"20182ef4-8012-4f3b-87a7-6e632e86e9e2\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-zcqzc\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"test-aruna-123-node-group-687e2c91ce\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-zcqzc\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-zcqzc\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-07-31T17:10:11Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-07-31T17:10:13Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-07-31T17:10:13Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-07-31T17:10:12Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://ba3fe6443ba08de76a4562e5d29589a6039f63a005710e33c952e2de41505dee\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-07-31T17:10:12Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.10.102.92\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.220.116\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.220.116\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-07-31T17:10:11Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jul 31 17:10:17.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 replace -f - --namespace=kubectl-9230'
Jul 31 17:10:18.111: INFO: stderr: ""
Jul 31 17:10:18.111: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1795
Jul 31 17:10:18.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 delete pods e2e-test-httpd-pod --namespace=kubectl-9230'
Jul 31 17:10:23.208: INFO: stderr: ""
Jul 31 17:10:23.208: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:10:23.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9230" for this suite.

• [SLOW TEST:10.647 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1786
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":280,"completed":153,"skipped":2573,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:10:23.220: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5355
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service nodeport-service with the type=NodePort in namespace services-5355
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-5355
STEP: creating replication controller externalsvc in namespace services-5355
I0731 17:10:23.439218      21 runners.go:189] Created replication controller with name: externalsvc, namespace: services-5355, replica count: 2
I0731 17:10:26.490990      21 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jul 31 17:10:26.519: INFO: Creating new exec pod
Jul 31 17:10:30.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=services-5355 execpodcsxmd -- /bin/sh -x -c nslookup nodeport-service'
Jul 31 17:10:30.840: INFO: stderr: "+ nslookup nodeport-service\n"
Jul 31 17:10:30.840: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-5355.svc.cluster.local\tcanonical name = externalsvc.services-5355.svc.cluster.local.\nName:\texternalsvc.services-5355.svc.cluster.local\nAddress: 10.98.68.242\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5355, will wait for the garbage collector to delete the pods
Jul 31 17:10:30.899: INFO: Deleting ReplicationController externalsvc took: 4.787749ms
Jul 31 17:10:31.499: INFO: Terminating ReplicationController externalsvc pods took: 600.215836ms
Jul 31 17:10:36.031: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:10:36.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5355" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:12.860 seconds]
[sig-network] Services
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":280,"completed":154,"skipped":2580,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:10:36.083: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1976
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-2c633f76-4270-4096-bb92-958efe4d623e
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:10:40.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1976" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":155,"skipped":2608,"failed":0}
SS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:10:40.348: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6551
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name secret-emptykey-test-edb831dc-1de5-4fa0-ab93-937373779edd
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:10:40.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6551" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":280,"completed":156,"skipped":2610,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:10:40.538: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3884
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test env composition
Jul 31 17:10:40.694: INFO: Waiting up to 5m0s for pod "var-expansion-5a1f5228-ce83-4a8d-a744-b944f7b6ff75" in namespace "var-expansion-3884" to be "success or failure"
Jul 31 17:10:40.705: INFO: Pod "var-expansion-5a1f5228-ce83-4a8d-a744-b944f7b6ff75": Phase="Pending", Reason="", readiness=false. Elapsed: 10.965684ms
Jul 31 17:10:42.709: INFO: Pod "var-expansion-5a1f5228-ce83-4a8d-a744-b944f7b6ff75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014547887s
Jul 31 17:10:44.712: INFO: Pod "var-expansion-5a1f5228-ce83-4a8d-a744-b944f7b6ff75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017912449s
STEP: Saw pod success
Jul 31 17:10:44.712: INFO: Pod "var-expansion-5a1f5228-ce83-4a8d-a744-b944f7b6ff75" satisfied condition "success or failure"
Jul 31 17:10:44.714: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod var-expansion-5a1f5228-ce83-4a8d-a744-b944f7b6ff75 container dapi-container: <nil>
STEP: delete the pod
Jul 31 17:10:44.738: INFO: Waiting for pod var-expansion-5a1f5228-ce83-4a8d-a744-b944f7b6ff75 to disappear
Jul 31 17:10:44.742: INFO: Pod var-expansion-5a1f5228-ce83-4a8d-a744-b944f7b6ff75 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:10:44.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3884" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":280,"completed":157,"skipped":2636,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:10:44.752: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-306
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-756b4044-7374-48bc-8072-7dcd1d7f4f6c
STEP: Creating a pod to test consume configMaps
Jul 31 17:10:44.901: INFO: Waiting up to 5m0s for pod "pod-configmaps-21df7d40-47ff-4e47-a520-4847a012fa3a" in namespace "configmap-306" to be "success or failure"
Jul 31 17:10:44.906: INFO: Pod "pod-configmaps-21df7d40-47ff-4e47-a520-4847a012fa3a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.470958ms
Jul 31 17:10:46.911: INFO: Pod "pod-configmaps-21df7d40-47ff-4e47-a520-4847a012fa3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009653357s
Jul 31 17:10:48.913: INFO: Pod "pod-configmaps-21df7d40-47ff-4e47-a520-4847a012fa3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011925046s
STEP: Saw pod success
Jul 31 17:10:48.913: INFO: Pod "pod-configmaps-21df7d40-47ff-4e47-a520-4847a012fa3a" satisfied condition "success or failure"
Jul 31 17:10:48.915: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-configmaps-21df7d40-47ff-4e47-a520-4847a012fa3a container configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 17:10:48.941: INFO: Waiting for pod pod-configmaps-21df7d40-47ff-4e47-a520-4847a012fa3a to disappear
Jul 31 17:10:48.945: INFO: Pod pod-configmaps-21df7d40-47ff-4e47-a520-4847a012fa3a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:10:48.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-306" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":158,"skipped":2643,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:10:48.966: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-706
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1754
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 31 17:10:49.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-706'
Jul 31 17:10:49.199: INFO: stderr: ""
Jul 31 17:10:49.199: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1759
Jul 31 17:10:49.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 delete pods e2e-test-httpd-pod --namespace=kubectl-706'
Jul 31 17:10:53.254: INFO: stderr: ""
Jul 31 17:10:53.254: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:10:53.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-706" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":280,"completed":159,"skipped":2643,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:10:53.268: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7266
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1681
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 31 17:10:53.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-7266'
Jul 31 17:10:53.485: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 31 17:10:53.485: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
Jul 31 17:10:53.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 delete jobs e2e-test-httpd-job --namespace=kubectl-7266'
Jul 31 17:10:53.571: INFO: stderr: ""
Jul 31 17:10:53.571: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:10:53.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7266" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run job should create a job from an image when restart is OnFailure  [Conformance]","total":280,"completed":160,"skipped":2649,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:10:53.588: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1440
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-58f17d01-ca55-4083-a0c0-07a1f72ddf0d
STEP: Creating a pod to test consume configMaps
Jul 31 17:10:53.818: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-603a9dc1-def5-4daa-80ff-df59ea04d26a" in namespace "projected-1440" to be "success or failure"
Jul 31 17:10:53.826: INFO: Pod "pod-projected-configmaps-603a9dc1-def5-4daa-80ff-df59ea04d26a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.750525ms
Jul 31 17:10:55.830: INFO: Pod "pod-projected-configmaps-603a9dc1-def5-4daa-80ff-df59ea04d26a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011314763s
Jul 31 17:10:57.833: INFO: Pod "pod-projected-configmaps-603a9dc1-def5-4daa-80ff-df59ea04d26a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014335095s
STEP: Saw pod success
Jul 31 17:10:57.833: INFO: Pod "pod-projected-configmaps-603a9dc1-def5-4daa-80ff-df59ea04d26a" satisfied condition "success or failure"
Jul 31 17:10:57.836: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-projected-configmaps-603a9dc1-def5-4daa-80ff-df59ea04d26a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 17:10:57.870: INFO: Waiting for pod pod-projected-configmaps-603a9dc1-def5-4daa-80ff-df59ea04d26a to disappear
Jul 31 17:10:57.874: INFO: Pod pod-projected-configmaps-603a9dc1-def5-4daa-80ff-df59ea04d26a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:10:57.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1440" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":161,"skipped":2690,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:10:57.886: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7449
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 31 17:10:58.044: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1c974122-dafc-42cd-957c-1180f027da84" in namespace "downward-api-7449" to be "success or failure"
Jul 31 17:10:58.048: INFO: Pod "downwardapi-volume-1c974122-dafc-42cd-957c-1180f027da84": Phase="Pending", Reason="", readiness=false. Elapsed: 3.678638ms
Jul 31 17:11:00.052: INFO: Pod "downwardapi-volume-1c974122-dafc-42cd-957c-1180f027da84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008024642s
Jul 31 17:11:02.056: INFO: Pod "downwardapi-volume-1c974122-dafc-42cd-957c-1180f027da84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011432885s
STEP: Saw pod success
Jul 31 17:11:02.056: INFO: Pod "downwardapi-volume-1c974122-dafc-42cd-957c-1180f027da84" satisfied condition "success or failure"
Jul 31 17:11:02.058: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod downwardapi-volume-1c974122-dafc-42cd-957c-1180f027da84 container client-container: <nil>
STEP: delete the pod
Jul 31 17:11:02.088: INFO: Waiting for pod downwardapi-volume-1c974122-dafc-42cd-957c-1180f027da84 to disappear
Jul 31 17:11:02.090: INFO: Pod downwardapi-volume-1c974122-dafc-42cd-957c-1180f027da84 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:11:02.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7449" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":162,"skipped":2696,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:11:02.101: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9732
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-508ccf3f-e466-4c4d-af6a-9ccf2dd92196
STEP: Creating a pod to test consume configMaps
Jul 31 17:11:02.271: INFO: Waiting up to 5m0s for pod "pod-configmaps-7eb87c1a-27a0-456f-8711-97f9860df030" in namespace "configmap-9732" to be "success or failure"
Jul 31 17:11:02.287: INFO: Pod "pod-configmaps-7eb87c1a-27a0-456f-8711-97f9860df030": Phase="Pending", Reason="", readiness=false. Elapsed: 15.927004ms
Jul 31 17:11:04.290: INFO: Pod "pod-configmaps-7eb87c1a-27a0-456f-8711-97f9860df030": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018951352s
Jul 31 17:11:06.292: INFO: Pod "pod-configmaps-7eb87c1a-27a0-456f-8711-97f9860df030": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021564015s
STEP: Saw pod success
Jul 31 17:11:06.292: INFO: Pod "pod-configmaps-7eb87c1a-27a0-456f-8711-97f9860df030" satisfied condition "success or failure"
Jul 31 17:11:06.295: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-configmaps-7eb87c1a-27a0-456f-8711-97f9860df030 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 17:11:06.334: INFO: Waiting for pod pod-configmaps-7eb87c1a-27a0-456f-8711-97f9860df030 to disappear
Jul 31 17:11:06.338: INFO: Pod pod-configmaps-7eb87c1a-27a0-456f-8711-97f9860df030 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:11:06.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9732" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":163,"skipped":2704,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:11:06.346: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3104
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jul 31 17:11:11.050: INFO: Successfully updated pod "labelsupdatefc40989e-44f2-408d-9140-16b683906f01"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:11:13.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3104" for this suite.

• [SLOW TEST:6.748 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":164,"skipped":2713,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:11:13.100: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3657
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's args
Jul 31 17:11:13.259: INFO: Waiting up to 5m0s for pod "var-expansion-e88b2921-dafb-4595-9d1e-2c5c3a769e86" in namespace "var-expansion-3657" to be "success or failure"
Jul 31 17:11:13.270: INFO: Pod "var-expansion-e88b2921-dafb-4595-9d1e-2c5c3a769e86": Phase="Pending", Reason="", readiness=false. Elapsed: 11.346875ms
Jul 31 17:11:15.273: INFO: Pod "var-expansion-e88b2921-dafb-4595-9d1e-2c5c3a769e86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014343202s
Jul 31 17:11:17.278: INFO: Pod "var-expansion-e88b2921-dafb-4595-9d1e-2c5c3a769e86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018642539s
STEP: Saw pod success
Jul 31 17:11:17.278: INFO: Pod "var-expansion-e88b2921-dafb-4595-9d1e-2c5c3a769e86" satisfied condition "success or failure"
Jul 31 17:11:17.280: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod var-expansion-e88b2921-dafb-4595-9d1e-2c5c3a769e86 container dapi-container: <nil>
STEP: delete the pod
Jul 31 17:11:17.306: INFO: Waiting for pod var-expansion-e88b2921-dafb-4595-9d1e-2c5c3a769e86 to disappear
Jul 31 17:11:17.320: INFO: Pod var-expansion-e88b2921-dafb-4595-9d1e-2c5c3a769e86 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:11:17.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3657" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":280,"completed":165,"skipped":2762,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:11:17.329: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-553
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-60f5d7b5-2c4b-44da-ba75-a2ca516846d8 in namespace container-probe-553
Jul 31 17:11:21.510: INFO: Started pod liveness-60f5d7b5-2c4b-44da-ba75-a2ca516846d8 in namespace container-probe-553
STEP: checking the pod's current state and verifying that restartCount is present
Jul 31 17:11:21.513: INFO: Initial restart count of pod liveness-60f5d7b5-2c4b-44da-ba75-a2ca516846d8 is 0
Jul 31 17:11:35.543: INFO: Restart count of pod container-probe-553/liveness-60f5d7b5-2c4b-44da-ba75-a2ca516846d8 is now 1 (14.030253519s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:11:35.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-553" for this suite.

• [SLOW TEST:18.253 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":166,"skipped":2776,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:11:35.582: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3855
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 17:11:35.755: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:11:39.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3855" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":280,"completed":167,"skipped":2778,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:11:39.956: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-5849
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9951
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8706
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:12:11.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5849" for this suite.
STEP: Destroying namespace "nsdeletetest-9951" for this suite.
Jul 31 17:12:11.452: INFO: Namespace nsdeletetest-9951 was already deleted
STEP: Destroying namespace "nsdeletetest-8706" for this suite.

• [SLOW TEST:31.500 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":280,"completed":168,"skipped":2782,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:12:11.457: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-264
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-a9698a63-34f2-4bf4-9c11-345663de6b18 in namespace container-probe-264
Jul 31 17:12:15.623: INFO: Started pod busybox-a9698a63-34f2-4bf4-9c11-345663de6b18 in namespace container-probe-264
STEP: checking the pod's current state and verifying that restartCount is present
Jul 31 17:12:15.626: INFO: Initial restart count of pod busybox-a9698a63-34f2-4bf4-9c11-345663de6b18 is 0
Jul 31 17:13:05.713: INFO: Restart count of pod container-probe-264/busybox-a9698a63-34f2-4bf4-9c11-345663de6b18 is now 1 (50.087054161s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:13:05.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-264" for this suite.

• [SLOW TEST:54.294 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":169,"skipped":2794,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:13:05.750: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6011
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-a10791f9-defd-4873-ac6d-d5a20155bb9f
STEP: Creating a pod to test consume configMaps
Jul 31 17:13:05.983: INFO: Waiting up to 5m0s for pod "pod-configmaps-b0bf3b51-03d9-42fb-8ec4-f8f398c35412" in namespace "configmap-6011" to be "success or failure"
Jul 31 17:13:05.986: INFO: Pod "pod-configmaps-b0bf3b51-03d9-42fb-8ec4-f8f398c35412": Phase="Pending", Reason="", readiness=false. Elapsed: 2.974719ms
Jul 31 17:13:07.994: INFO: Pod "pod-configmaps-b0bf3b51-03d9-42fb-8ec4-f8f398c35412": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010859685s
Jul 31 17:13:09.997: INFO: Pod "pod-configmaps-b0bf3b51-03d9-42fb-8ec4-f8f398c35412": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014000221s
STEP: Saw pod success
Jul 31 17:13:09.997: INFO: Pod "pod-configmaps-b0bf3b51-03d9-42fb-8ec4-f8f398c35412" satisfied condition "success or failure"
Jul 31 17:13:09.999: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-configmaps-b0bf3b51-03d9-42fb-8ec4-f8f398c35412 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 17:13:10.026: INFO: Waiting for pod pod-configmaps-b0bf3b51-03d9-42fb-8ec4-f8f398c35412 to disappear
Jul 31 17:13:10.036: INFO: Pod pod-configmaps-b0bf3b51-03d9-42fb-8ec4-f8f398c35412 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:13:10.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6011" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":170,"skipped":2799,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:13:10.045: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-3451
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:13:10.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3451" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":280,"completed":171,"skipped":2808,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:13:10.219: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3790
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap that has name configmap-test-emptyKey-a6da8cb5-3acd-4259-aa3c-adc342290c41
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:13:10.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3790" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":280,"completed":172,"skipped":2810,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:13:10.386: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7386
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 31 17:13:10.548: INFO: Waiting up to 5m0s for pod "pod-74a8994b-e42a-4a3c-9b31-6600e0f1db1e" in namespace "emptydir-7386" to be "success or failure"
Jul 31 17:13:10.559: INFO: Pod "pod-74a8994b-e42a-4a3c-9b31-6600e0f1db1e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.882601ms
Jul 31 17:13:12.563: INFO: Pod "pod-74a8994b-e42a-4a3c-9b31-6600e0f1db1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015677252s
Jul 31 17:13:14.568: INFO: Pod "pod-74a8994b-e42a-4a3c-9b31-6600e0f1db1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02059208s
STEP: Saw pod success
Jul 31 17:13:14.568: INFO: Pod "pod-74a8994b-e42a-4a3c-9b31-6600e0f1db1e" satisfied condition "success or failure"
Jul 31 17:13:14.575: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-74a8994b-e42a-4a3c-9b31-6600e0f1db1e container test-container: <nil>
STEP: delete the pod
Jul 31 17:13:14.642: INFO: Waiting for pod pod-74a8994b-e42a-4a3c-9b31-6600e0f1db1e to disappear
Jul 31 17:13:14.646: INFO: Pod pod-74a8994b-e42a-4a3c-9b31-6600e0f1db1e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:13:14.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7386" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":173,"skipped":2839,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:13:14.674: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-2394
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9399
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2161
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:13:21.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2394" for this suite.
STEP: Destroying namespace "nsdeletetest-9399" for this suite.
Jul 31 17:13:21.266: INFO: Namespace nsdeletetest-9399 was already deleted
STEP: Destroying namespace "nsdeletetest-2161" for this suite.

• [SLOW TEST:6.597 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":280,"completed":174,"skipped":2867,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:13:21.273: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7938
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-af881a10-c68c-4ca6-a9e1-3edebc0f5bbd
STEP: Creating a pod to test consume secrets
Jul 31 17:13:21.439: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-21270be8-251c-4524-b63c-84470260ad8e" in namespace "projected-7938" to be "success or failure"
Jul 31 17:13:21.445: INFO: Pod "pod-projected-secrets-21270be8-251c-4524-b63c-84470260ad8e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.420311ms
Jul 31 17:13:23.448: INFO: Pod "pod-projected-secrets-21270be8-251c-4524-b63c-84470260ad8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008858889s
Jul 31 17:13:25.451: INFO: Pod "pod-projected-secrets-21270be8-251c-4524-b63c-84470260ad8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011934923s
STEP: Saw pod success
Jul 31 17:13:25.451: INFO: Pod "pod-projected-secrets-21270be8-251c-4524-b63c-84470260ad8e" satisfied condition "success or failure"
Jul 31 17:13:25.453: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-projected-secrets-21270be8-251c-4524-b63c-84470260ad8e container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 31 17:13:25.479: INFO: Waiting for pod pod-projected-secrets-21270be8-251c-4524-b63c-84470260ad8e to disappear
Jul 31 17:13:25.484: INFO: Pod pod-projected-secrets-21270be8-251c-4524-b63c-84470260ad8e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:13:25.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7938" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":175,"skipped":2900,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:13:25.501: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1391
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 17:13:25.649: INFO: Creating deployment "webserver-deployment"
Jul 31 17:13:25.652: INFO: Waiting for observed generation 1
Jul 31 17:13:27.660: INFO: Waiting for all required pods to come up
Jul 31 17:13:27.663: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jul 31 17:13:31.695: INFO: Waiting for deployment "webserver-deployment" to complete
Jul 31 17:13:31.699: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jul 31 17:13:31.708: INFO: Updating deployment webserver-deployment
Jul 31 17:13:31.708: INFO: Waiting for observed generation 2
Jul 31 17:13:33.717: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jul 31 17:13:33.724: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jul 31 17:13:33.728: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jul 31 17:13:33.744: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jul 31 17:13:33.744: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jul 31 17:13:33.747: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jul 31 17:13:33.762: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jul 31 17:13:33.763: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jul 31 17:13:33.774: INFO: Updating deployment webserver-deployment
Jul 31 17:13:33.774: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jul 31 17:13:33.859: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jul 31 17:13:35.901: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jul 31 17:13:35.910: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-1391 /apis/apps/v1/namespaces/deployment-1391/deployments/webserver-deployment e8dc3075-1a07-4509-becd-0266d2e7a50c 229936 3 2020-07-31 17:13:25 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006780418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-07-31 17:13:33 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-07-31 17:13:33 +0000 UTC,LastTransitionTime:2020-07-31 17:13:25 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jul 31 17:13:35.913: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-1391 /apis/apps/v1/namespaces/deployment-1391/replicasets/webserver-deployment-c7997dcc8 dcc58131-5941-4e74-85fd-7b78ad189b6d 229931 3 2020-07-31 17:13:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment e8dc3075-1a07-4509-becd-0266d2e7a50c 0xc0067575e7 0xc0067575e8}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006757658 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 31 17:13:35.913: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jul 31 17:13:35.913: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-1391 /apis/apps/v1/namespaces/deployment-1391/replicasets/webserver-deployment-595b5b9587 265ef31e-decd-435b-a1b5-a5db49a9b890 229920 3 2020-07-31 17:13:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment e8dc3075-1a07-4509-becd-0266d2e7a50c 0xc0067574f7 0xc0067574f8}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006757568 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jul 31 17:13:35.918: INFO: Pod "webserver-deployment-595b5b9587-4k8sf" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-4k8sf webserver-deployment-595b5b9587- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-595b5b9587-4k8sf 57322495-b6fe-458d-a454-1abc8ef8dfd8 230002 0 2020-07-31 17:13:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.55.114/32 cni.projectcalico.org/podIPs:192.168.55.114/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 265ef31e-decd-435b-a1b5-a5db49a9b890 0xc006757b97 0xc006757b98}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-a92c781fbd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.93,PodIP:,StartTime:2020-07-31 17:13:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.918: INFO: Pod "webserver-deployment-595b5b9587-5tvzh" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-5tvzh webserver-deployment-595b5b9587- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-595b5b9587-5tvzh 526ee1fc-8744-44e3-a609-f3b41a4918d1 229925 0 2020-07-31 17:13:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 265ef31e-decd-435b-a1b5-a5db49a9b890 0xc006757cf7 0xc006757cf8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-687e2c91ce,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.92,PodIP:,StartTime:2020-07-31 17:13:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.919: INFO: Pod "webserver-deployment-595b5b9587-bpmr5" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bpmr5 webserver-deployment-595b5b9587- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-595b5b9587-bpmr5 300b202c-decc-4493-b607-fe627be59937 229962 0 2020-07-31 17:13:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 265ef31e-decd-435b-a1b5-a5db49a9b890 0xc006757e47 0xc006757e48}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-a8f122b701,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.94,PodIP:,StartTime:2020-07-31 17:13:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.919: INFO: Pod "webserver-deployment-595b5b9587-bttw6" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bttw6 webserver-deployment-595b5b9587- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-595b5b9587-bttw6 fa3a848e-e97b-4fe9-8abf-25f9c7fc882f 229941 0 2020-07-31 17:13:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 265ef31e-decd-435b-a1b5-a5db49a9b890 0xc006757f97 0xc006757f98}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-a8f122b701,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.94,PodIP:,StartTime:2020-07-31 17:13:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.919: INFO: Pod "webserver-deployment-595b5b9587-bvtfr" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bvtfr webserver-deployment-595b5b9587- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-595b5b9587-bvtfr 32a26b28-bdda-4a38-9513-0e4413a4a25b 229976 0 2020-07-31 17:13:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 265ef31e-decd-435b-a1b5-a5db49a9b890 0xc0067a40e7 0xc0067a40e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-a8f122b701,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.94,PodIP:,StartTime:2020-07-31 17:13:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.919: INFO: Pod "webserver-deployment-595b5b9587-ddqxb" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-ddqxb webserver-deployment-595b5b9587- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-595b5b9587-ddqxb 10794bdf-c36e-449a-a88b-07ca8e10558e 229723 0 2020-07-31 17:13:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.55.109/32 cni.projectcalico.org/podIPs:192.168.55.109/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 265ef31e-decd-435b-a1b5-a5db49a9b890 0xc0067a4257 0xc0067a4258}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-a92c781fbd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.93,PodIP:192.168.55.109,StartTime:2020-07-31 17:13:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-31 17:13:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://f5ca0a94f723ad395674bf28f171a5631e020c1a513dcd6263f6a570bd6d599a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.55.109,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.919: INFO: Pod "webserver-deployment-595b5b9587-fdsn5" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-fdsn5 webserver-deployment-595b5b9587- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-595b5b9587-fdsn5 b22548c7-415d-4356-8df2-ac22f16b79a1 229751 0 2020-07-31 17:13:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.220.79/32 cni.projectcalico.org/podIPs:192.168.220.79/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 265ef31e-decd-435b-a1b5-a5db49a9b890 0xc0067a43c7 0xc0067a43c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-687e2c91ce,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.92,PodIP:192.168.220.79,StartTime:2020-07-31 17:13:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-31 17:13:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://fecbb764ff875f68e0ab98b186e2022877ef684b6a70c01c2ac7881d7c45ec4c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.220.79,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.919: INFO: Pod "webserver-deployment-595b5b9587-h6q9s" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-h6q9s webserver-deployment-595b5b9587- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-595b5b9587-h6q9s 2a508508-80ca-4aa6-8ed9-46ec9f41fbd3 229939 0 2020-07-31 17:13:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 265ef31e-decd-435b-a1b5-a5db49a9b890 0xc0067a4537 0xc0067a4538}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-687e2c91ce,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.92,PodIP:,StartTime:2020-07-31 17:13:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.920: INFO: Pod "webserver-deployment-595b5b9587-hz7jr" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-hz7jr webserver-deployment-595b5b9587- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-595b5b9587-hz7jr d278bc4d-82b2-4798-af40-e18ec16ee139 229761 0 2020-07-31 17:13:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.86.112/32 cni.projectcalico.org/podIPs:192.168.86.112/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 265ef31e-decd-435b-a1b5-a5db49a9b890 0xc0067a4687 0xc0067a4688}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-a8f122b701,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.94,PodIP:192.168.86.112,StartTime:2020-07-31 17:13:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-31 17:13:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://098be86c107a261ee4a38634b189c74906be42a280dfcdfc5f4d19e3667252d1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.86.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.920: INFO: Pod "webserver-deployment-595b5b9587-ll2sh" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-ll2sh webserver-deployment-595b5b9587- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-595b5b9587-ll2sh bfc0f402-d80e-48dd-9b6f-7badfb345d6b 229715 0 2020-07-31 17:13:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.55.111/32 cni.projectcalico.org/podIPs:192.168.55.111/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 265ef31e-decd-435b-a1b5-a5db49a9b890 0xc0067a47f7 0xc0067a47f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-a92c781fbd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.93,PodIP:192.168.55.111,StartTime:2020-07-31 17:13:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-31 17:13:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://673bd5a7c848015a8f5cde217d8ef8482aa217deb3ec666d09680ba90a69e653,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.55.111,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.920: INFO: Pod "webserver-deployment-595b5b9587-mzdpl" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mzdpl webserver-deployment-595b5b9587- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-595b5b9587-mzdpl cf6f780d-8d43-45f6-8baa-11d0e63b1cd1 229987 0 2020-07-31 17:13:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 265ef31e-decd-435b-a1b5-a5db49a9b890 0xc0067a4967 0xc0067a4968}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-687e2c91ce,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.92,PodIP:,StartTime:2020-07-31 17:13:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.920: INFO: Pod "webserver-deployment-595b5b9587-n6jbz" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-n6jbz webserver-deployment-595b5b9587- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-595b5b9587-n6jbz b6eec8bf-201e-451b-95d9-d4c4318c951d 229949 0 2020-07-31 17:13:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 265ef31e-decd-435b-a1b5-a5db49a9b890 0xc0067a4ab7 0xc0067a4ab8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-687e2c91ce,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.92,PodIP:,StartTime:2020-07-31 17:13:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.920: INFO: Pod "webserver-deployment-595b5b9587-p8b4x" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-p8b4x webserver-deployment-595b5b9587- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-595b5b9587-p8b4x 4a69e8ce-e851-4b0c-ac0c-db43286d934a 229766 0 2020-07-31 17:13:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.86.111/32 cni.projectcalico.org/podIPs:192.168.86.111/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 265ef31e-decd-435b-a1b5-a5db49a9b890 0xc0067a4c07 0xc0067a4c08}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-a8f122b701,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.94,PodIP:192.168.86.111,StartTime:2020-07-31 17:13:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-31 17:13:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://4ece9b3844c8e0d601d7206991fa9c9d05995ef57b05e3b5423d24ac919d93cb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.86.111,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.920: INFO: Pod "webserver-deployment-595b5b9587-q82bx" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-q82bx webserver-deployment-595b5b9587- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-595b5b9587-q82bx 1682e9f9-bdee-4d72-a2c6-84ebf2b7862a 229719 0 2020-07-31 17:13:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.55.110/32 cni.projectcalico.org/podIPs:192.168.55.110/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 265ef31e-decd-435b-a1b5-a5db49a9b890 0xc0067a4f37 0xc0067a4f38}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-a92c781fbd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.93,PodIP:192.168.55.110,StartTime:2020-07-31 17:13:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-31 17:13:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://1990012ef981f051837d5da615e580a4f5e406dd09f0c27e3e1fb4cef6e381eb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.55.110,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.920: INFO: Pod "webserver-deployment-595b5b9587-qd78h" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qd78h webserver-deployment-595b5b9587- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-595b5b9587-qd78h ecc64d6a-4173-4386-8fe9-86a8b95a39da 229953 0 2020-07-31 17:13:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 265ef31e-decd-435b-a1b5-a5db49a9b890 0xc0067a50a7 0xc0067a50a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-a92c781fbd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.93,PodIP:,StartTime:2020-07-31 17:13:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.920: INFO: Pod "webserver-deployment-595b5b9587-rk2gd" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rk2gd webserver-deployment-595b5b9587- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-595b5b9587-rk2gd 9332a75d-b65e-4ed7-b729-12afee1a6c5d 229971 0 2020-07-31 17:13:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 265ef31e-decd-435b-a1b5-a5db49a9b890 0xc0067a51f7 0xc0067a51f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-a8f122b701,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.94,PodIP:,StartTime:2020-07-31 17:13:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.921: INFO: Pod "webserver-deployment-595b5b9587-rmcnx" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rmcnx webserver-deployment-595b5b9587- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-595b5b9587-rmcnx 5e5541d5-6d9d-427c-94b0-08cf460619e2 229906 0 2020-07-31 17:13:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 265ef31e-decd-435b-a1b5-a5db49a9b890 0xc0067a5347 0xc0067a5348}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-687e2c91ce,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.92,PodIP:,StartTime:2020-07-31 17:13:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.921: INFO: Pod "webserver-deployment-595b5b9587-trwpm" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-trwpm webserver-deployment-595b5b9587- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-595b5b9587-trwpm 46ce13a3-fb36-44b7-8ccd-1c6dd99f4b81 229757 0 2020-07-31 17:13:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.220.78/32 cni.projectcalico.org/podIPs:192.168.220.78/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 265ef31e-decd-435b-a1b5-a5db49a9b890 0xc0067a5497 0xc0067a5498}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-687e2c91ce,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.92,PodIP:192.168.220.78,StartTime:2020-07-31 17:13:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-31 17:13:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://5ae9c3b93a0135595aa5c170c1d25ba6fabd4e393ee582f5ce8412e1be9a2e3f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.220.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.921: INFO: Pod "webserver-deployment-595b5b9587-vp6gx" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-vp6gx webserver-deployment-595b5b9587- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-595b5b9587-vp6gx 26945a65-55b9-4d3d-84fb-3b7d911daf2f 229764 0 2020-07-31 17:13:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.86.113/32 cni.projectcalico.org/podIPs:192.168.86.113/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 265ef31e-decd-435b-a1b5-a5db49a9b890 0xc0067a5607 0xc0067a5608}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-a8f122b701,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.94,PodIP:192.168.86.113,StartTime:2020-07-31 17:13:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-31 17:13:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://b144a144d6ec3d596700111da47002049e575671a2af55be8d87e0a5170c3b56,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.86.113,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.921: INFO: Pod "webserver-deployment-595b5b9587-w8jrp" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-w8jrp webserver-deployment-595b5b9587- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-595b5b9587-w8jrp 7c11fce7-773f-46fd-9891-07bf77ef5887 229938 0 2020-07-31 17:13:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 265ef31e-decd-435b-a1b5-a5db49a9b890 0xc0067a5777 0xc0067a5778}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-a92c781fbd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.93,PodIP:,StartTime:2020-07-31 17:13:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.921: INFO: Pod "webserver-deployment-c7997dcc8-5txjj" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-5txjj webserver-deployment-c7997dcc8- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-c7997dcc8-5txjj b09d5d63-b6eb-41c0-b56f-27c86f12c1a1 229998 0 2020-07-31 17:13:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.86.115/32 cni.projectcalico.org/podIPs:192.168.86.115/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 dcc58131-5941-4e74-85fd-7b78ad189b6d 0xc0067a58c7 0xc0067a58c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-a8f122b701,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.94,PodIP:,StartTime:2020-07-31 17:13:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.921: INFO: Pod "webserver-deployment-c7997dcc8-7gd6x" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-7gd6x webserver-deployment-c7997dcc8- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-c7997dcc8-7gd6x 790ca026-8b0e-45d4-a6ed-09b191a6d33c 229988 0 2020-07-31 17:13:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 dcc58131-5941-4e74-85fd-7b78ad189b6d 0xc0067a5a37 0xc0067a5a38}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-a8f122b701,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.94,PodIP:,StartTime:2020-07-31 17:13:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.922: INFO: Pod "webserver-deployment-c7997dcc8-84zd6" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-84zd6 webserver-deployment-c7997dcc8- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-c7997dcc8-84zd6 d487fbe6-1ff2-4f01-aac3-7b9f23ec125f 229870 0 2020-07-31 17:13:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.220.81/32 cni.projectcalico.org/podIPs:192.168.220.81/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 dcc58131-5941-4e74-85fd-7b78ad189b6d 0xc0067a5ba7 0xc0067a5ba8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-687e2c91ce,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.92,PodIP:,StartTime:2020-07-31 17:13:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.922: INFO: Pod "webserver-deployment-c7997dcc8-8hkkq" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-8hkkq webserver-deployment-c7997dcc8- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-c7997dcc8-8hkkq 9639882f-836e-40b6-a7f5-82d1d60a7dc3 229946 0 2020-07-31 17:13:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 dcc58131-5941-4e74-85fd-7b78ad189b6d 0xc0067a5d17 0xc0067a5d18}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-a92c781fbd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.93,PodIP:,StartTime:2020-07-31 17:13:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.922: INFO: Pod "webserver-deployment-c7997dcc8-8zht8" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-8zht8 webserver-deployment-c7997dcc8- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-c7997dcc8-8zht8 22e90edb-ed42-494c-82dc-c371873105d7 229852 0 2020-07-31 17:13:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.86.114/32 cni.projectcalico.org/podIPs:192.168.86.114/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 dcc58131-5941-4e74-85fd-7b78ad189b6d 0xc0067a5e87 0xc0067a5e88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-a8f122b701,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.94,PodIP:,StartTime:2020-07-31 17:13:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.922: INFO: Pod "webserver-deployment-c7997dcc8-94fjt" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-94fjt webserver-deployment-c7997dcc8- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-c7997dcc8-94fjt d2695435-dc06-4c85-b4c5-f7d8de1fc96c 229958 0 2020-07-31 17:13:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 dcc58131-5941-4e74-85fd-7b78ad189b6d 0xc0067a5ff7 0xc0067a5ff8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-a92c781fbd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.93,PodIP:,StartTime:2020-07-31 17:13:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.923: INFO: Pod "webserver-deployment-c7997dcc8-cg27h" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-cg27h webserver-deployment-c7997dcc8- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-c7997dcc8-cg27h d80af38f-b7b9-4be8-8752-c635c62123ff 229967 0 2020-07-31 17:13:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 dcc58131-5941-4e74-85fd-7b78ad189b6d 0xc00681e167 0xc00681e168}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-687e2c91ce,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.92,PodIP:,StartTime:2020-07-31 17:13:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.923: INFO: Pod "webserver-deployment-c7997dcc8-d9qxc" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-d9qxc webserver-deployment-c7997dcc8- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-c7997dcc8-d9qxc c7b9ff25-2001-41b8-b641-2769289d7061 229961 0 2020-07-31 17:13:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 dcc58131-5941-4e74-85fd-7b78ad189b6d 0xc00681e2d7 0xc00681e2d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-687e2c91ce,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.92,PodIP:,StartTime:2020-07-31 17:13:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.923: INFO: Pod "webserver-deployment-c7997dcc8-nt4tr" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-nt4tr webserver-deployment-c7997dcc8- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-c7997dcc8-nt4tr 3a09d0fc-13c5-4790-93eb-60faa89e8f38 229858 0 2020-07-31 17:13:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.55.113/32 cni.projectcalico.org/podIPs:192.168.55.113/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 dcc58131-5941-4e74-85fd-7b78ad189b6d 0xc00681e447 0xc00681e448}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-a92c781fbd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.93,PodIP:,StartTime:2020-07-31 17:13:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.923: INFO: Pod "webserver-deployment-c7997dcc8-p4v4j" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-p4v4j webserver-deployment-c7997dcc8- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-c7997dcc8-p4v4j 07f4f974-bc08-44ed-9208-ae9101177de7 229951 0 2020-07-31 17:13:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 dcc58131-5941-4e74-85fd-7b78ad189b6d 0xc00681e5b7 0xc00681e5b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-a8f122b701,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.94,PodIP:,StartTime:2020-07-31 17:13:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.923: INFO: Pod "webserver-deployment-c7997dcc8-qhtmb" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-qhtmb webserver-deployment-c7997dcc8- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-c7997dcc8-qhtmb 112e18dc-fe27-4dc4-a179-c2256b4a9d0b 229993 0 2020-07-31 17:13:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 dcc58131-5941-4e74-85fd-7b78ad189b6d 0xc00681e727 0xc00681e728}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-687e2c91ce,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.92,PodIP:,StartTime:2020-07-31 17:13:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.923: INFO: Pod "webserver-deployment-c7997dcc8-qls9h" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-qls9h webserver-deployment-c7997dcc8- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-c7997dcc8-qls9h 59437a26-1f18-4853-9f21-deceb74c7668 229937 0 2020-07-31 17:13:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.220.82/32 cni.projectcalico.org/podIPs:192.168.220.82/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 dcc58131-5941-4e74-85fd-7b78ad189b6d 0xc00681e897 0xc00681e898}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-687e2c91ce,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.92,PodIP:,StartTime:2020-07-31 17:13:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 17:13:35.923: INFO: Pod "webserver-deployment-c7997dcc8-vfvdq" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-vfvdq webserver-deployment-c7997dcc8- deployment-1391 /api/v1/namespaces/deployment-1391/pods/webserver-deployment-c7997dcc8-vfvdq cc4fac54-83e6-4f46-b263-680279429a18 229853 0 2020-07-31 17:13:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.55.112/32 cni.projectcalico.org/podIPs:192.168.55.112/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 dcc58131-5941-4e74-85fd-7b78ad189b6d 0xc00681ea07 0xc00681ea08}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h7v5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h7v5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h7v5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-aruna-123-node-group-a92c781fbd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-31 17:13:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.93,PodIP:,StartTime:2020-07-31 17:13:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:13:35.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1391" for this suite.

• [SLOW TEST:10.449 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":280,"completed":176,"skipped":2946,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:13:35.950: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6648
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:46
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Jul 31 17:13:42.155: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-776750169 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jul 31 17:13:57.373: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:13:57.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6648" for this suite.

• [SLOW TEST:21.439 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should be submitted and removed [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period should be submitted and removed [Conformance]","total":280,"completed":177,"skipped":2954,"failed":0}
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:13:57.396: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-792
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-792.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-792.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-792.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-792.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-792.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-792.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-792.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-792.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-792.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-792.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 31 17:14:01.623: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:01.628: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:01.630: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:01.633: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:01.644: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:01.648: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:01.651: INFO: Unable to read jessie_udp@dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:01.654: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:01.663: INFO: Lookups using dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local wheezy_udp@dns-test-service-2.dns-792.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-792.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local jessie_udp@dns-test-service-2.dns-792.svc.cluster.local jessie_tcp@dns-test-service-2.dns-792.svc.cluster.local]

Jul 31 17:14:06.669: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:06.674: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:06.678: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:06.683: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:06.692: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:06.695: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:06.698: INFO: Unable to read jessie_udp@dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:06.701: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:06.707: INFO: Lookups using dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local wheezy_udp@dns-test-service-2.dns-792.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-792.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local jessie_udp@dns-test-service-2.dns-792.svc.cluster.local jessie_tcp@dns-test-service-2.dns-792.svc.cluster.local]

Jul 31 17:14:11.667: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:11.673: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:11.680: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:11.684: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:11.702: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:11.707: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:11.710: INFO: Unable to read jessie_udp@dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:11.725: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:11.736: INFO: Lookups using dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local wheezy_udp@dns-test-service-2.dns-792.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-792.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local jessie_udp@dns-test-service-2.dns-792.svc.cluster.local jessie_tcp@dns-test-service-2.dns-792.svc.cluster.local]

Jul 31 17:14:16.672: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:16.685: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:16.689: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:16.692: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:16.703: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:16.706: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:16.709: INFO: Unable to read jessie_udp@dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:16.712: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:16.717: INFO: Lookups using dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local wheezy_udp@dns-test-service-2.dns-792.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-792.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local jessie_udp@dns-test-service-2.dns-792.svc.cluster.local jessie_tcp@dns-test-service-2.dns-792.svc.cluster.local]

Jul 31 17:14:21.670: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:21.675: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:21.680: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:21.684: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:21.702: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:21.706: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:21.708: INFO: Unable to read jessie_udp@dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:21.711: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:21.716: INFO: Lookups using dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local wheezy_udp@dns-test-service-2.dns-792.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-792.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local jessie_udp@dns-test-service-2.dns-792.svc.cluster.local jessie_tcp@dns-test-service-2.dns-792.svc.cluster.local]

Jul 31 17:14:26.667: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:26.671: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:26.674: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:26.677: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:26.688: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:26.690: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:26.692: INFO: Unable to read jessie_udp@dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:26.695: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-792.svc.cluster.local from pod dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc: the server could not find the requested resource (get pods dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc)
Jul 31 17:14:26.699: INFO: Lookups using dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local wheezy_udp@dns-test-service-2.dns-792.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-792.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-792.svc.cluster.local jessie_udp@dns-test-service-2.dns-792.svc.cluster.local jessie_tcp@dns-test-service-2.dns-792.svc.cluster.local]

Jul 31 17:14:31.710: INFO: DNS probes using dns-792/dns-test-c094b0e8-1ae5-4f92-a197-05f0badf0ccc succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:14:31.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-792" for this suite.

• [SLOW TEST:34.414 seconds]
[sig-network] DNS
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":280,"completed":178,"skipped":2954,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:14:31.804: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6829
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jul 31 17:14:31.953: INFO: Waiting up to 5m0s for pod "downward-api-2cf4f8a0-3b08-488b-a761-c3b5d3c1db86" in namespace "downward-api-6829" to be "success or failure"
Jul 31 17:14:31.963: INFO: Pod "downward-api-2cf4f8a0-3b08-488b-a761-c3b5d3c1db86": Phase="Pending", Reason="", readiness=false. Elapsed: 10.193885ms
Jul 31 17:14:33.967: INFO: Pod "downward-api-2cf4f8a0-3b08-488b-a761-c3b5d3c1db86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014133684s
Jul 31 17:14:35.971: INFO: Pod "downward-api-2cf4f8a0-3b08-488b-a761-c3b5d3c1db86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017577587s
STEP: Saw pod success
Jul 31 17:14:35.971: INFO: Pod "downward-api-2cf4f8a0-3b08-488b-a761-c3b5d3c1db86" satisfied condition "success or failure"
Jul 31 17:14:35.973: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod downward-api-2cf4f8a0-3b08-488b-a761-c3b5d3c1db86 container dapi-container: <nil>
STEP: delete the pod
Jul 31 17:14:36.006: INFO: Waiting for pod downward-api-2cf4f8a0-3b08-488b-a761-c3b5d3c1db86 to disappear
Jul 31 17:14:36.008: INFO: Pod downward-api-2cf4f8a0-3b08-488b-a761-c3b5d3c1db86 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:14:36.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6829" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":280,"completed":179,"skipped":2969,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:14:36.021: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1203
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 17:14:36.174: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jul 31 17:14:39.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 --namespace=crd-publish-openapi-1203 create -f -'
Jul 31 17:14:40.815: INFO: stderr: ""
Jul 31 17:14:40.815: INFO: stdout: "e2e-test-crd-publish-openapi-621-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jul 31 17:14:40.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 --namespace=crd-publish-openapi-1203 delete e2e-test-crd-publish-openapi-621-crds test-foo'
Jul 31 17:14:40.919: INFO: stderr: ""
Jul 31 17:14:40.919: INFO: stdout: "e2e-test-crd-publish-openapi-621-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jul 31 17:14:40.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 --namespace=crd-publish-openapi-1203 apply -f -'
Jul 31 17:14:41.117: INFO: stderr: ""
Jul 31 17:14:41.117: INFO: stdout: "e2e-test-crd-publish-openapi-621-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jul 31 17:14:41.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 --namespace=crd-publish-openapi-1203 delete e2e-test-crd-publish-openapi-621-crds test-foo'
Jul 31 17:14:41.213: INFO: stderr: ""
Jul 31 17:14:41.213: INFO: stdout: "e2e-test-crd-publish-openapi-621-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jul 31 17:14:41.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 --namespace=crd-publish-openapi-1203 create -f -'
Jul 31 17:14:41.386: INFO: rc: 1
Jul 31 17:14:41.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 --namespace=crd-publish-openapi-1203 apply -f -'
Jul 31 17:14:41.578: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jul 31 17:14:41.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 --namespace=crd-publish-openapi-1203 create -f -'
Jul 31 17:14:41.754: INFO: rc: 1
Jul 31 17:14:41.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 --namespace=crd-publish-openapi-1203 apply -f -'
Jul 31 17:14:41.952: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jul 31 17:14:41.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 explain e2e-test-crd-publish-openapi-621-crds'
Jul 31 17:14:42.191: INFO: stderr: ""
Jul 31 17:14:42.191: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-621-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jul 31 17:14:42.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 explain e2e-test-crd-publish-openapi-621-crds.metadata'
Jul 31 17:14:42.390: INFO: stderr: ""
Jul 31 17:14:42.390: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-621-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jul 31 17:14:42.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 explain e2e-test-crd-publish-openapi-621-crds.spec'
Jul 31 17:14:42.596: INFO: stderr: ""
Jul 31 17:14:42.596: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-621-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jul 31 17:14:42.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 explain e2e-test-crd-publish-openapi-621-crds.spec.bars'
Jul 31 17:14:42.789: INFO: stderr: ""
Jul 31 17:14:42.789: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-621-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jul 31 17:14:42.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 explain e2e-test-crd-publish-openapi-621-crds.spec.bars2'
Jul 31 17:14:42.966: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:14:46.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1203" for this suite.

• [SLOW TEST:10.481 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":280,"completed":180,"skipped":2994,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:14:46.506: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5972
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 31 17:14:46.655: INFO: Waiting up to 5m0s for pod "pod-f84938c3-1a64-4e76-81d0-9818585e1339" in namespace "emptydir-5972" to be "success or failure"
Jul 31 17:14:46.670: INFO: Pod "pod-f84938c3-1a64-4e76-81d0-9818585e1339": Phase="Pending", Reason="", readiness=false. Elapsed: 14.917722ms
Jul 31 17:14:48.673: INFO: Pod "pod-f84938c3-1a64-4e76-81d0-9818585e1339": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018096819s
Jul 31 17:14:50.676: INFO: Pod "pod-f84938c3-1a64-4e76-81d0-9818585e1339": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021251626s
STEP: Saw pod success
Jul 31 17:14:50.676: INFO: Pod "pod-f84938c3-1a64-4e76-81d0-9818585e1339" satisfied condition "success or failure"
Jul 31 17:14:50.678: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-f84938c3-1a64-4e76-81d0-9818585e1339 container test-container: <nil>
STEP: delete the pod
Jul 31 17:14:50.715: INFO: Waiting for pod pod-f84938c3-1a64-4e76-81d0-9818585e1339 to disappear
Jul 31 17:14:50.720: INFO: Pod pod-f84938c3-1a64-4e76-81d0-9818585e1339 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:14:50.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5972" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":181,"skipped":3002,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:14:50.741: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9935
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's command
Jul 31 17:14:50.892: INFO: Waiting up to 5m0s for pod "var-expansion-5fc19aaa-84cc-44e1-884f-d613e5c6722e" in namespace "var-expansion-9935" to be "success or failure"
Jul 31 17:14:50.900: INFO: Pod "var-expansion-5fc19aaa-84cc-44e1-884f-d613e5c6722e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.462823ms
Jul 31 17:14:52.904: INFO: Pod "var-expansion-5fc19aaa-84cc-44e1-884f-d613e5c6722e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011958229s
Jul 31 17:14:54.907: INFO: Pod "var-expansion-5fc19aaa-84cc-44e1-884f-d613e5c6722e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014913609s
STEP: Saw pod success
Jul 31 17:14:54.907: INFO: Pod "var-expansion-5fc19aaa-84cc-44e1-884f-d613e5c6722e" satisfied condition "success or failure"
Jul 31 17:14:54.909: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod var-expansion-5fc19aaa-84cc-44e1-884f-d613e5c6722e container dapi-container: <nil>
STEP: delete the pod
Jul 31 17:14:54.943: INFO: Waiting for pod var-expansion-5fc19aaa-84cc-44e1-884f-d613e5c6722e to disappear
Jul 31 17:14:54.948: INFO: Pod var-expansion-5fc19aaa-84cc-44e1-884f-d613e5c6722e no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:14:54.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9935" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":280,"completed":182,"skipped":3014,"failed":0}
SSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:14:54.959: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3490
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-3490/configmap-test-9d3439ea-ae5a-4bcc-8416-1f728751101a
STEP: Creating a pod to test consume configMaps
Jul 31 17:14:55.146: INFO: Waiting up to 5m0s for pod "pod-configmaps-5b2f1358-a8cf-41fd-8858-ff1963af1b17" in namespace "configmap-3490" to be "success or failure"
Jul 31 17:14:55.168: INFO: Pod "pod-configmaps-5b2f1358-a8cf-41fd-8858-ff1963af1b17": Phase="Pending", Reason="", readiness=false. Elapsed: 22.220415ms
Jul 31 17:14:57.171: INFO: Pod "pod-configmaps-5b2f1358-a8cf-41fd-8858-ff1963af1b17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025457281s
Jul 31 17:14:59.177: INFO: Pod "pod-configmaps-5b2f1358-a8cf-41fd-8858-ff1963af1b17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031103286s
STEP: Saw pod success
Jul 31 17:14:59.177: INFO: Pod "pod-configmaps-5b2f1358-a8cf-41fd-8858-ff1963af1b17" satisfied condition "success or failure"
Jul 31 17:14:59.179: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-configmaps-5b2f1358-a8cf-41fd-8858-ff1963af1b17 container env-test: <nil>
STEP: delete the pod
Jul 31 17:14:59.201: INFO: Waiting for pod pod-configmaps-5b2f1358-a8cf-41fd-8858-ff1963af1b17 to disappear
Jul 31 17:14:59.219: INFO: Pod pod-configmaps-5b2f1358-a8cf-41fd-8858-ff1963af1b17 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:14:59.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3490" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":280,"completed":183,"skipped":3018,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:14:59.230: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-2529
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jul 31 17:14:59.748: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jul 31 17:15:01.763: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812499, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812499, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812499, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812499, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 17:15:04.788: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 17:15:04.790: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:15:06.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2529" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

• [SLOW TEST:6.944 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":280,"completed":184,"skipped":3050,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:15:06.174: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6231
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-30fd0a8b-1ab7-4211-a6e9-d3c388062f5b in namespace container-probe-6231
Jul 31 17:15:10.358: INFO: Started pod liveness-30fd0a8b-1ab7-4211-a6e9-d3c388062f5b in namespace container-probe-6231
STEP: checking the pod's current state and verifying that restartCount is present
Jul 31 17:15:10.362: INFO: Initial restart count of pod liveness-30fd0a8b-1ab7-4211-a6e9-d3c388062f5b is 0
Jul 31 17:15:20.381: INFO: Restart count of pod container-probe-6231/liveness-30fd0a8b-1ab7-4211-a6e9-d3c388062f5b is now 1 (10.019267613s elapsed)
Jul 31 17:15:40.418: INFO: Restart count of pod container-probe-6231/liveness-30fd0a8b-1ab7-4211-a6e9-d3c388062f5b is now 2 (30.056140845s elapsed)
Jul 31 17:16:00.459: INFO: Restart count of pod container-probe-6231/liveness-30fd0a8b-1ab7-4211-a6e9-d3c388062f5b is now 3 (50.097124243s elapsed)
Jul 31 17:16:20.496: INFO: Restart count of pod container-probe-6231/liveness-30fd0a8b-1ab7-4211-a6e9-d3c388062f5b is now 4 (1m10.133735592s elapsed)
Jul 31 17:17:22.635: INFO: Restart count of pod container-probe-6231/liveness-30fd0a8b-1ab7-4211-a6e9-d3c388062f5b is now 5 (2m12.272762527s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:17:22.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6231" for this suite.

• [SLOW TEST:136.498 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":280,"completed":185,"skipped":3056,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:17:22.673: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7045
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:324
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the initial replication controller
Jul 31 17:17:22.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 create -f - --namespace=kubectl-7045'
Jul 31 17:17:23.091: INFO: stderr: ""
Jul 31 17:17:23.091: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 31 17:17:23.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7045'
Jul 31 17:17:23.179: INFO: stderr: ""
Jul 31 17:17:23.179: INFO: stdout: "update-demo-nautilus-dkfnh update-demo-nautilus-qn8v2 "
Jul 31 17:17:23.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods update-demo-nautilus-dkfnh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7045'
Jul 31 17:17:23.257: INFO: stderr: ""
Jul 31 17:17:23.257: INFO: stdout: ""
Jul 31 17:17:23.257: INFO: update-demo-nautilus-dkfnh is created but not running
Jul 31 17:17:28.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7045'
Jul 31 17:17:28.336: INFO: stderr: ""
Jul 31 17:17:28.336: INFO: stdout: "update-demo-nautilus-dkfnh update-demo-nautilus-qn8v2 "
Jul 31 17:17:28.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods update-demo-nautilus-dkfnh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7045'
Jul 31 17:17:28.407: INFO: stderr: ""
Jul 31 17:17:28.407: INFO: stdout: "true"
Jul 31 17:17:28.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods update-demo-nautilus-dkfnh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7045'
Jul 31 17:17:28.484: INFO: stderr: ""
Jul 31 17:17:28.484: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 31 17:17:28.484: INFO: validating pod update-demo-nautilus-dkfnh
Jul 31 17:17:28.489: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 31 17:17:28.489: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 31 17:17:28.489: INFO: update-demo-nautilus-dkfnh is verified up and running
Jul 31 17:17:28.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods update-demo-nautilus-qn8v2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7045'
Jul 31 17:17:28.569: INFO: stderr: ""
Jul 31 17:17:28.569: INFO: stdout: "true"
Jul 31 17:17:28.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods update-demo-nautilus-qn8v2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7045'
Jul 31 17:17:28.645: INFO: stderr: ""
Jul 31 17:17:28.645: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 31 17:17:28.645: INFO: validating pod update-demo-nautilus-qn8v2
Jul 31 17:17:28.651: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 31 17:17:28.651: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 31 17:17:28.651: INFO: update-demo-nautilus-qn8v2 is verified up and running
STEP: rolling-update to new replication controller
Jul 31 17:17:28.654: INFO: scanned /root for discovery docs: <nil>
Jul 31 17:17:28.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-7045'
Jul 31 17:17:53.078: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul 31 17:17:53.078: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 31 17:17:53.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7045'
Jul 31 17:17:53.163: INFO: stderr: ""
Jul 31 17:17:53.163: INFO: stdout: "update-demo-kitten-4jrq6 update-demo-kitten-x8mx5 update-demo-nautilus-qn8v2 "
STEP: Replicas for name=update-demo: expected=2 actual=3
Jul 31 17:17:58.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7045'
Jul 31 17:17:58.236: INFO: stderr: ""
Jul 31 17:17:58.237: INFO: stdout: "update-demo-kitten-4jrq6 update-demo-kitten-x8mx5 "
Jul 31 17:17:58.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods update-demo-kitten-4jrq6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7045'
Jul 31 17:17:58.318: INFO: stderr: ""
Jul 31 17:17:58.318: INFO: stdout: "true"
Jul 31 17:17:58.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods update-demo-kitten-4jrq6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7045'
Jul 31 17:17:58.412: INFO: stderr: ""
Jul 31 17:17:58.412: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul 31 17:17:58.412: INFO: validating pod update-demo-kitten-4jrq6
Jul 31 17:17:58.417: INFO: got data: {
  "image": "kitten.jpg"
}

Jul 31 17:17:58.417: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul 31 17:17:58.417: INFO: update-demo-kitten-4jrq6 is verified up and running
Jul 31 17:17:58.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods update-demo-kitten-x8mx5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7045'
Jul 31 17:17:58.499: INFO: stderr: ""
Jul 31 17:17:58.499: INFO: stdout: "true"
Jul 31 17:17:58.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods update-demo-kitten-x8mx5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7045'
Jul 31 17:17:58.570: INFO: stderr: ""
Jul 31 17:17:58.570: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul 31 17:17:58.570: INFO: validating pod update-demo-kitten-x8mx5
Jul 31 17:17:58.576: INFO: got data: {
  "image": "kitten.jpg"
}

Jul 31 17:17:58.576: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul 31 17:17:58.576: INFO: update-demo-kitten-x8mx5 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:17:58.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7045" for this suite.

• [SLOW TEST:35.921 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:322
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should do a rolling update of a replication controller  [Conformance]","total":280,"completed":186,"skipped":3083,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:17:58.594: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8553
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:18:03.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8553" for this suite.

• [SLOW TEST:5.195 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":280,"completed":187,"skipped":3112,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:18:03.789: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7229
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jul 31 17:18:04.036: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7229 /api/v1/namespaces/watch-7229/configmaps/e2e-watch-test-resource-version 03267b6f-1f8c-4f33-89b9-e52c1a67d76c 231757 0 2020-07-31 17:18:03 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 31 17:18:04.036: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7229 /api/v1/namespaces/watch-7229/configmaps/e2e-watch-test-resource-version 03267b6f-1f8c-4f33-89b9-e52c1a67d76c 231758 0 2020-07-31 17:18:03 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:18:04.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7229" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":280,"completed":188,"skipped":3118,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:18:04.059: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-3578
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:46
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:18:04.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-3578" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":280,"completed":189,"skipped":3149,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:18:04.268: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6142
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating secret secrets-6142/secret-test-a174fad8-0ff5-40e7-82de-7e4a85716bd8
STEP: Creating a pod to test consume secrets
Jul 31 17:18:04.425: INFO: Waiting up to 5m0s for pod "pod-configmaps-e21667ed-3bed-4224-a5f8-c11bda241b41" in namespace "secrets-6142" to be "success or failure"
Jul 31 17:18:04.436: INFO: Pod "pod-configmaps-e21667ed-3bed-4224-a5f8-c11bda241b41": Phase="Pending", Reason="", readiness=false. Elapsed: 10.817995ms
Jul 31 17:18:06.439: INFO: Pod "pod-configmaps-e21667ed-3bed-4224-a5f8-c11bda241b41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013549986s
Jul 31 17:18:08.442: INFO: Pod "pod-configmaps-e21667ed-3bed-4224-a5f8-c11bda241b41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016362525s
STEP: Saw pod success
Jul 31 17:18:08.442: INFO: Pod "pod-configmaps-e21667ed-3bed-4224-a5f8-c11bda241b41" satisfied condition "success or failure"
Jul 31 17:18:08.444: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-configmaps-e21667ed-3bed-4224-a5f8-c11bda241b41 container env-test: <nil>
STEP: delete the pod
Jul 31 17:18:08.487: INFO: Waiting for pod pod-configmaps-e21667ed-3bed-4224-a5f8-c11bda241b41 to disappear
Jul 31 17:18:08.490: INFO: Pod pod-configmaps-e21667ed-3bed-4224-a5f8-c11bda241b41 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:18:08.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6142" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":190,"skipped":3162,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:18:08.509: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1131
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 17:18:08.708: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"60ed973a-9ce1-404d-b36f-ea311aa6186f", Controller:(*bool)(0xc0013e7b4a), BlockOwnerDeletion:(*bool)(0xc0013e7b4b)}}
Jul 31 17:18:08.721: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"b3d53b2f-667e-4043-80cd-fdeadf37d833", Controller:(*bool)(0xc001078992), BlockOwnerDeletion:(*bool)(0xc001078993)}}
Jul 31 17:18:08.727: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"3d678842-4d00-443a-bfca-e93d971a556c", Controller:(*bool)(0xc0013e7f0a), BlockOwnerDeletion:(*bool)(0xc0013e7f0b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:18:13.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1131" for this suite.

• [SLOW TEST:5.249 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":280,"completed":191,"skipped":3172,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:18:13.758: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7992
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 17:18:14.497: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 31 17:18:16.505: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812694, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812694, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812694, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812694, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 17:18:19.527: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jul 31 17:18:19.553: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:18:19.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7992" for this suite.
STEP: Destroying namespace "webhook-7992-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.254 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":280,"completed":192,"skipped":3180,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:18:20.015: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3991
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jul 31 17:18:20.187: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:18:33.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3991" for this suite.

• [SLOW TEST:13.217 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":280,"completed":193,"skipped":3188,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:18:33.233: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7988
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jul 31 17:18:33.447: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
Jul 31 17:18:37.093: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:18:51.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7988" for this suite.

• [SLOW TEST:18.014 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":280,"completed":194,"skipped":3188,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:18:51.247: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1347
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jul 31 17:18:51.413: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1347 /api/v1/namespaces/watch-1347/configmaps/e2e-watch-test-label-changed bf0a5dc0-0040-4d94-ae97-fd64fa602994 232151 0 2020-07-31 17:18:50 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 31 17:18:51.414: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1347 /api/v1/namespaces/watch-1347/configmaps/e2e-watch-test-label-changed bf0a5dc0-0040-4d94-ae97-fd64fa602994 232152 0 2020-07-31 17:18:50 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul 31 17:18:51.415: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1347 /api/v1/namespaces/watch-1347/configmaps/e2e-watch-test-label-changed bf0a5dc0-0040-4d94-ae97-fd64fa602994 232153 0 2020-07-31 17:18:50 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jul 31 17:19:01.441: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1347 /api/v1/namespaces/watch-1347/configmaps/e2e-watch-test-label-changed bf0a5dc0-0040-4d94-ae97-fd64fa602994 232192 0 2020-07-31 17:18:50 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 31 17:19:01.442: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1347 /api/v1/namespaces/watch-1347/configmaps/e2e-watch-test-label-changed bf0a5dc0-0040-4d94-ae97-fd64fa602994 232193 0 2020-07-31 17:18:50 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jul 31 17:19:01.442: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1347 /api/v1/namespaces/watch-1347/configmaps/e2e-watch-test-label-changed bf0a5dc0-0040-4d94-ae97-fd64fa602994 232194 0 2020-07-31 17:18:50 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:19:01.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1347" for this suite.

• [SLOW TEST:10.206 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":280,"completed":195,"skipped":3194,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:19:01.454: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6240
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0731 17:19:32.137472      21 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 31 17:19:32.137: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:19:32.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6240" for this suite.

• [SLOW TEST:30.694 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":280,"completed":196,"skipped":3197,"failed":0}
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:19:32.148: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8274
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override command
Jul 31 17:19:32.308: INFO: Waiting up to 5m0s for pod "client-containers-54734179-7d51-49d5-93db-1263265e2d7a" in namespace "containers-8274" to be "success or failure"
Jul 31 17:19:32.337: INFO: Pod "client-containers-54734179-7d51-49d5-93db-1263265e2d7a": Phase="Pending", Reason="", readiness=false. Elapsed: 28.794555ms
Jul 31 17:19:34.340: INFO: Pod "client-containers-54734179-7d51-49d5-93db-1263265e2d7a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03187367s
Jul 31 17:19:36.344: INFO: Pod "client-containers-54734179-7d51-49d5-93db-1263265e2d7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03563844s
STEP: Saw pod success
Jul 31 17:19:36.344: INFO: Pod "client-containers-54734179-7d51-49d5-93db-1263265e2d7a" satisfied condition "success or failure"
Jul 31 17:19:36.347: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod client-containers-54734179-7d51-49d5-93db-1263265e2d7a container test-container: <nil>
STEP: delete the pod
Jul 31 17:19:36.380: INFO: Waiting for pod client-containers-54734179-7d51-49d5-93db-1263265e2d7a to disappear
Jul 31 17:19:36.385: INFO: Pod client-containers-54734179-7d51-49d5-93db-1263265e2d7a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:19:36.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8274" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":280,"completed":197,"skipped":3198,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:19:36.399: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4994
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 17:19:37.372: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 31 17:19:39.380: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812776, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812776, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812776, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731812776, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 17:19:42.399: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 17:19:42.402: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:19:43.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4994" for this suite.
STEP: Destroying namespace "webhook-4994-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.555 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":280,"completed":198,"skipped":3198,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:19:43.954: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1594
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 31 17:19:44.138: INFO: Waiting up to 5m0s for pod "pod-88f0f948-a613-4de2-ace4-68334cee68c6" in namespace "emptydir-1594" to be "success or failure"
Jul 31 17:19:44.150: INFO: Pod "pod-88f0f948-a613-4de2-ace4-68334cee68c6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.795499ms
Jul 31 17:19:46.153: INFO: Pod "pod-88f0f948-a613-4de2-ace4-68334cee68c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015323216s
Jul 31 17:19:48.156: INFO: Pod "pod-88f0f948-a613-4de2-ace4-68334cee68c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018185158s
STEP: Saw pod success
Jul 31 17:19:48.156: INFO: Pod "pod-88f0f948-a613-4de2-ace4-68334cee68c6" satisfied condition "success or failure"
Jul 31 17:19:48.158: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-88f0f948-a613-4de2-ace4-68334cee68c6 container test-container: <nil>
STEP: delete the pod
Jul 31 17:19:48.193: INFO: Waiting for pod pod-88f0f948-a613-4de2-ace4-68334cee68c6 to disappear
Jul 31 17:19:48.198: INFO: Pod pod-88f0f948-a613-4de2-ace4-68334cee68c6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:19:48.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1594" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":199,"skipped":3202,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:19:48.209: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9123
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 31 17:19:48.369: INFO: Waiting up to 5m0s for pod "pod-9c4c7319-643e-4af9-9050-21c8c5c55a6d" in namespace "emptydir-9123" to be "success or failure"
Jul 31 17:19:48.376: INFO: Pod "pod-9c4c7319-643e-4af9-9050-21c8c5c55a6d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.333738ms
Jul 31 17:19:50.384: INFO: Pod "pod-9c4c7319-643e-4af9-9050-21c8c5c55a6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015327294s
Jul 31 17:19:52.387: INFO: Pod "pod-9c4c7319-643e-4af9-9050-21c8c5c55a6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018323124s
STEP: Saw pod success
Jul 31 17:19:52.387: INFO: Pod "pod-9c4c7319-643e-4af9-9050-21c8c5c55a6d" satisfied condition "success or failure"
Jul 31 17:19:52.392: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-9c4c7319-643e-4af9-9050-21c8c5c55a6d container test-container: <nil>
STEP: delete the pod
Jul 31 17:19:52.432: INFO: Waiting for pod pod-9c4c7319-643e-4af9-9050-21c8c5c55a6d to disappear
Jul 31 17:19:52.435: INFO: Pod pod-9c4c7319-643e-4af9-9050-21c8c5c55a6d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:19:52.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9123" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":200,"skipped":3237,"failed":0}

------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:19:52.446: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9311
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-f356c5c1-8369-483e-bf3a-c83372efb295
STEP: Creating a pod to test consume secrets
Jul 31 17:19:52.603: INFO: Waiting up to 5m0s for pod "pod-secrets-b080e495-4afa-4c48-82ee-e959e2da7f53" in namespace "secrets-9311" to be "success or failure"
Jul 31 17:19:52.613: INFO: Pod "pod-secrets-b080e495-4afa-4c48-82ee-e959e2da7f53": Phase="Pending", Reason="", readiness=false. Elapsed: 9.779692ms
Jul 31 17:19:54.616: INFO: Pod "pod-secrets-b080e495-4afa-4c48-82ee-e959e2da7f53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013209783s
Jul 31 17:19:56.619: INFO: Pod "pod-secrets-b080e495-4afa-4c48-82ee-e959e2da7f53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016240231s
STEP: Saw pod success
Jul 31 17:19:56.619: INFO: Pod "pod-secrets-b080e495-4afa-4c48-82ee-e959e2da7f53" satisfied condition "success or failure"
Jul 31 17:19:56.623: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-secrets-b080e495-4afa-4c48-82ee-e959e2da7f53 container secret-env-test: <nil>
STEP: delete the pod
Jul 31 17:19:56.650: INFO: Waiting for pod pod-secrets-b080e495-4afa-4c48-82ee-e959e2da7f53 to disappear
Jul 31 17:19:56.659: INFO: Pod pod-secrets-b080e495-4afa-4c48-82ee-e959e2da7f53 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:19:56.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9311" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":280,"completed":201,"skipped":3237,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:19:56.671: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7056
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-9304a756-94e0-4d72-8596-57a81ad7827f in namespace container-probe-7056
Jul 31 17:20:00.837: INFO: Started pod busybox-9304a756-94e0-4d72-8596-57a81ad7827f in namespace container-probe-7056
STEP: checking the pod's current state and verifying that restartCount is present
Jul 31 17:20:00.840: INFO: Initial restart count of pod busybox-9304a756-94e0-4d72-8596-57a81ad7827f is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:24:01.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7056" for this suite.

• [SLOW TEST:244.647 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":202,"skipped":3265,"failed":0}
SS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:24:01.320: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jul 31 17:24:01.473: INFO: Waiting up to 5m0s for pod "downward-api-aa8ab580-c1fc-4f30-b35a-2b43fde92395" in namespace "downward-api-5" to be "success or failure"
Jul 31 17:24:01.482: INFO: Pod "downward-api-aa8ab580-c1fc-4f30-b35a-2b43fde92395": Phase="Pending", Reason="", readiness=false. Elapsed: 8.775154ms
Jul 31 17:24:03.485: INFO: Pod "downward-api-aa8ab580-c1fc-4f30-b35a-2b43fde92395": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012412658s
Jul 31 17:24:05.488: INFO: Pod "downward-api-aa8ab580-c1fc-4f30-b35a-2b43fde92395": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015378701s
STEP: Saw pod success
Jul 31 17:24:05.488: INFO: Pod "downward-api-aa8ab580-c1fc-4f30-b35a-2b43fde92395" satisfied condition "success or failure"
Jul 31 17:24:05.491: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod downward-api-aa8ab580-c1fc-4f30-b35a-2b43fde92395 container dapi-container: <nil>
STEP: delete the pod
Jul 31 17:24:05.512: INFO: Waiting for pod downward-api-aa8ab580-c1fc-4f30-b35a-2b43fde92395 to disappear
Jul 31 17:24:05.514: INFO: Pod downward-api-aa8ab580-c1fc-4f30-b35a-2b43fde92395 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:24:05.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":280,"completed":203,"skipped":3267,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:24:05.536: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2975
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 31 17:24:05.688: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ff11b47c-f1e0-4292-9953-7e407aa36c17" in namespace "projected-2975" to be "success or failure"
Jul 31 17:24:05.693: INFO: Pod "downwardapi-volume-ff11b47c-f1e0-4292-9953-7e407aa36c17": Phase="Pending", Reason="", readiness=false. Elapsed: 4.795608ms
Jul 31 17:24:07.696: INFO: Pod "downwardapi-volume-ff11b47c-f1e0-4292-9953-7e407aa36c17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008126088s
Jul 31 17:24:09.699: INFO: Pod "downwardapi-volume-ff11b47c-f1e0-4292-9953-7e407aa36c17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0105441s
STEP: Saw pod success
Jul 31 17:24:09.699: INFO: Pod "downwardapi-volume-ff11b47c-f1e0-4292-9953-7e407aa36c17" satisfied condition "success or failure"
Jul 31 17:24:09.701: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod downwardapi-volume-ff11b47c-f1e0-4292-9953-7e407aa36c17 container client-container: <nil>
STEP: delete the pod
Jul 31 17:24:09.723: INFO: Waiting for pod downwardapi-volume-ff11b47c-f1e0-4292-9953-7e407aa36c17 to disappear
Jul 31 17:24:09.729: INFO: Pod downwardapi-volume-ff11b47c-f1e0-4292-9953-7e407aa36c17 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:24:09.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2975" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":204,"skipped":3315,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:24:09.745: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2021
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 31 17:24:09.983: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:24:09.999: INFO: Number of nodes with available pods: 0
Jul 31 17:24:09.999: INFO: Node test-aruna-123-node-group-687e2c91ce is running more than one daemon pod
Jul 31 17:24:11.005: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:24:11.009: INFO: Number of nodes with available pods: 0
Jul 31 17:24:11.009: INFO: Node test-aruna-123-node-group-687e2c91ce is running more than one daemon pod
Jul 31 17:24:12.004: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:24:12.006: INFO: Number of nodes with available pods: 0
Jul 31 17:24:12.006: INFO: Node test-aruna-123-node-group-687e2c91ce is running more than one daemon pod
Jul 31 17:24:13.005: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:24:13.009: INFO: Number of nodes with available pods: 3
Jul 31 17:24:13.009: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jul 31 17:24:13.034: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:24:13.037: INFO: Number of nodes with available pods: 2
Jul 31 17:24:13.037: INFO: Node test-aruna-123-node-group-687e2c91ce is running more than one daemon pod
Jul 31 17:24:14.042: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:24:14.049: INFO: Number of nodes with available pods: 2
Jul 31 17:24:14.049: INFO: Node test-aruna-123-node-group-687e2c91ce is running more than one daemon pod
Jul 31 17:24:15.042: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:24:15.045: INFO: Number of nodes with available pods: 2
Jul 31 17:24:15.045: INFO: Node test-aruna-123-node-group-687e2c91ce is running more than one daemon pod
Jul 31 17:24:16.042: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:24:16.047: INFO: Number of nodes with available pods: 2
Jul 31 17:24:16.047: INFO: Node test-aruna-123-node-group-687e2c91ce is running more than one daemon pod
Jul 31 17:24:17.042: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:24:17.045: INFO: Number of nodes with available pods: 2
Jul 31 17:24:17.046: INFO: Node test-aruna-123-node-group-687e2c91ce is running more than one daemon pod
Jul 31 17:24:18.042: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:24:18.045: INFO: Number of nodes with available pods: 2
Jul 31 17:24:18.045: INFO: Node test-aruna-123-node-group-687e2c91ce is running more than one daemon pod
Jul 31 17:24:19.042: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:24:19.045: INFO: Number of nodes with available pods: 2
Jul 31 17:24:19.045: INFO: Node test-aruna-123-node-group-687e2c91ce is running more than one daemon pod
Jul 31 17:24:20.041: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:24:20.044: INFO: Number of nodes with available pods: 2
Jul 31 17:24:20.044: INFO: Node test-aruna-123-node-group-687e2c91ce is running more than one daemon pod
Jul 31 17:24:21.045: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:24:21.049: INFO: Number of nodes with available pods: 2
Jul 31 17:24:21.049: INFO: Node test-aruna-123-node-group-687e2c91ce is running more than one daemon pod
Jul 31 17:24:22.043: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:24:22.046: INFO: Number of nodes with available pods: 2
Jul 31 17:24:22.046: INFO: Node test-aruna-123-node-group-687e2c91ce is running more than one daemon pod
Jul 31 17:24:23.041: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:24:23.043: INFO: Number of nodes with available pods: 2
Jul 31 17:24:23.043: INFO: Node test-aruna-123-node-group-687e2c91ce is running more than one daemon pod
Jul 31 17:24:24.043: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:24:24.049: INFO: Number of nodes with available pods: 2
Jul 31 17:24:24.049: INFO: Node test-aruna-123-node-group-687e2c91ce is running more than one daemon pod
Jul 31 17:24:25.043: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:24:25.046: INFO: Number of nodes with available pods: 2
Jul 31 17:24:25.046: INFO: Node test-aruna-123-node-group-687e2c91ce is running more than one daemon pod
Jul 31 17:24:26.043: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:24:26.045: INFO: Number of nodes with available pods: 3
Jul 31 17:24:26.045: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2021, will wait for the garbage collector to delete the pods
Jul 31 17:24:26.157: INFO: Deleting DaemonSet.extensions daemon-set took: 56.259758ms
Jul 31 17:24:26.657: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.354139ms
Jul 31 17:24:35.460: INFO: Number of nodes with available pods: 0
Jul 31 17:24:35.460: INFO: Number of running nodes: 0, number of available pods: 0
Jul 31 17:24:35.462: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2021/daemonsets","resourceVersion":"233684"},"items":null}

Jul 31 17:24:35.464: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2021/pods","resourceVersion":"233684"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:24:35.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2021" for this suite.

• [SLOW TEST:25.742 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":280,"completed":205,"skipped":3324,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:24:35.487: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7169
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 17:24:36.251: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 31 17:24:38.257: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813075, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813075, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813075, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813075, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 17:24:41.284: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a validating webhook configuration
Jul 31 17:24:41.313: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:24:41.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7169" for this suite.
STEP: Destroying namespace "webhook-7169-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.092 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":280,"completed":206,"skipped":3325,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:24:41.582: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-712
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with configMap that has name projected-configmap-test-upd-b584cc7b-d687-4853-a1e8-421a75e925f2
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-b584cc7b-d687-4853-a1e8-421a75e925f2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:26:00.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-712" for this suite.

• [SLOW TEST:78.678 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":207,"skipped":3359,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:26:00.260: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8275
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 31 17:26:00.411: INFO: Waiting up to 5m0s for pod "pod-9dc8a421-b51a-4d72-b7d4-e8c1b9c44188" in namespace "emptydir-8275" to be "success or failure"
Jul 31 17:26:00.417: INFO: Pod "pod-9dc8a421-b51a-4d72-b7d4-e8c1b9c44188": Phase="Pending", Reason="", readiness=false. Elapsed: 5.546021ms
Jul 31 17:26:02.420: INFO: Pod "pod-9dc8a421-b51a-4d72-b7d4-e8c1b9c44188": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008778954s
Jul 31 17:26:04.423: INFO: Pod "pod-9dc8a421-b51a-4d72-b7d4-e8c1b9c44188": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012047141s
STEP: Saw pod success
Jul 31 17:26:04.423: INFO: Pod "pod-9dc8a421-b51a-4d72-b7d4-e8c1b9c44188" satisfied condition "success or failure"
Jul 31 17:26:04.425: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-9dc8a421-b51a-4d72-b7d4-e8c1b9c44188 container test-container: <nil>
STEP: delete the pod
Jul 31 17:26:04.445: INFO: Waiting for pod pod-9dc8a421-b51a-4d72-b7d4-e8c1b9c44188 to disappear
Jul 31 17:26:04.454: INFO: Pod pod-9dc8a421-b51a-4d72-b7d4-e8c1b9c44188 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:26:04.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8275" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":208,"skipped":3370,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:26:04.463: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6522
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:26:08.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6522" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":280,"completed":209,"skipped":3410,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:26:08.673: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9159
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 17:26:08.818: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:26:09.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9159" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":280,"completed":210,"skipped":3427,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:26:09.855: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9392
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jul 31 17:26:10.009: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 31 17:26:10.023: INFO: Waiting for terminating namespaces to be deleted...
Jul 31 17:26:10.025: INFO: 
Logging pods the kubelet thinks is on node test-aruna-123-node-group-687e2c91ce before test
Jul 31 17:26:10.032: INFO: calico-node-98r4p from kube-system started at 2020-07-30 20:35:01 +0000 UTC (1 container statuses recorded)
Jul 31 17:26:10.032: INFO: 	Container calico-node ready: true, restart count 0
Jul 31 17:26:10.032: INFO: nvidia-device-plugin-daemonset-wjtwk from kube-system started at 2020-07-31 16:51:11 +0000 UTC (1 container statuses recorded)
Jul 31 17:26:10.032: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Jul 31 17:26:10.032: INFO: sonobuoy from sonobuoy started at 2020-07-31 16:21:46 +0000 UTC (1 container statuses recorded)
Jul 31 17:26:10.032: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 31 17:26:10.032: INFO: kube-proxy-9nrgh from kube-system started at 2020-07-30 20:35:01 +0000 UTC (1 container statuses recorded)
Jul 31 17:26:10.032: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 17:26:10.032: INFO: metallb-speaker-bp7cw from ccp started at 2020-07-31 16:51:00 +0000 UTC (1 container statuses recorded)
Jul 31 17:26:10.032: INFO: 	Container metallb-speaker ready: true, restart count 0
Jul 31 17:26:10.032: INFO: ccp-monitor-prometheus-pushgateway-7d5b6d448b-hl5dh from ccp started at 2020-07-31 16:51:00 +0000 UTC (1 container statuses recorded)
Jul 31 17:26:10.032: INFO: 	Container prometheus-pushgateway ready: true, restart count 0
Jul 31 17:26:10.032: INFO: ccp-monitor-prometheus-node-exporter-p4jvn from ccp started at 2020-07-31 16:51:11 +0000 UTC (1 container statuses recorded)
Jul 31 17:26:10.032: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jul 31 17:26:10.032: INFO: nginx-ingress-controller-jt5pp from ccp started at 2020-07-31 16:51:21 +0000 UTC (1 container statuses recorded)
Jul 31 17:26:10.032: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jul 31 17:26:10.032: INFO: client-containers-1a66b798-5889-42cc-941d-b63eb39359c9 from containers-6522 started at 2020-07-31 17:26:02 +0000 UTC (1 container statuses recorded)
Jul 31 17:26:10.032: INFO: 	Container test-container ready: true, restart count 0
Jul 31 17:26:10.032: INFO: sonobuoy-systemd-logs-daemon-set-21a6c8c28f874b58-rcrfg from sonobuoy started at 2020-07-31 16:22:15 +0000 UTC (2 container statuses recorded)
Jul 31 17:26:10.032: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul 31 17:26:10.032: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 17:26:10.032: INFO: 
Logging pods the kubelet thinks is on node test-aruna-123-node-group-a8f122b701 before test
Jul 31 17:26:10.043: INFO: metallb-speaker-g2jrx from ccp started at 2020-07-30 20:35:04 +0000 UTC (1 container statuses recorded)
Jul 31 17:26:10.044: INFO: 	Container metallb-speaker ready: true, restart count 0
Jul 31 17:26:10.044: INFO: calico-node-fkmgx from kube-system started at 2020-07-30 20:34:54 +0000 UTC (1 container statuses recorded)
Jul 31 17:26:10.044: INFO: 	Container calico-node ready: true, restart count 0
Jul 31 17:26:10.044: INFO: cert-manager-7c4fdf69b7-ct9hx from ccp started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 17:26:10.044: INFO: 	Container cert-manager ready: true, restart count 0
Jul 31 17:26:10.044: INFO: nginx-ingress-default-backend-6b546bb848-d7ms8 from ccp started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 17:26:10.044: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jul 31 17:26:10.044: INFO: kube-proxy-rpj2g from kube-system started at 2020-07-30 20:34:54 +0000 UTC (1 container statuses recorded)
Jul 31 17:26:10.044: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 17:26:10.044: INFO: nginx-ingress-controller-vv7zf from ccp started at 2020-07-30 20:35:04 +0000 UTC (1 container statuses recorded)
Jul 31 17:26:10.044: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jul 31 17:26:10.044: INFO: metallb-controller-6c8c5fd7fd-9n8gm from ccp started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 17:26:10.044: INFO: 	Container metallb-controller ready: true, restart count 0
Jul 31 17:26:10.044: INFO: ccp-monitor-prometheus-server-67d55955c7-b54b2 from ccp started at 2020-07-30 20:37:51 +0000 UTC (3 container statuses recorded)
Jul 31 17:26:10.044: INFO: 	Container nginx-proxy ready: true, restart count 0
Jul 31 17:26:10.044: INFO: 	Container prometheus-server ready: true, restart count 0
Jul 31 17:26:10.044: INFO: 	Container prometheus-server-configmap-reload ready: true, restart count 0
Jul 31 17:26:10.044: INFO: nvidia-device-plugin-daemonset-b7rb6 from kube-system started at 2020-07-30 20:35:04 +0000 UTC (1 container statuses recorded)
Jul 31 17:26:10.044: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Jul 31 17:26:10.044: INFO: ccp-monitor-prometheus-node-exporter-82955 from ccp started at 2020-07-30 20:37:46 +0000 UTC (1 container statuses recorded)
Jul 31 17:26:10.044: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jul 31 17:26:10.044: INFO: sonobuoy-systemd-logs-daemon-set-21a6c8c28f874b58-64p9f from sonobuoy started at 2020-07-31 16:22:16 +0000 UTC (2 container statuses recorded)
Jul 31 17:26:10.044: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul 31 17:26:10.044: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 17:26:10.044: INFO: ccp-helm-operator-749fb6dc7-rrdzz from ccp started at 2020-07-31 16:51:01 +0000 UTC (1 container statuses recorded)
Jul 31 17:26:10.044: INFO: 	Container ccp-helm-operator ready: true, restart count 0
Jul 31 17:26:10.044: INFO: 
Logging pods the kubelet thinks is on node test-aruna-123-node-group-a92c781fbd before test
Jul 31 17:26:10.053: INFO: ccp-monitor-grafana-78fd4cc979-qn6mq from ccp started at 2020-07-31 16:51:02 +0000 UTC (1 container statuses recorded)
Jul 31 17:26:10.053: INFO: 	Container grafana ready: true, restart count 0
Jul 31 17:26:10.053: INFO: calico-node-txdhs from kube-system started at 2020-07-30 20:35:00 +0000 UTC (1 container statuses recorded)
Jul 31 17:26:10.053: INFO: 	Container calico-node ready: true, restart count 0
Jul 31 17:26:10.053: INFO: sonobuoy-e2e-job-e7094f6bfc7c4c8d from sonobuoy started at 2020-07-31 16:22:17 +0000 UTC (2 container statuses recorded)
Jul 31 17:26:10.053: INFO: 	Container e2e ready: true, restart count 0
Jul 31 17:26:10.053: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 17:26:10.053: INFO: ccp-monitor-prometheus-alertmanager-57f5689547-wvfrs from ccp started at 2020-07-30 20:37:51 +0000 UTC (2 container statuses recorded)
Jul 31 17:26:10.053: INFO: 	Container prometheus-alertmanager ready: true, restart count 0
Jul 31 17:26:10.053: INFO: 	Container prometheus-alertmanager-configmap-reload ready: true, restart count 0
Jul 31 17:26:10.053: INFO: kube-proxy-q4q9v from kube-system started at 2020-07-30 20:35:00 +0000 UTC (1 container statuses recorded)
Jul 31 17:26:10.054: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 17:26:10.054: INFO: nginx-ingress-controller-2447g from ccp started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 17:26:10.054: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jul 31 17:26:10.054: INFO: sonobuoy-systemd-logs-daemon-set-21a6c8c28f874b58-5vrgt from sonobuoy started at 2020-07-31 16:22:17 +0000 UTC (2 container statuses recorded)
Jul 31 17:26:10.054: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul 31 17:26:10.054: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 17:26:10.054: INFO: metallb-speaker-q6n4f from ccp started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 17:26:10.054: INFO: 	Container metallb-speaker ready: true, restart count 0
Jul 31 17:26:10.054: INFO: ccp-monitor-prometheus-node-exporter-559kz from ccp started at 2020-07-30 20:37:46 +0000 UTC (1 container statuses recorded)
Jul 31 17:26:10.054: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jul 31 17:26:10.054: INFO: ccp-monitor-prometheus-kube-state-metrics-7f7c9f986-xzrgl from ccp started at 2020-07-30 20:37:46 +0000 UTC (1 container statuses recorded)
Jul 31 17:26:10.055: INFO: 	Container prometheus-kube-state-metrics ready: true, restart count 0
Jul 31 17:26:10.055: INFO: ccp-monitor-grafana-set-datasource-qwx6l from ccp started at 2020-07-30 20:37:46 +0000 UTC (1 container statuses recorded)
Jul 31 17:26:10.055: INFO: 	Container ccp-monitor-grafana-set-datasource ready: false, restart count 0
Jul 31 17:26:10.055: INFO: nvidia-device-plugin-daemonset-4pt4d from kube-system started at 2020-07-30 20:35:10 +0000 UTC (1 container statuses recorded)
Jul 31 17:26:10.055: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Jul 31 17:26:10.055: INFO: kubernetes-dashboard-dbfcd4d-mkklx from ccp started at 2020-07-30 20:37:33 +0000 UTC (1 container statuses recorded)
Jul 31 17:26:10.055: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: verifying the node has the label node test-aruna-123-node-group-687e2c91ce
STEP: verifying the node has the label node test-aruna-123-node-group-a8f122b701
STEP: verifying the node has the label node test-aruna-123-node-group-a92c781fbd
Jul 31 17:26:10.150: INFO: Pod ccp-helm-operator-749fb6dc7-rrdzz requesting resource cpu=0m on Node test-aruna-123-node-group-a8f122b701
Jul 31 17:26:10.151: INFO: Pod ccp-monitor-grafana-78fd4cc979-qn6mq requesting resource cpu=0m on Node test-aruna-123-node-group-a92c781fbd
Jul 31 17:26:10.151: INFO: Pod ccp-monitor-prometheus-alertmanager-57f5689547-wvfrs requesting resource cpu=0m on Node test-aruna-123-node-group-a92c781fbd
Jul 31 17:26:10.151: INFO: Pod ccp-monitor-prometheus-kube-state-metrics-7f7c9f986-xzrgl requesting resource cpu=0m on Node test-aruna-123-node-group-a92c781fbd
Jul 31 17:26:10.151: INFO: Pod ccp-monitor-prometheus-node-exporter-559kz requesting resource cpu=0m on Node test-aruna-123-node-group-a92c781fbd
Jul 31 17:26:10.151: INFO: Pod ccp-monitor-prometheus-node-exporter-82955 requesting resource cpu=0m on Node test-aruna-123-node-group-a8f122b701
Jul 31 17:26:10.151: INFO: Pod ccp-monitor-prometheus-node-exporter-p4jvn requesting resource cpu=0m on Node test-aruna-123-node-group-687e2c91ce
Jul 31 17:26:10.151: INFO: Pod ccp-monitor-prometheus-pushgateway-7d5b6d448b-hl5dh requesting resource cpu=0m on Node test-aruna-123-node-group-687e2c91ce
Jul 31 17:26:10.151: INFO: Pod ccp-monitor-prometheus-server-67d55955c7-b54b2 requesting resource cpu=0m on Node test-aruna-123-node-group-a8f122b701
Jul 31 17:26:10.151: INFO: Pod cert-manager-7c4fdf69b7-ct9hx requesting resource cpu=0m on Node test-aruna-123-node-group-a8f122b701
Jul 31 17:26:10.151: INFO: Pod kubernetes-dashboard-dbfcd4d-mkklx requesting resource cpu=100m on Node test-aruna-123-node-group-a92c781fbd
Jul 31 17:26:10.151: INFO: Pod metallb-controller-6c8c5fd7fd-9n8gm requesting resource cpu=100m on Node test-aruna-123-node-group-a8f122b701
Jul 31 17:26:10.151: INFO: Pod metallb-speaker-bp7cw requesting resource cpu=100m on Node test-aruna-123-node-group-687e2c91ce
Jul 31 17:26:10.151: INFO: Pod metallb-speaker-g2jrx requesting resource cpu=100m on Node test-aruna-123-node-group-a8f122b701
Jul 31 17:26:10.151: INFO: Pod metallb-speaker-q6n4f requesting resource cpu=100m on Node test-aruna-123-node-group-a92c781fbd
Jul 31 17:26:10.151: INFO: Pod nginx-ingress-controller-2447g requesting resource cpu=0m on Node test-aruna-123-node-group-a92c781fbd
Jul 31 17:26:10.151: INFO: Pod nginx-ingress-controller-jt5pp requesting resource cpu=0m on Node test-aruna-123-node-group-687e2c91ce
Jul 31 17:26:10.151: INFO: Pod nginx-ingress-controller-vv7zf requesting resource cpu=0m on Node test-aruna-123-node-group-a8f122b701
Jul 31 17:26:10.152: INFO: Pod nginx-ingress-default-backend-6b546bb848-d7ms8 requesting resource cpu=0m on Node test-aruna-123-node-group-a8f122b701
Jul 31 17:26:10.152: INFO: Pod client-containers-1a66b798-5889-42cc-941d-b63eb39359c9 requesting resource cpu=0m on Node test-aruna-123-node-group-687e2c91ce
Jul 31 17:26:10.152: INFO: Pod calico-node-98r4p requesting resource cpu=250m on Node test-aruna-123-node-group-687e2c91ce
Jul 31 17:26:10.152: INFO: Pod calico-node-fkmgx requesting resource cpu=250m on Node test-aruna-123-node-group-a8f122b701
Jul 31 17:26:10.152: INFO: Pod calico-node-txdhs requesting resource cpu=250m on Node test-aruna-123-node-group-a92c781fbd
Jul 31 17:26:10.152: INFO: Pod kube-proxy-9nrgh requesting resource cpu=0m on Node test-aruna-123-node-group-687e2c91ce
Jul 31 17:26:10.152: INFO: Pod kube-proxy-q4q9v requesting resource cpu=0m on Node test-aruna-123-node-group-a92c781fbd
Jul 31 17:26:10.152: INFO: Pod kube-proxy-rpj2g requesting resource cpu=0m on Node test-aruna-123-node-group-a8f122b701
Jul 31 17:26:10.152: INFO: Pod nvidia-device-plugin-daemonset-4pt4d requesting resource cpu=0m on Node test-aruna-123-node-group-a92c781fbd
Jul 31 17:26:10.152: INFO: Pod nvidia-device-plugin-daemonset-b7rb6 requesting resource cpu=0m on Node test-aruna-123-node-group-a8f122b701
Jul 31 17:26:10.152: INFO: Pod nvidia-device-plugin-daemonset-wjtwk requesting resource cpu=0m on Node test-aruna-123-node-group-687e2c91ce
Jul 31 17:26:10.152: INFO: Pod sonobuoy requesting resource cpu=0m on Node test-aruna-123-node-group-687e2c91ce
Jul 31 17:26:10.152: INFO: Pod sonobuoy-e2e-job-e7094f6bfc7c4c8d requesting resource cpu=0m on Node test-aruna-123-node-group-a92c781fbd
Jul 31 17:26:10.152: INFO: Pod sonobuoy-systemd-logs-daemon-set-21a6c8c28f874b58-5vrgt requesting resource cpu=0m on Node test-aruna-123-node-group-a92c781fbd
Jul 31 17:26:10.152: INFO: Pod sonobuoy-systemd-logs-daemon-set-21a6c8c28f874b58-64p9f requesting resource cpu=0m on Node test-aruna-123-node-group-a8f122b701
Jul 31 17:26:10.152: INFO: Pod sonobuoy-systemd-logs-daemon-set-21a6c8c28f874b58-rcrfg requesting resource cpu=0m on Node test-aruna-123-node-group-687e2c91ce
STEP: Starting Pods to consume most of the cluster CPU.
Jul 31 17:26:10.152: INFO: Creating a pod which consumes cpu=1085m on Node test-aruna-123-node-group-a92c781fbd
Jul 31 17:26:10.171: INFO: Creating a pod which consumes cpu=1155m on Node test-aruna-123-node-group-687e2c91ce
Jul 31 17:26:10.191: INFO: Creating a pod which consumes cpu=1085m on Node test-aruna-123-node-group-a8f122b701
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-74fae138-c401-47be-8c9d-78f1f51841b8.1626e654c40295a3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9392/filler-pod-74fae138-c401-47be-8c9d-78f1f51841b8 to test-aruna-123-node-group-a92c781fbd]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-74fae138-c401-47be-8c9d-78f1f51841b8.1626e655272ec72b], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-74fae138-c401-47be-8c9d-78f1f51841b8.1626e655504225c4], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-74fae138-c401-47be-8c9d-78f1f51841b8.1626e65558fb2535], Reason = [Created], Message = [Created container filler-pod-74fae138-c401-47be-8c9d-78f1f51841b8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-74fae138-c401-47be-8c9d-78f1f51841b8.1626e65565db74d4], Reason = [Started], Message = [Started container filler-pod-74fae138-c401-47be-8c9d-78f1f51841b8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ed1f7368-bbe4-4ee5-90c7-1026ccc5941b.1626e654c6d26f95], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9392/filler-pod-ed1f7368-bbe4-4ee5-90c7-1026ccc5941b to test-aruna-123-node-group-a8f122b701]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ed1f7368-bbe4-4ee5-90c7-1026ccc5941b.1626e654f14be87d], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ed1f7368-bbe4-4ee5-90c7-1026ccc5941b.1626e6551765a1e0], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ed1f7368-bbe4-4ee5-90c7-1026ccc5941b.1626e6552094757b], Reason = [Created], Message = [Created container filler-pod-ed1f7368-bbe4-4ee5-90c7-1026ccc5941b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ed1f7368-bbe4-4ee5-90c7-1026ccc5941b.1626e6552df4d241], Reason = [Started], Message = [Started container filler-pod-ed1f7368-bbe4-4ee5-90c7-1026ccc5941b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f207d267-ca49-4e61-a5d7-b0fb0096a0f1.1626e654c58b76ff], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9392/filler-pod-f207d267-ca49-4e61-a5d7-b0fb0096a0f1 to test-aruna-123-node-group-687e2c91ce]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f207d267-ca49-4e61-a5d7-b0fb0096a0f1.1626e654d1ba12a0], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f207d267-ca49-4e61-a5d7-b0fb0096a0f1.1626e654dba9aff4], Reason = [Created], Message = [Created container filler-pod-f207d267-ca49-4e61-a5d7-b0fb0096a0f1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f207d267-ca49-4e61-a5d7-b0fb0096a0f1.1626e654ebcabc3c], Reason = [Started], Message = [Started container filler-pod-f207d267-ca49-4e61-a5d7-b0fb0096a0f1]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1626e655b66b7ab8], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 3 Insufficient cpu.]
STEP: removing the label node off the node test-aruna-123-node-group-a8f122b701
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node test-aruna-123-node-group-a92c781fbd
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node test-aruna-123-node-group-687e2c91ce
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:26:15.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9392" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:5.708 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":280,"completed":211,"skipped":3438,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:26:15.567: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9701
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting the proxy server
Jul 31 17:26:15.715: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-776750169 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:26:15.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9701" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":280,"completed":212,"skipped":3464,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:26:15.784: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-974
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 31 17:26:15.935: INFO: Waiting up to 5m0s for pod "pod-d89766c2-a6fb-4e79-8a7a-528f43a0f297" in namespace "emptydir-974" to be "success or failure"
Jul 31 17:26:15.949: INFO: Pod "pod-d89766c2-a6fb-4e79-8a7a-528f43a0f297": Phase="Pending", Reason="", readiness=false. Elapsed: 13.500076ms
Jul 31 17:26:17.955: INFO: Pod "pod-d89766c2-a6fb-4e79-8a7a-528f43a0f297": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018898806s
Jul 31 17:26:19.964: INFO: Pod "pod-d89766c2-a6fb-4e79-8a7a-528f43a0f297": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028187003s
STEP: Saw pod success
Jul 31 17:26:19.964: INFO: Pod "pod-d89766c2-a6fb-4e79-8a7a-528f43a0f297" satisfied condition "success or failure"
Jul 31 17:26:19.970: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-d89766c2-a6fb-4e79-8a7a-528f43a0f297 container test-container: <nil>
STEP: delete the pod
Jul 31 17:26:20.028: INFO: Waiting for pod pod-d89766c2-a6fb-4e79-8a7a-528f43a0f297 to disappear
Jul 31 17:26:20.034: INFO: Pod pod-d89766c2-a6fb-4e79-8a7a-528f43a0f297 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:26:20.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-974" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":213,"skipped":3522,"failed":0}

------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:26:20.056: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8357
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 31 17:26:23.305: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:26:23.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8357" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":280,"completed":214,"skipped":3522,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:26:23.351: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4645
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service endpoint-test2 in namespace services-4645
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4645 to expose endpoints map[]
Jul 31 17:26:23.543: INFO: successfully validated that service endpoint-test2 in namespace services-4645 exposes endpoints map[] (11.834395ms elapsed)
STEP: Creating pod pod1 in namespace services-4645
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4645 to expose endpoints map[pod1:[80]]
Jul 31 17:26:26.591: INFO: successfully validated that service endpoint-test2 in namespace services-4645 exposes endpoints map[pod1:[80]] (3.03705078s elapsed)
STEP: Creating pod pod2 in namespace services-4645
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4645 to expose endpoints map[pod1:[80] pod2:[80]]
Jul 31 17:26:29.636: INFO: successfully validated that service endpoint-test2 in namespace services-4645 exposes endpoints map[pod1:[80] pod2:[80]] (3.037285496s elapsed)
STEP: Deleting pod pod1 in namespace services-4645
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4645 to expose endpoints map[pod2:[80]]
Jul 31 17:26:29.658: INFO: successfully validated that service endpoint-test2 in namespace services-4645 exposes endpoints map[pod2:[80]] (15.094665ms elapsed)
STEP: Deleting pod pod2 in namespace services-4645
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4645 to expose endpoints map[]
Jul 31 17:26:29.676: INFO: successfully validated that service endpoint-test2 in namespace services-4645 exposes endpoints map[] (8.690166ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:26:29.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4645" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:6.373 seconds]
[sig-network] Services
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":280,"completed":215,"skipped":3531,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:26:29.724: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-2666
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jul 31 17:26:30.455: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jul 31 17:26:32.462: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813189, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813189, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813190, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813189, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 17:26:35.483: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 17:26:35.486: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:26:36.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2666" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

• [SLOW TEST:7.118 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":280,"completed":216,"skipped":3541,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:26:36.843: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5706
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:26:37.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5706" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":280,"completed":217,"skipped":3551,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:26:37.071: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2740
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-projected-all-test-volume-262f164a-d485-4aaf-96bf-6f1f3877761f
STEP: Creating secret with name secret-projected-all-test-volume-85b839c7-8b71-4fa8-8594-014f23543b81
STEP: Creating a pod to test Check all projections for projected volume plugin
Jul 31 17:26:37.288: INFO: Waiting up to 5m0s for pod "projected-volume-2aabbcc9-425b-4576-bf2f-a9aceaa07ef0" in namespace "projected-2740" to be "success or failure"
Jul 31 17:26:37.297: INFO: Pod "projected-volume-2aabbcc9-425b-4576-bf2f-a9aceaa07ef0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.364825ms
Jul 31 17:26:39.301: INFO: Pod "projected-volume-2aabbcc9-425b-4576-bf2f-a9aceaa07ef0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012107391s
Jul 31 17:26:41.304: INFO: Pod "projected-volume-2aabbcc9-425b-4576-bf2f-a9aceaa07ef0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015251427s
STEP: Saw pod success
Jul 31 17:26:41.304: INFO: Pod "projected-volume-2aabbcc9-425b-4576-bf2f-a9aceaa07ef0" satisfied condition "success or failure"
Jul 31 17:26:41.307: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod projected-volume-2aabbcc9-425b-4576-bf2f-a9aceaa07ef0 container projected-all-volume-test: <nil>
STEP: delete the pod
Jul 31 17:26:41.322: INFO: Waiting for pod projected-volume-2aabbcc9-425b-4576-bf2f-a9aceaa07ef0 to disappear
Jul 31 17:26:41.326: INFO: Pod projected-volume-2aabbcc9-425b-4576-bf2f-a9aceaa07ef0 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:26:41.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2740" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":280,"completed":218,"skipped":3586,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:26:41.336: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7507
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7507
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-7507
I0731 17:26:41.564797      21 runners.go:189] Created replication controller with name: externalname-service, namespace: services-7507, replica count: 2
I0731 17:26:44.615230      21 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 31 17:26:44.615: INFO: Creating new exec pod
Jul 31 17:26:49.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=services-7507 execpodd6x4n -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jul 31 17:26:51.101: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 31 17:26:51.101: INFO: stdout: ""
Jul 31 17:26:51.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=services-7507 execpodd6x4n -- /bin/sh -x -c nc -zv -t -w 2 10.97.137.16 80'
Jul 31 17:26:51.409: INFO: stderr: "+ nc -zv -t -w 2 10.97.137.16 80\nConnection to 10.97.137.16 80 port [tcp/http] succeeded!\n"
Jul 31 17:26:51.409: INFO: stdout: ""
Jul 31 17:26:51.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=services-7507 execpodd6x4n -- /bin/sh -x -c nc -zv -t -w 2 10.10.102.94 31572'
Jul 31 17:26:51.663: INFO: stderr: "+ nc -zv -t -w 2 10.10.102.94 31572\nConnection to 10.10.102.94 31572 port [tcp/31572] succeeded!\n"
Jul 31 17:26:51.663: INFO: stdout: ""
Jul 31 17:26:51.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=services-7507 execpodd6x4n -- /bin/sh -x -c nc -zv -t -w 2 10.10.102.92 31572'
Jul 31 17:26:51.942: INFO: stderr: "+ nc -zv -t -w 2 10.10.102.92 31572\nConnection to 10.10.102.92 31572 port [tcp/31572] succeeded!\n"
Jul 31 17:26:51.942: INFO: stdout: ""
Jul 31 17:26:51.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=services-7507 execpodd6x4n -- /bin/sh -x -c nc -zv -t -w 2 10.10.102.94 31572'
Jul 31 17:26:52.213: INFO: stderr: "+ nc -zv -t -w 2 10.10.102.94 31572\nConnection to 10.10.102.94 31572 port [tcp/31572] succeeded!\n"
Jul 31 17:26:52.213: INFO: stdout: ""
Jul 31 17:26:52.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 exec --namespace=services-7507 execpodd6x4n -- /bin/sh -x -c nc -zv -t -w 2 10.10.102.92 31572'
Jul 31 17:26:52.469: INFO: stderr: "+ nc -zv -t -w 2 10.10.102.92 31572\nConnection to 10.10.102.92 31572 port [tcp/31572] succeeded!\n"
Jul 31 17:26:52.469: INFO: stdout: ""
Jul 31 17:26:52.469: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:26:52.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7507" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:11.204 seconds]
[sig-network] Services
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":280,"completed":219,"skipped":3637,"failed":0}
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:26:52.540: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4402
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-a14a1ba9-e2e0-4863-8471-404f0649e415
STEP: Creating secret with name s-test-opt-upd-43316f51-a3d8-4fad-b19a-e0f37881a248
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-a14a1ba9-e2e0-4863-8471-404f0649e415
STEP: Updating secret s-test-opt-upd-43316f51-a3d8-4fad-b19a-e0f37881a248
STEP: Creating secret with name s-test-opt-create-827e75c8-4deb-4aca-a3d5-537e146c5e76
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:27:00.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4402" for this suite.

• [SLOW TEST:8.399 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":220,"skipped":3637,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:27:00.939: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9173
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-fbnv
STEP: Creating a pod to test atomic-volume-subpath
Jul 31 17:27:01.100: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-fbnv" in namespace "subpath-9173" to be "success or failure"
Jul 31 17:27:01.105: INFO: Pod "pod-subpath-test-configmap-fbnv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.441374ms
Jul 31 17:27:03.110: INFO: Pod "pod-subpath-test-configmap-fbnv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009159354s
Jul 31 17:27:05.114: INFO: Pod "pod-subpath-test-configmap-fbnv": Phase="Running", Reason="", readiness=true. Elapsed: 4.01322054s
Jul 31 17:27:07.118: INFO: Pod "pod-subpath-test-configmap-fbnv": Phase="Running", Reason="", readiness=true. Elapsed: 6.017404029s
Jul 31 17:27:09.121: INFO: Pod "pod-subpath-test-configmap-fbnv": Phase="Running", Reason="", readiness=true. Elapsed: 8.020278236s
Jul 31 17:27:11.126: INFO: Pod "pod-subpath-test-configmap-fbnv": Phase="Running", Reason="", readiness=true. Elapsed: 10.025167286s
Jul 31 17:27:13.129: INFO: Pod "pod-subpath-test-configmap-fbnv": Phase="Running", Reason="", readiness=true. Elapsed: 12.028370273s
Jul 31 17:27:15.132: INFO: Pod "pod-subpath-test-configmap-fbnv": Phase="Running", Reason="", readiness=true. Elapsed: 14.031277072s
Jul 31 17:27:17.139: INFO: Pod "pod-subpath-test-configmap-fbnv": Phase="Running", Reason="", readiness=true. Elapsed: 16.038788114s
Jul 31 17:27:19.142: INFO: Pod "pod-subpath-test-configmap-fbnv": Phase="Running", Reason="", readiness=true. Elapsed: 18.041889873s
Jul 31 17:27:21.146: INFO: Pod "pod-subpath-test-configmap-fbnv": Phase="Running", Reason="", readiness=true. Elapsed: 20.045369008s
Jul 31 17:27:23.149: INFO: Pod "pod-subpath-test-configmap-fbnv": Phase="Running", Reason="", readiness=true. Elapsed: 22.048938405s
Jul 31 17:27:25.161: INFO: Pod "pod-subpath-test-configmap-fbnv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.061151636s
STEP: Saw pod success
Jul 31 17:27:25.162: INFO: Pod "pod-subpath-test-configmap-fbnv" satisfied condition "success or failure"
Jul 31 17:27:25.164: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-subpath-test-configmap-fbnv container test-container-subpath-configmap-fbnv: <nil>
STEP: delete the pod
Jul 31 17:27:25.222: INFO: Waiting for pod pod-subpath-test-configmap-fbnv to disappear
Jul 31 17:27:25.224: INFO: Pod pod-subpath-test-configmap-fbnv no longer exists
STEP: Deleting pod pod-subpath-test-configmap-fbnv
Jul 31 17:27:25.224: INFO: Deleting pod "pod-subpath-test-configmap-fbnv" in namespace "subpath-9173"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:27:25.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9173" for this suite.

• [SLOW TEST:24.297 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":280,"completed":221,"skipped":3664,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:27:25.236: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5729
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jul 31 17:27:25.393: INFO: Created pod &Pod{ObjectMeta:{dns-5729  dns-5729 /api/v1/namespaces/dns-5729/pods/dns-5729 71ec1ca1-4726-43e0-bf11-88c5673e5ed3 234954 0 2020-07-31 17:27:24 +0000 UTC <nil> <nil> map[] map[] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jmmwh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jmmwh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jmmwh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
STEP: Verifying customized DNS suffix list is configured on pod...
Jul 31 17:27:29.408: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5729 PodName:dns-5729 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 17:27:29.408: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Verifying customized DNS server is configured on pod...
Jul 31 17:27:29.581: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5729 PodName:dns-5729 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 17:27:29.581: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
Jul 31 17:27:29.800: INFO: Deleting pod dns-5729...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:27:29.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5729" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":280,"completed":222,"skipped":3669,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:27:29.830: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6741
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 31 17:27:29.991: INFO: Waiting up to 5m0s for pod "downwardapi-volume-de5f3789-b002-45c3-b8b8-4e7e4bba901d" in namespace "downward-api-6741" to be "success or failure"
Jul 31 17:27:29.995: INFO: Pod "downwardapi-volume-de5f3789-b002-45c3-b8b8-4e7e4bba901d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.584832ms
Jul 31 17:27:31.998: INFO: Pod "downwardapi-volume-de5f3789-b002-45c3-b8b8-4e7e4bba901d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007250114s
Jul 31 17:27:34.002: INFO: Pod "downwardapi-volume-de5f3789-b002-45c3-b8b8-4e7e4bba901d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011247749s
STEP: Saw pod success
Jul 31 17:27:34.002: INFO: Pod "downwardapi-volume-de5f3789-b002-45c3-b8b8-4e7e4bba901d" satisfied condition "success or failure"
Jul 31 17:27:34.007: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod downwardapi-volume-de5f3789-b002-45c3-b8b8-4e7e4bba901d container client-container: <nil>
STEP: delete the pod
Jul 31 17:27:34.033: INFO: Waiting for pod downwardapi-volume-de5f3789-b002-45c3-b8b8-4e7e4bba901d to disappear
Jul 31 17:27:34.039: INFO: Pod downwardapi-volume-de5f3789-b002-45c3-b8b8-4e7e4bba901d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:27:34.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6741" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":223,"skipped":3691,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:27:34.050: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7764
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 17:27:38.294: INFO: Waiting up to 5m0s for pod "client-envvars-b2a900a3-220f-41a1-bb5e-62e9817aca70" in namespace "pods-7764" to be "success or failure"
Jul 31 17:27:38.302: INFO: Pod "client-envvars-b2a900a3-220f-41a1-bb5e-62e9817aca70": Phase="Pending", Reason="", readiness=false. Elapsed: 7.97163ms
Jul 31 17:27:40.306: INFO: Pod "client-envvars-b2a900a3-220f-41a1-bb5e-62e9817aca70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01258467s
Jul 31 17:27:42.310: INFO: Pod "client-envvars-b2a900a3-220f-41a1-bb5e-62e9817aca70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016308132s
STEP: Saw pod success
Jul 31 17:27:42.311: INFO: Pod "client-envvars-b2a900a3-220f-41a1-bb5e-62e9817aca70" satisfied condition "success or failure"
Jul 31 17:27:42.312: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod client-envvars-b2a900a3-220f-41a1-bb5e-62e9817aca70 container env3cont: <nil>
STEP: delete the pod
Jul 31 17:27:42.344: INFO: Waiting for pod client-envvars-b2a900a3-220f-41a1-bb5e-62e9817aca70 to disappear
Jul 31 17:27:42.348: INFO: Pod client-envvars-b2a900a3-220f-41a1-bb5e-62e9817aca70 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:27:42.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7764" for this suite.

• [SLOW TEST:8.307 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":280,"completed":224,"skipped":3709,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:27:42.358: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-8865
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jul 31 17:27:42.514: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the sample API server.
Jul 31 17:27:43.259: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jul 31 17:27:45.340: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813262, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813262, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813262, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813262, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 17:27:47.344: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813262, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813262, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813262, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813262, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 17:27:49.344: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813262, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813262, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813262, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813262, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 17:27:51.344: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813262, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813262, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813262, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813262, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 17:27:53.344: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813262, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813262, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813262, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813262, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 17:27:55.345: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813262, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813262, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813262, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813262, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 17:27:57.344: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813262, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813262, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813262, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813262, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 17:28:02.174: INFO: Waited 2.801647226s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:28:02.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8865" for this suite.

• [SLOW TEST:20.684 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]","total":280,"completed":225,"skipped":3715,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:28:03.044: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6026
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jul 31 17:28:03.212: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6026 /api/v1/namespaces/watch-6026/configmaps/e2e-watch-test-configmap-a 03d5262f-74b1-487b-b1a2-acc1863e0096 235275 0 2020-07-31 17:28:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 31 17:28:03.212: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6026 /api/v1/namespaces/watch-6026/configmaps/e2e-watch-test-configmap-a 03d5262f-74b1-487b-b1a2-acc1863e0096 235275 0 2020-07-31 17:28:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jul 31 17:28:13.219: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6026 /api/v1/namespaces/watch-6026/configmaps/e2e-watch-test-configmap-a 03d5262f-74b1-487b-b1a2-acc1863e0096 235327 0 2020-07-31 17:28:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul 31 17:28:13.219: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6026 /api/v1/namespaces/watch-6026/configmaps/e2e-watch-test-configmap-a 03d5262f-74b1-487b-b1a2-acc1863e0096 235327 0 2020-07-31 17:28:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jul 31 17:28:23.227: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6026 /api/v1/namespaces/watch-6026/configmaps/e2e-watch-test-configmap-a 03d5262f-74b1-487b-b1a2-acc1863e0096 235360 0 2020-07-31 17:28:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 31 17:28:23.227: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6026 /api/v1/namespaces/watch-6026/configmaps/e2e-watch-test-configmap-a 03d5262f-74b1-487b-b1a2-acc1863e0096 235360 0 2020-07-31 17:28:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jul 31 17:28:33.233: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6026 /api/v1/namespaces/watch-6026/configmaps/e2e-watch-test-configmap-a 03d5262f-74b1-487b-b1a2-acc1863e0096 235390 0 2020-07-31 17:28:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 31 17:28:33.233: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6026 /api/v1/namespaces/watch-6026/configmaps/e2e-watch-test-configmap-a 03d5262f-74b1-487b-b1a2-acc1863e0096 235390 0 2020-07-31 17:28:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jul 31 17:28:43.241: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6026 /api/v1/namespaces/watch-6026/configmaps/e2e-watch-test-configmap-b 2ef9e972-dfad-4fcf-8b9e-a07d845e3ed3 235421 0 2020-07-31 17:28:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 31 17:28:43.241: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6026 /api/v1/namespaces/watch-6026/configmaps/e2e-watch-test-configmap-b 2ef9e972-dfad-4fcf-8b9e-a07d845e3ed3 235421 0 2020-07-31 17:28:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jul 31 17:28:53.252: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6026 /api/v1/namespaces/watch-6026/configmaps/e2e-watch-test-configmap-b 2ef9e972-dfad-4fcf-8b9e-a07d845e3ed3 235452 0 2020-07-31 17:28:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 31 17:28:53.252: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6026 /api/v1/namespaces/watch-6026/configmaps/e2e-watch-test-configmap-b 2ef9e972-dfad-4fcf-8b9e-a07d845e3ed3 235452 0 2020-07-31 17:28:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:29:03.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6026" for this suite.

• [SLOW TEST:60.218 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":280,"completed":226,"skipped":3726,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:29:03.263: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7130
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run default
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1489
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 31 17:29:03.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-7130'
Jul 31 17:29:03.490: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 31 17:29:03.491: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1495
Jul 31 17:29:03.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 delete deployment e2e-test-httpd-deployment --namespace=kubectl-7130'
Jul 31 17:29:03.626: INFO: stderr: ""
Jul 31 17:29:03.626: INFO: stdout: "deployment.extensions \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:29:03.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7130" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run default should create an rc or deployment from an image  [Conformance]","total":280,"completed":227,"skipped":3754,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:29:03.649: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8364
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 31 17:29:03.861: INFO: Waiting up to 5m0s for pod "downwardapi-volume-be7e884b-7984-4996-a634-b2bb23ec5133" in namespace "projected-8364" to be "success or failure"
Jul 31 17:29:03.870: INFO: Pod "downwardapi-volume-be7e884b-7984-4996-a634-b2bb23ec5133": Phase="Pending", Reason="", readiness=false. Elapsed: 9.678511ms
Jul 31 17:29:05.873: INFO: Pod "downwardapi-volume-be7e884b-7984-4996-a634-b2bb23ec5133": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012634953s
Jul 31 17:29:07.877: INFO: Pod "downwardapi-volume-be7e884b-7984-4996-a634-b2bb23ec5133": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016090963s
STEP: Saw pod success
Jul 31 17:29:07.877: INFO: Pod "downwardapi-volume-be7e884b-7984-4996-a634-b2bb23ec5133" satisfied condition "success or failure"
Jul 31 17:29:07.879: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod downwardapi-volume-be7e884b-7984-4996-a634-b2bb23ec5133 container client-container: <nil>
STEP: delete the pod
Jul 31 17:29:07.906: INFO: Waiting for pod downwardapi-volume-be7e884b-7984-4996-a634-b2bb23ec5133 to disappear
Jul 31 17:29:07.913: INFO: Pod downwardapi-volume-be7e884b-7984-4996-a634-b2bb23ec5133 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:29:07.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8364" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":228,"skipped":3795,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:29:07.922: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-3340
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test hostPath mode
Jul 31 17:29:08.082: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-3340" to be "success or failure"
Jul 31 17:29:08.085: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.499773ms
Jul 31 17:29:10.088: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005458112s
Jul 31 17:29:12.091: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008671245s
STEP: Saw pod success
Jul 31 17:29:12.091: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jul 31 17:29:12.093: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jul 31 17:29:12.113: INFO: Waiting for pod pod-host-path-test to disappear
Jul 31 17:29:12.124: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:29:12.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-3340" for this suite.
•{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":229,"skipped":3803,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:29:12.143: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8579
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 17:29:12.706: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 31 17:29:14.716: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813352, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813352, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813352, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813352, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 17:29:17.737: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 17:29:17.741: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7134-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:29:19.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8579" for this suite.
STEP: Destroying namespace "webhook-8579-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.311 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":280,"completed":230,"skipped":3809,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:29:19.454: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3626
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jul 31 17:29:23.652: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3626 PodName:pod-sharedvolume-6723b61c-3351-49a3-b789-4ccb15a4229c ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 17:29:23.652: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
Jul 31 17:29:23.822: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:29:23.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3626" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":280,"completed":231,"skipped":3845,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:29:23.832: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-1931
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:172
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating server pod server in namespace prestop-1931
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1931
STEP: Deleting pre-stop pod
Jul 31 17:29:37.071: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:29:37.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1931" for this suite.

• [SLOW TEST:13.264 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":280,"completed":232,"skipped":3855,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:29:37.097: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4045
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 31 17:29:37.258: INFO: Waiting up to 5m0s for pod "pod-d5a96c73-3760-4d5a-8b9e-c19b161d75fa" in namespace "emptydir-4045" to be "success or failure"
Jul 31 17:29:37.283: INFO: Pod "pod-d5a96c73-3760-4d5a-8b9e-c19b161d75fa": Phase="Pending", Reason="", readiness=false. Elapsed: 24.500903ms
Jul 31 17:29:39.287: INFO: Pod "pod-d5a96c73-3760-4d5a-8b9e-c19b161d75fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028904499s
Jul 31 17:29:41.291: INFO: Pod "pod-d5a96c73-3760-4d5a-8b9e-c19b161d75fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03228355s
STEP: Saw pod success
Jul 31 17:29:41.291: INFO: Pod "pod-d5a96c73-3760-4d5a-8b9e-c19b161d75fa" satisfied condition "success or failure"
Jul 31 17:29:41.293: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-d5a96c73-3760-4d5a-8b9e-c19b161d75fa container test-container: <nil>
STEP: delete the pod
Jul 31 17:29:41.314: INFO: Waiting for pod pod-d5a96c73-3760-4d5a-8b9e-c19b161d75fa to disappear
Jul 31 17:29:41.316: INFO: Pod pod-d5a96c73-3760-4d5a-8b9e-c19b161d75fa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:29:41.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4045" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":233,"skipped":3871,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:29:41.324: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-5720
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:29:45.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5720" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":280,"completed":234,"skipped":3892,"failed":0}
S
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:29:45.546: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7278
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:29:45.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7278" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":280,"completed":235,"skipped":3893,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:29:45.716: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5755
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 17:29:45.883: INFO: Create a RollingUpdate DaemonSet
Jul 31 17:29:45.888: INFO: Check that daemon pods launch on every node of the cluster
Jul 31 17:29:45.903: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:29:45.909: INFO: Number of nodes with available pods: 0
Jul 31 17:29:45.909: INFO: Node test-aruna-123-node-group-687e2c91ce is running more than one daemon pod
Jul 31 17:29:46.914: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:29:46.918: INFO: Number of nodes with available pods: 0
Jul 31 17:29:46.918: INFO: Node test-aruna-123-node-group-687e2c91ce is running more than one daemon pod
Jul 31 17:29:47.920: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:29:47.924: INFO: Number of nodes with available pods: 0
Jul 31 17:29:47.924: INFO: Node test-aruna-123-node-group-687e2c91ce is running more than one daemon pod
Jul 31 17:29:48.914: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:29:48.918: INFO: Number of nodes with available pods: 2
Jul 31 17:29:48.918: INFO: Node test-aruna-123-node-group-a92c781fbd is running more than one daemon pod
Jul 31 17:29:49.919: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:29:49.922: INFO: Number of nodes with available pods: 3
Jul 31 17:29:49.922: INFO: Number of running nodes: 3, number of available pods: 3
Jul 31 17:29:49.922: INFO: Update the DaemonSet to trigger a rollout
Jul 31 17:29:49.929: INFO: Updating DaemonSet daemon-set
Jul 31 17:29:52.999: INFO: Roll back the DaemonSet before rollout is complete
Jul 31 17:29:53.010: INFO: Updating DaemonSet daemon-set
Jul 31 17:29:53.010: INFO: Make sure DaemonSet rollback is complete
Jul 31 17:29:53.013: INFO: Wrong image for pod: daemon-set-kr6jc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jul 31 17:29:53.013: INFO: Pod daemon-set-kr6jc is not available
Jul 31 17:29:53.023: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:29:54.028: INFO: Wrong image for pod: daemon-set-kr6jc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jul 31 17:29:54.028: INFO: Pod daemon-set-kr6jc is not available
Jul 31 17:29:54.031: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:29:55.026: INFO: Wrong image for pod: daemon-set-kr6jc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jul 31 17:29:55.027: INFO: Pod daemon-set-kr6jc is not available
Jul 31 17:29:55.031: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 17:29:56.027: INFO: Pod daemon-set-47p7n is not available
Jul 31 17:29:56.032: INFO: DaemonSet pods can't tolerate node test-aruna-123-master-gro-1eb9ae3485 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5755, will wait for the garbage collector to delete the pods
Jul 31 17:29:56.103: INFO: Deleting DaemonSet.extensions daemon-set took: 4.825525ms
Jul 31 17:29:56.703: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.382647ms
Jul 31 17:30:05.506: INFO: Number of nodes with available pods: 0
Jul 31 17:30:05.506: INFO: Number of running nodes: 0, number of available pods: 0
Jul 31 17:30:05.508: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5755/daemonsets","resourceVersion":"236157"},"items":null}

Jul 31 17:30:05.512: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5755/pods","resourceVersion":"236158"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:30:05.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5755" for this suite.

• [SLOW TEST:19.819 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":280,"completed":236,"skipped":3907,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:30:05.537: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9428
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jul 31 17:30:10.254: INFO: Successfully updated pod "labelsupdate765b817b-a61f-4871-bdf1-be4364a8d414"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:30:12.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9428" for this suite.

• [SLOW TEST:6.764 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":237,"skipped":3955,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:30:12.301: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-6656
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
Jul 31 17:30:13.012: INFO: created pod pod-service-account-defaultsa
Jul 31 17:30:13.012: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jul 31 17:30:13.020: INFO: created pod pod-service-account-mountsa
Jul 31 17:30:13.020: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jul 31 17:30:13.033: INFO: created pod pod-service-account-nomountsa
Jul 31 17:30:13.033: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jul 31 17:30:13.047: INFO: created pod pod-service-account-defaultsa-mountspec
Jul 31 17:30:13.047: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jul 31 17:30:13.069: INFO: created pod pod-service-account-mountsa-mountspec
Jul 31 17:30:13.069: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jul 31 17:30:13.093: INFO: created pod pod-service-account-nomountsa-mountspec
Jul 31 17:30:13.093: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jul 31 17:30:13.122: INFO: created pod pod-service-account-defaultsa-nomountspec
Jul 31 17:30:13.122: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jul 31 17:30:13.128: INFO: created pod pod-service-account-mountsa-nomountspec
Jul 31 17:30:13.128: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jul 31 17:30:13.143: INFO: created pod pod-service-account-nomountsa-nomountspec
Jul 31 17:30:13.143: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:30:13.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6656" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":280,"completed":238,"skipped":3990,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:30:13.202: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6547
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 31 17:30:16.415: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:30:16.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6547" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":239,"skipped":4028,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:30:16.466: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-2546
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:344
Jul 31 17:30:16.635: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 31 17:31:16.668: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 17:31:16.671: INFO: Starting informer...
STEP: Starting pods...
Jul 31 17:31:16.887: INFO: Pod1 is running on test-aruna-123-node-group-687e2c91ce. Tainting Node
Jul 31 17:31:20.931: INFO: Pod2 is running on test-aruna-123-node-group-687e2c91ce. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jul 31 17:31:33.288: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jul 31 17:31:53.254: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:31:53.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-2546" for this suite.

• [SLOW TEST:96.856 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":280,"completed":240,"skipped":4066,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:31:53.323: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-9129
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jul 31 17:31:53.942: INFO: Pod name wrapped-volume-race-553ae4b0-1121-4ae6-a9d7-447619b21fee: Found 0 pods out of 5
Jul 31 17:31:58.949: INFO: Pod name wrapped-volume-race-553ae4b0-1121-4ae6-a9d7-447619b21fee: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-553ae4b0-1121-4ae6-a9d7-447619b21fee in namespace emptydir-wrapper-9129, will wait for the garbage collector to delete the pods
Jul 31 17:32:13.046: INFO: Deleting ReplicationController wrapped-volume-race-553ae4b0-1121-4ae6-a9d7-447619b21fee took: 8.799447ms
Jul 31 17:32:13.647: INFO: Terminating ReplicationController wrapped-volume-race-553ae4b0-1121-4ae6-a9d7-447619b21fee pods took: 600.225592ms
STEP: Creating RC which spawns configmap-volume pods
Jul 31 17:32:24.765: INFO: Pod name wrapped-volume-race-674e2df4-1616-4a4f-b026-7c29173071b5: Found 0 pods out of 5
Jul 31 17:32:29.770: INFO: Pod name wrapped-volume-race-674e2df4-1616-4a4f-b026-7c29173071b5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-674e2df4-1616-4a4f-b026-7c29173071b5 in namespace emptydir-wrapper-9129, will wait for the garbage collector to delete the pods
Jul 31 17:32:41.894: INFO: Deleting ReplicationController wrapped-volume-race-674e2df4-1616-4a4f-b026-7c29173071b5 took: 6.520867ms
Jul 31 17:32:42.494: INFO: Terminating ReplicationController wrapped-volume-race-674e2df4-1616-4a4f-b026-7c29173071b5 pods took: 600.290759ms
STEP: Creating RC which spawns configmap-volume pods
Jul 31 17:32:55.509: INFO: Pod name wrapped-volume-race-e44d7f6c-b813-4459-9005-606e4f051a3d: Found 0 pods out of 5
Jul 31 17:33:00.514: INFO: Pod name wrapped-volume-race-e44d7f6c-b813-4459-9005-606e4f051a3d: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e44d7f6c-b813-4459-9005-606e4f051a3d in namespace emptydir-wrapper-9129, will wait for the garbage collector to delete the pods
Jul 31 17:33:12.590: INFO: Deleting ReplicationController wrapped-volume-race-e44d7f6c-b813-4459-9005-606e4f051a3d took: 4.633192ms
Jul 31 17:33:13.291: INFO: Terminating ReplicationController wrapped-volume-race-e44d7f6c-b813-4459-9005-606e4f051a3d pods took: 700.209997ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:33:26.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9129" for this suite.

• [SLOW TEST:92.890 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":280,"completed":241,"skipped":4073,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:33:26.214: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4314
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 31 17:33:26.373: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7eac8b2d-593c-4af2-b98e-dd91478a1929" in namespace "downward-api-4314" to be "success or failure"
Jul 31 17:33:26.381: INFO: Pod "downwardapi-volume-7eac8b2d-593c-4af2-b98e-dd91478a1929": Phase="Pending", Reason="", readiness=false. Elapsed: 7.856065ms
Jul 31 17:33:28.389: INFO: Pod "downwardapi-volume-7eac8b2d-593c-4af2-b98e-dd91478a1929": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0163214s
Jul 31 17:33:30.393: INFO: Pod "downwardapi-volume-7eac8b2d-593c-4af2-b98e-dd91478a1929": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019476832s
STEP: Saw pod success
Jul 31 17:33:30.393: INFO: Pod "downwardapi-volume-7eac8b2d-593c-4af2-b98e-dd91478a1929" satisfied condition "success or failure"
Jul 31 17:33:30.395: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod downwardapi-volume-7eac8b2d-593c-4af2-b98e-dd91478a1929 container client-container: <nil>
STEP: delete the pod
Jul 31 17:33:30.471: INFO: Waiting for pod downwardapi-volume-7eac8b2d-593c-4af2-b98e-dd91478a1929 to disappear
Jul 31 17:33:30.478: INFO: Pod downwardapi-volume-7eac8b2d-593c-4af2-b98e-dd91478a1929 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:33:30.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4314" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":242,"skipped":4092,"failed":0}
SSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:33:30.495: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4809
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:178
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:33:30.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4809" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":280,"completed":243,"skipped":4095,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:33:30.704: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4502
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 31 17:33:30.856: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c7f56958-18e0-4852-b909-8d814607712f" in namespace "projected-4502" to be "success or failure"
Jul 31 17:33:30.864: INFO: Pod "downwardapi-volume-c7f56958-18e0-4852-b909-8d814607712f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.551961ms
Jul 31 17:33:32.867: INFO: Pod "downwardapi-volume-c7f56958-18e0-4852-b909-8d814607712f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011008921s
Jul 31 17:33:34.870: INFO: Pod "downwardapi-volume-c7f56958-18e0-4852-b909-8d814607712f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014155179s
STEP: Saw pod success
Jul 31 17:33:34.870: INFO: Pod "downwardapi-volume-c7f56958-18e0-4852-b909-8d814607712f" satisfied condition "success or failure"
Jul 31 17:33:34.872: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod downwardapi-volume-c7f56958-18e0-4852-b909-8d814607712f container client-container: <nil>
STEP: delete the pod
Jul 31 17:33:34.898: INFO: Waiting for pod downwardapi-volume-c7f56958-18e0-4852-b909-8d814607712f to disappear
Jul 31 17:33:34.908: INFO: Pod downwardapi-volume-c7f56958-18e0-4852-b909-8d814607712f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:33:34.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4502" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":244,"skipped":4097,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:33:34.936: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-2069
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 17:33:35.128: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-d7d50f4b-fed6-4301-9989-d291965945e3" in namespace "security-context-test-2069" to be "success or failure"
Jul 31 17:33:35.141: INFO: Pod "busybox-readonly-false-d7d50f4b-fed6-4301-9989-d291965945e3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.765555ms
Jul 31 17:33:37.145: INFO: Pod "busybox-readonly-false-d7d50f4b-fed6-4301-9989-d291965945e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017115854s
Jul 31 17:33:39.147: INFO: Pod "busybox-readonly-false-d7d50f4b-fed6-4301-9989-d291965945e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019758212s
Jul 31 17:33:39.147: INFO: Pod "busybox-readonly-false-d7d50f4b-fed6-4301-9989-d291965945e3" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:33:39.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2069" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":280,"completed":245,"skipped":4144,"failed":0}
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:33:39.153: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2174
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jul 31 17:33:39.309: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:33:42.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2174" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":280,"completed":246,"skipped":4145,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:33:42.931: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6250
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 31 17:33:43.114: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6d32c9cd-c03f-453a-9323-7889f7cd7797" in namespace "downward-api-6250" to be "success or failure"
Jul 31 17:33:43.132: INFO: Pod "downwardapi-volume-6d32c9cd-c03f-453a-9323-7889f7cd7797": Phase="Pending", Reason="", readiness=false. Elapsed: 17.848509ms
Jul 31 17:33:45.136: INFO: Pod "downwardapi-volume-6d32c9cd-c03f-453a-9323-7889f7cd7797": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021112709s
Jul 31 17:33:47.139: INFO: Pod "downwardapi-volume-6d32c9cd-c03f-453a-9323-7889f7cd7797": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024908513s
STEP: Saw pod success
Jul 31 17:33:47.140: INFO: Pod "downwardapi-volume-6d32c9cd-c03f-453a-9323-7889f7cd7797" satisfied condition "success or failure"
Jul 31 17:33:47.142: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod downwardapi-volume-6d32c9cd-c03f-453a-9323-7889f7cd7797 container client-container: <nil>
STEP: delete the pod
Jul 31 17:33:47.172: INFO: Waiting for pod downwardapi-volume-6d32c9cd-c03f-453a-9323-7889f7cd7797 to disappear
Jul 31 17:33:47.177: INFO: Pod downwardapi-volume-6d32c9cd-c03f-453a-9323-7889f7cd7797 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:33:47.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6250" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":247,"skipped":4150,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:33:47.186: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2623
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0731 17:33:57.403869      21 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 31 17:33:57.404: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:33:57.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2623" for this suite.

• [SLOW TEST:10.229 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":280,"completed":248,"skipped":4159,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:33:57.415: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-593
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-593
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating statefulset ss in namespace statefulset-593
Jul 31 17:33:57.594: INFO: Found 0 stateful pods, waiting for 1
Jul 31 17:34:07.597: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jul 31 17:34:07.613: INFO: Deleting all statefulset in ns statefulset-593
Jul 31 17:34:07.626: INFO: Scaling statefulset ss to 0
Jul 31 17:34:27.684: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 17:34:27.686: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:34:27.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-593" for this suite.

• [SLOW TEST:30.307 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":280,"completed":249,"skipped":4169,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:34:27.723: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4542
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:34:40.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4542" for this suite.

• [SLOW TEST:13.239 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":280,"completed":250,"skipped":4181,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:34:40.963: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-8678
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jul 31 17:34:46.180: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:34:46.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8678" for this suite.

• [SLOW TEST:5.281 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":280,"completed":251,"skipped":4199,"failed":0}
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:34:46.244: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8168
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:34:50.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8168" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":280,"completed":252,"skipped":4200,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:34:50.423: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2164
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 31 17:34:50.578: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a7563cb8-4ead-463d-bd7f-8074b467125e" in namespace "projected-2164" to be "success or failure"
Jul 31 17:34:50.581: INFO: Pod "downwardapi-volume-a7563cb8-4ead-463d-bd7f-8074b467125e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.250343ms
Jul 31 17:34:52.584: INFO: Pod "downwardapi-volume-a7563cb8-4ead-463d-bd7f-8074b467125e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006013518s
Jul 31 17:34:54.587: INFO: Pod "downwardapi-volume-a7563cb8-4ead-463d-bd7f-8074b467125e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009361383s
STEP: Saw pod success
Jul 31 17:34:54.587: INFO: Pod "downwardapi-volume-a7563cb8-4ead-463d-bd7f-8074b467125e" satisfied condition "success or failure"
Jul 31 17:34:54.590: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod downwardapi-volume-a7563cb8-4ead-463d-bd7f-8074b467125e container client-container: <nil>
STEP: delete the pod
Jul 31 17:34:54.619: INFO: Waiting for pod downwardapi-volume-a7563cb8-4ead-463d-bd7f-8074b467125e to disappear
Jul 31 17:34:54.623: INFO: Pod downwardapi-volume-a7563cb8-4ead-463d-bd7f-8074b467125e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:34:54.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2164" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":253,"skipped":4217,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:34:54.630: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-19
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 17:34:55.431: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 31 17:34:57.438: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813694, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813694, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813695, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813694, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 17:35:00.454: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 17:35:00.458: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3355-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:35:01.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-19" for this suite.
STEP: Destroying namespace "webhook-19-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.012 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":280,"completed":254,"skipped":4218,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:35:01.646: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2425
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-38b5ca29-d20a-4a8e-b6c1-ca9ad31f0ea4
STEP: Creating a pod to test consume secrets
Jul 31 17:35:01.829: INFO: Waiting up to 5m0s for pod "pod-secrets-0c5cbc15-0069-4212-aef5-de1a67c7f8c1" in namespace "secrets-2425" to be "success or failure"
Jul 31 17:35:01.844: INFO: Pod "pod-secrets-0c5cbc15-0069-4212-aef5-de1a67c7f8c1": Phase="Pending", Reason="", readiness=false. Elapsed: 15.225155ms
Jul 31 17:35:03.849: INFO: Pod "pod-secrets-0c5cbc15-0069-4212-aef5-de1a67c7f8c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020615612s
Jul 31 17:35:05.855: INFO: Pod "pod-secrets-0c5cbc15-0069-4212-aef5-de1a67c7f8c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026294283s
STEP: Saw pod success
Jul 31 17:35:05.855: INFO: Pod "pod-secrets-0c5cbc15-0069-4212-aef5-de1a67c7f8c1" satisfied condition "success or failure"
Jul 31 17:35:05.857: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-secrets-0c5cbc15-0069-4212-aef5-de1a67c7f8c1 container secret-volume-test: <nil>
STEP: delete the pod
Jul 31 17:35:05.879: INFO: Waiting for pod pod-secrets-0c5cbc15-0069-4212-aef5-de1a67c7f8c1 to disappear
Jul 31 17:35:05.881: INFO: Pod pod-secrets-0c5cbc15-0069-4212-aef5-de1a67c7f8c1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:35:05.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2425" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":255,"skipped":4237,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:35:05.892: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6702
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Jul 31 17:35:06.646: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:35:06.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0731 17:35:06.646336      21 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-6702" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":280,"completed":256,"skipped":4238,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:35:06.662: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2780
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-2780
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 31 17:35:06.836: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 31 17:35:30.974: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.220.114:8080/dial?request=hostname&protocol=http&host=192.168.220.113&port=8080&tries=1'] Namespace:pod-network-test-2780 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 17:35:30.974: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
Jul 31 17:35:31.156: INFO: Waiting for responses: map[]
Jul 31 17:35:31.161: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.220.114:8080/dial?request=hostname&protocol=http&host=192.168.86.86&port=8080&tries=1'] Namespace:pod-network-test-2780 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 17:35:31.161: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
Jul 31 17:35:31.406: INFO: Waiting for responses: map[]
Jul 31 17:35:31.409: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.220.114:8080/dial?request=hostname&protocol=http&host=192.168.55.127&port=8080&tries=1'] Namespace:pod-network-test-2780 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 17:35:31.409: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
Jul 31 17:35:31.609: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:35:31.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2780" for this suite.

• [SLOW TEST:24.960 seconds]
[sig-network] Networking
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":257,"skipped":4264,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:35:31.624: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1712
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-dcde24a7-5d67-419e-b54c-247ea898c5e9
STEP: Creating a pod to test consume secrets
Jul 31 17:35:31.779: INFO: Waiting up to 5m0s for pod "pod-secrets-de0bb63d-f5d6-4a82-9a0f-e0a002c3235d" in namespace "secrets-1712" to be "success or failure"
Jul 31 17:35:31.783: INFO: Pod "pod-secrets-de0bb63d-f5d6-4a82-9a0f-e0a002c3235d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.536173ms
Jul 31 17:35:33.786: INFO: Pod "pod-secrets-de0bb63d-f5d6-4a82-9a0f-e0a002c3235d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007540115s
Jul 31 17:35:35.790: INFO: Pod "pod-secrets-de0bb63d-f5d6-4a82-9a0f-e0a002c3235d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010905284s
STEP: Saw pod success
Jul 31 17:35:35.790: INFO: Pod "pod-secrets-de0bb63d-f5d6-4a82-9a0f-e0a002c3235d" satisfied condition "success or failure"
Jul 31 17:35:35.791: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-secrets-de0bb63d-f5d6-4a82-9a0f-e0a002c3235d container secret-volume-test: <nil>
STEP: delete the pod
Jul 31 17:35:35.816: INFO: Waiting for pod pod-secrets-de0bb63d-f5d6-4a82-9a0f-e0a002c3235d to disappear
Jul 31 17:35:35.819: INFO: Pod pod-secrets-de0bb63d-f5d6-4a82-9a0f-e0a002c3235d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:35:35.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1712" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":258,"skipped":4277,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:35:35.829: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2465
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-2465
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 31 17:35:35.985: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 31 17:36:02.088: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.220.117:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2465 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 17:36:02.088: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
Jul 31 17:36:02.266: INFO: Found all expected endpoints: [netserver-0]
Jul 31 17:36:02.269: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.86.88:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2465 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 17:36:02.269: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
Jul 31 17:36:02.450: INFO: Found all expected endpoints: [netserver-1]
Jul 31 17:36:02.453: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.55.69:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2465 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 17:36:02.453: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
Jul 31 17:36:02.638: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:36:02.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2465" for this suite.

• [SLOW TEST:26.821 seconds]
[sig-network] Networking
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":259,"skipped":4295,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:36:02.650: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5310
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 17:36:03.247: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 31 17:36:05.255: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813762, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813762, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813762, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813762, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 17:36:08.275: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:36:08.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5310" for this suite.
STEP: Destroying namespace "webhook-5310-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.098 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":280,"completed":260,"skipped":4295,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:36:08.751: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-5619
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 17:36:08.907: INFO: (0) /api/v1/nodes/test-aruna-123-node-group-a8f122b701:10250/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 6.861796ms)
Jul 31 17:36:08.911: INFO: (1) /api/v1/nodes/test-aruna-123-node-group-a8f122b701:10250/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 3.210506ms)
Jul 31 17:36:08.915: INFO: (2) /api/v1/nodes/test-aruna-123-node-group-a8f122b701:10250/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 3.658942ms)
Jul 31 17:36:08.918: INFO: (3) /api/v1/nodes/test-aruna-123-node-group-a8f122b701:10250/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 3.585772ms)
Jul 31 17:36:08.922: INFO: (4) /api/v1/nodes/test-aruna-123-node-group-a8f122b701:10250/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 3.650326ms)
Jul 31 17:36:08.925: INFO: (5) /api/v1/nodes/test-aruna-123-node-group-a8f122b701:10250/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 2.913692ms)
Jul 31 17:36:08.931: INFO: (6) /api/v1/nodes/test-aruna-123-node-group-a8f122b701:10250/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 5.719915ms)
Jul 31 17:36:08.934: INFO: (7) /api/v1/nodes/test-aruna-123-node-group-a8f122b701:10250/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 2.969353ms)
Jul 31 17:36:08.937: INFO: (8) /api/v1/nodes/test-aruna-123-node-group-a8f122b701:10250/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 2.862058ms)
Jul 31 17:36:08.942: INFO: (9) /api/v1/nodes/test-aruna-123-node-group-a8f122b701:10250/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 5.474357ms)
Jul 31 17:36:08.950: INFO: (10) /api/v1/nodes/test-aruna-123-node-group-a8f122b701:10250/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 8.027975ms)
Jul 31 17:36:08.957: INFO: (11) /api/v1/nodes/test-aruna-123-node-group-a8f122b701:10250/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 6.663608ms)
Jul 31 17:36:08.961: INFO: (12) /api/v1/nodes/test-aruna-123-node-group-a8f122b701:10250/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 3.602954ms)
Jul 31 17:36:08.966: INFO: (13) /api/v1/nodes/test-aruna-123-node-group-a8f122b701:10250/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 4.545216ms)
Jul 31 17:36:08.969: INFO: (14) /api/v1/nodes/test-aruna-123-node-group-a8f122b701:10250/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 3.360222ms)
Jul 31 17:36:08.972: INFO: (15) /api/v1/nodes/test-aruna-123-node-group-a8f122b701:10250/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 3.009807ms)
Jul 31 17:36:08.981: INFO: (16) /api/v1/nodes/test-aruna-123-node-group-a8f122b701:10250/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 8.464541ms)
Jul 31 17:36:08.987: INFO: (17) /api/v1/nodes/test-aruna-123-node-group-a8f122b701:10250/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 5.914916ms)
Jul 31 17:36:08.990: INFO: (18) /api/v1/nodes/test-aruna-123-node-group-a8f122b701:10250/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 3.374833ms)
Jul 31 17:36:09.003: INFO: (19) /api/v1/nodes/test-aruna-123-node-group-a8f122b701:10250/proxy/logs/: <pre>
<a href="apparmor/">apparmor/</a>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a hr... (200; 12.680795ms)
[AfterEach] version v1
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:36:09.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5619" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":280,"completed":261,"skipped":4342,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:36:09.028: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-717
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 31 17:36:09.193: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0c5842a2-8fd7-4416-8dbf-d12c420a8056" in namespace "downward-api-717" to be "success or failure"
Jul 31 17:36:09.204: INFO: Pod "downwardapi-volume-0c5842a2-8fd7-4416-8dbf-d12c420a8056": Phase="Pending", Reason="", readiness=false. Elapsed: 10.962896ms
Jul 31 17:36:11.207: INFO: Pod "downwardapi-volume-0c5842a2-8fd7-4416-8dbf-d12c420a8056": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014142227s
Jul 31 17:36:13.216: INFO: Pod "downwardapi-volume-0c5842a2-8fd7-4416-8dbf-d12c420a8056": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022863233s
STEP: Saw pod success
Jul 31 17:36:13.216: INFO: Pod "downwardapi-volume-0c5842a2-8fd7-4416-8dbf-d12c420a8056" satisfied condition "success or failure"
Jul 31 17:36:13.220: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod downwardapi-volume-0c5842a2-8fd7-4416-8dbf-d12c420a8056 container client-container: <nil>
STEP: delete the pod
Jul 31 17:36:13.258: INFO: Waiting for pod downwardapi-volume-0c5842a2-8fd7-4416-8dbf-d12c420a8056 to disappear
Jul 31 17:36:13.267: INFO: Pod downwardapi-volume-0c5842a2-8fd7-4416-8dbf-d12c420a8056 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:36:13.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-717" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":262,"skipped":4384,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:36:13.277: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4115
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:36:18.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4115" for this suite.

• [SLOW TEST:5.410 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":280,"completed":263,"skipped":4387,"failed":0}
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:36:18.687: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1316
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override all
Jul 31 17:36:18.851: INFO: Waiting up to 5m0s for pod "client-containers-8b8d7422-0a34-4e5b-b260-0f4b79aca345" in namespace "containers-1316" to be "success or failure"
Jul 31 17:36:18.861: INFO: Pod "client-containers-8b8d7422-0a34-4e5b-b260-0f4b79aca345": Phase="Pending", Reason="", readiness=false. Elapsed: 10.350044ms
Jul 31 17:36:20.864: INFO: Pod "client-containers-8b8d7422-0a34-4e5b-b260-0f4b79aca345": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013185264s
Jul 31 17:36:22.867: INFO: Pod "client-containers-8b8d7422-0a34-4e5b-b260-0f4b79aca345": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016693404s
STEP: Saw pod success
Jul 31 17:36:22.867: INFO: Pod "client-containers-8b8d7422-0a34-4e5b-b260-0f4b79aca345" satisfied condition "success or failure"
Jul 31 17:36:22.874: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod client-containers-8b8d7422-0a34-4e5b-b260-0f4b79aca345 container test-container: <nil>
STEP: delete the pod
Jul 31 17:36:22.908: INFO: Waiting for pod client-containers-8b8d7422-0a34-4e5b-b260-0f4b79aca345 to disappear
Jul 31 17:36:22.913: INFO: Pod client-containers-8b8d7422-0a34-4e5b-b260-0f4b79aca345 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:36:22.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1316" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":280,"completed":264,"skipped":4390,"failed":0}
SSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:36:22.925: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9205
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 17:36:23.098: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:36:27.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9205" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":280,"completed":265,"skipped":4395,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:36:27.152: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4999
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 17:36:27.989: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 31 17:36:29.998: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813787, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813787, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813787, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63731813787, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 17:36:33.011: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jul 31 17:36:37.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 attach --namespace=webhook-4999 to-be-attached-pod -i -c=container1'
Jul 31 17:36:37.134: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:36:37.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4999" for this suite.
STEP: Destroying namespace "webhook-4999-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.086 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":280,"completed":266,"skipped":4430,"failed":0}
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:36:37.238: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4553
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jul 31 17:36:37.386: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:36:41.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4553" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":280,"completed":267,"skipped":4432,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:36:41.934: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3285
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jul 31 17:36:42.091: INFO: Pod name pod-release: Found 0 pods out of 1
Jul 31 17:36:47.094: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:36:47.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3285" for this suite.

• [SLOW TEST:5.229 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":280,"completed":268,"skipped":4441,"failed":0}
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:36:47.163: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5497
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-projected-6kxj
STEP: Creating a pod to test atomic-volume-subpath
Jul 31 17:36:47.349: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-6kxj" in namespace "subpath-5497" to be "success or failure"
Jul 31 17:36:47.364: INFO: Pod "pod-subpath-test-projected-6kxj": Phase="Pending", Reason="", readiness=false. Elapsed: 15.316658ms
Jul 31 17:36:49.371: INFO: Pod "pod-subpath-test-projected-6kxj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021534813s
Jul 31 17:36:51.374: INFO: Pod "pod-subpath-test-projected-6kxj": Phase="Running", Reason="", readiness=true. Elapsed: 4.024888595s
Jul 31 17:36:53.377: INFO: Pod "pod-subpath-test-projected-6kxj": Phase="Running", Reason="", readiness=true. Elapsed: 6.027799157s
Jul 31 17:36:55.381: INFO: Pod "pod-subpath-test-projected-6kxj": Phase="Running", Reason="", readiness=true. Elapsed: 8.032313174s
Jul 31 17:36:57.384: INFO: Pod "pod-subpath-test-projected-6kxj": Phase="Running", Reason="", readiness=true. Elapsed: 10.035091074s
Jul 31 17:36:59.388: INFO: Pod "pod-subpath-test-projected-6kxj": Phase="Running", Reason="", readiness=true. Elapsed: 12.038687975s
Jul 31 17:37:01.393: INFO: Pod "pod-subpath-test-projected-6kxj": Phase="Running", Reason="", readiness=true. Elapsed: 14.043659908s
Jul 31 17:37:03.407: INFO: Pod "pod-subpath-test-projected-6kxj": Phase="Running", Reason="", readiness=true. Elapsed: 16.057712939s
Jul 31 17:37:05.411: INFO: Pod "pod-subpath-test-projected-6kxj": Phase="Running", Reason="", readiness=true. Elapsed: 18.061685388s
Jul 31 17:37:07.414: INFO: Pod "pod-subpath-test-projected-6kxj": Phase="Running", Reason="", readiness=true. Elapsed: 20.06517148s
Jul 31 17:37:09.417: INFO: Pod "pod-subpath-test-projected-6kxj": Phase="Running", Reason="", readiness=true. Elapsed: 22.067898958s
Jul 31 17:37:11.420: INFO: Pod "pod-subpath-test-projected-6kxj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.07117895s
STEP: Saw pod success
Jul 31 17:37:11.420: INFO: Pod "pod-subpath-test-projected-6kxj" satisfied condition "success or failure"
Jul 31 17:37:11.424: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-subpath-test-projected-6kxj container test-container-subpath-projected-6kxj: <nil>
STEP: delete the pod
Jul 31 17:37:11.455: INFO: Waiting for pod pod-subpath-test-projected-6kxj to disappear
Jul 31 17:37:11.458: INFO: Pod pod-subpath-test-projected-6kxj no longer exists
STEP: Deleting pod pod-subpath-test-projected-6kxj
Jul 31 17:37:11.458: INFO: Deleting pod "pod-subpath-test-projected-6kxj" in namespace "subpath-5497"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:37:11.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5497" for this suite.

• [SLOW TEST:24.312 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":280,"completed":269,"skipped":4443,"failed":0}
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:37:11.475: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-738
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Jul 31 17:37:11.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 create -f - --namespace=kubectl-738'
Jul 31 17:37:13.086: INFO: stderr: ""
Jul 31 17:37:13.087: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 31 17:37:13.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-738'
Jul 31 17:37:13.167: INFO: stderr: ""
Jul 31 17:37:13.167: INFO: stdout: "update-demo-nautilus-gbtqg update-demo-nautilus-vvn9g "
Jul 31 17:37:13.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods update-demo-nautilus-gbtqg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jul 31 17:37:13.229: INFO: stderr: ""
Jul 31 17:37:13.229: INFO: stdout: ""
Jul 31 17:37:13.229: INFO: update-demo-nautilus-gbtqg is created but not running
Jul 31 17:37:18.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-738'
Jul 31 17:37:18.306: INFO: stderr: ""
Jul 31 17:37:18.306: INFO: stdout: "update-demo-nautilus-gbtqg update-demo-nautilus-vvn9g "
Jul 31 17:37:18.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods update-demo-nautilus-gbtqg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jul 31 17:37:18.382: INFO: stderr: ""
Jul 31 17:37:18.382: INFO: stdout: "true"
Jul 31 17:37:18.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods update-demo-nautilus-gbtqg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jul 31 17:37:18.461: INFO: stderr: ""
Jul 31 17:37:18.461: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 31 17:37:18.461: INFO: validating pod update-demo-nautilus-gbtqg
Jul 31 17:37:18.467: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 31 17:37:18.467: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 31 17:37:18.467: INFO: update-demo-nautilus-gbtqg is verified up and running
Jul 31 17:37:18.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods update-demo-nautilus-vvn9g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jul 31 17:37:18.552: INFO: stderr: ""
Jul 31 17:37:18.552: INFO: stdout: "true"
Jul 31 17:37:18.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods update-demo-nautilus-vvn9g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jul 31 17:37:18.631: INFO: stderr: ""
Jul 31 17:37:18.631: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 31 17:37:18.631: INFO: validating pod update-demo-nautilus-vvn9g
Jul 31 17:37:18.637: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 31 17:37:18.637: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 31 17:37:18.637: INFO: update-demo-nautilus-vvn9g is verified up and running
STEP: scaling down the replication controller
Jul 31 17:37:18.639: INFO: scanned /root for discovery docs: <nil>
Jul 31 17:37:18.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-738'
Jul 31 17:37:19.737: INFO: stderr: ""
Jul 31 17:37:19.737: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 31 17:37:19.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-738'
Jul 31 17:37:19.824: INFO: stderr: ""
Jul 31 17:37:19.824: INFO: stdout: "update-demo-nautilus-gbtqg update-demo-nautilus-vvn9g "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 31 17:37:24.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-738'
Jul 31 17:37:24.888: INFO: stderr: ""
Jul 31 17:37:24.888: INFO: stdout: "update-demo-nautilus-gbtqg update-demo-nautilus-vvn9g "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 31 17:37:29.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-738'
Jul 31 17:37:29.953: INFO: stderr: ""
Jul 31 17:37:29.954: INFO: stdout: "update-demo-nautilus-gbtqg "
Jul 31 17:37:29.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods update-demo-nautilus-gbtqg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jul 31 17:37:30.015: INFO: stderr: ""
Jul 31 17:37:30.015: INFO: stdout: "true"
Jul 31 17:37:30.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods update-demo-nautilus-gbtqg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jul 31 17:37:30.083: INFO: stderr: ""
Jul 31 17:37:30.083: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 31 17:37:30.083: INFO: validating pod update-demo-nautilus-gbtqg
Jul 31 17:37:30.087: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 31 17:37:30.087: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 31 17:37:30.087: INFO: update-demo-nautilus-gbtqg is verified up and running
STEP: scaling up the replication controller
Jul 31 17:37:30.088: INFO: scanned /root for discovery docs: <nil>
Jul 31 17:37:30.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-738'
Jul 31 17:37:31.204: INFO: stderr: ""
Jul 31 17:37:31.204: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 31 17:37:31.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-738'
Jul 31 17:37:31.276: INFO: stderr: ""
Jul 31 17:37:31.276: INFO: stdout: "update-demo-nautilus-gbtqg update-demo-nautilus-r6bk7 "
Jul 31 17:37:31.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods update-demo-nautilus-gbtqg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jul 31 17:37:31.356: INFO: stderr: ""
Jul 31 17:37:31.356: INFO: stdout: "true"
Jul 31 17:37:31.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods update-demo-nautilus-gbtqg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jul 31 17:37:31.432: INFO: stderr: ""
Jul 31 17:37:31.432: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 31 17:37:31.432: INFO: validating pod update-demo-nautilus-gbtqg
Jul 31 17:37:31.442: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 31 17:37:31.442: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 31 17:37:31.442: INFO: update-demo-nautilus-gbtqg is verified up and running
Jul 31 17:37:31.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods update-demo-nautilus-r6bk7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jul 31 17:37:31.509: INFO: stderr: ""
Jul 31 17:37:31.509: INFO: stdout: ""
Jul 31 17:37:31.509: INFO: update-demo-nautilus-r6bk7 is created but not running
Jul 31 17:37:36.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-738'
Jul 31 17:37:36.580: INFO: stderr: ""
Jul 31 17:37:36.580: INFO: stdout: "update-demo-nautilus-gbtqg update-demo-nautilus-r6bk7 "
Jul 31 17:37:36.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods update-demo-nautilus-gbtqg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jul 31 17:37:36.649: INFO: stderr: ""
Jul 31 17:37:36.649: INFO: stdout: "true"
Jul 31 17:37:36.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods update-demo-nautilus-gbtqg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jul 31 17:37:36.719: INFO: stderr: ""
Jul 31 17:37:36.719: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 31 17:37:36.719: INFO: validating pod update-demo-nautilus-gbtqg
Jul 31 17:37:36.723: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 31 17:37:36.723: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 31 17:37:36.723: INFO: update-demo-nautilus-gbtqg is verified up and running
Jul 31 17:37:36.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods update-demo-nautilus-r6bk7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jul 31 17:37:36.800: INFO: stderr: ""
Jul 31 17:37:36.800: INFO: stdout: "true"
Jul 31 17:37:36.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods update-demo-nautilus-r6bk7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-738'
Jul 31 17:37:36.880: INFO: stderr: ""
Jul 31 17:37:36.881: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 31 17:37:36.881: INFO: validating pod update-demo-nautilus-r6bk7
Jul 31 17:37:36.884: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 31 17:37:36.884: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 31 17:37:36.884: INFO: update-demo-nautilus-r6bk7 is verified up and running
STEP: using delete to clean up resources
Jul 31 17:37:36.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 delete --grace-period=0 --force -f - --namespace=kubectl-738'
Jul 31 17:37:36.967: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 31 17:37:36.967: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 31 17:37:36.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-738'
Jul 31 17:37:37.041: INFO: stderr: "No resources found in kubectl-738 namespace.\n"
Jul 31 17:37:37.042: INFO: stdout: ""
Jul 31 17:37:37.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods -l name=update-demo --namespace=kubectl-738 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 31 17:37:37.123: INFO: stderr: ""
Jul 31 17:37:37.123: INFO: stdout: "update-demo-nautilus-gbtqg\nupdate-demo-nautilus-r6bk7\n"
Jul 31 17:37:37.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-738'
Jul 31 17:37:37.698: INFO: stderr: "No resources found in kubectl-738 namespace.\n"
Jul 31 17:37:37.698: INFO: stdout: ""
Jul 31 17:37:37.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods -l name=update-demo --namespace=kubectl-738 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 31 17:37:37.774: INFO: stderr: ""
Jul 31 17:37:37.774: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:37:37.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-738" for this suite.

• [SLOW TEST:26.314 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":280,"completed":270,"skipped":4443,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:37:37.789: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5957
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:37:54.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5957" for this suite.

• [SLOW TEST:16.285 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":280,"completed":271,"skipped":4460,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:37:54.075: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3428
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 31 17:37:54.226: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2e081e2c-30e0-4c73-8011-2f52de419544" in namespace "downward-api-3428" to be "success or failure"
Jul 31 17:37:54.228: INFO: Pod "downwardapi-volume-2e081e2c-30e0-4c73-8011-2f52de419544": Phase="Pending", Reason="", readiness=false. Elapsed: 2.223952ms
Jul 31 17:37:56.232: INFO: Pod "downwardapi-volume-2e081e2c-30e0-4c73-8011-2f52de419544": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005949811s
Jul 31 17:37:58.235: INFO: Pod "downwardapi-volume-2e081e2c-30e0-4c73-8011-2f52de419544": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00870106s
STEP: Saw pod success
Jul 31 17:37:58.235: INFO: Pod "downwardapi-volume-2e081e2c-30e0-4c73-8011-2f52de419544" satisfied condition "success or failure"
Jul 31 17:37:58.237: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod downwardapi-volume-2e081e2c-30e0-4c73-8011-2f52de419544 container client-container: <nil>
STEP: delete the pod
Jul 31 17:37:58.256: INFO: Waiting for pod downwardapi-volume-2e081e2c-30e0-4c73-8011-2f52de419544 to disappear
Jul 31 17:37:58.267: INFO: Pod downwardapi-volume-2e081e2c-30e0-4c73-8011-2f52de419544 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:37:58.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3428" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":272,"skipped":4473,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:37:58.281: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6758
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:38:23.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6758" for this suite.

• [SLOW TEST:25.414 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":280,"completed":273,"skipped":4497,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:38:23.695: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2916
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-82a0ee6a-8e23-450a-b989-d307ef918c47
STEP: Creating a pod to test consume secrets
Jul 31 17:38:23.860: INFO: Waiting up to 5m0s for pod "pod-secrets-54f3a79c-1392-4c2d-ae1b-684386b7e37c" in namespace "secrets-2916" to be "success or failure"
Jul 31 17:38:23.866: INFO: Pod "pod-secrets-54f3a79c-1392-4c2d-ae1b-684386b7e37c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.602747ms
Jul 31 17:38:25.870: INFO: Pod "pod-secrets-54f3a79c-1392-4c2d-ae1b-684386b7e37c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009968152s
Jul 31 17:38:27.873: INFO: Pod "pod-secrets-54f3a79c-1392-4c2d-ae1b-684386b7e37c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013016879s
STEP: Saw pod success
Jul 31 17:38:27.873: INFO: Pod "pod-secrets-54f3a79c-1392-4c2d-ae1b-684386b7e37c" satisfied condition "success or failure"
Jul 31 17:38:27.875: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-secrets-54f3a79c-1392-4c2d-ae1b-684386b7e37c container secret-volume-test: <nil>
STEP: delete the pod
Jul 31 17:38:27.909: INFO: Waiting for pod pod-secrets-54f3a79c-1392-4c2d-ae1b-684386b7e37c to disappear
Jul 31 17:38:27.914: INFO: Pod pod-secrets-54f3a79c-1392-4c2d-ae1b-684386b7e37c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:38:27.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2916" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":274,"skipped":4498,"failed":0}
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:38:27.927: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1513
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-1513/configmap-test-10cd1601-cf26-41e2-bb13-08920e152bd9
STEP: Creating a pod to test consume configMaps
Jul 31 17:38:28.137: INFO: Waiting up to 5m0s for pod "pod-configmaps-e5f98f2d-f356-4982-82e9-96eaafe1dc4a" in namespace "configmap-1513" to be "success or failure"
Jul 31 17:38:28.144: INFO: Pod "pod-configmaps-e5f98f2d-f356-4982-82e9-96eaafe1dc4a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.245246ms
Jul 31 17:38:30.148: INFO: Pod "pod-configmaps-e5f98f2d-f356-4982-82e9-96eaafe1dc4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010501716s
Jul 31 17:38:32.152: INFO: Pod "pod-configmaps-e5f98f2d-f356-4982-82e9-96eaafe1dc4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014408811s
STEP: Saw pod success
Jul 31 17:38:32.152: INFO: Pod "pod-configmaps-e5f98f2d-f356-4982-82e9-96eaafe1dc4a" satisfied condition "success or failure"
Jul 31 17:38:32.155: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-configmaps-e5f98f2d-f356-4982-82e9-96eaafe1dc4a container env-test: <nil>
STEP: delete the pod
Jul 31 17:38:32.177: INFO: Waiting for pod pod-configmaps-e5f98f2d-f356-4982-82e9-96eaafe1dc4a to disappear
Jul 31 17:38:32.185: INFO: Pod pod-configmaps-e5f98f2d-f356-4982-82e9-96eaafe1dc4a no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:38:32.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1513" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":275,"skipped":4505,"failed":0}

------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:38:32.200: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3997
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating pod
Jul 31 17:38:36.423: INFO: Pod pod-hostip-038fcb20-1004-4f67-b29d-c3176ae4a9da has hostIP: 10.10.102.92
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:38:36.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3997" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":280,"completed":276,"skipped":4505,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:38:36.438: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1715
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on tmpfs
Jul 31 17:38:36.587: INFO: Waiting up to 5m0s for pod "pod-f809bdd1-c9b2-4e15-9fc8-ede9e1e3c42a" in namespace "emptydir-1715" to be "success or failure"
Jul 31 17:38:36.590: INFO: Pod "pod-f809bdd1-c9b2-4e15-9fc8-ede9e1e3c42a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.162708ms
Jul 31 17:38:38.597: INFO: Pod "pod-f809bdd1-c9b2-4e15-9fc8-ede9e1e3c42a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009386624s
Jul 31 17:38:40.600: INFO: Pod "pod-f809bdd1-c9b2-4e15-9fc8-ede9e1e3c42a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012753391s
STEP: Saw pod success
Jul 31 17:38:40.600: INFO: Pod "pod-f809bdd1-c9b2-4e15-9fc8-ede9e1e3c42a" satisfied condition "success or failure"
Jul 31 17:38:40.602: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-f809bdd1-c9b2-4e15-9fc8-ede9e1e3c42a container test-container: <nil>
STEP: delete the pod
Jul 31 17:38:40.629: INFO: Waiting for pod pod-f809bdd1-c9b2-4e15-9fc8-ede9e1e3c42a to disappear
Jul 31 17:38:40.633: INFO: Pod pod-f809bdd1-c9b2-4e15-9fc8-ede9e1e3c42a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:38:40.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1715" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":277,"skipped":4523,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:38:40.642: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8842
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1275
STEP: creating the pod
Jul 31 17:38:40.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 create -f - --namespace=kubectl-8842'
Jul 31 17:38:41.024: INFO: stderr: ""
Jul 31 17:38:41.024: INFO: stdout: "pod/pause created\n"
Jul 31 17:38:41.024: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jul 31 17:38:41.024: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8842" to be "running and ready"
Jul 31 17:38:41.031: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.524475ms
Jul 31 17:38:43.034: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009709748s
Jul 31 17:38:45.039: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.014516533s
Jul 31 17:38:45.039: INFO: Pod "pause" satisfied condition "running and ready"
Jul 31 17:38:45.039: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: adding the label testing-label with value testing-label-value to a pod
Jul 31 17:38:45.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 label pods pause testing-label=testing-label-value --namespace=kubectl-8842'
Jul 31 17:38:45.114: INFO: stderr: ""
Jul 31 17:38:45.114: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jul 31 17:38:45.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pod pause -L testing-label --namespace=kubectl-8842'
Jul 31 17:38:45.185: INFO: stderr: ""
Jul 31 17:38:45.185: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jul 31 17:38:45.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 label pods pause testing-label- --namespace=kubectl-8842'
Jul 31 17:38:45.260: INFO: stderr: ""
Jul 31 17:38:45.261: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jul 31 17:38:45.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pod pause -L testing-label --namespace=kubectl-8842'
Jul 31 17:38:45.327: INFO: stderr: ""
Jul 31 17:38:45.327: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1282
STEP: using delete to clean up resources
Jul 31 17:38:45.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 delete --grace-period=0 --force -f - --namespace=kubectl-8842'
Jul 31 17:38:45.404: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 31 17:38:45.404: INFO: stdout: "pod \"pause\" force deleted\n"
Jul 31 17:38:45.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get rc,svc -l name=pause --no-headers --namespace=kubectl-8842'
Jul 31 17:38:45.483: INFO: stderr: "No resources found in kubectl-8842 namespace.\n"
Jul 31 17:38:45.483: INFO: stdout: ""
Jul 31 17:38:45.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-776750169 get pods -l name=pause --namespace=kubectl-8842 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 31 17:38:45.551: INFO: stderr: ""
Jul 31 17:38:45.551: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:38:45.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8842" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":280,"completed":278,"skipped":4531,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:38:45.560: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-2509
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 31 17:38:45.705: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-4d6313cf-c25f-4cde-8d47-84a4ece7e809" in namespace "security-context-test-2509" to be "success or failure"
Jul 31 17:38:45.713: INFO: Pod "busybox-privileged-false-4d6313cf-c25f-4cde-8d47-84a4ece7e809": Phase="Pending", Reason="", readiness=false. Elapsed: 7.893764ms
Jul 31 17:38:47.716: INFO: Pod "busybox-privileged-false-4d6313cf-c25f-4cde-8d47-84a4ece7e809": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011133141s
Jul 31 17:38:49.720: INFO: Pod "busybox-privileged-false-4d6313cf-c25f-4cde-8d47-84a4ece7e809": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015466219s
Jul 31 17:38:49.720: INFO: Pod "busybox-privileged-false-4d6313cf-c25f-4cde-8d47-84a4ece7e809" satisfied condition "success or failure"
Jul 31 17:38:49.733: INFO: Got logs for pod "busybox-privileged-false-4d6313cf-c25f-4cde-8d47-84a4ece7e809": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:38:49.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2509" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":279,"skipped":4543,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 31 17:38:49.745: INFO: >>> kubeConfig: /tmp/kubeconfig-776750169
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4839
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-b7f6c4a2-191a-4a9b-a5b6-8dc1d6bfa04a
STEP: Creating a pod to test consume secrets
Jul 31 17:38:49.918: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bfa35223-587a-44a5-b8ad-8579b8fc07d3" in namespace "projected-4839" to be "success or failure"
Jul 31 17:38:49.925: INFO: Pod "pod-projected-secrets-bfa35223-587a-44a5-b8ad-8579b8fc07d3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.67097ms
Jul 31 17:38:51.928: INFO: Pod "pod-projected-secrets-bfa35223-587a-44a5-b8ad-8579b8fc07d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010094693s
Jul 31 17:38:53.932: INFO: Pod "pod-projected-secrets-bfa35223-587a-44a5-b8ad-8579b8fc07d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013240943s
STEP: Saw pod success
Jul 31 17:38:53.932: INFO: Pod "pod-projected-secrets-bfa35223-587a-44a5-b8ad-8579b8fc07d3" satisfied condition "success or failure"
Jul 31 17:38:53.934: INFO: Trying to get logs from node test-aruna-123-node-group-687e2c91ce pod pod-projected-secrets-bfa35223-587a-44a5-b8ad-8579b8fc07d3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 31 17:38:53.962: INFO: Waiting for pod pod-projected-secrets-bfa35223-587a-44a5-b8ad-8579b8fc07d3 to disappear
Jul 31 17:38:53.970: INFO: Pod pod-projected-secrets-bfa35223-587a-44a5-b8ad-8579b8fc07d3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 31 17:38:53.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4839" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":280,"skipped":4553,"failed":0}
SSSSSSSSSSJul 31 17:38:53.986: INFO: Running AfterSuite actions on all nodes
Jul 31 17:38:53.986: INFO: Running AfterSuite actions on node 1
Jul 31 17:38:53.986: INFO: Skipping dumping logs from cluster
{"msg":"Test Suite completed","total":280,"completed":280,"skipped":4563,"failed":0}

Ran 280 of 4843 Specs in 4564.639 seconds
SUCCESS! -- 280 Passed | 0 Failed | 0 Pending | 4563 Skipped
PASS

Ginkgo ran 1 suite in 1h16m6.242089346s
Test Suite Passed
