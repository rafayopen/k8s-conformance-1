I0611 07:41:08.194130      23 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-515000098
I0611 07:41:08.194155      23 test_context.go:419] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0611 07:41:08.194305      23 e2e.go:109] Starting e2e run "11151d3c-36a3-4297-b0b0-de0dedcd1ebd" on Ginkgo node 1
{"msg":"Test Suite starting","total":280,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1591861266 - Will randomize all specs
Will run 280 of 4843 specs

Jun 11 07:41:08.208: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 07:41:08.210: INFO: Waiting up to 30m0s for all (but 7) nodes to be schedulable
Jun 11 07:41:08.225: INFO: Unschedulable nodes:
Jun 11 07:41:08.225: INFO: -> ip-10-0-133-101.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated route-reflector NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 07:41:08.225: INFO: -> ip-10-0-132-151.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated elasticsearch-data NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 07:41:08.225: INFO: -> ip-10-0-130-153.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated monitoring NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 07:41:08.225: INFO: -> ip-10-0-139-38.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated route-reflector NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 07:41:08.225: INFO: -> ip-10-0-129-167.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated elasticsearch-data NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 07:41:08.225: INFO: -> ip-10-0-137-193.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated elasticsearch-data NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 07:41:08.225: INFO: -> ip-10-0-129-22.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated route-reflector NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 07:41:08.225: INFO: ================================
Jun 11 07:41:08.238: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun 11 07:41:08.291: INFO: 78 / 78 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun 11 07:41:08.291: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Jun 11 07:41:08.291: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun 11 07:41:08.301: INFO: 17 / 17 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jun 11 07:41:08.301: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-route-reflector' (0 seconds elapsed)
Jun 11 07:41:08.301: INFO: 20 / 20 pods ready in namespaw ce 'kube-system' in daemonset 'ebs-csi-node' (0 seconds elapsed)
Jun 11 07:41:08.301: INFO: 20 / 20 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jun 11 07:41:08.301: INFO: e2e test version: v1.17.6
Jun 11 07:41:08.302: INFO: kube-apiserver version: v1.17.6
Jun 11 07:41:08.302: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 07:41:08.307: INFO: Cluster IP family: ipv4
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:41:08.307: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename webhook
Jun 11 07:41:08.347: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Jun 11 07:41:08.360: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-273
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 07:41:08.968: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 11 07:41:10.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458068, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458068, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458068, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458068, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 07:41:13.998: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 07:41:14.002: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2449-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:41:14.690: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "webhook-273" for this suite.
STEP: Destroying namespace "webhook-273-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.472 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":280,"completed":1,"skipped":5,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:41:14.779: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9435
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name secret-emptykey-test-9a6e10c6-b0e2-4900-92fa-d2a4afca1627
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:41:14.931: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "secrets-9435" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":280,"completed":2,"skipped":46,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:41:14.949: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-715
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service multi-endpoint-test in namespace services-715
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-715 to expose endpoints map[]
Jun 11 07:41:15.133: INFO: successfully validated that service multi-endpoint-test in namespace services-715 exposes endpoints map[] (10.800561ms elapsed)
STEP: Creating pod pod1 in namespace services-715
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-715 to expose endpoints map[pod1:[100]]
Jun 11 07:41:17.173: INFO: successfully validated that service multi-endpoint-test in namespace services-715 exposes endpoints map[pod1:[100]] (2.028023408s elapsed)
STEP: Creating pod pod2 in namespace services-715
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-715 to expose endpoints map[pod1:[100] pod2:[101]]
Jun 11 07:41:19.216: INFO: successfully validated that service multi-endpoint-test in namespace services-715 exposes endpoints map[pod1:[100] pod2:[101]] (2.03470178s elapsed)
STEP: Deleting pod pod1 in namespace services-715
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-715 to expose endpoints map[pod2:[101]]
Jun 11 07:41:20.242: INFO: successfully validated that service multi-endpoint-test in namespace services-715 exposes endpoints map[pod2:[101]] (1.01780779s elapsed)
STEP: Deleting pod pod2 in namespace services-715
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-715 to expose endpoints map[]
Jun 11 07:41:20.256: INFO: successfully validated that service multi-endpoint-test in namespace services-715 exposes endpoints map[] (5.927818ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:41:20.300: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "services-715" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:5.375 seconds]
[sig-network] Services
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":280,"completed":3,"skipped":54,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:41:20.324: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7723
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 07:41:20.983: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 11 07:41:22.996: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458080, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458080, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458081, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458080, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 07:41:26.013: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:41:26.236: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "webhook-7723" for this suite.
STEP: Destroying namespace "webhook-7723-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.012 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":280,"completed":4,"skipped":54,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:41:26.336: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6999
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 07:41:26.923: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 07:41:29.947: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:41:29.983: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "webhook-6999" for this suite.
STEP: Destroying namespace "webhook-6999-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":280,"completed":5,"skipped":72,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:41:30.068: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2035
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod test-webserver-b53b9a4d-56ae-4c0d-bceb-0b9aa2ac987d in namespace container-probe-2035
Jun 11 07:41:32.235: INFO: Started pod test-webserver-b53b9a4d-56ae-4c0d-bceb-0b9aa2ac987d in namespace container-probe-2035
STEP: checking the pod's current state and verifying that restartCount is present
Jun 11 07:41:32.241: INFO: Initial restart count of pod test-webserver-b53b9a4d-56ae-4c0d-bceb-0b9aa2ac987d is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:45:32.801: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "container-probe-2035" for this suite.

• [SLOW TEST:242.750 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":6,"skipped":93,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:45:32.818: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8077
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:324
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the initial replication controller
Jun 11 07:45:32.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 create -f - --namespace=kubectl-8077'
Jun 11 07:45:33.471: INFO: stderr: ""
Jun 11 07:45:33.471: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 11 07:45:33.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8077'
Jun 11 07:45:33.603: INFO: stderr: ""
Jun 11 07:45:33.603: INFO: stdout: "update-demo-nautilus-2lz6s update-demo-nautilus-w6zp5 "
Jun 11 07:45:33.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods update-demo-nautilus-2lz6s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8077'
Jun 11 07:45:33.732: INFO: stderr: ""
Jun 11 07:45:33.732: INFO: stdout: ""
Jun 11 07:45:33.732: INFO: update-demo-nautilus-2lz6s is created but not running
Jun 11 07:45:38.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8077'
Jun 11 07:45:38.868: INFO: stderr: ""
Jun 11 07:45:38.868: INFO: stdout: "update-demo-nautilus-2lz6s update-demo-nautilus-w6zp5 "
Jun 11 07:45:38.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods update-demo-nautilus-2lz6s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8077'
Jun 11 07:45:38.995: INFO: stderr: ""
Jun 11 07:45:38.995: INFO: stdout: "true"
Jun 11 07:45:38.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods update-demo-nautilus-2lz6s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8077'
Jun 11 07:45:39.121: INFO: stderr: ""
Jun 11 07:45:39.121: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 11 07:45:39.121: INFO: validating pod update-demo-nautilus-2lz6s
Jun 11 07:45:39.127: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 11 07:45:39.127: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 11 07:45:39.127: INFO: update-demo-nautilus-2lz6s is verified up and running
Jun 11 07:45:39.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods update-demo-nautilus-w6zp5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8077'
Jun 11 07:45:39.255: INFO: stderr: ""
Jun 11 07:45:39.255: INFO: stdout: "true"
Jun 11 07:45:39.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods update-demo-nautilus-w6zp5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8077'
Jun 11 07:45:39.383: INFO: stderr: ""
Jun 11 07:45:39.383: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 11 07:45:39.383: INFO: validating pod update-demo-nautilus-w6zp5
Jun 11 07:45:39.389: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 11 07:45:39.389: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 11 07:45:39.389: INFO: update-demo-nautilus-w6zp5 is verified up and running
STEP: rolling-update to new replication controller
Jun 11 07:45:39.391: INFO: scanned /root for discovery docs: <nil>
Jun 11 07:45:39.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-8077'
Jun 11 07:45:52.084: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jun 11 07:45:52.084: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 11 07:45:52.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8077'
Jun 11 07:45:52.236: INFO: stderr: ""
Jun 11 07:45:52.236: INFO: stdout: "update-demo-kitten-2rvfq update-demo-kitten-fwkqd "
Jun 11 07:45:52.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods update-demo-kitten-2rvfq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8077'
Jun 11 07:45:52.369: INFO: stderr: ""
Jun 11 07:45:52.369: INFO: stdout: "true"
Jun 11 07:45:52.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods update-demo-kitten-2rvfq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8077'
Jun 11 07:45:52.500: INFO: stderr: ""
Jun 11 07:45:52.500: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jun 11 07:45:52.500: INFO: validating pod update-demo-kitten-2rvfq
Jun 11 07:45:52.505: INFO: got data: {
  "image": "kitten.jpg"
}

Jun 11 07:45:52.505: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jun 11 07:45:52.505: INFO: update-demo-kitten-2rvfq is verified up and running
Jun 11 07:45:52.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods update-demo-kitten-fwkqd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8077'
Jun 11 07:45:52.631: INFO: stderr: ""
Jun 11 07:45:52.632: INFO: stdout: "true"
Jun 11 07:45:52.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods update-demo-kitten-fwkqd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8077'
Jun 11 07:45:52.760: INFO: stderr: ""
Jun 11 07:45:52.760: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jun 11 07:45:52.760: INFO: validating pod update-demo-kitten-fwkqd
Jun 11 07:45:52.766: INFO: got data: {
  "image": "kitten.jpg"
}

Jun 11 07:45:52.766: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jun 11 07:45:52.766: INFO: update-demo-kitten-fwkqd is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:45:52.766: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "kubectl-8077" for this suite.

• [SLOW TEST:19.964 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:322
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should do a rolling update of a replication controller  [Conformance]","total":280,"completed":7,"skipped":96,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:45:52.782: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9940
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1525
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 11 07:45:52.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-9940'
Jun 11 07:45:53.035: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 11 07:45:53.035: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Jun 11 07:45:53.044: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-c6qd5]
Jun 11 07:45:53.044: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-c6qd5" in namespace "kubectl-9940" to be "running and ready"
Jun 11 07:45:53.050: INFO: Pod "e2e-test-httpd-rc-c6qd5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.855976ms
Jun 11 07:45:55.054: INFO: Pod "e2e-test-httpd-rc-c6qd5": Phase="Running", Reason="", readiness=true. Elapsed: 2.010116401s
Jun 11 07:45:55.055: INFO: Pod "e2e-test-httpd-rc-c6qd5" satisfied condition "running and ready"
Jun 11 07:45:55.055: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-c6qd5]
Jun 11 07:45:55.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 logs rc/e2e-test-httpd-rc --namespace=kubectl-9940'
Jun 11 07:45:55.191: INFO: stderr: ""
Jun 11 07:45:55.191: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 192.168.42.35. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 192.168.42.35. Set the 'ServerName' directive globally to suppress this message\n[Thu Jun 11 07:45:54.552753 2020] [mpm_event:notice] [pid 1:tid 140047029627752] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Thu Jun 11 07:45:54.552797 2020] [core:notice] [pid 1:tid 140047029627752] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1530
Jun 11 07:45:55.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 delete rc e2e-test-httpd-rc --namespace=kubectl-9940'
Jun 11 07:45:55.331: INFO: stderr: ""
Jun 11 07:45:55.331: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:45:55.331: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "kubectl-9940" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run rc should create an rc from an image  [Conformance]","total":280,"completed":8,"skipped":105,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:45:55.349: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5660
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 07:45:55.503: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jun 11 07:46:00.508: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 11 07:46:00.508: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun 11 07:46:02.513: INFO: Creating deployment "test-rollover-deployment"
Jun 11 07:46:02.522: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun 11 07:46:04.529: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun 11 07:46:04.538: INFO: Ensure that both replica sets have 1 created replica
Jun 11 07:46:04.545: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jun 11 07:46:04.554: INFO: Updating deployment test-rollover-deployment
Jun 11 07:46:04.554: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun 11 07:46:06.562: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun 11 07:46:06.571: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun 11 07:46:06.579: INFO: all replica sets need to contain the pod-template-hash label
Jun 11 07:46:06.579: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458362, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458362, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458365, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458362, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 11 07:46:08.587: INFO: all replica sets need to contain the pod-template-hash label
Jun 11 07:46:08.587: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458362, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458362, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458365, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458362, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 11 07:46:10.589: INFO: all replica sets need to contain the pod-template-hash label
Jun 11 07:46:10.589: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458362, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458362, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458365, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458362, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 11 07:46:12.588: INFO: all replica sets need to contain the pod-template-hash label
Jun 11 07:46:12.588: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458362, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458362, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458365, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458362, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 11 07:46:14.588: INFO: all replica sets need to contain the pod-template-hash label
Jun 11 07:46:14.588: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458362, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458362, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458365, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458362, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 11 07:46:16.588: INFO:
Jun 11 07:46:16.588: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jun 11 07:46:16.599: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-5660 /apis/apps/v1/namespaces/deployment-5660/deployments/test-rollover-deployment 203a6e27-5c05-47ac-9937-b3501f87d13b 95636 2 2020-06-11 07:46:02 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0007cc998 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-06-11 07:46:02 +0000 UTC,LastTransitionTime:2020-06-11 07:46:02 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-574d6dfbff" has successfully progressed.,LastUpdateTime:2020-06-11 07:46:15 +0000 UTC,LastTransitionTime:2020-06-11 07:46:02 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 11 07:46:16.603: INFO: New ReplicaSet "test-rollover-deployment-574d6dfbff" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-574d6dfbff  deployment-5660 /apis/apps/v1/namespaces/deployment-5660/replicasets/test-rollover-deployment-574d6dfbff c83eee92-6f78-4f1e-95bc-883314e71c36 95625 2 2020-06-11 07:46:04 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 203a6e27-5c05-47ac-9937-b3501f87d13b 0xc0007cd1f7 0xc0007cd1f8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 574d6dfbff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0007cd278 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 11 07:46:16.603: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun 11 07:46:16.603: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5660 /apis/apps/v1/namespaces/deployment-5660/replicasets/test-rollover-controller df042071-3b12-4ccb-a0fd-6c846f627d69 95634 2 2020-06-11 07:45:55 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 203a6e27-5c05-47ac-9937-b3501f87d13b 0xc0007cd117 0xc0007cd118}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0007cd188 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 11 07:46:16.603: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-5660 /apis/apps/v1/namespaces/deployment-5660/replicasets/test-rollover-deployment-f6c94f66c 14a9cb61-7345-447f-ada8-fd7a947b6b49 95508 2 2020-06-11 07:46:02 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 203a6e27-5c05-47ac-9937-b3501f87d13b 0xc0007cd2e0 0xc0007cd2e1}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0007cd358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 11 07:46:16.608: INFO: Pod "test-rollover-deployment-574d6dfbff-hhfc2" is available:
&Pod{ObjectMeta:{test-rollover-deployment-574d6dfbff-hhfc2 test-rollover-deployment-574d6dfbff- deployment-5660 /api/v1/namespaces/deployment-5660/pods/test-rollover-deployment-574d6dfbff-hhfc2 af462a5b-74b0-431c-8ce1-e90084e2655d 95532 0 2020-06-11 07:46:04 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[cni.projectcalico.org/podIP:192.168.42.33/32 cni.projectcalico.org/podIPs:192.168.42.33/32] [{apps/v1 ReplicaSet test-rollover-deployment-574d6dfbff c83eee92-6f78-4f1e-95bc-883314e71c36 0xc0007cd8d7 0xc0007cd8d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mtxt6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mtxt6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mtxt6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-136-38.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:46:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:46:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:46:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:46:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.136.38,PodIP:192.168.42.33,StartTime:2020-06-11 07:46:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-11 07:46:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:containerd://8ebdc7358a04bc1f5681bae396f3e805ef30e1e8e522d2951e3b3ad41cb6e985,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.42.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:46:16.608: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "deployment-5660" for this suite.

• [SLOW TEST:21.274 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":280,"completed":9,"skipped":134,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:46:16.623: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3210
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 07:46:17.080: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 11 07:46:19.093: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458377, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458377, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458377, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458377, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 07:46:22.109: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:46:22.188: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "webhook-3210" for this suite.
STEP: Destroying namespace "webhook-3210-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.665 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":280,"completed":10,"skipped":154,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:46:22.288: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8289
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-3b6e29e7-bbc2-4598-b669-91f0b7dd54f4
STEP: Creating a pod to test consume secrets
Jun 11 07:46:22.452: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b9e7f246-698c-4428-b00d-d32a45393f21" in namespace "projected-8289" to be "success or failure"
Jun 11 07:46:22.459: INFO: Pod "pod-projected-secrets-b9e7f246-698c-4428-b00d-d32a45393f21": Phase="Pending", Reason="", readiness=false. Elapsed: 6.662162ms
Jun 11 07:46:24.463: INFO: Pod "pod-projected-secrets-b9e7f246-698c-4428-b00d-d32a45393f21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011210452s
STEP: Saw pod success
Jun 11 07:46:24.463: INFO: Pod "pod-projected-secrets-b9e7f246-698c-4428-b00d-d32a45393f21" satisfied condition "success or failure"
Jun 11 07:46:24.467: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-projected-secrets-b9e7f246-698c-4428-b00d-d32a45393f21 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 11 07:46:24.496: INFO: Waiting for pod pod-projected-secrets-b9e7f246-698c-4428-b00d-d32a45393f21 to disappear
Jun 11 07:46:24.500: INFO: Pod pod-projected-secrets-b9e7f246-698c-4428-b00d-d32a45393f21 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:46:24.500: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "projected-8289" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":11,"skipped":160,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:46:24.516: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4457
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:46:35.726: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "resourcequota-4457" for this suite.

• [SLOW TEST:11.226 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":280,"completed":12,"skipped":172,"failed":0}
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:46:35.742: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9565
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating api versions
Jun 11 07:46:35.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 api-versions'
Jun 11 07:46:35.954: INFO: stderr: ""
Jun 11 07:46:35.954: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nauthorizedlister.workspaces.kommander.mesosphere.io/v1alpha1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncertmanager.k8s.io/v1alpha1\nconfig.gatekeeper.sh/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncore.kubefed.io/v1alpha1\ncore.kubefed.io/v1beta1\ncrd.projectcalico.org/v1\ncustom.metrics.k8s.io/v1beta1\ndex.coreos.com/v1\ndex.mesosphere.io/v1alpha1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nkommander.mesosphere.io/v1beta1\nkubeaddons.mesosphere.io/v1beta1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmulticlusterdns.kubefed.io/v1alpha1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nscheduling.kubefed.io/v1alpha1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplates.gatekeeper.sh/v1alpha1\ntemplates.gatekeeper.sh/v1beta1\ntypes.kubefed.io/v1beta1\nv1\nvelero.io/v1\nwebhook.certmanager.k8s.io/v1beta1\nwebhook.federation.kommander.mesosphere.io/v1beta1\nwebhook.federation.workspaces.kommander.mesosphere.io/v1alpha1\nwebhook.licensing.kommander.mesosphere.io/v1beta1\nwebhook.provisioning.kommander.mesosphere.io/v1beta1\nworkspaces.kommander.mesosphere.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:46:35.954: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "kubectl-9565" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":280,"completed":13,"skipped":172,"failed":0}

------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:46:35.970: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2562
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 11 07:46:36.182: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:36.182: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:36.182: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:36.182: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:36.182: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:36.183: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:36.183: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:36.183: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:36.183: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:36.183: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:36.188: INFO: Number of nodes with available pods: 0
Jun 11 07:46:36.188: INFO: Node ip-10-0-128-119.us-west-2.compute.internal is running more than one daemon pod
Jun 11 07:46:37.197: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:37.197: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:37.197: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:37.197: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:37.197: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:37.197: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:37.197: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:37.197: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:37.197: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:37.197: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:37.202: INFO: Number of nodes with available pods: 0
Jun 11 07:46:37.202: INFO: Node ip-10-0-128-119.us-west-2.compute.internal is running more than one daemon pod
Jun 11 07:46:38.196: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:38.196: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:38.196: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:38.196: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:38.196: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:38.196: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:38.196: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:38.196: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:38.196: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:38.196: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:38.201: INFO: Number of nodes with available pods: 6
Jun 11 07:46:38.201: INFO: Node ip-10-0-128-119.us-west-2.compute.internal is running more than one daemon pod
Jun 11 07:46:39.196: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:39.197: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:39.197: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:39.197: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:39.197: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:39.197: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:39.197: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:39.197: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:39.197: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:39.197: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:39.202: INFO: Number of nodes with available pods: 10
Jun 11 07:46:39.202: INFO: Number of running nodes: 10, number of available pods: 10
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jun 11 07:46:39.225: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:39.225: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:39.225: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:39.225: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:39.225: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:39.225: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:39.225: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:39.225: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:39.225: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:39.225: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:39.230: INFO: Number of nodes with available pods: 9
Jun 11 07:46:39.230: INFO: Node ip-10-0-130-174.us-west-2.compute.internal is running more than one daemon pod
Jun 11 07:46:40.238: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:40.238: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:40.238: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:40.238: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:40.239: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:40.239: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:40.239: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:40.239: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:40.239: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:40.239: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:40.243: INFO: Number of nodes with available pods: 9
Jun 11 07:46:40.243: INFO: Node ip-10-0-130-174.us-west-2.compute.internal is running more than one daemon pod
Jun 11 07:46:41.240: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:41.241: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:41.241: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:41.241: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:41.241: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:41.241: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:41.241: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:41.241: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:41.241: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:41.241: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:41.247: INFO: Number of nodes with available pods: 9
Jun 11 07:46:41.247: INFO: Node ip-10-0-130-174.us-west-2.compute.internal is running more than one daemon pod
Jun 11 07:46:42.261: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:42.261: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:42.261: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:42.261: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:42.261: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:42.261: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:42.261: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:42.261: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:42.261: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:42.261: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:42.266: INFO: Number of nodes with available pods: 9
Jun 11 07:46:42.267: INFO: Node ip-10-0-130-174.us-west-2.compute.internal is running more than one daemon pod
Jun 11 07:46:43.238: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:43.238: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:43.238: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:43.238: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:43.238: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:43.238: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:43.238: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:43.238: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:43.239: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:43.239: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:43.244: INFO: Number of nodes with available pods: 9
Jun 11 07:46:43.244: INFO: Node ip-10-0-130-174.us-west-2.compute.internal is running more than one daemon pod
Jun 11 07:46:44.239: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:44.239: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:44.239: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:44.239: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:44.240: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:44.240: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:44.240: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:44.240: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:44.240: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:44.240: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:44.244: INFO: Number of nodes with available pods: 9
Jun 11 07:46:44.244: INFO: Node ip-10-0-130-174.us-west-2.compute.internal is running more than one daemon pod
Jun 11 07:46:45.238: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:45.238: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:45.238: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:45.238: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:45.238: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:45.238: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:45.238: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:45.238: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:45.238: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:45.238: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:45.243: INFO: Number of nodes with available pods: 9
Jun 11 07:46:45.243: INFO: Node ip-10-0-130-174.us-west-2.compute.internal is running more than one daemon pod
Jun 11 07:46:46.238: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:46.238: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:46.238: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:46.238: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:46.238: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:46.238: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:46.238: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:46.238: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:46.238: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:46.238: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:46.243: INFO: Number of nodes with available pods: 9
Jun 11 07:46:46.243: INFO: Node ip-10-0-130-174.us-west-2.compute.internal is running more than one daemon pod
Jun 11 07:46:47.238: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:47.238: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:47.238: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:47.238: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:47.238: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:47.238: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:47.238: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:47.238: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:47.238: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:47.238: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:47.244: INFO: Number of nodes with available pods: 9
Jun 11 07:46:47.244: INFO: Node ip-10-0-130-174.us-west-2.compute.internal is running more than one daemon pod
Jun 11 07:46:48.238: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:48.238: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:48.238: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:48.238: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:48.238: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:48.239: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:48.239: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:48.239: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:48.239: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:48.239: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:48.243: INFO: Number of nodes with available pods: 9
Jun 11 07:46:48.243: INFO: Node ip-10-0-130-174.us-west-2.compute.internal is running more than one daemon pod
Jun 11 07:46:49.238: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:49.238: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:49.238: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:49.238: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:49.238: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:49.238: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:49.238: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:49.238: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:49.238: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:49.238: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:46:49.243: INFO: Number of nodes with available pods: 10
Jun 11 07:46:49.243: INFO: Number of running nodes: 10, number of available pods: 10
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2562, will wait for the garbage collector to delete the pods
Jun 11 07:46:49.311: INFO: Deleting DaemonSet.extensions daemon-set took: 10.779405ms
Jun 11 07:46:50.911: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.600239846s
Jun 11 07:46:57.515: INFO: Number of nodes with available pods: 0
Jun 11 07:46:57.516: INFO: Number of running nodes: 0, number of available pods: 0
Jun 11 07:46:57.520: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2562/daemonsets","resourceVersion":"96453"},"items":null}

Jun 11 07:46:57.524: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2562/pods","resourceVersion":"96453"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:46:57.570: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "daemonsets-2562" for this suite.

• [SLOW TEST:21.615 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":280,"completed":14,"skipped":172,"failed":0}
SSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:46:57.585: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7931
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jun 11 07:46:57.741: INFO: Waiting up to 5m0s for pod "downward-api-ea9e1b23-bc4d-40aa-9298-fe90162c5581" in namespace "downward-api-7931" to be "success or failure"
Jun 11 07:46:57.745: INFO: Pod "downward-api-ea9e1b23-bc4d-40aa-9298-fe90162c5581": Phase="Pending", Reason="", readiness=false. Elapsed: 3.849825ms
Jun 11 07:46:59.750: INFO: Pod "downward-api-ea9e1b23-bc4d-40aa-9298-fe90162c5581": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008155368s
Jun 11 07:47:01.754: INFO: Pod "downward-api-ea9e1b23-bc4d-40aa-9298-fe90162c5581": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01290861s
STEP: Saw pod success
Jun 11 07:47:01.754: INFO: Pod "downward-api-ea9e1b23-bc4d-40aa-9298-fe90162c5581" satisfied condition "success or failure"
Jun 11 07:47:01.758: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod downward-api-ea9e1b23-bc4d-40aa-9298-fe90162c5581 container dapi-container: <nil>
STEP: delete the pod
Jun 11 07:47:01.781: INFO: Waiting for pod downward-api-ea9e1b23-bc4d-40aa-9298-fe90162c5581 to disappear
Jun 11 07:47:01.784: INFO: Pod downward-api-ea9e1b23-bc4d-40aa-9298-fe90162c5581 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:47:01.784: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "downward-api-7931" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":280,"completed":15,"skipped":177,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period
  should be submitted and removed [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:47:01.799: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1369
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:46
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Jun 11 07:47:03.977: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-515000098 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jun 11 07:47:19.054: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:47:19.057: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "pods-1369" for this suite.

• [SLOW TEST:17.280 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should be submitted and removed [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period should be submitted and removed [Conformance]","total":280,"completed":16,"skipped":203,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:47:19.079: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7251
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 11 07:47:21.762: INFO: Successfully updated pod "pod-update-activedeadlineseconds-eabbae68-3b5f-4ac5-bd73-bd0647b3a7b3"
Jun 11 07:47:21.762: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-eabbae68-3b5f-4ac5-bd73-bd0647b3a7b3" in namespace "pods-7251" to be "terminated due to deadline exceeded"
Jun 11 07:47:21.766: INFO: Pod "pod-update-activedeadlineseconds-eabbae68-3b5f-4ac5-bd73-bd0647b3a7b3": Phase="Running", Reason="", readiness=true. Elapsed: 3.641467ms
Jun 11 07:47:23.770: INFO: Pod "pod-update-activedeadlineseconds-eabbae68-3b5f-4ac5-bd73-bd0647b3a7b3": Phase="Running", Reason="", readiness=true. Elapsed: 2.007851385s
Jun 11 07:47:25.774: INFO: Pod "pod-update-activedeadlineseconds-eabbae68-3b5f-4ac5-bd73-bd0647b3a7b3": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.012162685s
Jun 11 07:47:25.774: INFO: Pod "pod-update-activedeadlineseconds-eabbae68-3b5f-4ac5-bd73-bd0647b3a7b3" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:47:25.774: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "pods-7251" for this suite.

• [SLOW TEST:6.711 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":280,"completed":17,"skipped":216,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:47:25.790: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5899
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: executing a command with run --rm and attach with stdin
Jun 11 07:47:25.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 --namespace=kubectl-5899 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jun 11 07:47:27.977: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jun 11 07:47:27.977: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:47:29.984: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "kubectl-5899" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run --rm job should create a job from an image, then delete the job  [Conformance]","total":280,"completed":18,"skipped":230,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:47:30.000: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2834
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Jun 11 07:47:30.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 create -f - --namespace=kubectl-2834'
Jun 11 07:47:30.459: INFO: stderr: ""
Jun 11 07:47:30.459: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 11 07:47:30.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2834'
Jun 11 07:47:30.593: INFO: stderr: ""
Jun 11 07:47:30.593: INFO: stdout: "update-demo-nautilus-4gvzb update-demo-nautilus-8pls9 "
Jun 11 07:47:30.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods update-demo-nautilus-4gvzb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2834'
Jun 11 07:47:30.722: INFO: stderr: ""
Jun 11 07:47:30.722: INFO: stdout: ""
Jun 11 07:47:30.722: INFO: update-demo-nautilus-4gvzb is created but not running
Jun 11 07:47:35.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2834'
Jun 11 07:47:35.861: INFO: stderr: ""
Jun 11 07:47:35.861: INFO: stdout: "update-demo-nautilus-4gvzb update-demo-nautilus-8pls9 "
Jun 11 07:47:35.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods update-demo-nautilus-4gvzb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2834'
Jun 11 07:47:35.999: INFO: stderr: ""
Jun 11 07:47:35.999: INFO: stdout: "true"
Jun 11 07:47:35.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods update-demo-nautilus-4gvzb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2834'
Jun 11 07:47:36.128: INFO: stderr: ""
Jun 11 07:47:36.128: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 11 07:47:36.128: INFO: validating pod update-demo-nautilus-4gvzb
Jun 11 07:47:36.133: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 11 07:47:36.133: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 11 07:47:36.133: INFO: update-demo-nautilus-4gvzb is verified up and running
Jun 11 07:47:36.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods update-demo-nautilus-8pls9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2834'
Jun 11 07:47:36.262: INFO: stderr: ""
Jun 11 07:47:36.262: INFO: stdout: "true"
Jun 11 07:47:36.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods update-demo-nautilus-8pls9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2834'
Jun 11 07:47:36.395: INFO: stderr: ""
Jun 11 07:47:36.395: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 11 07:47:36.395: INFO: validating pod update-demo-nautilus-8pls9
Jun 11 07:47:36.401: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 11 07:47:36.401: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 11 07:47:36.401: INFO: update-demo-nautilus-8pls9 is verified up and running
STEP: scaling down the replication controller
Jun 11 07:47:36.404: INFO: scanned /root for discovery docs: <nil>
Jun 11 07:47:36.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-2834'
Jun 11 07:47:36.593: INFO: stderr: ""
Jun 11 07:47:36.593: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 11 07:47:36.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2834'
Jun 11 07:47:36.723: INFO: stderr: ""
Jun 11 07:47:36.723: INFO: stdout: "update-demo-nautilus-4gvzb update-demo-nautilus-8pls9 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 11 07:47:41.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2834'
Jun 11 07:47:41.855: INFO: stderr: ""
Jun 11 07:47:41.855: INFO: stdout: "update-demo-nautilus-4gvzb update-demo-nautilus-8pls9 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 11 07:47:46.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2834'
Jun 11 07:47:46.987: INFO: stderr: ""
Jun 11 07:47:46.987: INFO: stdout: "update-demo-nautilus-8pls9 "
Jun 11 07:47:46.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods update-demo-nautilus-8pls9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2834'
Jun 11 07:47:47.113: INFO: stderr: ""
Jun 11 07:47:47.113: INFO: stdout: "true"
Jun 11 07:47:47.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods update-demo-nautilus-8pls9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2834'
Jun 11 07:47:47.246: INFO: stderr: ""
Jun 11 07:47:47.246: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 11 07:47:47.246: INFO: validating pod update-demo-nautilus-8pls9
Jun 11 07:47:47.251: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 11 07:47:47.251: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 11 07:47:47.251: INFO: update-demo-nautilus-8pls9 is verified up and running
STEP: scaling up the replication controller
Jun 11 07:47:47.253: INFO: scanned /root for discovery docs: <nil>
Jun 11 07:47:47.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-2834'
Jun 11 07:47:47.442: INFO: stderr: ""
Jun 11 07:47:47.442: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 11 07:47:47.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2834'
Jun 11 07:47:47.575: INFO: stderr: ""
Jun 11 07:47:47.575: INFO: stdout: "update-demo-nautilus-8pls9 update-demo-nautilus-trzdh "
Jun 11 07:47:47.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods update-demo-nautilus-8pls9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2834'
Jun 11 07:47:47.704: INFO: stderr: ""
Jun 11 07:47:47.704: INFO: stdout: "true"
Jun 11 07:47:47.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods update-demo-nautilus-8pls9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2834'
Jun 11 07:47:47.840: INFO: stderr: ""
Jun 11 07:47:47.840: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 11 07:47:47.840: INFO: validating pod update-demo-nautilus-8pls9
Jun 11 07:47:47.846: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 11 07:47:47.846: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 11 07:47:47.846: INFO: update-demo-nautilus-8pls9 is verified up and running
Jun 11 07:47:47.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods update-demo-nautilus-trzdh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2834'
Jun 11 07:47:47.976: INFO: stderr: ""
Jun 11 07:47:47.976: INFO: stdout: ""
Jun 11 07:47:47.976: INFO: update-demo-nautilus-trzdh is created but not running
Jun 11 07:47:52.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2834'
Jun 11 07:47:53.113: INFO: stderr: ""
Jun 11 07:47:53.113: INFO: stdout: "update-demo-nautilus-8pls9 update-demo-nautilus-trzdh "
Jun 11 07:47:53.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods update-demo-nautilus-8pls9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2834'
Jun 11 07:47:53.244: INFO: stderr: ""
Jun 11 07:47:53.244: INFO: stdout: "true"
Jun 11 07:47:53.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods update-demo-nautilus-8pls9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2834'
Jun 11 07:47:53.381: INFO: stderr: ""
Jun 11 07:47:53.381: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 11 07:47:53.381: INFO: validating pod update-demo-nautilus-8pls9
Jun 11 07:47:53.386: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 11 07:47:53.386: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 11 07:47:53.386: INFO: update-demo-nautilus-8pls9 is verified up and running
Jun 11 07:47:53.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods update-demo-nautilus-trzdh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2834'
Jun 11 07:47:53.517: INFO: stderr: ""
Jun 11 07:47:53.518: INFO: stdout: "true"
Jun 11 07:47:53.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods update-demo-nautilus-trzdh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2834'
Jun 11 07:47:53.646: INFO: stderr: ""
Jun 11 07:47:53.646: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 11 07:47:53.646: INFO: validating pod update-demo-nautilus-trzdh
Jun 11 07:47:53.651: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 11 07:47:53.651: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 11 07:47:53.651: INFO: update-demo-nautilus-trzdh is verified up and running
STEP: using delete to clean up resources
Jun 11 07:47:53.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 delete --grace-period=0 --force -f - --namespace=kubectl-2834'
Jun 11 07:47:53.749: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 11 07:47:53.749: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 11 07:47:53.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2834'
Jun 11 07:47:53.920: INFO: stderr: "No resources found in kubectl-2834 namespace.\n"
Jun 11 07:47:53.920: INFO: stdout: ""
Jun 11 07:47:53.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods -l name=update-demo --namespace=kubectl-2834 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 11 07:47:54.051: INFO: stderr: ""
Jun 11 07:47:54.051: INFO: stdout: "update-demo-nautilus-8pls9\nupdate-demo-nautilus-trzdh\n"
Jun 11 07:47:54.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2834'
Jun 11 07:47:54.724: INFO: stderr: "No resources found in kubectl-2834 namespace.\n"
Jun 11 07:47:54.724: INFO: stdout: ""
Jun 11 07:47:54.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods -l name=update-demo --namespace=kubectl-2834 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 11 07:47:54.855: INFO: stderr: ""
Jun 11 07:47:54.855: INFO: stdout: "update-demo-nautilus-8pls9\nupdate-demo-nautilus-trzdh\n"
Jun 11 07:47:55.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2834'
Jun 11 07:47:55.226: INFO: stderr: "No resources found in kubectl-2834 namespace.\n"
Jun 11 07:47:55.226: INFO: stdout: ""
Jun 11 07:47:55.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods -l name=update-demo --namespace=kubectl-2834 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 11 07:47:55.359: INFO: stderr: ""
Jun 11 07:47:55.359: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:47:55.359: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "kubectl-2834" for this suite.

• [SLOW TEST:25.375 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":280,"completed":19,"skipped":240,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:47:55.375: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9522
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-f8701e42-97bd-4665-84ac-429f62c9f8ef
STEP: Creating a pod to test consume configMaps
Jun 11 07:47:55.537: INFO: Waiting up to 5m0s for pod "pod-configmaps-d18b7a4a-8ea4-4562-bfe8-ed8e67525fa5" in namespace "configmap-9522" to be "success or failure"
Jun 11 07:47:55.540: INFO: Pod "pod-configmaps-d18b7a4a-8ea4-4562-bfe8-ed8e67525fa5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.617438ms
Jun 11 07:47:57.545: INFO: Pod "pod-configmaps-d18b7a4a-8ea4-4562-bfe8-ed8e67525fa5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007976199s
STEP: Saw pod success
Jun 11 07:47:57.545: INFO: Pod "pod-configmaps-d18b7a4a-8ea4-4562-bfe8-ed8e67525fa5" satisfied condition "success or failure"
Jun 11 07:47:57.548: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-configmaps-d18b7a4a-8ea4-4562-bfe8-ed8e67525fa5 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 11 07:47:57.569: INFO: Waiting for pod pod-configmaps-d18b7a4a-8ea4-4562-bfe8-ed8e67525fa5 to disappear
Jun 11 07:47:57.573: INFO: Pod pod-configmaps-d18b7a4a-8ea4-4562-bfe8-ed8e67525fa5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:47:57.573: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "configmap-9522" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":20,"skipped":244,"failed":0}
SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:47:57.591: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-770
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jun 11 07:47:57.733: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:48:00.929: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "init-container-770" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":280,"completed":21,"skipped":251,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:48:00.947: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7984
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jun 11 07:48:01.093: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 07:48:07.038: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:48:28.532: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7984" for this suite.

• [SLOW TEST:27.601 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":280,"completed":22,"skipped":263,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:48:28.547: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3031
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:48:33.739: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "replication-controller-3031" for this suite.

• [SLOW TEST:5.206 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":280,"completed":23,"skipped":283,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:48:33.754: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5742
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-35d170fc-846d-41fa-b9c3-2ef70decf18f
STEP: Creating a pod to test consume configMaps
Jun 11 07:48:33.917: INFO: Waiting up to 5m0s for pod "pod-configmaps-d1853448-f655-4ae0-a1e2-f7772bb4358b" in namespace "configmap-5742" to be "success or failure"
Jun 11 07:48:33.921: INFO: Pod "pod-configmaps-d1853448-f655-4ae0-a1e2-f7772bb4358b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.789537ms
Jun 11 07:48:35.925: INFO: Pod "pod-configmaps-d1853448-f655-4ae0-a1e2-f7772bb4358b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007995241s
STEP: Saw pod success
Jun 11 07:48:35.925: INFO: Pod "pod-configmaps-d1853448-f655-4ae0-a1e2-f7772bb4358b" satisfied condition "success or failure"
Jun 11 07:48:35.929: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-configmaps-d1853448-f655-4ae0-a1e2-f7772bb4358b container configmap-volume-test: <nil>
STEP: delete the pod
Jun 11 07:48:35.958: INFO: Waiting for pod pod-configmaps-d1853448-f655-4ae0-a1e2-f7772bb4358b to disappear
Jun 11 07:48:35.962: INFO: Pod pod-configmaps-d1853448-f655-4ae0-a1e2-f7772bb4358b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:48:35.962: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "configmap-5742" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":24,"skipped":307,"failed":0}
SSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:48:35.977: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-43
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:48:58.374: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "container-runtime-43" for this suite.

• [SLOW TEST:22.411 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":280,"completed":25,"skipped":312,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Pods
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:48:58.389: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4254
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating pod
Jun 11 07:49:00.563: INFO: Pod pod-hostip-364ce91b-7861-4bc2-985a-c001fa8514b8 has hostIP: 10.0.136.38
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:49:00.563: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "pods-4254" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":280,"completed":26,"skipped":319,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:49:00.577: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1097
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 11 07:49:00.735: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dd841827-4bf2-497a-9c27-9662ecbd857c" in namespace "projected-1097" to be "success or failure"
Jun 11 07:49:00.739: INFO: Pod "downwardapi-volume-dd841827-4bf2-497a-9c27-9662ecbd857c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.87135ms
Jun 11 07:49:02.744: INFO: Pod "downwardapi-volume-dd841827-4bf2-497a-9c27-9662ecbd857c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008673872s
STEP: Saw pod success
Jun 11 07:49:02.744: INFO: Pod "downwardapi-volume-dd841827-4bf2-497a-9c27-9662ecbd857c" satisfied condition "success or failure"
Jun 11 07:49:02.748: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod downwardapi-volume-dd841827-4bf2-497a-9c27-9662ecbd857c container client-container: <nil>
STEP: delete the pod
Jun 11 07:49:02.772: INFO: Waiting for pod downwardapi-volume-dd841827-4bf2-497a-9c27-9662ecbd857c to disappear
Jun 11 07:49:02.777: INFO: Pod downwardapi-volume-dd841827-4bf2-497a-9c27-9662ecbd857c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:49:02.777: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "projected-1097" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":27,"skipped":328,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:49:02.792: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1144
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jun 11 07:49:02.939: INFO: Waiting up to 1m0s for all (but 7) nodes to be ready
Jun 11 07:49:02.963: INFO: Waiting for terminating namespaces to be deleted...
Jun 11 07:49:02.968: INFO:
Logging pods the kubelet thinks is on node ip-10-0-128-119.us-west-2.compute.internal before test
Jun 11 07:49:02.987: INFO: yakcl-licensing-cm-7c5cc586b5-xgtnw from kommander started at 2020-06-11 05:33:34 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:02.987: INFO: 	Container controller-manager ready: true, restart count 0
Jun 11 07:49:02.987: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 11 07:49:02.987: INFO: kube-proxy-b8rs5 from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:02.987: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 07:49:02.987: INFO: calico-node-wxzzc from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:02.987: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 07:49:02.987: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 07:49:02.987: INFO: cert-manager-kubeaddons-cainjector-6dcd94769b-v8x5p from cert-manager started at 2020-06-11 05:29:09 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:02.987: INFO: 	Container cainjector ready: true, restart count 0
Jun 11 07:49:02.987: INFO: elasticsearch-kubeaddons-client-6c56cc5c7-kpxr5 from kubeaddons started at 2020-06-11 05:30:43 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:02.987: INFO: 	Container elasticsearch ready: true, restart count 0
Jun 11 07:49:02.987: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-594c2 from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:02.987: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 07:49:02.987: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 07:49:02.987: INFO: ebs-csi-node-2w524 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 07:49:02.987: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 07:49:02.988: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 07:49:02.988: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 07:49:02.988: INFO: prometheus-kubeaddons-prometheus-node-exporter-fz2j2 from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:02.988: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 07:49:02.988: INFO: kubefed-controller-manager-78b769f688-8mjvz from kommander started at 2020-06-11 05:33:45 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:02.988: INFO: 	Container controller-manager ready: true, restart count 0
Jun 11 07:49:02.988: INFO: fluentbit-kubeaddons-fluent-bit-hfm9r from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:02.988: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 07:49:02.988: INFO:
Logging pods the kubelet thinks is on node ip-10-0-129-105.us-west-2.compute.internal before test
Jun 11 07:49:03.008: INFO: kube-proxy-7vnlr from kube-system started at 2020-06-11 05:26:20 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.008: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 07:49:03.008: INFO: ebs-csi-node-jzjgw from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 07:49:03.008: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 07:49:03.008: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 07:49:03.008: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 07:49:03.008: INFO: fluentbit-kubeaddons-fluent-bit-nbsn6 from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.009: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 07:49:03.009: INFO: kube-oidc-proxy-kubeaddons-545d6df8-w9v44 from kubeaddons started at 2020-06-11 05:31:42 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.009: INFO: 	Container kube-oidc-proxy ready: true, restart count 0
Jun 11 07:49:03.009: INFO: prometheus-kubeaddons-grafana-c8f7fcdb5-xjw4p from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:03.009: INFO: 	Container grafana ready: true, restart count 0
Jun 11 07:49:03.009: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Jun 11 07:49:03.009: INFO: prometheus-kubeaddons-prometheus-node-exporter-zl5mt from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.009: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 07:49:03.009: INFO: calico-node-tnwfg from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:03.009: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 07:49:03.009: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 07:49:03.009: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-cxrgp from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:03.009: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 07:49:03.009: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 07:49:03.009: INFO: opsportal-kubeaddons-kommander-ui-689b97997f-x45gn from kubeaddons started at 2020-06-11 05:28:52 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.009: INFO: 	Container opsportal-kubeaddons-kommander-ui ready: true, restart count 0
Jun 11 07:49:03.009: INFO: prometheusadapter-kubeaddons-prometheus-adapter-77bc665f9-sjxp8 from kubeaddons started at 2020-06-11 05:33:06 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.009: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 11 07:49:03.009: INFO:
Logging pods the kubelet thinks is on node ip-10-0-129-30.us-west-2.compute.internal before test
Jun 11 07:49:03.029: INFO: fluentbit-kubeaddons-fluent-bit-mpwz7 from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.029: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 07:49:03.029: INFO: kommander-kubeaddons-kommander-ui-86b4f5f88f-pvgsw from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.029: INFO: 	Container kommander-kubeaddons-kommander-ui ready: true, restart count 0
Jun 11 07:49:03.029: INFO: tiller-deploy-6cdf7f9d6f-d6b2p from kube-system started at 2020-06-11 05:28:13 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.029: INFO: 	Container tiller ready: true, restart count 0
Jun 11 07:49:03.029: INFO: minio-1 from velero started at 2020-06-11 05:32:15 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.029: INFO: 	Container minio ready: true, restart count 0
Jun 11 07:49:03.029: INFO: kommander-kubecost-thanos-query-79fc7d8747-fm6rq from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.029: INFO: 	Container thanos-query ready: true, restart count 0
Jun 11 07:49:03.029: INFO: kommander-kubeaddons-kube-state-metrics-6d77646c89-bvrqg from kommander started at 2020-06-11 05:33:33 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.029: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 11 07:49:03.029: INFO: kube-proxy-td2b5 from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.029: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 07:49:03.029: INFO: calico-node-vpvmh from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:03.029: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 07:49:03.029: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 07:49:03.029: INFO: kommander-kubeaddons-cost-analyzer-7cfd4cb9cd-6tfnq from kommander started at 2020-06-11 05:33:41 +0000 UTC (3 container statuses recorded)
Jun 11 07:49:03.029: INFO: 	Container cost-analyzer-frontend ready: true, restart count 0
Jun 11 07:49:03.029: INFO: 	Container cost-analyzer-server ready: true, restart count 0
Jun 11 07:49:03.029: INFO: 	Container cost-model ready: true, restart count 0
Jun 11 07:49:03.029: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-5wq4g from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:03.029: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 07:49:03.029: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 07:49:03.029: INFO: dstorageclass-controller-manager-5c966c767f-h5rsz from kubeaddons started at 2020-06-11 05:29:47 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:03.029: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 11 07:49:03.029: INFO: 	Container manager ready: true, restart count 0
Jun 11 07:49:03.029: INFO: prometheus-kubeaddons-prometheus-node-exporter-dwrbk from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.029: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 07:49:03.029: INFO: kommander-kubeaddons-karma-8df6cfdb6-4jg59 from kommander started at 2020-06-11 05:33:33 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.029: INFO: 	Container karma ready: true, restart count 0
Jun 11 07:49:03.029: INFO: kubernetes-dashboard-549989bcdf-n9ncj from kubeaddons started at 2020-06-11 07:15:29 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.029: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jun 11 07:49:03.029: INFO: ebs-csi-node-s8248 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 07:49:03.029: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 07:49:03.029: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 07:49:03.029: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 07:49:03.029: INFO: traefik-forward-auth-kubeaddons-6675968b94-fjznl from kubeaddons started at 2020-06-11 05:37:16 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.029: INFO: 	Container traefik-forward-auth ready: true, restart count 1
Jun 11 07:49:03.029: INFO:
Logging pods the kubelet thinks is on node ip-10-0-130-174.us-west-2.compute.internal before test
Jun 11 07:49:03.049: INFO: prometheus-kubeaddons-prometheus-node-exporter-mzcfk from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.049: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 07:49:03.049: INFO: yakcl-licensing-webhook-6d876f844d-mrvm5 from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.049: INFO: 	Container webhook ready: true, restart count 0
Jun 11 07:49:03.049: INFO: kube-proxy-6htkf from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.049: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 07:49:03.049: INFO: kommander-kubeaddons-thanos-query-6cddb86b55-f4g4l from kommander started at 2020-06-11 05:33:35 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.049: INFO: 	Container thanos-query ready: true, restart count 0
Jun 11 07:49:03.049: INFO: elasticsearchexporter-kubeaddons-elasticsearch-exporter-84pdc2n from kubeaddons started at 2020-06-11 05:36:03 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.049: INFO: 	Container elasticsearch-exporter ready: true, restart count 0
Jun 11 07:49:03.049: INFO: ebs-csi-controller-0 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (6 container statuses recorded)
Jun 11 07:49:03.049: INFO: 	Container csi-attacher ready: true, restart count 0
Jun 11 07:49:03.049: INFO: 	Container csi-provisioner ready: true, restart count 0
Jun 11 07:49:03.049: INFO: 	Container csi-resizer ready: true, restart count 0
Jun 11 07:49:03.049: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jun 11 07:49:03.049: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 07:49:03.049: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 07:49:03.049: INFO: kubefed-admission-webhook-7b4997895b-x5h84 from kommander started at 2020-06-11 05:33:38 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.049: INFO: 	Container admission-webhook ready: true, restart count 0
Jun 11 07:49:03.049: INFO: kibana-kubeaddons-c8c7b687-4lqrx from kubeaddons started at 2020-06-11 07:15:29 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:03.049: INFO: 	Container initialize-kibana-index ready: true, restart count 0
Jun 11 07:49:03.049: INFO: 	Container kibana ready: true, restart count 0
Jun 11 07:49:03.049: INFO: calico-node-hrkxr from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:03.049: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 07:49:03.049: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 07:49:03.049: INFO: ebs-csi-node-bfvm8 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 07:49:03.049: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 07:49:03.049: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 07:49:03.049: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 07:49:03.049: INFO: kommander-kubeaddons-grafana-66c558d6f5-ntspl from kommander started at 2020-06-11 05:33:32 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:03.049: INFO: 	Container grafana ready: true, restart count 0
Jun 11 07:49:03.049: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Jun 11 07:49:03.049: INFO: kommander-kubeaddons-prometheus-alertmanager-7cdccf8c44-h9grs from kommander started at 2020-06-11 05:33:38 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:03.049: INFO: 	Container prometheus-alertmanager ready: true, restart count 0
Jun 11 07:49:03.049: INFO: 	Container prometheus-alertmanager-configmap-reload ready: true, restart count 0
Jun 11 07:49:03.049: INFO: fluentbit-kubeaddons-fluent-bit-xx74f from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.049: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 07:49:03.049: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-zvgtl from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:03.049: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 07:49:03.049: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 07:49:03.049: INFO: cert-manager-kubeaddons-webhook-77fbc6d59b-d67w5 from cert-manager started at 2020-06-11 05:29:09 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.049: INFO: 	Container cert-manager ready: true, restart count 1
Jun 11 07:49:03.049: INFO:
Logging pods the kubelet thinks is on node ip-10-0-132-48.us-west-2.compute.internal before test
Jun 11 07:49:03.064: INFO: calico-node-wr8kq from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:03.064: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 07:49:03.064: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 07:49:03.064: INFO: cert-manager-kubeaddons-7d7f98fbc6-86r7x from cert-manager started at 2020-06-11 05:29:09 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.064: INFO: 	Container cert-manager ready: true, restart count 0
Jun 11 07:49:03.064: INFO: prometheus-kubeaddons-prometheus-node-exporter-cqrh7 from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.064: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 07:49:03.064: INFO: yakcl-federation-cm-55469d7b6b-f6xhf from kommander started at 2020-06-11 05:33:33 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:03.064: INFO: 	Container controller-manager ready: true, restart count 0
Jun 11 07:49:03.064: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 11 07:49:03.064: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-ft9g5 from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:03.064: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 07:49:03.064: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 07:49:03.064: INFO: kube-proxy-q79z5 from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.064: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 07:49:03.064: INFO: ebs-csi-node-xzdsx from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 07:49:03.064: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 07:49:03.064: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 07:49:03.064: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 07:49:03.064: INFO: elasticsearch-kubeaddons-master-0 from kubeaddons started at 2020-06-11 05:31:03 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.064: INFO: 	Container elasticsearch ready: true, restart count 0
Jun 11 07:49:03.064: INFO: fluentbit-kubeaddons-fluent-bit-2wt5v from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.064: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 07:49:03.064: INFO:
Logging pods the kubelet thinks is on node ip-10-0-132-52.us-west-2.compute.internal before test
Jun 11 07:49:03.081: INFO: kube-proxy-w486c from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.081: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 07:49:03.081: INFO: calico-node-6mfwv from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:03.081: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 07:49:03.081: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 07:49:03.081: INFO: traefik-kubeaddons-64fbc79c9-54zfn from kubeaddons started at 2020-06-11 05:31:03 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.081: INFO: 	Container traefik-kubeaddons ready: true, restart count 0
Jun 11 07:49:03.081: INFO: kubefed-controller-manager-78b769f688-rmx64 from kommander started at 2020-06-11 05:33:38 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.081: INFO: 	Container controller-manager ready: true, restart count 0
Jun 11 07:49:03.081: INFO: traefik-kubeaddons-1.72.19-zdq6l from kubeaddons started at 2020-06-11 05:29:49 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.081: INFO: 	Container traefik ready: false, restart count 0
Jun 11 07:49:03.081: INFO: ebs-csi-node-cf4v4 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 07:49:03.081: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 07:49:03.081: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 07:49:03.081: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 07:49:03.081: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-tssmj from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:03.081: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 07:49:03.081: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 07:49:03.081: INFO: kubeaddons-controller-manager-986f46689-sk6x8 from kubeaddons started at 2020-06-11 05:27:57 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.081: INFO: 	Container manager ready: true, restart count 0
Jun 11 07:49:03.081: INFO: prometheus-kubeaddons-prometheus-node-exporter-nff74 from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.081: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 07:49:03.081: INFO: elasticsearch-kubeaddons-client-6c56cc5c7-zp9fl from kubeaddons started at 2020-06-11 05:30:43 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.081: INFO: 	Container elasticsearch ready: true, restart count 0
Jun 11 07:49:03.081: INFO: fluentbit-kubeaddons-fluent-bit-78mbt from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.081: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 07:49:03.081: INFO:
Logging pods the kubelet thinks is on node ip-10-0-135-119.us-west-2.compute.internal before test
Jun 11 07:49:03.098: INFO: calico-node-sq7sb from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:03.098: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 07:49:03.098: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 07:49:03.098: INFO: external-dns-kubeaddons-765d55b455-vlxcv from kubeaddons started at 2020-06-11 05:28:25 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.098: INFO: 	Container external-dns ready: true, restart count 0
Jun 11 07:49:03.098: INFO: minio-2 from velero started at 2020-06-11 05:32:12 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.098: INFO: 	Container minio ready: true, restart count 0
Jun 11 07:49:03.098: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-b7drp from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:03.098: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 07:49:03.098: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 07:49:03.098: INFO: kube-proxy-xnz2c from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.098: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 07:49:03.098: INFO: prometheus-kubeaddons-prometheus-node-exporter-4fllq from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.098: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 07:49:03.098: INFO: alertmanager-prometheus-kubeaddons-prom-alertmanager-0 from kubeaddons started at 2020-06-11 05:32:06 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:03.098: INFO: 	Container alertmanager ready: true, restart count 0
Jun 11 07:49:03.098: INFO: 	Container config-reloader ready: true, restart count 0
Jun 11 07:49:03.098: INFO: ebs-csi-node-w9whr from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 07:49:03.098: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 07:49:03.098: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 07:49:03.098: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 07:49:03.098: INFO: dex-kubeaddons-dex-controller-7bd5fc575c-xmnlq from kubeaddons started at 2020-06-11 05:31:09 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:03.098: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 11 07:49:03.098: INFO: 	Container manager ready: true, restart count 0
Jun 11 07:49:03.098: INFO: velero-kubeaddons-5d85fcdcb9-gqb5c from velero started at 2020-06-11 05:32:08 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.098: INFO: 	Container velero ready: true, restart count 3
Jun 11 07:49:03.098: INFO: elasticsearch-kubeaddons-master-1 from kubeaddons started at 2020-06-11 05:32:29 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.098: INFO: 	Container elasticsearch ready: true, restart count 0
Jun 11 07:49:03.098: INFO: fluentbit-kubeaddons-fluent-bit-6lwhn from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.098: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 07:49:03.098: INFO:
Logging pods the kubelet thinks is on node ip-10-0-136-38.us-west-2.compute.internal before test
Jun 11 07:49:03.106: INFO: cost-analyzer-checks-1591861200-kj2b7 from kommander started at 2020-06-11 07:40:08 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.106: INFO: 	Container cost-analyzer-checks ready: false, restart count 0
Jun 11 07:49:03.106: INFO: pod-hostip-364ce91b-7861-4bc2-985a-c001fa8514b8 from pods-4254 started at 2020-06-11 07:48:58 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.106: INFO: 	Container test ready: true, restart count 0
Jun 11 07:49:03.106: INFO: calico-node-tzhxw from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:03.106: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 07:49:03.106: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 07:49:03.106: INFO: sonobuoy from sonobuoy started at 2020-06-11 07:40:59 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.106: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 11 07:49:03.106: INFO: prometheus-kubeaddons-prometheus-node-exporter-fj26x from kubeaddons started at 2020-06-11 07:36:06 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.106: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 07:49:03.106: INFO: fluentbit-kubeaddons-fluent-bit-flmsm from kubeaddons started at 2020-06-11 07:35:36 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.106: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 07:49:03.106: INFO: kube-proxy-dz75m from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.106: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 07:49:03.106: INFO: ebs-csi-node-8xl6p from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 07:49:03.106: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 07:49:03.106: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 07:49:03.106: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 07:49:03.106: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-lgn88 from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:03.106: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 07:49:03.106: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 07:49:03.107: INFO: cost-analyzer-checks-1591860600-79ssz from kommander started at 2020-06-11 07:30:06 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.107: INFO: 	Container cost-analyzer-checks ready: false, restart count 0
Jun 11 07:49:03.107: INFO:
Logging pods the kubelet thinks is on node ip-10-0-137-210.us-west-2.compute.internal before test
Jun 11 07:49:03.123: INFO: kube-proxy-qj7tx from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.123: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 07:49:03.123: INFO: minio-3 from velero started at 2020-06-11 05:32:15 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.123: INFO: 	Container minio ready: true, restart count 0
Jun 11 07:49:03.123: INFO: prometheus-kubeaddons-kube-state-metrics-6599df558b-mmf9r from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.123: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 11 07:49:03.123: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-6cvjn from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:03.123: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 07:49:03.123: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 07:49:03.123: INFO: cost-analyzer-checks-1591860000-mwznw from kommander started at 2020-06-11 07:20:04 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.123: INFO: 	Container cost-analyzer-checks ready: false, restart count 0
Jun 11 07:49:03.123: INFO: calico-node-vkrzv from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:03.123: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 07:49:03.123: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 07:49:03.123: INFO: ebs-csi-node-frsf2 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 07:49:03.123: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 07:49:03.123: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 07:49:03.123: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 07:49:03.123: INFO: prometheus-kubeaddons-prometheus-node-exporter-8z4lv from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.123: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 07:49:03.123: INFO: fluentbit-kubeaddons-fluent-bit-hv9bs from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.123: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 07:49:03.123: INFO: dex-kubeaddons-696dbdb6c6-5prn9 from kubeaddons started at 2020-06-11 05:37:11 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.123: INFO: 	Container main ready: true, restart count 0
Jun 11 07:49:03.123: INFO: opsportal-landing-6f6865b688-6r5kn from kubeaddons started at 2020-06-11 05:28:52 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.123: INFO: 	Container opsportal-landing ready: true, restart count 0
Jun 11 07:49:03.123: INFO: gatekeeper-kubeaddons-776f4b5c96-jqsq8 from kubeaddons started at 2020-06-11 05:29:52 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.123: INFO: 	Container manager ready: true, restart count 0
Jun 11 07:49:03.123: INFO: elasticsearch-kubeaddons-master-2 from kubeaddons started at 2020-06-11 05:33:29 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.123: INFO: 	Container elasticsearch ready: true, restart count 0
Jun 11 07:49:03.123: INFO:
Logging pods the kubelet thinks is on node ip-10-0-138-28.us-west-2.compute.internal before test
Jun 11 07:49:03.142: INFO: reloader-kubeaddons-reloader-7d4bd64cfb-4qjm4 from kubeaddons started at 2020-06-11 05:28:30 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.142: INFO: 	Container reloader-kubeaddons-reloader ready: true, restart count 0
Jun 11 07:49:03.142: INFO: kommander-kubeaddons-kubeaddons-catalog-6654f856df-kndt7 from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.142: INFO: 	Container kubeaddons-catalog ready: true, restart count 0
Jun 11 07:49:03.142: INFO: yakcl-federation-webhook-6fcc46596-smpfh from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.142: INFO: 	Container webhook ready: true, restart count 0
Jun 11 07:49:03.142: INFO: yakcl-federation-utility-apiserver-7d57699df9-wp7bs from kommander started at 2020-06-11 05:33:33 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.142: INFO: 	Container server ready: true, restart count 0
Jun 11 07:49:03.142: INFO: ebs-csi-snapshot-controller-0 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.142: INFO: 	Container ebs-csi-snapshot-controller ready: true, restart count 0
Jun 11 07:49:03.142: INFO: minio-0 from velero started at 2020-06-11 05:32:12 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.142: INFO: 	Container minio ready: true, restart count 0
Jun 11 07:49:03.142: INFO: yakcl-federation-authorizedlister-5db7b69bfd-zl4mt from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.142: INFO: 	Container authorizedlister ready: true, restart count 0
Jun 11 07:49:03.142: INFO: calico-node-fzn27 from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:03.142: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 07:49:03.142: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 07:49:03.142: INFO: prometheus-kubeaddons-prom-operator-767c8d59cb-44z76 from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:03.142: INFO: 	Container prometheus-operator ready: true, restart count 0
Jun 11 07:49:03.142: INFO: 	Container tls-proxy ready: true, restart count 0
Jun 11 07:49:03.142: INFO: dex-k8s-authenticator-kubeaddons-748fcc984d-vdpxg from kubeaddons started at 2020-06-11 05:31:50 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.142: INFO: 	Container dex-k8s-authenticator ready: true, restart count 5
Jun 11 07:49:03.142: INFO: elasticsearch-kubeaddons-client-6c56cc5c7-blbht from kubeaddons started at 2020-06-11 07:15:29 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.142: INFO: 	Container elasticsearch ready: true, restart count 0
Jun 11 07:49:03.142: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-lf27c from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 07:49:03.142: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 07:49:03.142: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 07:49:03.142: INFO: kube-proxy-pp8td from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.142: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 07:49:03.142: INFO: ebs-csi-node-nbd7n from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 07:49:03.142: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 07:49:03.142: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 07:49:03.142: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 07:49:03.142: INFO: prometheus-kubeaddons-prometheus-node-exporter-4c6c2 from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.142: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 07:49:03.142: INFO: kommander-kubeaddons-prometheus-server-86d645cc98-5bbdz from kommander started at 2020-06-11 05:33:41 +0000 UTC (3 container statuses recorded)
Jun 11 07:49:03.142: INFO: 	Container prometheus-server ready: true, restart count 0
Jun 11 07:49:03.142: INFO: 	Container prometheus-server-configmap-reload ready: true, restart count 0
Jun 11 07:49:03.142: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 11 07:49:03.143: INFO: fluentbit-kubeaddons-fluent-bit-46z8w from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 07:49:03.143: INFO: 	Container fluent-bit ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-a84b5c1f-9dfb-40d8-934d-3273610061cd 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-a84b5c1f-9dfb-40d8-934d-3273610061cd off the node ip-10-0-136-38.us-west-2.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-a84b5c1f-9dfb-40d8-934d-3273610061cd
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:49:11.265: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "sched-pred-1144" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:8.488 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":280,"completed":28,"skipped":360,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:49:11.280: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3508
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 07:49:11.893: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 07:49:14.917: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 07:49:14.921: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:49:16.070: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "webhook-3508" for this suite.
STEP: Destroying namespace "webhook-3508-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":280,"completed":29,"skipped":374,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:49:16.168: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-841
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name projected-secret-test-d2257bef-1eab-45a9-afb5-4448df8606b9
STEP: Creating a pod to test consume secrets
Jun 11 07:49:16.340: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9a688c80-6218-4ac6-b92b-c6b9d54eeaf1" in namespace "projected-841" to be "success or failure"
Jun 11 07:49:16.346: INFO: Pod "pod-projected-secrets-9a688c80-6218-4ac6-b92b-c6b9d54eeaf1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.256094ms
Jun 11 07:49:18.350: INFO: Pod "pod-projected-secrets-9a688c80-6218-4ac6-b92b-c6b9d54eeaf1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009508855s
STEP: Saw pod success
Jun 11 07:49:18.350: INFO: Pod "pod-projected-secrets-9a688c80-6218-4ac6-b92b-c6b9d54eeaf1" satisfied condition "success or failure"
Jun 11 07:49:18.354: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-projected-secrets-9a688c80-6218-4ac6-b92b-c6b9d54eeaf1 container secret-volume-test: <nil>
STEP: delete the pod
Jun 11 07:49:18.377: INFO: Waiting for pod pod-projected-secrets-9a688c80-6218-4ac6-b92b-c6b9d54eeaf1 to disappear
Jun 11 07:49:18.384: INFO: Pod pod-projected-secrets-9a688c80-6218-4ac6-b92b-c6b9d54eeaf1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:49:18.384: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "projected-841" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":30,"skipped":384,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:49:18.399: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2472
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 07:49:18.548: INFO: Creating deployment "webserver-deployment"
Jun 11 07:49:18.553: INFO: Waiting for observed generation 1
Jun 11 07:49:20.562: INFO: Waiting for all required pods to come up
Jun 11 07:49:20.569: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jun 11 07:49:22.580: INFO: Waiting for deployment "webserver-deployment" to complete
Jun 11 07:49:22.588: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jun 11 07:49:22.597: INFO: Updating deployment webserver-deployment
Jun 11 07:49:22.597: INFO: Waiting for observed generation 2
Jun 11 07:49:24.605: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun 11 07:49:24.609: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun 11 07:49:24.613: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 11 07:49:24.624: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun 11 07:49:24.624: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun 11 07:49:24.627: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 11 07:49:24.634: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jun 11 07:49:24.634: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jun 11 07:49:24.643: INFO: Updating deployment webserver-deployment
Jun 11 07:49:24.643: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jun 11 07:49:24.650: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun 11 07:49:24.656: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jun 11 07:49:24.665: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-2472 /apis/apps/v1/namespaces/deployment-2472/deployments/webserver-deployment 3f321673-0726-48c9-9465-e469ab57a4f6 98823 3 2020-06-11 07:49:18 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc007338fb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-06-11 07:49:21 +0000 UTC,LastTransitionTime:2020-06-11 07:49:21 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-06-11 07:49:22 +0000 UTC,LastTransitionTime:2020-06-11 07:49:18 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jun 11 07:49:24.674: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-2472 /apis/apps/v1/namespaces/deployment-2472/replicasets/webserver-deployment-c7997dcc8 aa77f22c-909e-4e27-9879-8c4b792212e8 98826 3 2020-06-11 07:49:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 3f321673-0726-48c9-9465-e469ab57a4f6 0xc0073397b7 0xc0073397b8}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc007339878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 11 07:49:24.674: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jun 11 07:49:24.674: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-2472 /apis/apps/v1/namespaces/deployment-2472/replicasets/webserver-deployment-595b5b9587 7af63bd8-817a-4e4f-8f9e-ea1e06ebd3ee 98824 3 2020-06-11 07:49:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 3f321673-0726-48c9-9465-e469ab57a4f6 0xc0073395f7 0xc0073395f8}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc007339688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jun 11 07:49:24.694: INFO: Pod "webserver-deployment-595b5b9587-2v77c" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2v77c webserver-deployment-595b5b9587- deployment-2472 /api/v1/namespaces/deployment-2472/pods/webserver-deployment-595b5b9587-2v77c ba6315e7-bcfc-422c-94af-da971ad55f21 98841 0 2020-06-11 07:49:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 7af63bd8-817a-4e4f-8f9e-ea1e06ebd3ee 0xc00737c577 0xc00737c578}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kk9m6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kk9m6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kk9m6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-136-38.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 07:49:24.694: INFO: Pod "webserver-deployment-595b5b9587-4fkgf" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-4fkgf webserver-deployment-595b5b9587- deployment-2472 /api/v1/namespaces/deployment-2472/pods/webserver-deployment-595b5b9587-4fkgf 2d695855-6fad-4a45-9a6a-f6da65749460 98654 0 2020-06-11 07:49:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.226.90/32 cni.projectcalico.org/podIPs:192.168.226.90/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 7af63bd8-817a-4e4f-8f9e-ea1e06ebd3ee 0xc00737c6c0 0xc00737c6c1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kk9m6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kk9m6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kk9m6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-48.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.132.48,PodIP:192.168.226.90,StartTime:2020-06-11 07:49:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-11 07:49:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://49f2c108ca2b405efcb8b2c6a231b94941a08017608aa98924b75e226bd99a9a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.226.90,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 07:49:24.694: INFO: Pod "webserver-deployment-595b5b9587-54xwx" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-54xwx webserver-deployment-595b5b9587- deployment-2472 /api/v1/namespaces/deployment-2472/pods/webserver-deployment-595b5b9587-54xwx ac0d552d-865c-422a-8e62-49d79f33df37 98844 0 2020-06-11 07:49:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 7af63bd8-817a-4e4f-8f9e-ea1e06ebd3ee 0xc00737c8d7 0xc00737c8d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kk9m6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kk9m6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kk9m6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-129-105.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 07:49:24.694: INFO: Pod "webserver-deployment-595b5b9587-6zgx5" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-6zgx5 webserver-deployment-595b5b9587- deployment-2472 /api/v1/namespaces/deployment-2472/pods/webserver-deployment-595b5b9587-6zgx5 1385ab94-bbf1-4dc7-9766-162e4c6f67e9 98670 0 2020-06-11 07:49:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.42.60/32 cni.projectcalico.org/podIPs:192.168.42.60/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 7af63bd8-817a-4e4f-8f9e-ea1e06ebd3ee 0xc00737ca30 0xc00737ca31}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kk9m6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kk9m6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kk9m6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-136-38.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.136.38,PodIP:192.168.42.60,StartTime:2020-06-11 07:49:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-11 07:49:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://91209c6f8d244d5cd0b0466acc9b9260a6921cde6ff0ec876c8f2efcffb22a78,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.42.60,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 07:49:24.695: INFO: Pod "webserver-deployment-595b5b9587-b6szc" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-b6szc webserver-deployment-595b5b9587- deployment-2472 /api/v1/namespaces/deployment-2472/pods/webserver-deployment-595b5b9587-b6szc 6446d865-cb9a-4991-932f-64c3750bb769 98651 0 2020-06-11 07:49:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.74.241/32 cni.projectcalico.org/podIPs:192.168.74.241/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 7af63bd8-817a-4e4f-8f9e-ea1e06ebd3ee 0xc00737cbf7 0xc00737cbf8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kk9m6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kk9m6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kk9m6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-174.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.174,PodIP:192.168.74.241,StartTime:2020-06-11 07:49:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-11 07:49:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://cb73f640d52fc6b69eed1bc3e13904c3093d62bcf3e88880d0a552d00f2835c8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.74.241,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 07:49:24.695: INFO: Pod "webserver-deployment-595b5b9587-ch7p9" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-ch7p9 webserver-deployment-595b5b9587- deployment-2472 /api/v1/namespaces/deployment-2472/pods/webserver-deployment-595b5b9587-ch7p9 05fd7f4b-e296-409b-ab69-3e2a6fafb0d3 98849 0 2020-06-11 07:49:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 7af63bd8-817a-4e4f-8f9e-ea1e06ebd3ee 0xc00737ce17 0xc00737ce18}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kk9m6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kk9m6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kk9m6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 07:49:24.695: INFO: Pod "webserver-deployment-595b5b9587-dwwjt" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-dwwjt webserver-deployment-595b5b9587- deployment-2472 /api/v1/namespaces/deployment-2472/pods/webserver-deployment-595b5b9587-dwwjt a54e5da1-9ae2-4cc8-8e73-ce5490b67bc7 98847 0 2020-06-11 07:49:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 7af63bd8-817a-4e4f-8f9e-ea1e06ebd3ee 0xc00737cf27 0xc00737cf28}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kk9m6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kk9m6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kk9m6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 07:49:24.695: INFO: Pod "webserver-deployment-595b5b9587-fm2hs" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-fm2hs webserver-deployment-595b5b9587- deployment-2472 /api/v1/namespaces/deployment-2472/pods/webserver-deployment-595b5b9587-fm2hs 2abeea64-a473-4581-84d8-139d185265c1 98663 0 2020-06-11 07:49:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.23.81/32 cni.projectcalico.org/podIPs:192.168.23.81/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 7af63bd8-817a-4e4f-8f9e-ea1e06ebd3ee 0xc00737d077 0xc00737d078}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kk9m6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kk9m6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kk9m6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-52.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.132.52,PodIP:192.168.23.81,StartTime:2020-06-11 07:49:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-11 07:49:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://52abda5141060647e25d1eddbd7881b80025851d8c23084dcaf72aeed2f9c7c6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.23.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 07:49:24.695: INFO: Pod "webserver-deployment-595b5b9587-jwnxh" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-jwnxh webserver-deployment-595b5b9587- deployment-2472 /api/v1/namespaces/deployment-2472/pods/webserver-deployment-595b5b9587-jwnxh 7028a57a-0f19-4123-b89b-8b5e6b0fa9e5 98666 0 2020-06-11 07:49:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.169.26/32 cni.projectcalico.org/podIPs:192.168.169.26/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 7af63bd8-817a-4e4f-8f9e-ea1e06ebd3ee 0xc00737d237 0xc00737d238}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kk9m6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kk9m6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kk9m6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-135-119.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.135.119,PodIP:192.168.169.26,StartTime:2020-06-11 07:49:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-11 07:49:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://00bbc536cb4138658f78188dec843809ee788691d4b1c6bdb2625b6ed5fb0306,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.169.26,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 07:49:24.695: INFO: Pod "webserver-deployment-595b5b9587-kj822" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-kj822 webserver-deployment-595b5b9587- deployment-2472 /api/v1/namespaces/deployment-2472/pods/webserver-deployment-595b5b9587-kj822 199ec2e2-51f4-4f86-bc1c-e36008ec4eaa 98846 0 2020-06-11 07:49:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 7af63bd8-817a-4e4f-8f9e-ea1e06ebd3ee 0xc00737d3a7 0xc00737d3a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kk9m6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kk9m6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kk9m6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 07:49:24.695: INFO: Pod "webserver-deployment-595b5b9587-n89rw" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-n89rw webserver-deployment-595b5b9587- deployment-2472 /api/v1/namespaces/deployment-2472/pods/webserver-deployment-595b5b9587-n89rw afec129d-c2d1-4a9e-ad6c-07ca675a78de 98857 0 2020-06-11 07:49:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 7af63bd8-817a-4e4f-8f9e-ea1e06ebd3ee 0xc00737d4d7 0xc00737d4d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kk9m6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kk9m6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kk9m6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-136-38.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.136.38,PodIP:,StartTime:2020-06-11 07:49:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 07:49:24.696: INFO: Pod "webserver-deployment-595b5b9587-plgtw" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-plgtw webserver-deployment-595b5b9587- deployment-2472 /api/v1/namespaces/deployment-2472/pods/webserver-deployment-595b5b9587-plgtw 0f1f401b-79fa-4142-8911-a29cb86b21b2 98645 0 2020-06-11 07:49:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.208.132/32 cni.projectcalico.org/podIPs:192.168.208.132/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 7af63bd8-817a-4e4f-8f9e-ea1e06ebd3ee 0xc00737d647 0xc00737d648}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kk9m6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kk9m6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kk9m6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-129-30.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.129.30,PodIP:192.168.208.132,StartTime:2020-06-11 07:49:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-11 07:49:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://ab04d5b69d69acc7d26e9c0279f79586e32c4f518bbf4a15f8948ca665e2a0dc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.208.132,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 07:49:24.696: INFO: Pod "webserver-deployment-595b5b9587-rtpnz" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rtpnz webserver-deployment-595b5b9587- deployment-2472 /api/v1/namespaces/deployment-2472/pods/webserver-deployment-595b5b9587-rtpnz 18022316-b12c-4a90-aa13-3a76f685e19b 98646 0 2020-06-11 07:49:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.239.221/32 cni.projectcalico.org/podIPs:192.168.239.221/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 7af63bd8-817a-4e4f-8f9e-ea1e06ebd3ee 0xc00737d827 0xc00737d828}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kk9m6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kk9m6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kk9m6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-137-210.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.137.210,PodIP:192.168.239.221,StartTime:2020-06-11 07:49:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-11 07:49:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://e1eebef8a352ad7d9f536bbb6eea12bfb7340766de17e9ddf0d30b10e81fb9e2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.239.221,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 07:49:24.696: INFO: Pod "webserver-deployment-595b5b9587-szqsl" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-szqsl webserver-deployment-595b5b9587- deployment-2472 /api/v1/namespaces/deployment-2472/pods/webserver-deployment-595b5b9587-szqsl 1342a3c5-ebee-4d8c-a022-cf483bf09cb3 98848 0 2020-06-11 07:49:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 7af63bd8-817a-4e4f-8f9e-ea1e06ebd3ee 0xc00737d9a7 0xc00737d9a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kk9m6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kk9m6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kk9m6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 07:49:24.696: INFO: Pod "webserver-deployment-595b5b9587-zg9zh" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-zg9zh webserver-deployment-595b5b9587- deployment-2472 /api/v1/namespaces/deployment-2472/pods/webserver-deployment-595b5b9587-zg9zh 36ad7a9f-a615-43aa-a748-303b99e0f438 98657 0 2020-06-11 07:49:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.28.214/32 cni.projectcalico.org/podIPs:192.168.28.214/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 7af63bd8-817a-4e4f-8f9e-ea1e06ebd3ee 0xc00737dab7 0xc00737dab8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kk9m6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kk9m6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kk9m6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-128-119.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.119,PodIP:192.168.28.214,StartTime:2020-06-11 07:49:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-11 07:49:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://49aba67dde5606f1197a67a505a117333709d6b0260b07c1171d6ba90c75f1ab,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.28.214,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 07:49:24.696: INFO: Pod "webserver-deployment-c7997dcc8-cdrhj" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-cdrhj webserver-deployment-c7997dcc8- deployment-2472 /api/v1/namespaces/deployment-2472/pods/webserver-deployment-c7997dcc8-cdrhj 03d2b1db-e1d7-4307-bdd6-72a9092cfac2 98783 0 2020-06-11 07:49:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.226.91/32 cni.projectcalico.org/podIPs:192.168.226.91/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 aa77f22c-909e-4e27-9879-8c4b792212e8 0xc00737dc57 0xc00737dc58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kk9m6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kk9m6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kk9m6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-48.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.132.48,PodIP:,StartTime:2020-06-11 07:49:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 07:49:24.696: INFO: Pod "webserver-deployment-c7997dcc8-f492x" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-f492x webserver-deployment-c7997dcc8- deployment-2472 /api/v1/namespaces/deployment-2472/pods/webserver-deployment-c7997dcc8-f492x b6fd22e8-981f-4b64-b53c-0bfb9e9f2852 98852 0 2020-06-11 07:49:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 aa77f22c-909e-4e27-9879-8c4b792212e8 0xc00737deb0 0xc00737deb1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kk9m6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kk9m6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kk9m6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-137-210.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 07:49:24.696: INFO: Pod "webserver-deployment-c7997dcc8-fd8lr" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-fd8lr webserver-deployment-c7997dcc8- deployment-2472 /api/v1/namespaces/deployment-2472/pods/webserver-deployment-c7997dcc8-fd8lr 9b149a20-e6cb-4081-a56a-76cd14503b54 98853 0 2020-06-11 07:49:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 aa77f22c-909e-4e27-9879-8c4b792212e8 0xc00737dff0 0xc00737dff1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kk9m6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kk9m6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kk9m6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-135-119.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 07:49:24.697: INFO: Pod "webserver-deployment-c7997dcc8-jkllq" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-jkllq webserver-deployment-c7997dcc8- deployment-2472 /api/v1/namespaces/deployment-2472/pods/webserver-deployment-c7997dcc8-jkllq b6ee09ce-95ab-4b3b-af0f-3b9bf7570b42 98837 0 2020-06-11 07:49:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 aa77f22c-909e-4e27-9879-8c4b792212e8 0xc0073a81d0 0xc0073a81d1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kk9m6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kk9m6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kk9m6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-129-30.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 07:49:24.697: INFO: Pod "webserver-deployment-c7997dcc8-l6lw5" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-l6lw5 webserver-deployment-c7997dcc8- deployment-2472 /api/v1/namespaces/deployment-2472/pods/webserver-deployment-c7997dcc8-l6lw5 b132714a-595c-431c-a6fa-78b605a7312a 98791 0 2020-06-11 07:49:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.42.57/32 cni.projectcalico.org/podIPs:192.168.42.57/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 aa77f22c-909e-4e27-9879-8c4b792212e8 0xc0073a83d0 0xc0073a83d1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kk9m6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kk9m6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kk9m6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-136-38.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.136.38,PodIP:,StartTime:2020-06-11 07:49:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 07:49:24.697: INFO: Pod "webserver-deployment-c7997dcc8-pqr2w" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-pqr2w webserver-deployment-c7997dcc8- deployment-2472 /api/v1/namespaces/deployment-2472/pods/webserver-deployment-c7997dcc8-pqr2w 63aff92e-9429-4c61-9deb-8a22efe76f89 98784 0 2020-06-11 07:49:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.42.54/32 cni.projectcalico.org/podIPs:192.168.42.54/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 aa77f22c-909e-4e27-9879-8c4b792212e8 0xc0073a85e0 0xc0073a85e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kk9m6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kk9m6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kk9m6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-136-38.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.136.38,PodIP:,StartTime:2020-06-11 07:49:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 07:49:24.697: INFO: Pod "webserver-deployment-c7997dcc8-shqx9" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-shqx9 webserver-deployment-c7997dcc8- deployment-2472 /api/v1/namespaces/deployment-2472/pods/webserver-deployment-c7997dcc8-shqx9 fd9e2ca1-575b-4c38-8b55-4c775216ba89 98781 0 2020-06-11 07:49:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.23.82/32 cni.projectcalico.org/podIPs:192.168.23.82/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 aa77f22c-909e-4e27-9879-8c4b792212e8 0xc0073a88e0 0xc0073a88e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kk9m6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kk9m6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kk9m6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-52.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.132.52,PodIP:,StartTime:2020-06-11 07:49:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 07:49:24.697: INFO: Pod "webserver-deployment-c7997dcc8-vxgmd" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-vxgmd webserver-deployment-c7997dcc8- deployment-2472 /api/v1/namespaces/deployment-2472/pods/webserver-deployment-c7997dcc8-vxgmd 1b2d550a-c9d5-40ad-8efd-b5012e4b3323 98782 0 2020-06-11 07:49:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.28.215/32 cni.projectcalico.org/podIPs:192.168.28.215/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 aa77f22c-909e-4e27-9879-8c4b792212e8 0xc0073a8a70 0xc0073a8a71}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kk9m6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kk9m6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kk9m6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-128-119.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:49:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.119,PodIP:,StartTime:2020-06-11 07:49:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:49:24.697: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "deployment-2472" for this suite.

• [SLOW TEST:6.339 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":280,"completed":31,"skipped":392,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:49:24.738: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7310
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 11 07:49:24.956: INFO: Waiting up to 5m0s for pod "pod-ddc80764-cc48-42fb-8140-d869f5523665" in namespace "emptydir-7310" to be "success or failure"
Jun 11 07:49:24.960: INFO: Pod "pod-ddc80764-cc48-42fb-8140-d869f5523665": Phase="Pending", Reason="", readiness=false. Elapsed: 3.988922ms
Jun 11 07:49:26.964: INFO: Pod "pod-ddc80764-cc48-42fb-8140-d869f5523665": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008079643s
Jun 11 07:49:28.968: INFO: Pod "pod-ddc80764-cc48-42fb-8140-d869f5523665": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012478406s
Jun 11 07:49:30.973: INFO: Pod "pod-ddc80764-cc48-42fb-8140-d869f5523665": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016600096s
Jun 11 07:49:32.977: INFO: Pod "pod-ddc80764-cc48-42fb-8140-d869f5523665": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.021154796s
STEP: Saw pod success
Jun 11 07:49:32.977: INFO: Pod "pod-ddc80764-cc48-42fb-8140-d869f5523665" satisfied condition "success or failure"
Jun 11 07:49:32.981: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-ddc80764-cc48-42fb-8140-d869f5523665 container test-container: <nil>
STEP: delete the pod
Jun 11 07:49:33.004: INFO: Waiting for pod pod-ddc80764-cc48-42fb-8140-d869f5523665 to disappear
Jun 11 07:49:33.008: INFO: Pod pod-ddc80764-cc48-42fb-8140-d869f5523665 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:49:33.008: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "emptydir-7310" for this suite.

• [SLOW TEST:8.286 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":32,"skipped":401,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:49:33.025: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8489
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jun 11 07:49:35.724: INFO: Successfully updated pod "annotationupdate56939c6c-7008-42b8-b0cb-02a51e5804c4"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:49:39.749: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "projected-8489" for this suite.

• [SLOW TEST:6.739 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":33,"skipped":407,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:49:39.764: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6587
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 11 07:49:39.979: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:39.979: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:39.979: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:39.979: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:39.979: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:39.979: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:39.979: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:39.979: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:39.979: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:39.979: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:39.983: INFO: Number of nodes with available pods: 0
Jun 11 07:49:39.983: INFO: Node ip-10-0-128-119.us-west-2.compute.internal is running more than one daemon pod
Jun 11 07:49:40.990: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:40.990: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:40.990: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:40.990: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:40.990: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:40.990: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:40.991: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:40.991: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:40.991: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:40.991: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:40.995: INFO: Number of nodes with available pods: 0
Jun 11 07:49:40.995: INFO: Node ip-10-0-128-119.us-west-2.compute.internal is running more than one daemon pod
Jun 11 07:49:41.990: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:41.990: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:41.991: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:41.991: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:41.991: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:41.991: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:41.991: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:41.991: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:41.991: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:41.991: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:41.998: INFO: Number of nodes with available pods: 2
Jun 11 07:49:41.998: INFO: Node ip-10-0-129-105.us-west-2.compute.internal is running more than one daemon pod
Jun 11 07:49:42.991: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:42.991: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:42.991: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:42.991: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:42.991: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:42.991: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:42.991: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:42.991: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:42.991: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:42.991: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:42.996: INFO: Number of nodes with available pods: 10
Jun 11 07:49:42.996: INFO: Number of running nodes: 10, number of available pods: 10
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jun 11 07:49:43.019: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:43.019: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:43.019: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:43.019: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:43.019: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:43.019: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:43.019: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:43.019: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:43.019: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:43.019: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:43.024: INFO: Number of nodes with available pods: 9
Jun 11 07:49:43.024: INFO: Node ip-10-0-136-38.us-west-2.compute.internal is running more than one daemon pod
Jun 11 07:49:44.031: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:44.031: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:44.031: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:44.031: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:44.031: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:44.031: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:44.031: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:44.031: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:44.031: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:44.031: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:44.036: INFO: Number of nodes with available pods: 9
Jun 11 07:49:44.036: INFO: Node ip-10-0-136-38.us-west-2.compute.internal is running more than one daemon pod
Jun 11 07:49:45.031: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:45.031: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:45.031: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:45.031: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:45.032: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:45.032: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:45.032: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:45.032: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:45.032: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:45.032: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:45.036: INFO: Number of nodes with available pods: 9
Jun 11 07:49:45.036: INFO: Node ip-10-0-136-38.us-west-2.compute.internal is running more than one daemon pod
Jun 11 07:49:46.031: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:46.032: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:46.032: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:46.032: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:46.032: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:46.032: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:46.032: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:46.032: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:46.032: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:46.032: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:49:46.037: INFO: Number of nodes with available pods: 10
Jun 11 07:49:46.037: INFO: Number of running nodes: 10, number of available pods: 10
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6587, will wait for the garbage collector to delete the pods
Jun 11 07:49:46.109: INFO: Deleting DaemonSet.extensions daemon-set took: 10.802878ms
Jun 11 07:49:47.709: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.600243382s
Jun 11 07:49:57.514: INFO: Number of nodes with available pods: 0
Jun 11 07:49:57.514: INFO: Number of running nodes: 0, number of available pods: 0
Jun 11 07:49:57.518: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6587/daemonsets","resourceVersion":"99957"},"items":null}

Jun 11 07:49:57.522: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6587/pods","resourceVersion":"99957"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:49:57.568: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "daemonsets-6587" for this suite.

• [SLOW TEST:17.819 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":280,"completed":34,"skipped":417,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:49:57.583: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-959
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:50:57.746: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "container-probe-959" for this suite.

• [SLOW TEST:60.181 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":280,"completed":35,"skipped":442,"failed":0}
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:50:57.764: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6529
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jun 11 07:50:57.924: INFO: Waiting up to 5m0s for pod "downward-api-062dc191-6b13-40b6-b9f7-df020f9848b7" in namespace "downward-api-6529" to be "success or failure"
Jun 11 07:50:57.927: INFO: Pod "downward-api-062dc191-6b13-40b6-b9f7-df020f9848b7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.784504ms
Jun 11 07:50:59.932: INFO: Pod "downward-api-062dc191-6b13-40b6-b9f7-df020f9848b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008301457s
Jun 11 07:51:01.936: INFO: Pod "downward-api-062dc191-6b13-40b6-b9f7-df020f9848b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012499072s
STEP: Saw pod success
Jun 11 07:51:01.936: INFO: Pod "downward-api-062dc191-6b13-40b6-b9f7-df020f9848b7" satisfied condition "success or failure"
Jun 11 07:51:01.940: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod downward-api-062dc191-6b13-40b6-b9f7-df020f9848b7 container dapi-container: <nil>
STEP: delete the pod
Jun 11 07:51:01.964: INFO: Waiting for pod downward-api-062dc191-6b13-40b6-b9f7-df020f9848b7 to disappear
Jun 11 07:51:01.969: INFO: Pod downward-api-062dc191-6b13-40b6-b9f7-df020f9848b7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:51:01.970: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "downward-api-6529" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":280,"completed":36,"skipped":442,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:51:01.986: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2331
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-fkzw
STEP: Creating a pod to test atomic-volume-subpath
Jun 11 07:51:02.155: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-fkzw" in namespace "subpath-2331" to be "success or failure"
Jun 11 07:51:02.159: INFO: Pod "pod-subpath-test-configmap-fkzw": Phase="Pending", Reason="", readiness=false. Elapsed: 3.853636ms
Jun 11 07:51:04.163: INFO: Pod "pod-subpath-test-configmap-fkzw": Phase="Running", Reason="", readiness=true. Elapsed: 2.007983297s
Jun 11 07:51:06.168: INFO: Pod "pod-subpath-test-configmap-fkzw": Phase="Running", Reason="", readiness=true. Elapsed: 4.012538912s
Jun 11 07:51:08.172: INFO: Pod "pod-subpath-test-configmap-fkzw": Phase="Running", Reason="", readiness=true. Elapsed: 6.016635527s
Jun 11 07:51:10.177: INFO: Pod "pod-subpath-test-configmap-fkzw": Phase="Running", Reason="", readiness=true. Elapsed: 8.021466063s
Jun 11 07:51:12.181: INFO: Pod "pod-subpath-test-configmap-fkzw": Phase="Running", Reason="", readiness=true. Elapsed: 10.025945805s
Jun 11 07:51:14.186: INFO: Pod "pod-subpath-test-configmap-fkzw": Phase="Running", Reason="", readiness=true. Elapsed: 12.030556483s
Jun 11 07:51:16.192: INFO: Pod "pod-subpath-test-configmap-fkzw": Phase="Running", Reason="", readiness=true. Elapsed: 14.036786751s
Jun 11 07:51:18.197: INFO: Pod "pod-subpath-test-configmap-fkzw": Phase="Running", Reason="", readiness=true. Elapsed: 16.041190289s
Jun 11 07:51:20.201: INFO: Pod "pod-subpath-test-configmap-fkzw": Phase="Running", Reason="", readiness=true. Elapsed: 18.045994595s
Jun 11 07:51:22.206: INFO: Pod "pod-subpath-test-configmap-fkzw": Phase="Running", Reason="", readiness=true. Elapsed: 20.050891815s
Jun 11 07:51:24.211: INFO: Pod "pod-subpath-test-configmap-fkzw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.055722878s
STEP: Saw pod success
Jun 11 07:51:24.211: INFO: Pod "pod-subpath-test-configmap-fkzw" satisfied condition "success or failure"
Jun 11 07:51:24.215: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-subpath-test-configmap-fkzw container test-container-subpath-configmap-fkzw: <nil>
STEP: delete the pod
Jun 11 07:51:24.238: INFO: Waiting for pod pod-subpath-test-configmap-fkzw to disappear
Jun 11 07:51:24.244: INFO: Pod pod-subpath-test-configmap-fkzw no longer exists
STEP: Deleting pod pod-subpath-test-configmap-fkzw
Jun 11 07:51:24.244: INFO: Deleting pod "pod-subpath-test-configmap-fkzw" in namespace "subpath-2331"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:51:24.248: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "subpath-2331" for this suite.

• [SLOW TEST:22.276 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":280,"completed":37,"skipped":443,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl run default
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:51:24.262: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8113
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run default
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1489
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 11 07:51:24.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-8113'
Jun 11 07:51:24.510: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 11 07:51:24.510: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1495
Jun 11 07:51:26.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 delete deployment e2e-test-httpd-deployment --namespace=kubectl-8113'
Jun 11 07:51:26.656: INFO: stderr: ""
Jun 11 07:51:26.656: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:51:26.656: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "kubectl-8113" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run default should create an rc or deployment from an image  [Conformance]","total":280,"completed":38,"skipped":446,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:51:26.671: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-315
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating replication controller my-hostname-basic-3fd33403-6202-424a-939c-c29becfaa329
Jun 11 07:51:26.827: INFO: Pod name my-hostname-basic-3fd33403-6202-424a-939c-c29becfaa329: Found 0 pods out of 1
Jun 11 07:51:31.832: INFO: Pod name my-hostname-basic-3fd33403-6202-424a-939c-c29becfaa329: Found 1 pods out of 1
Jun 11 07:51:31.832: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-3fd33403-6202-424a-939c-c29becfaa329" are running
Jun 11 07:51:31.835: INFO: Pod "my-hostname-basic-3fd33403-6202-424a-939c-c29becfaa329-lfgmg" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-11 07:51:26 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-11 07:51:28 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-11 07:51:28 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-11 07:51:26 +0000 UTC Reason: Message:}])
Jun 11 07:51:31.836: INFO: Trying to dial the pod
Jun 11 07:51:36.851: INFO: Controller my-hostname-basic-3fd33403-6202-424a-939c-c29becfaa329: Got expected result from replica 1 [my-hostname-basic-3fd33403-6202-424a-939c-c29becfaa329-lfgmg]: "my-hostname-basic-3fd33403-6202-424a-939c-c29becfaa329-lfgmg", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:51:36.851: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "replication-controller-315" for this suite.

• [SLOW TEST:10.196 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":39,"skipped":462,"failed":0}
SSSSS
------------------------------
[k8s.io] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:51:36.868: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2577
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 07:51:55.038: INFO: Container started at 2020-06-11 07:51:37 +0000 UTC, pod became ready at 2020-06-11 07:51:53 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:51:55.038: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "container-probe-2577" for this suite.

• [SLOW TEST:18.185 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":280,"completed":40,"skipped":467,"failed":0}
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:51:55.053: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6990
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 11 07:51:55.212: INFO: Waiting up to 5m0s for pod "pod-d63a1ac6-8bb0-475c-a526-f18727763086" in namespace "emptydir-6990" to be "success or failure"
Jun 11 07:51:55.216: INFO: Pod "pod-d63a1ac6-8bb0-475c-a526-f18727763086": Phase="Pending", Reason="", readiness=false. Elapsed: 3.927588ms
Jun 11 07:51:57.220: INFO: Pod "pod-d63a1ac6-8bb0-475c-a526-f18727763086": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008341768s
STEP: Saw pod success
Jun 11 07:51:57.220: INFO: Pod "pod-d63a1ac6-8bb0-475c-a526-f18727763086" satisfied condition "success or failure"
Jun 11 07:51:57.224: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-d63a1ac6-8bb0-475c-a526-f18727763086 container test-container: <nil>
STEP: delete the pod
Jun 11 07:51:57.247: INFO: Waiting for pod pod-d63a1ac6-8bb0-475c-a526-f18727763086 to disappear
Jun 11 07:51:57.252: INFO: Pod pod-d63a1ac6-8bb0-475c-a526-f18727763086 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:51:57.252: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "emptydir-6990" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":41,"skipped":467,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:51:57.267: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4328
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-c79a60df-6860-4191-8d00-dbf0a671d8cf
STEP: Creating a pod to test consume secrets
Jun 11 07:51:57.432: INFO: Waiting up to 5m0s for pod "pod-secrets-11cb2765-9548-442b-b191-c88e5bdaa616" in namespace "secrets-4328" to be "success or failure"
Jun 11 07:51:57.436: INFO: Pod "pod-secrets-11cb2765-9548-442b-b191-c88e5bdaa616": Phase="Pending", Reason="", readiness=false. Elapsed: 3.81091ms
Jun 11 07:51:59.440: INFO: Pod "pod-secrets-11cb2765-9548-442b-b191-c88e5bdaa616": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007898575s
STEP: Saw pod success
Jun 11 07:51:59.440: INFO: Pod "pod-secrets-11cb2765-9548-442b-b191-c88e5bdaa616" satisfied condition "success or failure"
Jun 11 07:51:59.443: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-secrets-11cb2765-9548-442b-b191-c88e5bdaa616 container secret-env-test: <nil>
STEP: delete the pod
Jun 11 07:51:59.468: INFO: Waiting for pod pod-secrets-11cb2765-9548-442b-b191-c88e5bdaa616 to disappear
Jun 11 07:51:59.473: INFO: Pod pod-secrets-11cb2765-9548-442b-b191-c88e5bdaa616 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:51:59.473: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "secrets-4328" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":280,"completed":42,"skipped":471,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:51:59.488: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-926
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jun 11 07:52:09.729: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0611 07:52:09.729428      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:52:09.729: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "gc-926" for this suite.

• [SLOW TEST:10.255 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":280,"completed":43,"skipped":474,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:52:09.743: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3303
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-89cc5f93-44ee-4a70-a928-897633d80cd2
STEP: Creating a pod to test consume configMaps
Jun 11 07:52:09.906: INFO: Waiting up to 5m0s for pod "pod-configmaps-66f4668a-6a02-4590-b7b5-72624589e68e" in namespace "configmap-3303" to be "success or failure"
Jun 11 07:52:09.910: INFO: Pod "pod-configmaps-66f4668a-6a02-4590-b7b5-72624589e68e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.913531ms
Jun 11 07:52:11.914: INFO: Pod "pod-configmaps-66f4668a-6a02-4590-b7b5-72624589e68e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008168255s
STEP: Saw pod success
Jun 11 07:52:11.914: INFO: Pod "pod-configmaps-66f4668a-6a02-4590-b7b5-72624589e68e" satisfied condition "success or failure"
Jun 11 07:52:11.918: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-configmaps-66f4668a-6a02-4590-b7b5-72624589e68e container configmap-volume-test: <nil>
STEP: delete the pod
Jun 11 07:52:11.941: INFO: Waiting for pod pod-configmaps-66f4668a-6a02-4590-b7b5-72624589e68e to disappear
Jun 11 07:52:11.945: INFO: Pod pod-configmaps-66f4668a-6a02-4590-b7b5-72624589e68e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:52:11.945: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "configmap-3303" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":44,"skipped":478,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:52:11.959: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6472
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1681
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 11 07:52:12.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-6472'
Jun 11 07:52:12.204: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 11 07:52:12.204: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
Jun 11 07:52:12.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 delete jobs e2e-test-httpd-job --namespace=kubectl-6472'
Jun 11 07:52:12.351: INFO: stderr: ""
Jun 11 07:52:12.351: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:52:12.351: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "kubectl-6472" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run job should create a job from an image when restart is OnFailure  [Conformance]","total":280,"completed":45,"skipped":482,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:52:12.368: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-513
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 11 07:52:16.566: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 11 07:52:16.570: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 11 07:52:18.570: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 11 07:52:18.575: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 11 07:52:20.570: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 11 07:52:20.574: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 11 07:52:22.570: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 11 07:52:22.574: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 11 07:52:24.570: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 11 07:52:24.575: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 11 07:52:26.570: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 11 07:52:26.574: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 11 07:52:28.570: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 11 07:52:28.574: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:52:28.582: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-513" for this suite.

• [SLOW TEST:16.228 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":280,"completed":46,"skipped":489,"failed":0}
S
------------------------------
[sig-storage] HostPath
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:52:28.597: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-8641
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test hostPath mode
Jun 11 07:52:28.757: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-8641" to be "success or failure"
Jun 11 07:52:28.761: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048205ms
Jun 11 07:52:30.766: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008361323s
STEP: Saw pod success
Jun 11 07:52:30.766: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jun 11 07:52:30.770: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jun 11 07:52:30.792: INFO: Waiting for pod pod-host-path-test to disappear
Jun 11 07:52:30.799: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:52:30.800: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "hostpath-8641" for this suite.
•{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":47,"skipped":490,"failed":0}

------------------------------
[k8s.io] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:52:30.814: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4289
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-f603bddf-678d-4e5e-bd34-dbca9cae036d in namespace container-probe-4289
Jun 11 07:52:34.980: INFO: Started pod busybox-f603bddf-678d-4e5e-bd34-dbca9cae036d in namespace container-probe-4289
STEP: checking the pod's current state and verifying that restartCount is present
Jun 11 07:52:34.984: INFO: Initial restart count of pod busybox-f603bddf-678d-4e5e-bd34-dbca9cae036d is 0
Jun 11 07:53:25.099: INFO: Restart count of pod container-probe-4289/busybox-f603bddf-678d-4e5e-bd34-dbca9cae036d is now 1 (50.114797825s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:53:25.115: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "container-probe-4289" for this suite.

• [SLOW TEST:54.316 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":48,"skipped":490,"failed":0}
SSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:53:25.130: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4479
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-projected-all-test-volume-ccbfde88-4251-4e5f-be1c-c2cca198b469
STEP: Creating secret with name secret-projected-all-test-volume-50cb6f81-df6d-404e-9765-75bfe7fbfb3f
STEP: Creating a pod to test Check all projections for projected volume plugin
Jun 11 07:53:25.298: INFO: Waiting up to 5m0s for pod "projected-volume-ae872957-6374-4af1-a13f-0088c6d075e7" in namespace "projected-4479" to be "success or failure"
Jun 11 07:53:25.302: INFO: Pod "projected-volume-ae872957-6374-4af1-a13f-0088c6d075e7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.762941ms
Jun 11 07:53:27.306: INFO: Pod "projected-volume-ae872957-6374-4af1-a13f-0088c6d075e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007953715s
Jun 11 07:53:29.311: INFO: Pod "projected-volume-ae872957-6374-4af1-a13f-0088c6d075e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012225791s
STEP: Saw pod success
Jun 11 07:53:29.311: INFO: Pod "projected-volume-ae872957-6374-4af1-a13f-0088c6d075e7" satisfied condition "success or failure"
Jun 11 07:53:29.314: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod projected-volume-ae872957-6374-4af1-a13f-0088c6d075e7 container projected-all-volume-test: <nil>
STEP: delete the pod
Jun 11 07:53:29.337: INFO: Waiting for pod projected-volume-ae872957-6374-4af1-a13f-0088c6d075e7 to disappear
Jun 11 07:53:29.340: INFO: Pod projected-volume-ae872957-6374-4af1-a13f-0088c6d075e7 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:53:29.340: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "projected-4479" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":280,"completed":49,"skipped":493,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:53:29.356: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6645
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 07:53:29.822: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 07:53:32.847: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:53:32.924: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "webhook-6645" for this suite.
STEP: Destroying namespace "webhook-6645-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":280,"completed":50,"skipped":516,"failed":0}
SS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:53:33.023: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2747
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jun 11 07:53:33.187: INFO: Waiting up to 5m0s for pod "downward-api-e641bdf8-e64b-4dfb-8f97-32f0ca3b0849" in namespace "downward-api-2747" to be "success or failure"
Jun 11 07:53:33.193: INFO: Pod "downward-api-e641bdf8-e64b-4dfb-8f97-32f0ca3b0849": Phase="Pending", Reason="", readiness=false. Elapsed: 5.665731ms
Jun 11 07:53:35.197: INFO: Pod "downward-api-e641bdf8-e64b-4dfb-8f97-32f0ca3b0849": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00981839s
Jun 11 07:53:37.201: INFO: Pod "downward-api-e641bdf8-e64b-4dfb-8f97-32f0ca3b0849": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014000861s
STEP: Saw pod success
Jun 11 07:53:37.201: INFO: Pod "downward-api-e641bdf8-e64b-4dfb-8f97-32f0ca3b0849" satisfied condition "success or failure"
Jun 11 07:53:37.205: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod downward-api-e641bdf8-e64b-4dfb-8f97-32f0ca3b0849 container dapi-container: <nil>
STEP: delete the pod
Jun 11 07:53:37.228: INFO: Waiting for pod downward-api-e641bdf8-e64b-4dfb-8f97-32f0ca3b0849 to disappear
Jun 11 07:53:37.232: INFO: Pod downward-api-e641bdf8-e64b-4dfb-8f97-32f0ca3b0849 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:53:37.232: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "downward-api-2747" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":280,"completed":51,"skipped":518,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:53:37.248: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2139
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 11 07:53:37.406: INFO: Waiting up to 5m0s for pod "downwardapi-volume-422814ad-6b29-470a-b543-a50985ba5f00" in namespace "projected-2139" to be "success or failure"
Jun 11 07:53:37.410: INFO: Pod "downwardapi-volume-422814ad-6b29-470a-b543-a50985ba5f00": Phase="Pending", Reason="", readiness=false. Elapsed: 3.869188ms
Jun 11 07:53:39.415: INFO: Pod "downwardapi-volume-422814ad-6b29-470a-b543-a50985ba5f00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008397174s
STEP: Saw pod success
Jun 11 07:53:39.415: INFO: Pod "downwardapi-volume-422814ad-6b29-470a-b543-a50985ba5f00" satisfied condition "success or failure"
Jun 11 07:53:39.419: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod downwardapi-volume-422814ad-6b29-470a-b543-a50985ba5f00 container client-container: <nil>
STEP: delete the pod
Jun 11 07:53:39.442: INFO: Waiting for pod downwardapi-volume-422814ad-6b29-470a-b543-a50985ba5f00 to disappear
Jun 11 07:53:39.448: INFO: Pod downwardapi-volume-422814ad-6b29-470a-b543-a50985ba5f00 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:53:39.448: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "projected-2139" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":280,"completed":52,"skipped":523,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:53:39.463: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3250
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-8f691e5d-0ae8-4517-94ef-aa280ab4a3e0
STEP: Creating a pod to test consume secrets
Jun 11 07:53:39.625: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6c495f10-0f9e-434a-92d8-4893d03cb495" in namespace "projected-3250" to be "success or failure"
Jun 11 07:53:39.629: INFO: Pod "pod-projected-secrets-6c495f10-0f9e-434a-92d8-4893d03cb495": Phase="Pending", Reason="", readiness=false. Elapsed: 3.746855ms
Jun 11 07:53:41.634: INFO: Pod "pod-projected-secrets-6c495f10-0f9e-434a-92d8-4893d03cb495": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008162793s
STEP: Saw pod success
Jun 11 07:53:41.634: INFO: Pod "pod-projected-secrets-6c495f10-0f9e-434a-92d8-4893d03cb495" satisfied condition "success or failure"
Jun 11 07:53:41.637: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-projected-secrets-6c495f10-0f9e-434a-92d8-4893d03cb495 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 11 07:53:41.662: INFO: Waiting for pod pod-projected-secrets-6c495f10-0f9e-434a-92d8-4893d03cb495 to disappear
Jun 11 07:53:41.667: INFO: Pod pod-projected-secrets-6c495f10-0f9e-434a-92d8-4893d03cb495 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:53:41.667: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "projected-3250" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":53,"skipped":535,"failed":0}
S
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:53:41.681: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-4151
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:163
Jun 11 07:53:41.830: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 11 07:54:41.916: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 07:54:41.920: INFO: Starting informer...
STEP: Starting pod...
Jun 11 07:54:42.136: INFO: Pod is running on ip-10-0-136-38.us-west-2.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jun 11 07:54:42.150: INFO: Pod wasn't evicted. Proceeding
Jun 11 07:54:42.150: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jun 11 07:55:57.168: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:55:57.168: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "taint-single-pod-4151" for this suite.

• [SLOW TEST:135.506 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":280,"completed":54,"skipped":536,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:55:57.188: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8148
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 11 07:55:57.350: INFO: Waiting up to 5m0s for pod "pod-007de2f0-15bd-4e81-af83-28fb66f6f368" in namespace "emptydir-8148" to be "success or failure"
Jun 11 07:55:57.353: INFO: Pod "pod-007de2f0-15bd-4e81-af83-28fb66f6f368": Phase="Pending", Reason="", readiness=false. Elapsed: 3.578131ms
Jun 11 07:55:59.358: INFO: Pod "pod-007de2f0-15bd-4e81-af83-28fb66f6f368": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00800193s
STEP: Saw pod success
Jun 11 07:55:59.358: INFO: Pod "pod-007de2f0-15bd-4e81-af83-28fb66f6f368" satisfied condition "success or failure"
Jun 11 07:55:59.362: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-007de2f0-15bd-4e81-af83-28fb66f6f368 container test-container: <nil>
STEP: delete the pod
Jun 11 07:55:59.391: INFO: Waiting for pod pod-007de2f0-15bd-4e81-af83-28fb66f6f368 to disappear
Jun 11 07:55:59.397: INFO: Pod pod-007de2f0-15bd-4e81-af83-28fb66f6f368 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:55:59.397: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "emptydir-8148" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":55,"skipped":570,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:55:59.412: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7913
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 07:55:59.559: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 11 07:56:04.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 --namespace=crd-publish-openapi-7913 create -f -'
Jun 11 07:56:09.493: INFO: stderr: ""
Jun 11 07:56:09.494: INFO: stdout: "e2e-test-crd-publish-openapi-7142-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 11 07:56:09.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 --namespace=crd-publish-openapi-7913 delete e2e-test-crd-publish-openapi-7142-crds test-cr'
Jun 11 07:56:09.637: INFO: stderr: ""
Jun 11 07:56:09.637: INFO: stdout: "e2e-test-crd-publish-openapi-7142-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jun 11 07:56:09.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 --namespace=crd-publish-openapi-7913 apply -f -'
Jun 11 07:56:09.997: INFO: stderr: ""
Jun 11 07:56:09.997: INFO: stdout: "e2e-test-crd-publish-openapi-7142-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 11 07:56:09.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 --namespace=crd-publish-openapi-7913 delete e2e-test-crd-publish-openapi-7142-crds test-cr'
Jun 11 07:56:10.137: INFO: stderr: ""
Jun 11 07:56:10.137: INFO: stdout: "e2e-test-crd-publish-openapi-7142-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun 11 07:56:10.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 explain e2e-test-crd-publish-openapi-7142-crds'
Jun 11 07:56:10.482: INFO: stderr: ""
Jun 11 07:56:10.482: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7142-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:56:15.868: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7913" for this suite.

• [SLOW TEST:16.473 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":280,"completed":56,"skipped":588,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:56:15.885: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8054
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-6998053b-8e8e-4963-96f5-2471d6ddb21b
STEP: Creating a pod to test consume configMaps
Jun 11 07:56:16.051: INFO: Waiting up to 5m0s for pod "pod-configmaps-46b1170c-1975-43ec-bda2-a5567ae16d3a" in namespace "configmap-8054" to be "success or failure"
Jun 11 07:56:16.056: INFO: Pod "pod-configmaps-46b1170c-1975-43ec-bda2-a5567ae16d3a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.06219ms
Jun 11 07:56:18.060: INFO: Pod "pod-configmaps-46b1170c-1975-43ec-bda2-a5567ae16d3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009248666s
STEP: Saw pod success
Jun 11 07:56:18.060: INFO: Pod "pod-configmaps-46b1170c-1975-43ec-bda2-a5567ae16d3a" satisfied condition "success or failure"
Jun 11 07:56:18.064: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-configmaps-46b1170c-1975-43ec-bda2-a5567ae16d3a container configmap-volume-test: <nil>
STEP: delete the pod
Jun 11 07:56:18.091: INFO: Waiting for pod pod-configmaps-46b1170c-1975-43ec-bda2-a5567ae16d3a to disappear
Jun 11 07:56:18.097: INFO: Pod pod-configmaps-46b1170c-1975-43ec-bda2-a5567ae16d3a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:56:18.097: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "configmap-8054" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":57,"skipped":663,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:56:18.113: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9309
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jun 11 07:56:18.274: INFO: Created pod &Pod{ObjectMeta:{dns-9309  dns-9309 /api/v1/namespaces/dns-9309/pods/dns-9309 244263d1-c7d6-41e7-9572-2dcb2b7a132c 104448 0 2020-06-11 07:56:18 +0000 UTC <nil> <nil> map[] map[] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qvnj5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qvnj5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qvnj5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
STEP: Verifying customized DNS suffix list is configured on pod...
Jun 11 07:56:20.284: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9309 PodName:dns-9309 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 07:56:20.284: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Verifying customized DNS server is configured on pod...
Jun 11 07:56:20.352: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9309 PodName:dns-9309 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 07:56:20.352: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 07:56:20.424: INFO: Deleting pod dns-9309...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:56:20.441: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "dns-9309" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":280,"completed":58,"skipped":700,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:56:20.459: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3056
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 07:56:21.017: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 11 07:56:23.030: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458981, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458981, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458981, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727458981, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 07:56:26.047: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:56:26.127: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "webhook-3056" for this suite.
STEP: Destroying namespace "webhook-3056-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.759 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":280,"completed":59,"skipped":709,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:56:26.218: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-752
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-752 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-752;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-752 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-752;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-752.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-752.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-752.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-752.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-752.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-752.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-752.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-752.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-752.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-752.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-752.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-752.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-752.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 212.5.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.5.212_udp@PTR;check="$$(dig +tcp +noall +answer +search 212.5.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.5.212_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-752 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-752;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-752 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-752;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-752.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-752.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-752.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-752.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-752.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-752.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-752.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-752.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-752.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-752.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-752.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-752.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-752.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 212.5.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.5.212_udp@PTR;check="$$(dig +tcp +noall +answer +search 212.5.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.5.212_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 11 07:56:30.545: INFO: DNS probes using dns-752/dns-test-39b98f2a-9d5a-439f-9cc2-e85d0286d28c succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:56:30.613: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "dns-752" for this suite.
•{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":280,"completed":60,"skipped":731,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:56:30.630: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1447
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating all guestbook components
Jun 11 07:56:30.779: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Jun 11 07:56:30.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 create -f - --namespace=kubectl-1447'
Jun 11 07:56:31.107: INFO: stderr: ""
Jun 11 07:56:31.107: INFO: stdout: "service/agnhost-slave created\n"
Jun 11 07:56:31.107: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Jun 11 07:56:31.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 create -f - --namespace=kubectl-1447'
Jun 11 07:56:31.434: INFO: stderr: ""
Jun 11 07:56:31.434: INFO: stdout: "service/agnhost-master created\n"
Jun 11 07:56:31.434: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun 11 07:56:31.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 create -f - --namespace=kubectl-1447'
Jun 11 07:56:31.781: INFO: stderr: ""
Jun 11 07:56:31.781: INFO: stdout: "service/frontend created\n"
Jun 11 07:56:31.781: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jun 11 07:56:31.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 create -f - --namespace=kubectl-1447'
Jun 11 07:56:32.098: INFO: stderr: ""
Jun 11 07:56:32.098: INFO: stdout: "deployment.apps/frontend created\n"
Jun 11 07:56:32.098: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 11 07:56:32.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 create -f - --namespace=kubectl-1447'
Jun 11 07:56:32.422: INFO: stderr: ""
Jun 11 07:56:32.422: INFO: stdout: "deployment.apps/agnhost-master created\n"
Jun 11 07:56:32.422: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 11 07:56:32.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 create -f - --namespace=kubectl-1447'
Jun 11 07:56:32.761: INFO: stderr: ""
Jun 11 07:56:32.761: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Jun 11 07:56:32.761: INFO: Waiting for all frontend pods to be Running.
Jun 11 07:56:37.812: INFO: Waiting for frontend to serve content.
Jun 11 07:56:37.825: INFO: Trying to add a new entry to the guestbook.
Jun 11 07:56:37.838: INFO: Verifying that added entry can be retrieved.
Jun 11 07:56:37.851: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Jun 11 07:56:42.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 delete --grace-period=0 --force -f - --namespace=kubectl-1447'
Jun 11 07:56:42.988: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 11 07:56:42.988: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Jun 11 07:56:42.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 delete --grace-period=0 --force -f - --namespace=kubectl-1447'
Jun 11 07:56:43.110: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 11 07:56:43.110: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Jun 11 07:56:43.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 delete --grace-period=0 --force -f - --namespace=kubectl-1447'
Jun 11 07:56:43.243: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 11 07:56:43.243: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 11 07:56:43.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 delete --grace-period=0 --force -f - --namespace=kubectl-1447'
Jun 11 07:56:43.343: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 11 07:56:43.343: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 11 07:56:43.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 delete --grace-period=0 --force -f - --namespace=kubectl-1447'
Jun 11 07:56:43.445: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 11 07:56:43.445: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Jun 11 07:56:43.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 delete --grace-period=0 --force -f - --namespace=kubectl-1447'
Jun 11 07:56:43.542: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 11 07:56:43.542: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:56:43.542: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "kubectl-1447" for this suite.

• [SLOW TEST:12.928 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:380
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":280,"completed":61,"skipped":750,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:56:43.559: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5390
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jun 11 07:56:47.746: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5390 PodName:pod-sharedvolume-1626789a-8110-46ce-8200-ed7c377f2d27 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 07:56:47.746: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 07:56:47.807: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:56:47.807: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "emptydir-5390" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":280,"completed":62,"skipped":896,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:56:47.822: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4030
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 07:56:48.235: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 11 07:56:50.247: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727459008, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727459008, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727459008, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727459008, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 07:56:53.263: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:56:53.331: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "webhook-4030" for this suite.
STEP: Destroying namespace "webhook-4030-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.600 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":280,"completed":63,"skipped":905,"failed":0}
SS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:56:53.422: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4389
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4389
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-4389
I0611 07:56:53.640638      23 runners.go:189] Created replication controller with name: externalname-service, namespace: services-4389, replica count: 2
I0611 07:56:56.691191      23 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady
Jun 11 07:56:56.691: INFO: Creating new exec pod
Jun 11 07:56:59.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=services-4389 execpodnpwmv -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jun 11 07:56:59.891: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 11 07:56:59.891: INFO: stdout: ""
Jun 11 07:56:59.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=services-4389 execpodnpwmv -- /bin/sh -x -c nc -zv -t -w 2 10.0.45.152 80'
Jun 11 07:57:00.071: INFO: stderr: "+ nc -zv -t -w 2 10.0.45.152 80\nConnection to 10.0.45.152 80 port [tcp/http] succeeded!\n"
Jun 11 07:57:00.071: INFO: stdout: ""
Jun 11 07:57:00.071: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:57:00.099: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "services-4389" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:6.692 seconds]
[sig-network] Services
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":280,"completed":64,"skipped":907,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:57:00.115: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9513
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 11 07:57:00.275: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6da622e8-13e8-4f13-b55c-a7ce41bf5c90" in namespace "downward-api-9513" to be "success or failure"
Jun 11 07:57:00.279: INFO: Pod "downwardapi-volume-6da622e8-13e8-4f13-b55c-a7ce41bf5c90": Phase="Pending", Reason="", readiness=false. Elapsed: 3.986422ms
Jun 11 07:57:02.283: INFO: Pod "downwardapi-volume-6da622e8-13e8-4f13-b55c-a7ce41bf5c90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008108394s
STEP: Saw pod success
Jun 11 07:57:02.284: INFO: Pod "downwardapi-volume-6da622e8-13e8-4f13-b55c-a7ce41bf5c90" satisfied condition "success or failure"
Jun 11 07:57:02.287: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod downwardapi-volume-6da622e8-13e8-4f13-b55c-a7ce41bf5c90 container client-container: <nil>
STEP: delete the pod
Jun 11 07:57:02.310: INFO: Waiting for pod downwardapi-volume-6da622e8-13e8-4f13-b55c-a7ce41bf5c90 to disappear
Jun 11 07:57:02.316: INFO: Pod downwardapi-volume-6da622e8-13e8-4f13-b55c-a7ce41bf5c90 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:57:02.316: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "downward-api-9513" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":65,"skipped":909,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:57:02.331: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1751
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on tmpfs
Jun 11 07:57:02.489: INFO: Waiting up to 5m0s for pod "pod-1872d462-a222-4352-b188-d614a55ec2c3" in namespace "emptydir-1751" to be "success or failure"
Jun 11 07:57:02.493: INFO: Pod "pod-1872d462-a222-4352-b188-d614a55ec2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.368623ms
Jun 11 07:57:04.498: INFO: Pod "pod-1872d462-a222-4352-b188-d614a55ec2c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008925644s
STEP: Saw pod success
Jun 11 07:57:04.498: INFO: Pod "pod-1872d462-a222-4352-b188-d614a55ec2c3" satisfied condition "success or failure"
Jun 11 07:57:04.502: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-1872d462-a222-4352-b188-d614a55ec2c3 container test-container: <nil>
STEP: delete the pod
Jun 11 07:57:04.526: INFO: Waiting for pod pod-1872d462-a222-4352-b188-d614a55ec2c3 to disappear
Jun 11 07:57:04.532: INFO: Pod pod-1872d462-a222-4352-b188-d614a55ec2c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:57:04.532: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "emptydir-1751" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":66,"skipped":931,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:57:04.549: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-885
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jun 11 07:57:04.696: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:57:09.113: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "init-container-885" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":280,"completed":67,"skipped":1000,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:57:09.128: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4792
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Jun 11 07:57:19.362: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0611 07:57:19.362443      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:57:19.362: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "gc-4792" for this suite.

• [SLOW TEST:10.250 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":280,"completed":68,"skipped":1001,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:57:19.378: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-5036
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun 11 07:57:20.396: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jun 11 07:57:22.409: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727459040, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727459040, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727459040, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727459040, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 07:57:25.425: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 07:57:25.429: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:57:26.631: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "crd-webhook-5036" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

• [SLOW TEST:7.347 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":280,"completed":69,"skipped":1015,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:57:26.726: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7963
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-8vf7
STEP: Creating a pod to test atomic-volume-subpath
Jun 11 07:57:26.937: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8vf7" in namespace "subpath-7963" to be "success or failure"
Jun 11 07:57:26.941: INFO: Pod "pod-subpath-test-configmap-8vf7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.250806ms
Jun 11 07:57:28.945: INFO: Pod "pod-subpath-test-configmap-8vf7": Phase="Running", Reason="", readiness=true. Elapsed: 2.008689246s
Jun 11 07:57:30.951: INFO: Pod "pod-subpath-test-configmap-8vf7": Phase="Running", Reason="", readiness=true. Elapsed: 4.013959002s
Jun 11 07:57:32.955: INFO: Pod "pod-subpath-test-configmap-8vf7": Phase="Running", Reason="", readiness=true. Elapsed: 6.018472674s
Jun 11 07:57:34.959: INFO: Pod "pod-subpath-test-configmap-8vf7": Phase="Running", Reason="", readiness=true. Elapsed: 8.022600045s
Jun 11 07:57:36.964: INFO: Pod "pod-subpath-test-configmap-8vf7": Phase="Running", Reason="", readiness=true. Elapsed: 10.027098671s
Jun 11 07:57:38.968: INFO: Pod "pod-subpath-test-configmap-8vf7": Phase="Running", Reason="", readiness=true. Elapsed: 12.03161376s
Jun 11 07:57:40.973: INFO: Pod "pod-subpath-test-configmap-8vf7": Phase="Running", Reason="", readiness=true. Elapsed: 14.036400517s
Jun 11 07:57:42.978: INFO: Pod "pod-subpath-test-configmap-8vf7": Phase="Running", Reason="", readiness=true. Elapsed: 16.040964532s
Jun 11 07:57:44.982: INFO: Pod "pod-subpath-test-configmap-8vf7": Phase="Running", Reason="", readiness=true. Elapsed: 18.045555088s
Jun 11 07:57:46.987: INFO: Pod "pod-subpath-test-configmap-8vf7": Phase="Running", Reason="", readiness=true. Elapsed: 20.050262158s
Jun 11 07:57:48.991: INFO: Pod "pod-subpath-test-configmap-8vf7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.054396294s
STEP: Saw pod success
Jun 11 07:57:48.991: INFO: Pod "pod-subpath-test-configmap-8vf7" satisfied condition "success or failure"
Jun 11 07:57:48.995: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-subpath-test-configmap-8vf7 container test-container-subpath-configmap-8vf7: <nil>
STEP: delete the pod
Jun 11 07:57:49.019: INFO: Waiting for pod pod-subpath-test-configmap-8vf7 to disappear
Jun 11 07:57:49.025: INFO: Pod pod-subpath-test-configmap-8vf7 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-8vf7
Jun 11 07:57:49.025: INFO: Deleting pod "pod-subpath-test-configmap-8vf7" in namespace "subpath-7963"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:57:49.028: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "subpath-7963" for this suite.

• [SLOW TEST:22.317 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":280,"completed":70,"skipped":1027,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:57:49.044: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1577
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 11 07:57:52.225: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:57:52.252: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "container-runtime-1577" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":71,"skipped":1047,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:57:52.268: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9306
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 07:57:52.863: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 11 07:57:54.875: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727459072, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727459072, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727459072, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727459072, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 07:57:57.892: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:57:58.105: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "webhook-9306" for this suite.
STEP: Destroying namespace "webhook-9306-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.923 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":280,"completed":72,"skipped":1101,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:57:58.191: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1330
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:178
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:57:58.359: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "pods-1330" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":280,"completed":73,"skipped":1112,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:57:58.379: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-1104
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 07:57:58.538: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-54831117-872c-430c-889f-a7e7308e8dee" in namespace "security-context-test-1104" to be "success or failure"
Jun 11 07:57:58.543: INFO: Pod "busybox-privileged-false-54831117-872c-430c-889f-a7e7308e8dee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.970495ms
Jun 11 07:58:00.547: INFO: Pod "busybox-privileged-false-54831117-872c-430c-889f-a7e7308e8dee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009121552s
Jun 11 07:58:02.552: INFO: Pod "busybox-privileged-false-54831117-872c-430c-889f-a7e7308e8dee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013751795s
Jun 11 07:58:02.552: INFO: Pod "busybox-privileged-false-54831117-872c-430c-889f-a7e7308e8dee" satisfied condition "success or failure"
Jun 11 07:58:02.560: INFO: Got logs for pod "busybox-privileged-false-54831117-872c-430c-889f-a7e7308e8dee": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:58:02.560: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "security-context-test-1104" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":74,"skipped":1121,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:58:02.575: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4898
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1275
STEP: creating the pod
Jun 11 07:58:02.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 create -f - --namespace=kubectl-4898'
Jun 11 07:58:03.064: INFO: stderr: ""
Jun 11 07:58:03.064: INFO: stdout: "pod/pause created\n"
Jun 11 07:58:03.064: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun 11 07:58:03.064: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4898" to be "running and ready"
Jun 11 07:58:03.069: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.581443ms
Jun 11 07:58:05.074: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.010246305s
Jun 11 07:58:05.074: INFO: Pod "pause" satisfied condition "running and ready"
Jun 11 07:58:05.074: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: adding the label testing-label with value testing-label-value to a pod
Jun 11 07:58:05.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 label pods pause testing-label=testing-label-value --namespace=kubectl-4898'
Jun 11 07:58:05.215: INFO: stderr: ""
Jun 11 07:58:05.215: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jun 11 07:58:05.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pod pause -L testing-label --namespace=kubectl-4898'
Jun 11 07:58:05.347: INFO: stderr: ""
Jun 11 07:58:05.347: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jun 11 07:58:05.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 label pods pause testing-label- --namespace=kubectl-4898'
Jun 11 07:58:05.484: INFO: stderr: ""
Jun 11 07:58:05.484: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jun 11 07:58:05.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pod pause -L testing-label --namespace=kubectl-4898'
Jun 11 07:58:05.616: INFO: stderr: ""
Jun 11 07:58:05.616: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1282
STEP: using delete to clean up resources
Jun 11 07:58:05.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 delete --grace-period=0 --force -f - --namespace=kubectl-4898'
Jun 11 07:58:05.717: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 11 07:58:05.717: INFO: stdout: "pod \"pause\" force deleted\n"
Jun 11 07:58:05.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get rc,svc -l name=pause --no-headers --namespace=kubectl-4898'
Jun 11 07:58:05.888: INFO: stderr: "No resources found in kubectl-4898 namespace.\n"
Jun 11 07:58:05.888: INFO: stdout: ""
Jun 11 07:58:05.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods -l name=pause --namespace=kubectl-4898 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 11 07:58:06.025: INFO: stderr: ""
Jun 11 07:58:06.025: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:58:06.025: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "kubectl-4898" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":280,"completed":75,"skipped":1138,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:58:06.043: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3826
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:58:17.237: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "resourcequota-3826" for this suite.

• [SLOW TEST:11.210 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":280,"completed":76,"skipped":1171,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:58:17.253: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-881
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1585
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 11 07:58:17.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-881'
Jun 11 07:58:17.499: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 11 07:58:17.499: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Jun 11 07:58:17.505: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Jun 11 07:58:17.509: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jun 11 07:58:17.518: INFO: scanned /root for discovery docs: <nil>
Jun 11 07:58:17.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-881'
Jun 11 07:58:28.516: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jun 11 07:58:28.516: INFO: stdout: "Created e2e-test-httpd-rc-6b481b94f994b190ff29492a7bbf844b\nScaling up e2e-test-httpd-rc-6b481b94f994b190ff29492a7bbf844b from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-6b481b94f994b190ff29492a7bbf844b up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-6b481b94f994b190ff29492a7bbf844b to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Jun 11 07:58:28.516: INFO: stdout: "Created e2e-test-httpd-rc-6b481b94f994b190ff29492a7bbf844b\nScaling up e2e-test-httpd-rc-6b481b94f994b190ff29492a7bbf844b from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-6b481b94f994b190ff29492a7bbf844b up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-6b481b94f994b190ff29492a7bbf844b to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Jun 11 07:58:28.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-881'
Jun 11 07:58:28.648: INFO: stderr: ""
Jun 11 07:58:28.648: INFO: stdout: "e2e-test-httpd-rc-6b481b94f994b190ff29492a7bbf844b-tmwj2 "
Jun 11 07:58:28.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods e2e-test-httpd-rc-6b481b94f994b190ff29492a7bbf844b-tmwj2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-881'
Jun 11 07:58:28.793: INFO: stderr: ""
Jun 11 07:58:28.793: INFO: stdout: "true"
Jun 11 07:58:28.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods e2e-test-httpd-rc-6b481b94f994b190ff29492a7bbf844b-tmwj2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-881'
Jun 11 07:58:28.923: INFO: stderr: ""
Jun 11 07:58:28.923: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Jun 11 07:58:28.923: INFO: e2e-test-httpd-rc-6b481b94f994b190ff29492a7bbf844b-tmwj2 is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
Jun 11 07:58:28.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 delete rc e2e-test-httpd-rc --namespace=kubectl-881'
Jun 11 07:58:29.067: INFO: stderr: ""
Jun 11 07:58:29.067: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:58:29.067: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "kubectl-881" for this suite.

• [SLOW TEST:11.832 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1580
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl rolling-update should support rolling-update to same image  [Conformance]","total":280,"completed":77,"skipped":1197,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:58:29.086: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4910
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 07:58:29.244: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jun 11 07:58:34.248: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 11 07:58:34.248: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jun 11 07:58:34.271: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4910 /apis/apps/v1/namespaces/deployment-4910/deployments/test-cleanup-deployment 84de3cba-b634-4ae0-bf51-9e22441d7340 106902 1 2020-06-11 07:58:34 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc008b0a988 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jun 11 07:58:34.275: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jun 11 07:58:34.275: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jun 11 07:58:34.275: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4910 /apis/apps/v1/namespaces/deployment-4910/replicasets/test-cleanup-controller 48e184e7-a980-402a-a96f-82d6b541a8be 106903 1 2020-06-11 07:58:29 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 84de3cba-b634-4ae0-bf51-9e22441d7340 0xc008672327 0xc008672328}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc008672388 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 11 07:58:34.281: INFO: Pod "test-cleanup-controller-p7jwl" is available:
&Pod{ObjectMeta:{test-cleanup-controller-p7jwl test-cleanup-controller- deployment-4910 /api/v1/namespaces/deployment-4910/pods/test-cleanup-controller-p7jwl 06749e91-b2c1-43c6-92b6-4802bc343b9d 106873 0 2020-06-11 07:58:29 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:192.168.42.57/32 cni.projectcalico.org/podIPs:192.168.42.57/32] [{apps/v1 ReplicaSet test-cleanup-controller 48e184e7-a980-402a-a96f-82d6b541a8be 0xc008c7ae57 0xc008c7ae58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-9525m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-9525m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-9525m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-136-38.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:58:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:58:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:58:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 07:58:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.136.38,PodIP:192.168.42.57,StartTime:2020-06-11 07:58:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-11 07:58:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://3ea7fa39f3ccc38943fdbe8bf1cb8beebc39305bf9354483d98bcd056918c0a7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.42.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:58:34.281: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "deployment-4910" for this suite.

• [SLOW TEST:5.213 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":280,"completed":78,"skipped":1227,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:58:34.299: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7837
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 11 07:58:34.462: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8a552e32-c0d6-4114-8254-ee483e08e90b" in namespace "downward-api-7837" to be "success or failure"
Jun 11 07:58:34.466: INFO: Pod "downwardapi-volume-8a552e32-c0d6-4114-8254-ee483e08e90b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.946498ms
Jun 11 07:58:36.477: INFO: Pod "downwardapi-volume-8a552e32-c0d6-4114-8254-ee483e08e90b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015473876s
STEP: Saw pod success
Jun 11 07:58:36.477: INFO: Pod "downwardapi-volume-8a552e32-c0d6-4114-8254-ee483e08e90b" satisfied condition "success or failure"
Jun 11 07:58:36.483: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod downwardapi-volume-8a552e32-c0d6-4114-8254-ee483e08e90b container client-container: <nil>
STEP: delete the pod
Jun 11 07:58:36.506: INFO: Waiting for pod downwardapi-volume-8a552e32-c0d6-4114-8254-ee483e08e90b to disappear
Jun 11 07:58:36.513: INFO: Pod downwardapi-volume-8a552e32-c0d6-4114-8254-ee483e08e90b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:58:36.513: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "downward-api-7837" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":280,"completed":79,"skipped":1257,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:58:36.528: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2272
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 11 07:58:36.691: INFO: Waiting up to 5m0s for pod "pod-e8c59660-fd70-450e-9f5e-a662601fda04" in namespace "emptydir-2272" to be "success or failure"
Jun 11 07:58:36.695: INFO: Pod "pod-e8c59660-fd70-450e-9f5e-a662601fda04": Phase="Pending", Reason="", readiness=false. Elapsed: 3.947578ms
Jun 11 07:58:38.700: INFO: Pod "pod-e8c59660-fd70-450e-9f5e-a662601fda04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008370162s
STEP: Saw pod success
Jun 11 07:58:38.700: INFO: Pod "pod-e8c59660-fd70-450e-9f5e-a662601fda04" satisfied condition "success or failure"
Jun 11 07:58:38.704: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-e8c59660-fd70-450e-9f5e-a662601fda04 container test-container: <nil>
STEP: delete the pod
Jun 11 07:58:38.727: INFO: Waiting for pod pod-e8c59660-fd70-450e-9f5e-a662601fda04 to disappear
Jun 11 07:58:38.733: INFO: Pod pod-e8c59660-fd70-450e-9f5e-a662601fda04 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:58:38.733: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "emptydir-2272" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":80,"skipped":1281,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:58:38.748: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4956
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-4c67cca1-5970-4e32-affd-ba5bf68b48b1
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-4c67cca1-5970-4e32-affd-ba5bf68b48b1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:58:42.964: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "configmap-4956" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":81,"skipped":1287,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:58:42.980: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5177
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 07:58:43.131: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 11 07:58:48.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 --namespace=crd-publish-openapi-5177 create -f -'
Jun 11 07:58:48.598: INFO: stderr: ""
Jun 11 07:58:48.598: INFO: stdout: "e2e-test-crd-publish-openapi-9104-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 11 07:58:48.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 --namespace=crd-publish-openapi-5177 delete e2e-test-crd-publish-openapi-9104-crds test-cr'
Jun 11 07:58:48.774: INFO: stderr: ""
Jun 11 07:58:48.774: INFO: stdout: "e2e-test-crd-publish-openapi-9104-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jun 11 07:58:48.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 --namespace=crd-publish-openapi-5177 apply -f -'
Jun 11 07:58:48.993: INFO: stderr: ""
Jun 11 07:58:48.993: INFO: stdout: "e2e-test-crd-publish-openapi-9104-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 11 07:58:48.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 --namespace=crd-publish-openapi-5177 delete e2e-test-crd-publish-openapi-9104-crds test-cr'
Jun 11 07:58:49.138: INFO: stderr: ""
Jun 11 07:58:49.138: INFO: stdout: "e2e-test-crd-publish-openapi-9104-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun 11 07:58:49.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 explain e2e-test-crd-publish-openapi-9104-crds'
Jun 11 07:58:49.374: INFO: stderr: ""
Jun 11 07:58:49.374: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9104-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:58:54.246: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5177" for this suite.

• [SLOW TEST:11.282 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":280,"completed":82,"skipped":1287,"failed":0}
SSS
------------------------------
[sig-network] Proxy version v1
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:58:54.261: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1390
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 07:58:54.433: INFO: (0) /api/v1/nodes/ip-10-0-129-105.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 16.36857ms)
Jun 11 07:58:54.438: INFO: (1) /api/v1/nodes/ip-10-0-129-105.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 5.520474ms)
Jun 11 07:58:54.445: INFO: (2) /api/v1/nodes/ip-10-0-129-105.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 6.145093ms)
Jun 11 07:58:54.450: INFO: (3) /api/v1/nodes/ip-10-0-129-105.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 5.479998ms)
Jun 11 07:58:54.455: INFO: (4) /api/v1/nodes/ip-10-0-129-105.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 5.414873ms)
Jun 11 07:58:54.461: INFO: (5) /api/v1/nodes/ip-10-0-129-105.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 5.303499ms)
Jun 11 07:58:54.466: INFO: (6) /api/v1/nodes/ip-10-0-129-105.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 5.290645ms)
Jun 11 07:58:54.471: INFO: (7) /api/v1/nodes/ip-10-0-129-105.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 5.166044ms)
Jun 11 07:58:54.477: INFO: (8) /api/v1/nodes/ip-10-0-129-105.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 5.291419ms)
Jun 11 07:58:54.482: INFO: (9) /api/v1/nodes/ip-10-0-129-105.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 5.351029ms)
Jun 11 07:58:54.487: INFO: (10) /api/v1/nodes/ip-10-0-129-105.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 5.254515ms)
Jun 11 07:58:54.493: INFO: (11) /api/v1/nodes/ip-10-0-129-105.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 5.25874ms)
Jun 11 07:58:54.498: INFO: (12) /api/v1/nodes/ip-10-0-129-105.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 5.284331ms)
Jun 11 07:58:54.503: INFO: (13) /api/v1/nodes/ip-10-0-129-105.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 5.268288ms)
Jun 11 07:58:54.508: INFO: (14) /api/v1/nodes/ip-10-0-129-105.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 5.18288ms)
Jun 11 07:58:54.514: INFO: (15) /api/v1/nodes/ip-10-0-129-105.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 5.246251ms)
Jun 11 07:58:54.519: INFO: (16) /api/v1/nodes/ip-10-0-129-105.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 5.057473ms)
Jun 11 07:58:54.524: INFO: (17) /api/v1/nodes/ip-10-0-129-105.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 5.245625ms)
Jun 11 07:58:54.530: INFO: (18) /api/v1/nodes/ip-10-0-129-105.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 5.549532ms)
Jun 11 07:58:54.535: INFO: (19) /api/v1/nodes/ip-10-0-129-105.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 5.201522ms)
[AfterEach] version v1
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:58:54.535: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "proxy-1390" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":280,"completed":83,"skipped":1290,"failed":0}
SSSS
------------------------------
[k8s.io] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:58:54.550: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8969
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 07:58:56.747: INFO: Waiting up to 5m0s for pod "client-envvars-05e476c9-6a0f-43ed-85e2-b784cf475ba9" in namespace "pods-8969" to be "success or failure"
Jun 11 07:58:56.751: INFO: Pod "client-envvars-05e476c9-6a0f-43ed-85e2-b784cf475ba9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.980475ms
Jun 11 07:58:58.756: INFO: Pod "client-envvars-05e476c9-6a0f-43ed-85e2-b784cf475ba9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008416691s
STEP: Saw pod success
Jun 11 07:58:58.756: INFO: Pod "client-envvars-05e476c9-6a0f-43ed-85e2-b784cf475ba9" satisfied condition "success or failure"
Jun 11 07:58:58.760: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod client-envvars-05e476c9-6a0f-43ed-85e2-b784cf475ba9 container env3cont: <nil>
STEP: delete the pod
Jun 11 07:58:58.786: INFO: Waiting for pod client-envvars-05e476c9-6a0f-43ed-85e2-b784cf475ba9 to disappear
Jun 11 07:58:58.792: INFO: Pod client-envvars-05e476c9-6a0f-43ed-85e2-b784cf475ba9 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:58:58.792: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "pods-8969" for this suite.
•{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":280,"completed":84,"skipped":1294,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:58:58.807: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6173
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Jun 11 07:59:29.500: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0611 07:59:29.500565      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:59:29.500: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "gc-6173" for this suite.

• [SLOW TEST:30.709 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":280,"completed":85,"skipped":1305,"failed":0}
SSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:59:29.515: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-4551
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
Jun 11 07:59:30.194: INFO: created pod pod-service-account-defaultsa
Jun 11 07:59:30.194: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jun 11 07:59:30.201: INFO: created pod pod-service-account-mountsa
Jun 11 07:59:30.201: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jun 11 07:59:30.208: INFO: created pod pod-service-account-nomountsa
Jun 11 07:59:30.208: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jun 11 07:59:30.217: INFO: created pod pod-service-account-defaultsa-mountspec
Jun 11 07:59:30.217: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jun 11 07:59:30.225: INFO: created pod pod-service-account-mountsa-mountspec
Jun 11 07:59:30.225: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jun 11 07:59:30.234: INFO: created pod pod-service-account-nomountsa-mountspec
Jun 11 07:59:30.234: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jun 11 07:59:30.242: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun 11 07:59:30.242: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jun 11 07:59:30.250: INFO: created pod pod-service-account-mountsa-nomountspec
Jun 11 07:59:30.250: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jun 11 07:59:30.261: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun 11 07:59:30.261: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 07:59:30.261: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "svcaccounts-4551" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":280,"completed":86,"skipped":1311,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 07:59:30.280: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2690
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 07:59:30.501: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jun 11 07:59:30.516: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:30.516: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:30.516: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:30.516: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:30.516: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:30.516: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:30.516: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:30.516: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:30.516: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:30.516: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:30.521: INFO: Number of nodes with available pods: 0
Jun 11 07:59:30.521: INFO: Node ip-10-0-128-119.us-west-2.compute.internal is running more than one daemon pod
Jun 11 07:59:31.529: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:31.529: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:31.529: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:31.529: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:31.529: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:31.529: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:31.529: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:31.529: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:31.529: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:31.529: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:31.534: INFO: Number of nodes with available pods: 0
Jun 11 07:59:31.534: INFO: Node ip-10-0-128-119.us-west-2.compute.internal is running more than one daemon pod
Jun 11 07:59:32.533: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:32.533: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:32.533: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:32.533: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:32.533: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:32.533: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:32.533: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:32.533: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:32.533: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:32.534: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:32.539: INFO: Number of nodes with available pods: 6
Jun 11 07:59:32.539: INFO: Node ip-10-0-129-105.us-west-2.compute.internal is running more than one daemon pod
Jun 11 07:59:33.528: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:33.528: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:33.528: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:33.529: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:33.529: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:33.529: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:33.529: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:33.529: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:33.529: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:33.529: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:33.534: INFO: Number of nodes with available pods: 9
Jun 11 07:59:33.534: INFO: Node ip-10-0-136-38.us-west-2.compute.internal is running more than one daemon pod
Jun 11 07:59:34.528: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:34.528: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:34.528: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:34.529: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:34.529: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:34.529: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:34.529: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:34.529: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:34.529: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:34.529: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:34.534: INFO: Number of nodes with available pods: 9
Jun 11 07:59:34.534: INFO: Node ip-10-0-136-38.us-west-2.compute.internal is running more than one daemon pod
Jun 11 07:59:35.529: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:35.529: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:35.529: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:35.529: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:35.529: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:35.529: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:35.529: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:35.529: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:35.529: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:35.529: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:35.535: INFO: Number of nodes with available pods: 10
Jun 11 07:59:35.535: INFO: Number of running nodes: 10, number of available pods: 10
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jun 11 07:59:35.572: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:35.572: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:35.572: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:35.572: INFO: Wrong image for pod: daemon-set-mfpsg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:35.572: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:35.572: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:35.572: INFO: Wrong image for pod: daemon-set-stjfb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:35.572: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:35.572: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:35.572: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:35.580: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:35.580: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:35.580: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:35.580: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:35.580: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:35.580: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:35.580: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:35.580: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:35.580: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:35.580: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:36.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:36.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:36.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:36.586: INFO: Wrong image for pod: daemon-set-mfpsg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:36.586: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:36.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:36.586: INFO: Wrong image for pod: daemon-set-stjfb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:36.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:36.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:36.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:36.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:36.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:36.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:36.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:36.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:36.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:36.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:36.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:36.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:36.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:37.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:37.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:37.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:37.586: INFO: Wrong image for pod: daemon-set-mfpsg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:37.586: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:37.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:37.586: INFO: Wrong image for pod: daemon-set-stjfb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:37.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:37.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:37.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:37.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:37.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:37.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:37.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:37.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:37.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:37.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:37.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:37.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:37.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:38.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:38.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:38.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:38.586: INFO: Wrong image for pod: daemon-set-mfpsg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:38.586: INFO: Pod daemon-set-mfpsg is not available
Jun 11 07:59:38.586: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:38.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:38.586: INFO: Wrong image for pod: daemon-set-stjfb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:38.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:38.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:38.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:38.594: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:38.594: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:38.594: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:38.594: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:38.594: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:38.594: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:38.594: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:38.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:38.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:38.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:39.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:39.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:39.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:39.586: INFO: Wrong image for pod: daemon-set-mfpsg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:39.586: INFO: Pod daemon-set-mfpsg is not available
Jun 11 07:59:39.586: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:39.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:39.586: INFO: Wrong image for pod: daemon-set-stjfb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:39.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:39.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:39.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:39.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:39.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:39.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:39.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:39.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:39.594: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:39.594: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:39.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:39.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:39.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:40.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:40.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:40.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:40.586: INFO: Wrong image for pod: daemon-set-mfpsg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:40.586: INFO: Pod daemon-set-mfpsg is not available
Jun 11 07:59:40.586: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:40.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:40.586: INFO: Wrong image for pod: daemon-set-stjfb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:40.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:40.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:40.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:40.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:40.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:40.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:40.594: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:40.594: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:40.594: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:40.594: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:40.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:40.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:40.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:41.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:41.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:41.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:41.586: INFO: Wrong image for pod: daemon-set-mfpsg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:41.586: INFO: Pod daemon-set-mfpsg is not available
Jun 11 07:59:41.586: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:41.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:41.586: INFO: Wrong image for pod: daemon-set-stjfb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:41.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:41.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:41.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:41.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:41.594: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:41.594: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:41.594: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:41.594: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:41.594: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:41.594: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:41.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:41.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:41.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:42.585: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:42.585: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:42.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:42.586: INFO: Wrong image for pod: daemon-set-mfpsg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:42.586: INFO: Pod daemon-set-mfpsg is not available
Jun 11 07:59:42.586: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:42.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:42.586: INFO: Wrong image for pod: daemon-set-stjfb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:42.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:42.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:42.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:42.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:42.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:42.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:42.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:42.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:42.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:42.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:42.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:42.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:42.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:43.585: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:43.585: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:43.585: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:43.585: INFO: Wrong image for pod: daemon-set-mfpsg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:43.585: INFO: Pod daemon-set-mfpsg is not available
Jun 11 07:59:43.585: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:43.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:43.585: INFO: Wrong image for pod: daemon-set-stjfb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:43.585: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:43.585: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:43.585: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:43.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:43.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:43.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:43.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:43.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:43.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:43.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:43.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:43.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:43.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:44.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:44.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:44.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:44.586: INFO: Wrong image for pod: daemon-set-mfpsg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:44.586: INFO: Pod daemon-set-mfpsg is not available
Jun 11 07:59:44.586: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:44.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:44.586: INFO: Wrong image for pod: daemon-set-stjfb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:44.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:44.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:44.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:44.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:44.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:44.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:44.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:44.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:44.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:44.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:44.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:44.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:44.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:45.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:45.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:45.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:45.586: INFO: Wrong image for pod: daemon-set-mfpsg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:45.586: INFO: Pod daemon-set-mfpsg is not available
Jun 11 07:59:45.586: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:45.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:45.586: INFO: Wrong image for pod: daemon-set-stjfb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:45.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:45.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:45.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:45.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:45.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:45.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:45.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:45.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:45.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:45.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:45.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:45.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:45.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:46.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:46.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:46.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:46.586: INFO: Wrong image for pod: daemon-set-mfpsg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:46.586: INFO: Pod daemon-set-mfpsg is not available
Jun 11 07:59:46.586: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:46.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:46.586: INFO: Wrong image for pod: daemon-set-stjfb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:46.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:46.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:46.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:46.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:46.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:46.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:46.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:46.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:46.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:46.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:46.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:46.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:46.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:47.585: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:47.585: INFO: Pod daemon-set-hptbf is not available
Jun 11 07:59:47.585: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:47.585: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:47.585: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:47.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:47.585: INFO: Wrong image for pod: daemon-set-stjfb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:47.585: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:47.585: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:47.585: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:47.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:47.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:47.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:47.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:47.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:47.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:47.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:47.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:47.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:47.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:48.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:48.586: INFO: Pod daemon-set-hptbf is not available
Jun 11 07:59:48.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:48.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:48.586: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:48.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:48.586: INFO: Wrong image for pod: daemon-set-stjfb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:48.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:48.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:48.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:48.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:48.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:48.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:48.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:48.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:48.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:48.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:48.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:48.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:48.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:49.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:49.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:49.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:49.586: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:49.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:49.586: INFO: Wrong image for pod: daemon-set-stjfb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:49.586: INFO: Pod daemon-set-stjfb is not available
Jun 11 07:59:49.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:49.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:49.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:49.594: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:49.594: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:49.594: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:49.594: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:49.594: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:49.594: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:49.594: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:49.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:49.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:49.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:50.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:50.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:50.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:50.586: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:50.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:50.586: INFO: Wrong image for pod: daemon-set-stjfb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:50.586: INFO: Pod daemon-set-stjfb is not available
Jun 11 07:59:50.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:50.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:50.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:50.594: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:50.594: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:50.594: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:50.594: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:50.594: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:50.594: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:50.594: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:50.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:50.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:50.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:51.585: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:51.585: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:51.585: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:51.585: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:51.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:51.585: INFO: Wrong image for pod: daemon-set-stjfb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:51.585: INFO: Pod daemon-set-stjfb is not available
Jun 11 07:59:51.585: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:51.585: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:51.585: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:51.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:51.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:51.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:51.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:51.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:51.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:51.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:51.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:51.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:51.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:52.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:52.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:52.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:52.586: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:52.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:52.586: INFO: Wrong image for pod: daemon-set-stjfb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:52.586: INFO: Pod daemon-set-stjfb is not available
Jun 11 07:59:52.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:52.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:52.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:52.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:52.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:52.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:52.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:52.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:52.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:52.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:52.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:52.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:52.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:53.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:53.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:53.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:53.586: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:53.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:53.586: INFO: Wrong image for pod: daemon-set-stjfb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:53.586: INFO: Pod daemon-set-stjfb is not available
Jun 11 07:59:53.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:53.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:53.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:53.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:53.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:53.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:53.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:53.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:53.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:53.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:53.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:53.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:53.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:54.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:54.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:54.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:54.586: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:54.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:54.586: INFO: Wrong image for pod: daemon-set-stjfb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:54.586: INFO: Pod daemon-set-stjfb is not available
Jun 11 07:59:54.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:54.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:54.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:54.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:54.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:54.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:54.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:54.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:54.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:54.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:54.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:54.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:54.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:55.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:55.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:55.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:55.586: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:55.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:55.586: INFO: Wrong image for pod: daemon-set-stjfb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:55.586: INFO: Pod daemon-set-stjfb is not available
Jun 11 07:59:55.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:55.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:55.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:55.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:55.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:55.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:55.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:55.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:55.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:55.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:55.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:55.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:55.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:56.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:56.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:56.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:56.586: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:56.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:56.586: INFO: Wrong image for pod: daemon-set-stjfb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:56.586: INFO: Pod daemon-set-stjfb is not available
Jun 11 07:59:56.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:56.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:56.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:56.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:56.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:56.594: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:56.594: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:56.594: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:56.594: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:56.594: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:56.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:56.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:56.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:57.585: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:57.586: INFO: Pod daemon-set-6rfkp is not available
Jun 11 07:59:57.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:57.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:57.586: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:57.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:57.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:57.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:57.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:57.594: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:57.594: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:57.594: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:57.594: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:57.594: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:57.594: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:57.595: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:57.595: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:57.595: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:57.595: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:58.585: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:58.585: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:58.585: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:58.585: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:58.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:58.585: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:58.585: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:58.585: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:58.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:58.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:58.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:58.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:58.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:58.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:58.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:58.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:58.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:58.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:59.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:59.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:59.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:59.586: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:59.586: INFO: Pod daemon-set-nzrm5 is not available
Jun 11 07:59:59.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:59.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:59.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:59.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 07:59:59.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:59.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:59.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:59.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:59.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:59.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:59.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:59.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:59.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 07:59:59.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:00.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:00.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:00.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:00.586: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:00.586: INFO: Pod daemon-set-nzrm5 is not available
Jun 11 08:00:00.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:00.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:00.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:00.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:00.594: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:00.594: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:00.594: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:00.594: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:00.594: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:00.594: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:00.594: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:00.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:00.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:00.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:01.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:01.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:01.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:01.586: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:01.586: INFO: Pod daemon-set-nzrm5 is not available
Jun 11 08:00:01.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:01.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:01.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:01.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:01.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:01.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:01.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:01.594: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:01.594: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:01.594: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:01.594: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:01.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:01.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:01.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:02.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:02.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:02.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:02.586: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:02.586: INFO: Pod daemon-set-nzrm5 is not available
Jun 11 08:00:02.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:02.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:02.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:02.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:02.594: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:02.594: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:02.594: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:02.594: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:02.594: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:02.594: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:02.594: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:02.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:02.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:02.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:03.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:03.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:03.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:03.586: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:03.586: INFO: Pod daemon-set-nzrm5 is not available
Jun 11 08:00:03.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:03.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:03.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:03.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:03.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:03.594: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:03.594: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:03.594: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:03.594: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:03.594: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:03.594: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:03.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:03.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:03.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:04.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:04.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:04.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:04.586: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:04.586: INFO: Pod daemon-set-nzrm5 is not available
Jun 11 08:00:04.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:04.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:04.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:04.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:04.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:04.594: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:04.594: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:04.594: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:04.594: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:04.594: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:04.594: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:04.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:04.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:04.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:05.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:05.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:05.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:05.586: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:05.586: INFO: Pod daemon-set-nzrm5 is not available
Jun 11 08:00:05.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:05.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:05.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:05.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:05.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:05.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:05.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:05.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:05.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:05.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:05.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:05.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:05.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:05.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:06.585: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:06.585: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:06.585: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:06.585: INFO: Wrong image for pod: daemon-set-nzrm5. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:06.585: INFO: Pod daemon-set-nzrm5 is not available
Jun 11 08:00:06.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:06.585: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:06.585: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:06.585: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:06.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:06.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:06.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:06.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:06.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:06.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:06.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:06.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:06.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:06.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:07.585: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:07.585: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:07.585: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:07.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:07.585: INFO: Pod daemon-set-tf82g is not available
Jun 11 08:00:07.585: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:07.585: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:07.585: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:07.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:07.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:07.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:07.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:07.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:07.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:07.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:07.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:07.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:07.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:08.585: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:08.585: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:08.585: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:08.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:08.585: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:08.585: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:08.585: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:08.592: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:08.592: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:08.592: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:08.592: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:08.592: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:08.592: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:08.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:08.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:08.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:08.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:09.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:09.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:09.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:09.586: INFO: Pod daemon-set-lbrj4 is not available
Jun 11 08:00:09.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:09.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:09.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:09.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:09.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:09.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:09.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:09.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:09.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:09.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:09.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:09.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:09.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:09.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:10.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:10.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:10.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:10.586: INFO: Pod daemon-set-lbrj4 is not available
Jun 11 08:00:10.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:10.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:10.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:10.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:10.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:10.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:10.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:10.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:10.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:10.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:10.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:10.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:10.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:10.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:11.585: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:11.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:11.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:11.586: INFO: Pod daemon-set-lbrj4 is not available
Jun 11 08:00:11.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:11.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:11.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:11.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:11.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:11.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:11.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:11.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:11.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:11.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:11.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:11.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:11.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:11.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:12.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:12.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:12.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:12.586: INFO: Pod daemon-set-lbrj4 is not available
Jun 11 08:00:12.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:12.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:12.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:12.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:12.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:12.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:12.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:12.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:12.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:12.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:12.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:12.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:12.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:12.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:13.585: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:13.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:13.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:13.586: INFO: Pod daemon-set-lbrj4 is not available
Jun 11 08:00:13.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:13.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:13.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:13.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:13.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:13.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:13.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:13.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:13.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:13.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:13.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:13.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:13.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:13.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:14.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:14.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:14.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:14.586: INFO: Pod daemon-set-lbrj4 is not available
Jun 11 08:00:14.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:14.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:14.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:14.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:14.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:14.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:14.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:14.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:14.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:14.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:14.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:14.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:14.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:14.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:15.585: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:15.585: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:15.585: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:15.585: INFO: Pod daemon-set-lbrj4 is not available
Jun 11 08:00:15.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:15.585: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:15.585: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:15.585: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:15.592: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:15.592: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:15.592: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:15.592: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:15.592: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:15.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:15.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:15.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:15.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:15.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:16.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:16.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:16.586: INFO: Wrong image for pod: daemon-set-lbrj4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:16.586: INFO: Pod daemon-set-lbrj4 is not available
Jun 11 08:00:16.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:16.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:16.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:16.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:16.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:16.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:16.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:16.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:16.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:16.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:16.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:16.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:16.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:16.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:17.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:17.586: INFO: Pod daemon-set-6mzvj is not available
Jun 11 08:00:17.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:17.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:17.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:17.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:17.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:17.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:17.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:17.594: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:17.594: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:17.594: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:17.594: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:17.594: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:17.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:17.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:17.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:18.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:18.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:18.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:18.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:18.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:18.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:18.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:18.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:18.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:18.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:18.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:18.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:18.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:18.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:18.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:18.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:19.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:19.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:19.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:19.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:19.586: INFO: Pod daemon-set-tls84 is not available
Jun 11 08:00:19.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:19.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:19.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:19.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:19.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:19.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:19.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:19.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:19.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:19.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:19.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:19.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:20.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:20.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:20.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:20.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:20.586: INFO: Pod daemon-set-tls84 is not available
Jun 11 08:00:20.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:20.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:20.594: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:20.594: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:20.594: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:20.594: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:20.594: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:20.594: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:20.594: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:20.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:20.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:20.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:21.585: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:21.585: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:21.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:21.585: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:21.585: INFO: Pod daemon-set-tls84 is not available
Jun 11 08:00:21.585: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:21.585: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:21.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:21.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:21.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:21.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:21.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:21.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:21.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:21.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:21.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:21.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:22.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:22.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:22.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:22.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:22.586: INFO: Pod daemon-set-tls84 is not available
Jun 11 08:00:22.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:22.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:22.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:22.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:22.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:22.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:22.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:22.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:22.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:22.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:22.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:22.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:23.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:23.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:23.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:23.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:23.586: INFO: Pod daemon-set-tls84 is not available
Jun 11 08:00:23.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:23.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:23.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:23.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:23.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:23.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:23.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:23.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:23.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:23.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:23.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:23.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:24.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:24.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:24.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:24.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:24.586: INFO: Pod daemon-set-tls84 is not available
Jun 11 08:00:24.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:24.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:24.596: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:24.596: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:24.596: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:24.596: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:24.596: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:24.596: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:24.596: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:24.596: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:24.596: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:24.596: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:25.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:25.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:25.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:25.586: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:25.586: INFO: Pod daemon-set-tls84 is not available
Jun 11 08:00:25.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:25.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:25.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:25.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:25.594: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:25.594: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:25.594: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:25.594: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:25.594: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:25.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:25.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:25.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:26.585: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:26.585: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:26.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:26.585: INFO: Wrong image for pod: daemon-set-tls84. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:26.585: INFO: Pod daemon-set-tls84 is not available
Jun 11 08:00:26.585: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:26.585: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:26.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:26.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:26.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:26.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:26.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:26.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:26.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:26.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:26.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:26.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:27.585: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:27.585: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:27.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:27.585: INFO: Pod daemon-set-tc5ws is not available
Jun 11 08:00:27.585: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:27.585: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:27.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:27.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:27.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:27.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:27.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:27.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:27.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:27.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:27.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:27.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:28.585: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:28.585: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:28.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:28.585: INFO: Pod daemon-set-tc5ws is not available
Jun 11 08:00:28.585: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:28.585: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:28.592: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:28.592: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:28.592: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:28.592: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:28.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:28.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:28.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:28.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:28.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:28.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:29.585: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:29.585: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:29.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:29.585: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:29.585: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:29.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:29.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:29.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:29.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:29.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:29.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:29.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:29.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:29.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:29.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:30.585: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:30.585: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:30.585: INFO: Pod daemon-set-kqxgt is not available
Jun 11 08:00:30.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:30.585: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:30.585: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:30.592: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:30.592: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:30.592: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:30.592: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:30.592: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:30.592: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:30.592: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:30.592: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:30.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:30.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:31.587: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:31.587: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:31.587: INFO: Pod daemon-set-kqxgt is not available
Jun 11 08:00:31.587: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:31.587: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:31.587: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:31.594: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:31.594: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:31.594: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:31.594: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:31.594: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:31.594: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:31.594: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:31.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:31.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:31.595: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:32.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:32.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:32.586: INFO: Pod daemon-set-kqxgt is not available
Jun 11 08:00:32.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:32.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:32.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:32.594: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:32.594: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:32.594: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:32.594: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:32.594: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:32.594: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:32.594: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:32.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:32.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:32.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:33.585: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:33.585: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:33.585: INFO: Pod daemon-set-kqxgt is not available
Jun 11 08:00:33.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:33.585: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:33.585: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:33.613: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:33.613: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:33.613: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:33.613: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:33.613: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:33.613: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:33.613: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:33.613: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:33.613: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:33.613: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:34.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:34.586: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:34.586: INFO: Pod daemon-set-kqxgt is not available
Jun 11 08:00:34.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:34.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:34.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:34.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:34.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:34.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:34.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:34.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:34.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:34.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:34.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:34.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:34.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:35.585: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:35.585: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:35.585: INFO: Pod daemon-set-kqxgt is not available
Jun 11 08:00:35.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:35.585: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:35.585: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:35.592: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:35.592: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:35.592: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:35.592: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:35.592: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:35.592: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:35.592: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:35.592: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:35.592: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:35.592: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:36.585: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:36.585: INFO: Wrong image for pod: daemon-set-kqxgt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:36.585: INFO: Pod daemon-set-kqxgt is not available
Jun 11 08:00:36.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:36.585: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:36.585: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:36.592: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:36.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:36.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:36.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:36.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:36.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:36.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:36.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:36.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:36.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:37.585: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:37.585: INFO: Pod daemon-set-bk5fc is not available
Jun 11 08:00:37.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:37.585: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:37.585: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:37.592: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:37.592: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:37.592: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:37.592: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:37.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:37.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:37.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:37.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:37.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:37.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:38.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:38.586: INFO: Pod daemon-set-bk5fc is not available
Jun 11 08:00:38.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:38.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:38.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:38.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:38.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:38.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:38.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:38.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:38.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:38.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:38.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:38.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:38.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:39.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:39.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:39.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:39.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:39.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:39.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:39.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:39.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:39.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:39.594: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:39.594: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:39.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:39.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:39.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:40.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:40.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:40.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:40.586: INFO: Wrong image for pod: daemon-set-z6pfw. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:40.586: INFO: Pod daemon-set-z6pfw is not available
Jun 11 08:00:40.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:40.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:40.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:40.594: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:40.594: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:40.594: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:40.594: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:40.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:40.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:40.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:41.585: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:41.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:41.585: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:41.585: INFO: Pod daemon-set-xvd62 is not available
Jun 11 08:00:41.592: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:41.592: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:41.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:41.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:41.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:41.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:41.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:41.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:41.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:41.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:42.586: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:42.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:42.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:42.586: INFO: Pod daemon-set-xvd62 is not available
Jun 11 08:00:42.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:42.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:42.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:42.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:42.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:42.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:42.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:42.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:42.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:42.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:43.585: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:43.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:43.585: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:43.592: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:43.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:43.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:43.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:43.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:43.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:43.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:43.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:43.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:43.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:44.587: INFO: Wrong image for pod: daemon-set-2rscv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:44.587: INFO: Pod daemon-set-2rscv is not available
Jun 11 08:00:44.587: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:44.587: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:44.594: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:44.594: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:44.594: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:44.594: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:44.594: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:44.594: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:44.594: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:44.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:44.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:44.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:45.585: INFO: Pod daemon-set-mxm9f is not available
Jun 11 08:00:45.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:45.585: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:45.592: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:45.592: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:45.592: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:45.592: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:45.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:45.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:45.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:45.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:45.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:45.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:46.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:46.585: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:46.592: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:46.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:46.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:46.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:46.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:46.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:46.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:46.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:46.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:46.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:47.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:47.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:47.586: INFO: Pod daemon-set-wfbll is not available
Jun 11 08:00:47.594: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:47.594: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:47.594: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:47.594: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:47.594: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:47.594: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:47.594: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:47.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:47.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:47.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:48.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:48.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:48.586: INFO: Pod daemon-set-wfbll is not available
Jun 11 08:00:48.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:48.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:48.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:48.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:48.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:48.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:48.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:48.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:48.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:48.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:49.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:49.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:49.586: INFO: Pod daemon-set-wfbll is not available
Jun 11 08:00:49.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:49.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:49.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:49.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:49.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:49.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:49.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:49.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:49.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:49.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:50.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:50.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:50.586: INFO: Pod daemon-set-wfbll is not available
Jun 11 08:00:50.594: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:50.594: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:50.594: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:50.594: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:50.594: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:50.594: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:50.594: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:50.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:50.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:50.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:51.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:51.585: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:51.585: INFO: Pod daemon-set-wfbll is not available
Jun 11 08:00:51.595: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:51.595: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:51.596: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:51.596: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:51.596: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:51.596: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:51.596: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:51.596: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:51.596: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:51.596: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:52.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:52.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:52.586: INFO: Pod daemon-set-wfbll is not available
Jun 11 08:00:52.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:52.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:52.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:52.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:52.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:52.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:52.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:52.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:52.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:52.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:53.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:53.585: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:53.585: INFO: Pod daemon-set-wfbll is not available
Jun 11 08:00:53.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:53.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:53.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:53.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:53.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:53.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:53.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:53.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:53.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:53.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:54.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:54.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:54.586: INFO: Pod daemon-set-wfbll is not available
Jun 11 08:00:54.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:54.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:54.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:54.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:54.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:54.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:54.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:54.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:54.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:54.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:55.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:55.586: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:55.586: INFO: Pod daemon-set-wfbll is not available
Jun 11 08:00:55.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:55.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:55.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:55.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:55.594: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:55.594: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:55.594: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:55.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:55.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:55.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:56.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:56.585: INFO: Wrong image for pod: daemon-set-wfbll. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:56.585: INFO: Pod daemon-set-wfbll is not available
Jun 11 08:00:56.592: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:56.592: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:56.592: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:56.592: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:56.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:56.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:56.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:56.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:56.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:56.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:57.585: INFO: Pod daemon-set-bvlrd is not available
Jun 11 08:00:57.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:57.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:57.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:57.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:57.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:57.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:57.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:57.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:57.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:57.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:57.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:58.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:58.592: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:58.592: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:58.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:58.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:58.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:58.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:58.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:58.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:58.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:58.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:59.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:00:59.585: INFO: Pod daemon-set-shz4w is not available
Jun 11 08:00:59.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:59.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:59.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:59.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:59.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:59.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:59.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:59.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:59.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:00:59.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:00.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:01:00.585: INFO: Pod daemon-set-shz4w is not available
Jun 11 08:01:00.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:00.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:00.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:00.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:00.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:00.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:00.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:00.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:00.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:00.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:01.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:01:01.586: INFO: Pod daemon-set-shz4w is not available
Jun 11 08:01:01.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:01.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:01.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:01.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:01.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:01.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:01.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:01.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:01.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:01.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:02.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:01:02.586: INFO: Pod daemon-set-shz4w is not available
Jun 11 08:01:02.594: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:02.594: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:02.594: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:02.594: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:02.594: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:02.594: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:02.594: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:02.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:02.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:02.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:03.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:01:03.585: INFO: Pod daemon-set-shz4w is not available
Jun 11 08:01:03.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:03.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:03.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:03.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:03.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:03.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:03.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:03.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:03.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:03.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:04.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:01:04.586: INFO: Pod daemon-set-shz4w is not available
Jun 11 08:01:04.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:04.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:04.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:04.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:04.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:04.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:04.594: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:04.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:04.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:04.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:05.585: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:01:05.585: INFO: Pod daemon-set-shz4w is not available
Jun 11 08:01:05.592: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:05.592: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:05.592: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:05.592: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:05.592: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:05.592: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:05.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:05.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:05.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:05.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:06.586: INFO: Wrong image for pod: daemon-set-shz4w. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun 11 08:01:06.586: INFO: Pod daemon-set-shz4w is not available
Jun 11 08:01:06.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:06.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:06.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:06.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:06.594: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:06.594: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:06.594: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:06.594: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:06.594: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:06.594: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:07.585: INFO: Pod daemon-set-wflsm is not available
Jun 11 08:01:07.592: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:07.593: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:07.593: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:07.593: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:07.593: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:07.593: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:07.593: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:07.593: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:07.593: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:07.593: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jun 11 08:01:07.600: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:07.600: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:07.600: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:07.600: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:07.600: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:07.600: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:07.600: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:07.600: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:07.600: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:07.600: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:07.605: INFO: Number of nodes with available pods: 9
Jun 11 08:01:07.605: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Jun 11 08:01:08.613: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:08.613: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:08.613: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:08.613: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:08.613: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:08.613: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:08.613: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:08.613: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:08.613: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:08.613: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:01:08.618: INFO: Number of nodes with available pods: 10
Jun 11 08:01:08.618: INFO: Number of running nodes: 10, number of available pods: 10
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2690, will wait for the garbage collector to delete the pods
Jun 11 08:01:08.705: INFO: Deleting DaemonSet.extensions daemon-set took: 11.880569ms
Jun 11 08:01:10.305: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.600282926s
Jun 11 08:01:17.010: INFO: Number of nodes with available pods: 0
Jun 11 08:01:17.010: INFO: Number of running nodes: 0, number of available pods: 0
Jun 11 08:01:17.014: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2690/daemonsets","resourceVersion":"109274"},"items":null}

Jun 11 08:01:17.018: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2690/pods","resourceVersion":"109274"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:01:17.064: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "daemonsets-2690" for this suite.

• [SLOW TEST:106.800 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":280,"completed":87,"skipped":1312,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:01:17.080: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8141
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:01:22.294: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "watch-8141" for this suite.

• [SLOW TEST:5.311 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":280,"completed":88,"skipped":1320,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:01:22.392: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4406
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-4406
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Jun 11 08:01:22.558: INFO: Found 0 stateful pods, waiting for 3
Jun 11 08:01:32.564: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 11 08:01:32.564: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 11 08:01:32.564: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun 11 08:01:32.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=statefulset-4406 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 11 08:01:32.751: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 11 08:01:32.751: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 11 08:01:32.751: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jun 11 08:01:42.788: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jun 11 08:01:52.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=statefulset-4406 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 11 08:01:52.990: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 11 08:01:52.990: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 11 08:01:52.990: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 11 08:02:03.017: INFO: Waiting for StatefulSet statefulset-4406/ss2 to complete update
Jun 11 08:02:03.017: INFO: Waiting for Pod statefulset-4406/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 11 08:02:03.017: INFO: Waiting for Pod statefulset-4406/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 11 08:02:13.026: INFO: Waiting for StatefulSet statefulset-4406/ss2 to complete update
Jun 11 08:02:13.026: INFO: Waiting for Pod statefulset-4406/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 11 08:02:13.026: INFO: Waiting for Pod statefulset-4406/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 11 08:02:23.026: INFO: Waiting for StatefulSet statefulset-4406/ss2 to complete update
Jun 11 08:02:23.026: INFO: Waiting for Pod statefulset-4406/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Jun 11 08:02:33.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=statefulset-4406 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 11 08:02:33.196: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 11 08:02:33.196: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 11 08:02:33.196: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 11 08:02:43.235: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jun 11 08:02:53.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=statefulset-4406 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 11 08:02:53.436: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 11 08:02:53.436: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 11 08:02:53.436: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jun 11 08:03:13.461: INFO: Deleting all statefulset in ns statefulset-4406
Jun 11 08:03:13.465: INFO: Scaling statefulset ss2 to 0
Jun 11 08:03:53.482: INFO: Waiting for statefulset status.replicas updated to 0
Jun 11 08:03:53.486: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:03:53.505: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "statefulset-4406" for this suite.

• [SLOW TEST:151.128 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":280,"completed":89,"skipped":1322,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:03:53.520: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9896
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service nodeport-service with the type=NodePort in namespace services-9896
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-9896
STEP: creating replication controller externalsvc in namespace services-9896
I0611 08:03:53.706109      23 runners.go:189] Created replication controller with name: externalsvc, namespace: services-9896, replica count: 2
I0611 08:03:56.756536      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady
STEP: changing the NodePort service to type=ExternalName
Jun 11 08:03:56.787: INFO: Creating new exec pod
Jun 11 08:03:58.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=services-9896 execpod9kns6 -- /bin/sh -x -c nslookup nodeport-service'
Jun 11 08:03:59.001: INFO: stderr: "+ nslookup nodeport-service\n"
Jun 11 08:03:59.001: INFO: stdout: "Server:\t\t10.0.0.10\nAddress:\t10.0.0.10#53\n\nnodeport-service.services-9896.svc.cluster.local\tcanonical name = externalsvc.services-9896.svc.cluster.local.\nName:\texternalsvc.services-9896.svc.cluster.local\nAddress: 10.0.56.225\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9896, will wait for the garbage collector to delete the pods
Jun 11 08:03:59.067: INFO: Deleting ReplicationController externalsvc took: 11.729634ms
Jun 11 08:03:59.167: INFO: Terminating ReplicationController externalsvc pods took: 100.281616ms
Jun 11 08:04:06.896: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:04:06.917: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "services-9896" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:13.413 seconds]
[sig-network] Services
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":280,"completed":90,"skipped":1351,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:04:06.934: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2653
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 08:04:07.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 version'
Jun 11 08:04:07.155: INFO: stderr: ""
Jun 11 08:04:07.155: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.6\", GitCommit:\"d32e40e20d167e103faf894261614c5b45c44198\", GitTreeState:\"clean\", BuildDate:\"2020-05-20T13:16:24Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.6\", GitCommit:\"d32e40e20d167e103faf894261614c5b45c44198\", GitTreeState:\"clean\", BuildDate:\"2020-05-20T13:08:34Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:04:07.155: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "kubectl-2653" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":280,"completed":91,"skipped":1378,"failed":0}
S
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:04:07.175: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-9173
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 08:04:07.326: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: creating replication controller svc-latency-rc in namespace svc-latency-9173
I0611 08:04:07.338266      23 runners.go:189] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9173, replica count: 1
I0611 08:04:08.388734      23 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady
Jun 11 08:04:08.502: INFO: Created: latency-svc-j9wzb
Jun 11 08:04:08.509: INFO: Got endpoints: latency-svc-j9wzb [20.444881ms]
Jun 11 08:04:08.522: INFO: Created: latency-svc-qzv9b
Jun 11 08:04:08.528: INFO: Got endpoints: latency-svc-qzv9b [18.632432ms]
Jun 11 08:04:08.529: INFO: Created: latency-svc-mcvpr
Jun 11 08:04:08.536: INFO: Got endpoints: latency-svc-mcvpr [27.258825ms]
Jun 11 08:04:08.539: INFO: Created: latency-svc-fjfmd
Jun 11 08:04:08.546: INFO: Got endpoints: latency-svc-fjfmd [36.750623ms]
Jun 11 08:04:08.550: INFO: Created: latency-svc-x8pnv
Jun 11 08:04:08.555: INFO: Got endpoints: latency-svc-x8pnv [45.933446ms]
Jun 11 08:04:08.557: INFO: Created: latency-svc-7xjqs
Jun 11 08:04:08.564: INFO: Got endpoints: latency-svc-7xjqs [55.192144ms]
Jun 11 08:04:08.566: INFO: Created: latency-svc-6tz9x
Jun 11 08:04:08.571: INFO: Got endpoints: latency-svc-6tz9x [61.796392ms]
Jun 11 08:04:08.574: INFO: Created: latency-svc-h4k67
Jun 11 08:04:08.580: INFO: Got endpoints: latency-svc-h4k67 [70.716633ms]
Jun 11 08:04:08.582: INFO: Created: latency-svc-t88qp
Jun 11 08:04:08.587: INFO: Got endpoints: latency-svc-t88qp [77.644389ms]
Jun 11 08:04:08.590: INFO: Created: latency-svc-wq4gw
Jun 11 08:04:08.599: INFO: Got endpoints: latency-svc-wq4gw [89.591973ms]
Jun 11 08:04:08.602: INFO: Created: latency-svc-qwnzc
Jun 11 08:04:08.608: INFO: Got endpoints: latency-svc-qwnzc [98.86904ms]
Jun 11 08:04:08.611: INFO: Created: latency-svc-dqrp8
Jun 11 08:04:08.617: INFO: Got endpoints: latency-svc-dqrp8 [108.252124ms]
Jun 11 08:04:08.621: INFO: Created: latency-svc-k6n4w
Jun 11 08:04:08.626: INFO: Got endpoints: latency-svc-k6n4w [116.847179ms]
Jun 11 08:04:08.629: INFO: Created: latency-svc-5mmdd
Jun 11 08:04:08.635: INFO: Got endpoints: latency-svc-5mmdd [125.398208ms]
Jun 11 08:04:08.637: INFO: Created: latency-svc-72n9w
Jun 11 08:04:08.641: INFO: Got endpoints: latency-svc-72n9w [132.080582ms]
Jun 11 08:04:08.645: INFO: Created: latency-svc-x9svl
Jun 11 08:04:08.650: INFO: Got endpoints: latency-svc-x9svl [141.273445ms]
Jun 11 08:04:08.653: INFO: Created: latency-svc-4mdgz
Jun 11 08:04:08.659: INFO: Got endpoints: latency-svc-4mdgz [131.751411ms]
Jun 11 08:04:08.664: INFO: Created: latency-svc-m7kkf
Jun 11 08:04:08.669: INFO: Got endpoints: latency-svc-m7kkf [132.991436ms]
Jun 11 08:04:08.672: INFO: Created: latency-svc-6vwvq
Jun 11 08:04:08.677: INFO: Got endpoints: latency-svc-6vwvq [130.862668ms]
Jun 11 08:04:08.680: INFO: Created: latency-svc-689gf
Jun 11 08:04:08.687: INFO: Got endpoints: latency-svc-689gf [131.690736ms]
Jun 11 08:04:08.698: INFO: Created: latency-svc-4sjrv
Jun 11 08:04:08.699: INFO: Got endpoints: latency-svc-4sjrv [134.035889ms]
Jun 11 08:04:08.707: INFO: Created: latency-svc-pd9rv
Jun 11 08:04:08.711: INFO: Got endpoints: latency-svc-pd9rv [140.379411ms]
Jun 11 08:04:08.715: INFO: Created: latency-svc-lsj76
Jun 11 08:04:08.723: INFO: Got endpoints: latency-svc-lsj76 [143.473535ms]
Jun 11 08:04:08.725: INFO: Created: latency-svc-29bnm
Jun 11 08:04:08.731: INFO: Got endpoints: latency-svc-29bnm [144.087754ms]
Jun 11 08:04:08.734: INFO: Created: latency-svc-hjd2p
Jun 11 08:04:08.740: INFO: Got endpoints: latency-svc-hjd2p [141.187082ms]
Jun 11 08:04:08.743: INFO: Created: latency-svc-zff8z
Jun 11 08:04:08.748: INFO: Got endpoints: latency-svc-zff8z [140.171462ms]
Jun 11 08:04:08.752: INFO: Created: latency-svc-j9j5h
Jun 11 08:04:08.757: INFO: Got endpoints: latency-svc-j9j5h [139.268926ms]
Jun 11 08:04:08.760: INFO: Created: latency-svc-82g8b
Jun 11 08:04:08.766: INFO: Got endpoints: latency-svc-82g8b [140.283657ms]
Jun 11 08:04:08.768: INFO: Created: latency-svc-5fx9f
Jun 11 08:04:08.773: INFO: Got endpoints: latency-svc-5fx9f [138.271718ms]
Jun 11 08:04:08.776: INFO: Created: latency-svc-8x956
Jun 11 08:04:08.781: INFO: Got endpoints: latency-svc-8x956 [139.948463ms]
Jun 11 08:04:08.785: INFO: Created: latency-svc-q6tbp
Jun 11 08:04:08.790: INFO: Got endpoints: latency-svc-q6tbp [139.67602ms]
Jun 11 08:04:08.793: INFO: Created: latency-svc-k4w92
Jun 11 08:04:08.799: INFO: Got endpoints: latency-svc-k4w92 [139.563194ms]
Jun 11 08:04:08.802: INFO: Created: latency-svc-ldhwb
Jun 11 08:04:08.807: INFO: Got endpoints: latency-svc-ldhwb [137.95566ms]
Jun 11 08:04:08.811: INFO: Created: latency-svc-22mwg
Jun 11 08:04:08.816: INFO: Got endpoints: latency-svc-22mwg [139.047701ms]
Jun 11 08:04:08.820: INFO: Created: latency-svc-gdpdg
Jun 11 08:04:08.826: INFO: Got endpoints: latency-svc-gdpdg [138.901819ms]
Jun 11 08:04:08.829: INFO: Created: latency-svc-k4pr2
Jun 11 08:04:08.834: INFO: Got endpoints: latency-svc-k4pr2 [135.447809ms]
Jun 11 08:04:08.836: INFO: Created: latency-svc-bfx5q
Jun 11 08:04:08.844: INFO: Created: latency-svc-x5wm2
Jun 11 08:04:08.852: INFO: Created: latency-svc-pwgsl
Jun 11 08:04:08.858: INFO: Got endpoints: latency-svc-bfx5q [146.547068ms]
Jun 11 08:04:08.861: INFO: Created: latency-svc-qg7zh
Jun 11 08:04:08.870: INFO: Created: latency-svc-p8wjj
Jun 11 08:04:08.877: INFO: Created: latency-svc-kcw7n
Jun 11 08:04:08.885: INFO: Created: latency-svc-7gzlj
Jun 11 08:04:08.893: INFO: Created: latency-svc-hfv8d
Jun 11 08:04:08.900: INFO: Created: latency-svc-qdzrx
Jun 11 08:04:08.907: INFO: Got endpoints: latency-svc-x5wm2 [183.888237ms]
Jun 11 08:04:08.910: INFO: Created: latency-svc-gmhjs
Jun 11 08:04:08.918: INFO: Created: latency-svc-l5fh6
Jun 11 08:04:08.926: INFO: Created: latency-svc-pn2k7
Jun 11 08:04:08.934: INFO: Created: latency-svc-xd6k6
Jun 11 08:04:08.942: INFO: Created: latency-svc-jhw6x
Jun 11 08:04:08.949: INFO: Created: latency-svc-6c2fs
Jun 11 08:04:08.957: INFO: Created: latency-svc-mwvns
Jun 11 08:04:08.958: INFO: Got endpoints: latency-svc-pwgsl [227.297084ms]
Jun 11 08:04:08.966: INFO: Created: latency-svc-ds8rp
Jun 11 08:04:08.978: INFO: Created: latency-svc-rv9xc
Jun 11 08:04:09.008: INFO: Got endpoints: latency-svc-qg7zh [268.082557ms]
Jun 11 08:04:09.023: INFO: Created: latency-svc-xcv7j
Jun 11 08:04:09.058: INFO: Got endpoints: latency-svc-p8wjj [309.887545ms]
Jun 11 08:04:09.071: INFO: Created: latency-svc-wx2kz
Jun 11 08:04:09.108: INFO: Got endpoints: latency-svc-kcw7n [350.7631ms]
Jun 11 08:04:09.120: INFO: Created: latency-svc-dfpsv
Jun 11 08:04:09.158: INFO: Got endpoints: latency-svc-7gzlj [391.47428ms]
Jun 11 08:04:09.170: INFO: Created: latency-svc-kwhh2
Jun 11 08:04:09.208: INFO: Got endpoints: latency-svc-hfv8d [434.716997ms]
Jun 11 08:04:09.221: INFO: Created: latency-svc-759lp
Jun 11 08:04:09.258: INFO: Got endpoints: latency-svc-qdzrx [476.899345ms]
Jun 11 08:04:09.270: INFO: Created: latency-svc-jbzvn
Jun 11 08:04:09.324: INFO: Got endpoints: latency-svc-gmhjs [533.704972ms]
Jun 11 08:04:09.364: INFO: Created: latency-svc-bhd8d
Jun 11 08:04:09.366: INFO: Got endpoints: latency-svc-l5fh6 [566.604424ms]
Jun 11 08:04:09.378: INFO: Created: latency-svc-55th9
Jun 11 08:04:09.408: INFO: Got endpoints: latency-svc-pn2k7 [600.60944ms]
Jun 11 08:04:09.427: INFO: Created: latency-svc-z2g62
Jun 11 08:04:09.458: INFO: Got endpoints: latency-svc-xd6k6 [642.436023ms]
Jun 11 08:04:09.472: INFO: Created: latency-svc-ccp4p
Jun 11 08:04:09.508: INFO: Got endpoints: latency-svc-jhw6x [682.025709ms]
Jun 11 08:04:09.520: INFO: Created: latency-svc-tqw29
Jun 11 08:04:09.558: INFO: Got endpoints: latency-svc-6c2fs [724.331157ms]
Jun 11 08:04:09.573: INFO: Created: latency-svc-5kqmt
Jun 11 08:04:09.608: INFO: Got endpoints: latency-svc-mwvns [750.476606ms]
Jun 11 08:04:09.621: INFO: Created: latency-svc-962b9
Jun 11 08:04:09.658: INFO: Got endpoints: latency-svc-ds8rp [751.030635ms]
Jun 11 08:04:09.671: INFO: Created: latency-svc-kltkd
Jun 11 08:04:09.709: INFO: Got endpoints: latency-svc-rv9xc [750.220876ms]
Jun 11 08:04:09.721: INFO: Created: latency-svc-jtjn2
Jun 11 08:04:09.758: INFO: Got endpoints: latency-svc-xcv7j [749.936957ms]
Jun 11 08:04:09.770: INFO: Created: latency-svc-qvcxw
Jun 11 08:04:09.808: INFO: Got endpoints: latency-svc-wx2kz [749.920164ms]
Jun 11 08:04:09.820: INFO: Created: latency-svc-r5q5x
Jun 11 08:04:09.859: INFO: Got endpoints: latency-svc-dfpsv [750.90156ms]
Jun 11 08:04:09.877: INFO: Created: latency-svc-mzmkq
Jun 11 08:04:09.908: INFO: Got endpoints: latency-svc-kwhh2 [749.963409ms]
Jun 11 08:04:09.921: INFO: Created: latency-svc-lf4np
Jun 11 08:04:09.958: INFO: Got endpoints: latency-svc-759lp [750.654353ms]
Jun 11 08:04:09.971: INFO: Created: latency-svc-296c2
Jun 11 08:04:10.008: INFO: Got endpoints: latency-svc-jbzvn [749.706112ms]
Jun 11 08:04:10.021: INFO: Created: latency-svc-5s4wg
Jun 11 08:04:10.059: INFO: Got endpoints: latency-svc-bhd8d [735.228476ms]
Jun 11 08:04:10.074: INFO: Created: latency-svc-92rc7
Jun 11 08:04:10.108: INFO: Got endpoints: latency-svc-55th9 [742.066522ms]
Jun 11 08:04:10.120: INFO: Created: latency-svc-gd495
Jun 11 08:04:10.158: INFO: Got endpoints: latency-svc-z2g62 [750.147491ms]
Jun 11 08:04:10.171: INFO: Created: latency-svc-n69wz
Jun 11 08:04:10.208: INFO: Got endpoints: latency-svc-ccp4p [749.549522ms]
Jun 11 08:04:10.220: INFO: Created: latency-svc-cgslk
Jun 11 08:04:10.258: INFO: Got endpoints: latency-svc-tqw29 [749.794819ms]
Jun 11 08:04:10.270: INFO: Created: latency-svc-fj85j
Jun 11 08:04:10.308: INFO: Got endpoints: latency-svc-5kqmt [749.709719ms]
Jun 11 08:04:10.321: INFO: Created: latency-svc-f7hl7
Jun 11 08:04:10.358: INFO: Got endpoints: latency-svc-962b9 [749.568811ms]
Jun 11 08:04:10.370: INFO: Created: latency-svc-x8fq9
Jun 11 08:04:10.408: INFO: Got endpoints: latency-svc-kltkd [749.60609ms]
Jun 11 08:04:10.420: INFO: Created: latency-svc-js8bs
Jun 11 08:04:10.458: INFO: Got endpoints: latency-svc-jtjn2 [748.966318ms]
Jun 11 08:04:10.470: INFO: Created: latency-svc-krx9q
Jun 11 08:04:10.508: INFO: Got endpoints: latency-svc-qvcxw [749.966478ms]
Jun 11 08:04:10.522: INFO: Created: latency-svc-fcwwv
Jun 11 08:04:10.558: INFO: Got endpoints: latency-svc-r5q5x [749.846065ms]
Jun 11 08:04:10.571: INFO: Created: latency-svc-mxl7p
Jun 11 08:04:10.609: INFO: Got endpoints: latency-svc-mzmkq [750.352017ms]
Jun 11 08:04:10.621: INFO: Created: latency-svc-sdq2f
Jun 11 08:04:10.659: INFO: Got endpoints: latency-svc-lf4np [750.959633ms]
Jun 11 08:04:10.672: INFO: Created: latency-svc-tlhmj
Jun 11 08:04:10.709: INFO: Got endpoints: latency-svc-296c2 [750.249965ms]
Jun 11 08:04:10.721: INFO: Created: latency-svc-wljml
Jun 11 08:04:10.758: INFO: Got endpoints: latency-svc-5s4wg [750.170976ms]
Jun 11 08:04:10.772: INFO: Created: latency-svc-sm5dd
Jun 11 08:04:10.808: INFO: Got endpoints: latency-svc-92rc7 [748.917082ms]
Jun 11 08:04:10.821: INFO: Created: latency-svc-bhjxf
Jun 11 08:04:10.858: INFO: Got endpoints: latency-svc-gd495 [750.404948ms]
Jun 11 08:04:10.870: INFO: Created: latency-svc-7grcb
Jun 11 08:04:10.909: INFO: Got endpoints: latency-svc-n69wz [750.363715ms]
Jun 11 08:04:10.920: INFO: Created: latency-svc-mj9xk
Jun 11 08:04:10.958: INFO: Got endpoints: latency-svc-cgslk [750.231076ms]
Jun 11 08:04:10.973: INFO: Created: latency-svc-965xz
Jun 11 08:04:11.008: INFO: Got endpoints: latency-svc-fj85j [750.506569ms]
Jun 11 08:04:11.021: INFO: Created: latency-svc-k4wmv
Jun 11 08:04:11.058: INFO: Got endpoints: latency-svc-f7hl7 [749.840739ms]
Jun 11 08:04:11.070: INFO: Created: latency-svc-vk87b
Jun 11 08:04:11.108: INFO: Got endpoints: latency-svc-x8fq9 [749.869067ms]
Jun 11 08:04:11.121: INFO: Created: latency-svc-k4zm8
Jun 11 08:04:11.158: INFO: Got endpoints: latency-svc-js8bs [750.190274ms]
Jun 11 08:04:11.171: INFO: Created: latency-svc-wqtdf
Jun 11 08:04:11.208: INFO: Got endpoints: latency-svc-krx9q [750.34352ms]
Jun 11 08:04:11.221: INFO: Created: latency-svc-w99tg
Jun 11 08:04:11.258: INFO: Got endpoints: latency-svc-fcwwv [749.506787ms]
Jun 11 08:04:11.270: INFO: Created: latency-svc-7xb6h
Jun 11 08:04:11.308: INFO: Got endpoints: latency-svc-mxl7p [750.340567ms]
Jun 11 08:04:11.320: INFO: Created: latency-svc-kt5hl
Jun 11 08:04:11.358: INFO: Got endpoints: latency-svc-sdq2f [748.953414ms]
Jun 11 08:04:11.370: INFO: Created: latency-svc-9s9cw
Jun 11 08:04:11.408: INFO: Got endpoints: latency-svc-tlhmj [748.598687ms]
Jun 11 08:04:11.420: INFO: Created: latency-svc-xf7l9
Jun 11 08:04:11.458: INFO: Got endpoints: latency-svc-wljml [749.434317ms]
Jun 11 08:04:11.471: INFO: Created: latency-svc-tsx2n
Jun 11 08:04:11.508: INFO: Got endpoints: latency-svc-sm5dd [749.69372ms]
Jun 11 08:04:11.521: INFO: Created: latency-svc-7h5k4
Jun 11 08:04:11.558: INFO: Got endpoints: latency-svc-bhjxf [749.982615ms]
Jun 11 08:04:11.571: INFO: Created: latency-svc-vnbv2
Jun 11 08:04:11.608: INFO: Got endpoints: latency-svc-7grcb [749.988597ms]
Jun 11 08:04:11.621: INFO: Created: latency-svc-9vl6b
Jun 11 08:04:11.658: INFO: Got endpoints: latency-svc-mj9xk [749.325422ms]
Jun 11 08:04:11.671: INFO: Created: latency-svc-qwx6m
Jun 11 08:04:11.708: INFO: Got endpoints: latency-svc-965xz [749.456487ms]
Jun 11 08:04:11.722: INFO: Created: latency-svc-rmqvd
Jun 11 08:04:11.759: INFO: Got endpoints: latency-svc-k4wmv [750.574954ms]
Jun 11 08:04:11.771: INFO: Created: latency-svc-gcwss
Jun 11 08:04:11.808: INFO: Got endpoints: latency-svc-vk87b [749.952707ms]
Jun 11 08:04:11.821: INFO: Created: latency-svc-b9hxl
Jun 11 08:04:11.860: INFO: Got endpoints: latency-svc-k4zm8 [751.775777ms]
Jun 11 08:04:11.872: INFO: Created: latency-svc-c6lcb
Jun 11 08:04:11.908: INFO: Got endpoints: latency-svc-wqtdf [750.160043ms]
Jun 11 08:04:11.920: INFO: Created: latency-svc-z8fnx
Jun 11 08:04:11.958: INFO: Got endpoints: latency-svc-w99tg [750.024828ms]
Jun 11 08:04:11.971: INFO: Created: latency-svc-8kc4l
Jun 11 08:04:12.008: INFO: Got endpoints: latency-svc-7xb6h [750.170654ms]
Jun 11 08:04:12.021: INFO: Created: latency-svc-k62wg
Jun 11 08:04:12.058: INFO: Got endpoints: latency-svc-kt5hl [749.220239ms]
Jun 11 08:04:12.071: INFO: Created: latency-svc-jx8tz
Jun 11 08:04:12.108: INFO: Got endpoints: latency-svc-9s9cw [750.262418ms]
Jun 11 08:04:12.122: INFO: Created: latency-svc-h4hkb
Jun 11 08:04:12.158: INFO: Got endpoints: latency-svc-xf7l9 [750.136521ms]
Jun 11 08:04:12.171: INFO: Created: latency-svc-hl47x
Jun 11 08:04:12.209: INFO: Got endpoints: latency-svc-tsx2n [750.491697ms]
Jun 11 08:04:12.221: INFO: Created: latency-svc-gg4wq
Jun 11 08:04:12.258: INFO: Got endpoints: latency-svc-7h5k4 [749.628711ms]
Jun 11 08:04:12.270: INFO: Created: latency-svc-6blqf
Jun 11 08:04:12.308: INFO: Got endpoints: latency-svc-vnbv2 [749.944564ms]
Jun 11 08:04:12.321: INFO: Created: latency-svc-77hfk
Jun 11 08:04:12.358: INFO: Got endpoints: latency-svc-9vl6b [749.777763ms]
Jun 11 08:04:12.377: INFO: Created: latency-svc-8r87t
Jun 11 08:04:12.408: INFO: Got endpoints: latency-svc-qwx6m [749.587032ms]
Jun 11 08:04:12.420: INFO: Created: latency-svc-5v9kr
Jun 11 08:04:12.459: INFO: Got endpoints: latency-svc-rmqvd [751.045278ms]
Jun 11 08:04:12.476: INFO: Created: latency-svc-mwhvz
Jun 11 08:04:12.508: INFO: Got endpoints: latency-svc-gcwss [749.60552ms]
Jun 11 08:04:12.521: INFO: Created: latency-svc-g9htp
Jun 11 08:04:12.560: INFO: Got endpoints: latency-svc-b9hxl [751.737774ms]
Jun 11 08:04:12.573: INFO: Created: latency-svc-bmz76
Jun 11 08:04:12.609: INFO: Got endpoints: latency-svc-c6lcb [748.911258ms]
Jun 11 08:04:12.623: INFO: Created: latency-svc-zs5bt
Jun 11 08:04:12.658: INFO: Got endpoints: latency-svc-z8fnx [749.437498ms]
Jun 11 08:04:12.672: INFO: Created: latency-svc-cnpnc
Jun 11 08:04:12.708: INFO: Got endpoints: latency-svc-8kc4l [750.067827ms]
Jun 11 08:04:12.722: INFO: Created: latency-svc-9hxsg
Jun 11 08:04:12.758: INFO: Got endpoints: latency-svc-k62wg [750.42902ms]
Jun 11 08:04:12.774: INFO: Created: latency-svc-9fjpn
Jun 11 08:04:12.808: INFO: Got endpoints: latency-svc-jx8tz [750.539869ms]
Jun 11 08:04:12.823: INFO: Created: latency-svc-9qfkw
Jun 11 08:04:12.858: INFO: Got endpoints: latency-svc-h4hkb [749.384469ms]
Jun 11 08:04:12.871: INFO: Created: latency-svc-4sk5h
Jun 11 08:04:12.909: INFO: Got endpoints: latency-svc-hl47x [750.849434ms]
Jun 11 08:04:12.921: INFO: Created: latency-svc-2ffn8
Jun 11 08:04:12.959: INFO: Got endpoints: latency-svc-gg4wq [750.048771ms]
Jun 11 08:04:12.974: INFO: Created: latency-svc-tql64
Jun 11 08:04:13.008: INFO: Got endpoints: latency-svc-6blqf [750.594136ms]
Jun 11 08:04:13.027: INFO: Created: latency-svc-dxkpn
Jun 11 08:04:13.058: INFO: Got endpoints: latency-svc-77hfk [750.22359ms]
Jun 11 08:04:13.072: INFO: Created: latency-svc-4c2mk
Jun 11 08:04:13.108: INFO: Got endpoints: latency-svc-8r87t [750.098423ms]
Jun 11 08:04:13.122: INFO: Created: latency-svc-dvpzc
Jun 11 08:04:13.158: INFO: Got endpoints: latency-svc-5v9kr [750.596804ms]
Jun 11 08:04:13.172: INFO: Created: latency-svc-g74jb
Jun 11 08:04:13.208: INFO: Got endpoints: latency-svc-mwhvz [749.080453ms]
Jun 11 08:04:13.221: INFO: Created: latency-svc-ccn4n
Jun 11 08:04:13.257: INFO: Got endpoints: latency-svc-g9htp [749.027994ms]
Jun 11 08:04:13.273: INFO: Created: latency-svc-klndw
Jun 11 08:04:13.308: INFO: Got endpoints: latency-svc-bmz76 [748.259309ms]
Jun 11 08:04:13.320: INFO: Created: latency-svc-sr2lr
Jun 11 08:04:13.358: INFO: Got endpoints: latency-svc-zs5bt [749.465189ms]
Jun 11 08:04:13.371: INFO: Created: latency-svc-hxp8w
Jun 11 08:04:13.408: INFO: Got endpoints: latency-svc-cnpnc [749.962354ms]
Jun 11 08:04:13.420: INFO: Created: latency-svc-g2jmt
Jun 11 08:04:13.458: INFO: Got endpoints: latency-svc-9hxsg [749.828677ms]
Jun 11 08:04:13.471: INFO: Created: latency-svc-66mx6
Jun 11 08:04:13.508: INFO: Got endpoints: latency-svc-9fjpn [749.223227ms]
Jun 11 08:04:13.519: INFO: Created: latency-svc-m9jh4
Jun 11 08:04:13.558: INFO: Got endpoints: latency-svc-9qfkw [749.652277ms]
Jun 11 08:04:13.570: INFO: Created: latency-svc-rggm4
Jun 11 08:04:13.609: INFO: Got endpoints: latency-svc-4sk5h [750.959777ms]
Jun 11 08:04:13.621: INFO: Created: latency-svc-wcnbv
Jun 11 08:04:13.658: INFO: Got endpoints: latency-svc-2ffn8 [749.193352ms]
Jun 11 08:04:13.670: INFO: Created: latency-svc-nq4lv
Jun 11 08:04:13.708: INFO: Got endpoints: latency-svc-tql64 [749.513354ms]
Jun 11 08:04:13.728: INFO: Created: latency-svc-j2znb
Jun 11 08:04:13.758: INFO: Got endpoints: latency-svc-dxkpn [749.370815ms]
Jun 11 08:04:13.770: INFO: Created: latency-svc-rb7n9
Jun 11 08:04:13.808: INFO: Got endpoints: latency-svc-4c2mk [749.798264ms]
Jun 11 08:04:13.821: INFO: Created: latency-svc-cqvvq
Jun 11 08:04:13.858: INFO: Got endpoints: latency-svc-dvpzc [750.005866ms]
Jun 11 08:04:13.871: INFO: Created: latency-svc-ngd68
Jun 11 08:04:13.908: INFO: Got endpoints: latency-svc-g74jb [750.044729ms]
Jun 11 08:04:13.921: INFO: Created: latency-svc-rpwb4
Jun 11 08:04:13.958: INFO: Got endpoints: latency-svc-ccn4n [750.177806ms]
Jun 11 08:04:13.970: INFO: Created: latency-svc-65vcg
Jun 11 08:04:14.009: INFO: Got endpoints: latency-svc-klndw [751.1137ms]
Jun 11 08:04:14.021: INFO: Created: latency-svc-5j8jw
Jun 11 08:04:14.059: INFO: Got endpoints: latency-svc-sr2lr [750.473707ms]
Jun 11 08:04:14.071: INFO: Created: latency-svc-tqh8t
Jun 11 08:04:14.108: INFO: Got endpoints: latency-svc-hxp8w [750.253248ms]
Jun 11 08:04:14.122: INFO: Created: latency-svc-wkjqm
Jun 11 08:04:14.158: INFO: Got endpoints: latency-svc-g2jmt [750.234718ms]
Jun 11 08:04:14.170: INFO: Created: latency-svc-pc4v6
Jun 11 08:04:14.208: INFO: Got endpoints: latency-svc-66mx6 [750.041745ms]
Jun 11 08:04:14.221: INFO: Created: latency-svc-vmf7t
Jun 11 08:04:14.258: INFO: Got endpoints: latency-svc-m9jh4 [750.234668ms]
Jun 11 08:04:14.271: INFO: Created: latency-svc-kvcdg
Jun 11 08:04:14.308: INFO: Got endpoints: latency-svc-rggm4 [750.310347ms]
Jun 11 08:04:14.322: INFO: Created: latency-svc-tp8b5
Jun 11 08:04:14.359: INFO: Got endpoints: latency-svc-wcnbv [750.30181ms]
Jun 11 08:04:14.373: INFO: Created: latency-svc-s5zzj
Jun 11 08:04:14.408: INFO: Got endpoints: latency-svc-nq4lv [750.381679ms]
Jun 11 08:04:14.421: INFO: Created: latency-svc-4qlxh
Jun 11 08:04:14.458: INFO: Got endpoints: latency-svc-j2znb [749.886954ms]
Jun 11 08:04:14.472: INFO: Created: latency-svc-6j5td
Jun 11 08:04:14.509: INFO: Got endpoints: latency-svc-rb7n9 [750.758816ms]
Jun 11 08:04:14.522: INFO: Created: latency-svc-p2hb4
Jun 11 08:04:14.559: INFO: Got endpoints: latency-svc-cqvvq [750.624721ms]
Jun 11 08:04:14.571: INFO: Created: latency-svc-rvwzp
Jun 11 08:04:14.608: INFO: Got endpoints: latency-svc-ngd68 [750.080415ms]
Jun 11 08:04:14.623: INFO: Created: latency-svc-p9zjt
Jun 11 08:04:14.659: INFO: Got endpoints: latency-svc-rpwb4 [750.388951ms]
Jun 11 08:04:14.671: INFO: Created: latency-svc-2jstd
Jun 11 08:04:14.709: INFO: Got endpoints: latency-svc-65vcg [750.641419ms]
Jun 11 08:04:14.721: INFO: Created: latency-svc-hc8tz
Jun 11 08:04:14.758: INFO: Got endpoints: latency-svc-5j8jw [749.553566ms]
Jun 11 08:04:14.772: INFO: Created: latency-svc-swkhw
Jun 11 08:04:14.809: INFO: Got endpoints: latency-svc-tqh8t [750.044541ms]
Jun 11 08:04:14.822: INFO: Created: latency-svc-59bdb
Jun 11 08:04:14.858: INFO: Got endpoints: latency-svc-wkjqm [749.24614ms]
Jun 11 08:04:14.871: INFO: Created: latency-svc-zrdpk
Jun 11 08:04:14.908: INFO: Got endpoints: latency-svc-pc4v6 [750.038075ms]
Jun 11 08:04:14.921: INFO: Created: latency-svc-mlfs9
Jun 11 08:04:14.959: INFO: Got endpoints: latency-svc-vmf7t [750.595929ms]
Jun 11 08:04:14.971: INFO: Created: latency-svc-zppd4
Jun 11 08:04:15.010: INFO: Got endpoints: latency-svc-kvcdg [752.511754ms]
Jun 11 08:04:15.024: INFO: Created: latency-svc-nfmv8
Jun 11 08:04:15.058: INFO: Got endpoints: latency-svc-tp8b5 [749.749096ms]
Jun 11 08:04:15.071: INFO: Created: latency-svc-mfv9g
Jun 11 08:04:15.109: INFO: Got endpoints: latency-svc-s5zzj [750.336163ms]
Jun 11 08:04:15.122: INFO: Created: latency-svc-mz5j4
Jun 11 08:04:15.158: INFO: Got endpoints: latency-svc-4qlxh [750.140449ms]
Jun 11 08:04:15.171: INFO: Created: latency-svc-5fvq8
Jun 11 08:04:15.208: INFO: Got endpoints: latency-svc-6j5td [749.193952ms]
Jun 11 08:04:15.220: INFO: Created: latency-svc-7hm88
Jun 11 08:04:15.258: INFO: Got endpoints: latency-svc-p2hb4 [749.463684ms]
Jun 11 08:04:15.270: INFO: Created: latency-svc-whssj
Jun 11 08:04:15.308: INFO: Got endpoints: latency-svc-rvwzp [749.438458ms]
Jun 11 08:04:15.322: INFO: Created: latency-svc-ttwkz
Jun 11 08:04:15.361: INFO: Got endpoints: latency-svc-p9zjt [752.962523ms]
Jun 11 08:04:15.376: INFO: Created: latency-svc-7hg42
Jun 11 08:04:15.408: INFO: Got endpoints: latency-svc-2jstd [749.070598ms]
Jun 11 08:04:15.420: INFO: Created: latency-svc-2b65p
Jun 11 08:04:15.458: INFO: Got endpoints: latency-svc-hc8tz [748.963051ms]
Jun 11 08:04:15.470: INFO: Created: latency-svc-vg9b4
Jun 11 08:04:15.508: INFO: Got endpoints: latency-svc-swkhw [750.050814ms]
Jun 11 08:04:15.520: INFO: Created: latency-svc-px7bh
Jun 11 08:04:15.558: INFO: Got endpoints: latency-svc-59bdb [749.31568ms]
Jun 11 08:04:15.570: INFO: Created: latency-svc-lfdrx
Jun 11 08:04:15.608: INFO: Got endpoints: latency-svc-zrdpk [749.904587ms]
Jun 11 08:04:15.620: INFO: Created: latency-svc-qd7x4
Jun 11 08:04:15.658: INFO: Got endpoints: latency-svc-mlfs9 [749.54216ms]
Jun 11 08:04:15.670: INFO: Created: latency-svc-pq8xs
Jun 11 08:04:15.708: INFO: Got endpoints: latency-svc-zppd4 [749.166876ms]
Jun 11 08:04:15.729: INFO: Created: latency-svc-4cnkv
Jun 11 08:04:15.808: INFO: Got endpoints: latency-svc-nfmv8 [797.324768ms]
Jun 11 08:04:15.822: INFO: Created: latency-svc-2fsjg
Jun 11 08:04:15.858: INFO: Got endpoints: latency-svc-mfv9g [799.852144ms]
Jun 11 08:04:15.871: INFO: Created: latency-svc-cnnpr
Jun 11 08:04:15.909: INFO: Got endpoints: latency-svc-mz5j4 [799.21903ms]
Jun 11 08:04:15.922: INFO: Created: latency-svc-85qt8
Jun 11 08:04:15.958: INFO: Got endpoints: latency-svc-5fvq8 [799.820098ms]
Jun 11 08:04:15.971: INFO: Created: latency-svc-ttr7n
Jun 11 08:04:16.008: INFO: Got endpoints: latency-svc-7hm88 [800.806866ms]
Jun 11 08:04:16.021: INFO: Created: latency-svc-k6c55
Jun 11 08:04:16.058: INFO: Got endpoints: latency-svc-whssj [799.789066ms]
Jun 11 08:04:16.069: INFO: Created: latency-svc-bnwsk
Jun 11 08:04:16.108: INFO: Got endpoints: latency-svc-ttwkz [800.219592ms]
Jun 11 08:04:16.121: INFO: Created: latency-svc-j6tbc
Jun 11 08:04:16.158: INFO: Got endpoints: latency-svc-7hg42 [796.646596ms]
Jun 11 08:04:16.171: INFO: Created: latency-svc-gdfm2
Jun 11 08:04:16.208: INFO: Got endpoints: latency-svc-2b65p [800.287371ms]
Jun 11 08:04:16.220: INFO: Created: latency-svc-lnj7s
Jun 11 08:04:16.258: INFO: Got endpoints: latency-svc-vg9b4 [800.460264ms]
Jun 11 08:04:16.270: INFO: Created: latency-svc-mlrg7
Jun 11 08:04:16.308: INFO: Got endpoints: latency-svc-px7bh [799.688183ms]
Jun 11 08:04:16.321: INFO: Created: latency-svc-pbhqd
Jun 11 08:04:16.358: INFO: Got endpoints: latency-svc-lfdrx [800.469163ms]
Jun 11 08:04:16.372: INFO: Created: latency-svc-ft45h
Jun 11 08:04:16.409: INFO: Got endpoints: latency-svc-qd7x4 [800.83371ms]
Jun 11 08:04:16.458: INFO: Got endpoints: latency-svc-pq8xs [800.580814ms]
Jun 11 08:04:16.509: INFO: Got endpoints: latency-svc-4cnkv [800.717122ms]
Jun 11 08:04:16.558: INFO: Got endpoints: latency-svc-2fsjg [750.47181ms]
Jun 11 08:04:16.608: INFO: Got endpoints: latency-svc-cnnpr [749.915191ms]
Jun 11 08:04:16.658: INFO: Got endpoints: latency-svc-85qt8 [749.238533ms]
Jun 11 08:04:16.708: INFO: Got endpoints: latency-svc-ttr7n [749.410498ms]
Jun 11 08:04:16.758: INFO: Got endpoints: latency-svc-k6c55 [749.778868ms]
Jun 11 08:04:16.808: INFO: Got endpoints: latency-svc-bnwsk [749.842515ms]
Jun 11 08:04:16.858: INFO: Got endpoints: latency-svc-j6tbc [749.219706ms]
Jun 11 08:04:16.909: INFO: Got endpoints: latency-svc-gdfm2 [750.559526ms]
Jun 11 08:04:16.959: INFO: Got endpoints: latency-svc-lnj7s [750.704835ms]
Jun 11 08:04:17.008: INFO: Got endpoints: latency-svc-mlrg7 [749.655821ms]
Jun 11 08:04:17.058: INFO: Got endpoints: latency-svc-pbhqd [750.419516ms]
Jun 11 08:04:17.108: INFO: Got endpoints: latency-svc-ft45h [749.78722ms]
Jun 11 08:04:17.108: INFO: Latencies: [18.632432ms 27.258825ms 36.750623ms 45.933446ms 55.192144ms 61.796392ms 70.716633ms 77.644389ms 89.591973ms 98.86904ms 108.252124ms 116.847179ms 125.398208ms 130.862668ms 131.690736ms 131.751411ms 132.080582ms 132.991436ms 134.035889ms 135.447809ms 137.95566ms 138.271718ms 138.901819ms 139.047701ms 139.268926ms 139.563194ms 139.67602ms 139.948463ms 140.171462ms 140.283657ms 140.379411ms 141.187082ms 141.273445ms 143.473535ms 144.087754ms 146.547068ms 183.888237ms 227.297084ms 268.082557ms 309.887545ms 350.7631ms 391.47428ms 434.716997ms 476.899345ms 533.704972ms 566.604424ms 600.60944ms 642.436023ms 682.025709ms 724.331157ms 735.228476ms 742.066522ms 748.259309ms 748.598687ms 748.911258ms 748.917082ms 748.953414ms 748.963051ms 748.966318ms 749.027994ms 749.070598ms 749.080453ms 749.166876ms 749.193352ms 749.193952ms 749.219706ms 749.220239ms 749.223227ms 749.238533ms 749.24614ms 749.31568ms 749.325422ms 749.370815ms 749.384469ms 749.410498ms 749.434317ms 749.437498ms 749.438458ms 749.456487ms 749.463684ms 749.465189ms 749.506787ms 749.513354ms 749.54216ms 749.549522ms 749.553566ms 749.568811ms 749.587032ms 749.60552ms 749.60609ms 749.628711ms 749.652277ms 749.655821ms 749.69372ms 749.706112ms 749.709719ms 749.749096ms 749.777763ms 749.778868ms 749.78722ms 749.794819ms 749.798264ms 749.828677ms 749.840739ms 749.842515ms 749.846065ms 749.869067ms 749.886954ms 749.904587ms 749.915191ms 749.920164ms 749.936957ms 749.944564ms 749.952707ms 749.962354ms 749.963409ms 749.966478ms 749.982615ms 749.988597ms 750.005866ms 750.024828ms 750.038075ms 750.041745ms 750.044541ms 750.044729ms 750.048771ms 750.050814ms 750.067827ms 750.080415ms 750.098423ms 750.136521ms 750.140449ms 750.147491ms 750.160043ms 750.170654ms 750.170976ms 750.177806ms 750.190274ms 750.220876ms 750.22359ms 750.231076ms 750.234668ms 750.234718ms 750.249965ms 750.253248ms 750.262418ms 750.30181ms 750.310347ms 750.336163ms 750.340567ms 750.34352ms 750.352017ms 750.363715ms 750.381679ms 750.388951ms 750.404948ms 750.419516ms 750.42902ms 750.47181ms 750.473707ms 750.476606ms 750.491697ms 750.506569ms 750.539869ms 750.559526ms 750.574954ms 750.594136ms 750.595929ms 750.596804ms 750.624721ms 750.641419ms 750.654353ms 750.704835ms 750.758816ms 750.849434ms 750.90156ms 750.959633ms 750.959777ms 751.030635ms 751.045278ms 751.1137ms 751.737774ms 751.775777ms 752.511754ms 752.962523ms 796.646596ms 797.324768ms 799.21903ms 799.688183ms 799.789066ms 799.820098ms 799.852144ms 800.219592ms 800.287371ms 800.460264ms 800.469163ms 800.580814ms 800.717122ms 800.806866ms 800.83371ms]
Jun 11 08:04:17.108: INFO: 50 %ile: 749.794819ms
Jun 11 08:04:17.109: INFO: 90 %ile: 751.1137ms
Jun 11 08:04:17.109: INFO: 99 %ile: 800.806866ms
Jun 11 08:04:17.109: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:04:17.109: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "svc-latency-9173" for this suite.

• [SLOW TEST:9.956 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":280,"completed":92,"skipped":1379,"failed":0}
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:04:17.131: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4563
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:04:33.390: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "resourcequota-4563" for this suite.

• [SLOW TEST:16.275 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":280,"completed":93,"skipped":1379,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:04:33.406: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2502
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 08:04:33.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 create -f - --namespace=kubectl-2502'
Jun 11 08:04:33.868: INFO: stderr: ""
Jun 11 08:04:33.868: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Jun 11 08:04:33.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 create -f - --namespace=kubectl-2502'
Jun 11 08:04:34.082: INFO: stderr: ""
Jun 11 08:04:34.082: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Jun 11 08:04:35.087: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 11 08:04:35.087: INFO: Found 0 / 1
Jun 11 08:04:36.087: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 11 08:04:36.087: INFO: Found 1 / 1
Jun 11 08:04:36.087: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 11 08:04:36.090: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 11 08:04:36.090: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 11 08:04:36.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 describe pod agnhost-master-w5gxx --namespace=kubectl-2502'
Jun 11 08:04:36.234: INFO: stderr: ""
Jun 11 08:04:36.234: INFO: stdout: "Name:         agnhost-master-w5gxx\nNamespace:    kubectl-2502\nPriority:     0\nNode:         ip-10-0-136-38.us-west-2.compute.internal/10.0.136.38\nStart Time:   Thu, 11 Jun 2020 08:04:33 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 192.168.42.30/32\n              cni.projectcalico.org/podIPs: 192.168.42.30/32\nStatus:       Running\nIP:           192.168.42.30\nIPs:\n  IP:           192.168.42.30\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   containerd://881a1954bf3c76cbfaae03dbdbb7e3c58a5a8d5c7611b31d22b3291284427358\n    Image:          gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Image ID:       gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 11 Jun 2020 08:04:34 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-hhb46 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-hhb46:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-hhb46\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                                                Message\n  ----    ------     ----       ----                                                -------\n  Normal  Scheduled  <unknown>  default-scheduler                                   Successfully assigned kubectl-2502/agnhost-master-w5gxx to ip-10-0-136-38.us-west-2.compute.internal\n  Normal  Pulling    2s         kubelet, ip-10-0-136-38.us-west-2.compute.internal  Pulling image \"gcr.io/kubernetes-e2e-test-images/agnhost:2.8\"\n  Normal  Pulled     2s         kubelet, ip-10-0-136-38.us-west-2.compute.internal  Successfully pulled image \"gcr.io/kubernetes-e2e-test-images/agnhost:2.8\"\n  Normal  Created    2s         kubelet, ip-10-0-136-38.us-west-2.compute.internal  Created container agnhost-master\n  Normal  Started    2s         kubelet, ip-10-0-136-38.us-west-2.compute.internal  Started container agnhost-master\n"
Jun 11 08:04:36.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 describe rc agnhost-master --namespace=kubectl-2502'
Jun 11 08:04:36.383: INFO: stderr: ""
Jun 11 08:04:36.383: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-2502\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-master-w5gxx\n"
Jun 11 08:04:36.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 describe service agnhost-master --namespace=kubectl-2502'
Jun 11 08:04:36.525: INFO: stderr: ""
Jun 11 08:04:36.525: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-2502\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                10.0.55.154\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.42.30:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jun 11 08:04:36.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 describe node ip-10-0-128-119.us-west-2.compute.internal'
Jun 11 08:04:36.713: INFO: stderr: ""
Jun 11 08:04:36.713: INFO: stdout: "Name:               ip-10-0-128-119.us-west-2.compute.internal\nRoles:              <none>\nLabels:             autoscaling.k8s.io/nodegroup=worker\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m5.xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-west-2\n                    failure-domain.beta.kubernetes.io/zone=us-west-2a\n                    konvoy.mesosphere.com/inventory_hostname=10.0.128.119\n                    konvoy.mesosphere.com/node_pool=worker\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-128-119.us-west-2.compute.internal\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=m5.xlarge\n                    topology.ebs.csi.aws.com/zone=us-west-2a\n                    topology.kubernetes.io/region=us-west-2\n                    topology.kubernetes.io/zone=us-west-2a\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"ebs.csi.aws.com\":\"i-06298ef26098faf0e\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.128.119/32\n                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.28.192\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 11 Jun 2020 05:26:19 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-0-128-119.us-west-2.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 11 Jun 2020 08:04:35 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 11 Jun 2020 05:27:30 +0000   Thu, 11 Jun 2020 05:27:30 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 11 Jun 2020 08:02:13 +0000   Thu, 11 Jun 2020 05:26:19 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 11 Jun 2020 08:02:13 +0000   Thu, 11 Jun 2020 05:26:19 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 11 Jun 2020 08:02:13 +0000   Thu, 11 Jun 2020 05:26:19 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 11 Jun 2020 08:02:13 +0000   Thu, 11 Jun 2020 05:27:29 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.0.128.119\n  ExternalIP:   52.39.222.66\n  Hostname:     ip-10-0-128-119.us-west-2.compute.internal\n  InternalDNS:  ip-10-0-128-119.us-west-2.compute.internal\n  ExternalDNS:  ec2-52-39-222-66.us-west-2.compute.amazonaws.com\nCapacity:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         4\n  ephemeral-storage:           83874796Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      15791212Ki\n  pods:                        110\nAllocatable:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         4\n  ephemeral-storage:           77299011866\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      15688812Ki\n  pods:                        110\nSystem Info:\n  Machine ID:                 3d5c05376530a2eb49e3e90576f83c5b\n  System UUID:                EC221487-FB3E-4AFE-7643-FFB57EE19F43\n  Boot ID:                    a9148782-8cc5-40f7-9dcc-0f8c46395690\n  Kernel Version:             3.10.0-1062.12.1.el7.x86_64\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.3.4\n  Kubelet Version:            v1.17.6\n  Kube-Proxy Version:         v1.17.6\nProviderID:                   aws:///us-west-2a/i-06298ef26098faf0e\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  cert-manager                cert-manager-kubeaddons-cainjector-6dcd94769b-v8x5p        0 (0%)        0 (0%)      0 (0%)           0 (0%)         155m\n  kommander                   kubefed-controller-manager-78b769f688-8mjvz                100m (2%)     500m (12%)  64Mi (0%)        128Mi (0%)     150m\n  kommander                   yakcl-licensing-cm-7c5cc586b5-xgtnw                        500m (12%)    500m (12%)  128Mi (0%)       256Mi (1%)     151m\n  kube-system                 calico-node-wxzzc                                          300m (7%)     0 (0%)      32M (0%)         0 (0%)         157m\n  kube-system                 ebs-csi-node-2w524                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         153m\n  kube-system                 kube-proxy-b8rs5                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         158m\n  kubeaddons                  elasticsearch-kubeaddons-client-6c56cc5c7-kpxr5            1500m (37%)   2 (50%)     10000Mi (65%)    12000Mi (78%)  153m\n  kubeaddons                  fluentbit-kubeaddons-fluent-bit-hfm9r                      200m (5%)     0 (0%)      200Mi (1%)       750Mi (4%)     148m\n  kubeaddons                  prometheus-kubeaddons-prometheus-node-exporter-fz2j2       0 (0%)        0 (0%)      0 (0%)           0 (0%)         152m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-594c2    0 (0%)        0 (0%)      0 (0%)           0 (0%)         23m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests          Limits\n  --------                    --------          ------\n  cpu                         2600m (65%)       3 (75%)\n  memory                      10672658Ki (68%)  13134Mi (85%)\n  ephemeral-storage           0 (0%)            0 (0%)\n  attachable-volumes-aws-ebs  0                 0\nEvents:                       <none>\n"
Jun 11 08:04:36.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 describe namespace kubectl-2502'
Jun 11 08:04:36.859: INFO: stderr: ""
Jun 11 08:04:36.859: INFO: stdout: "Name:         kubectl-2502\nLabels:       e2e-framework=kubectl\n              e2e-run=11151d3c-36a3-4297-b0b0-de0dedcd1ebd\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:04:36.859: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "kubectl-2502" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":280,"completed":94,"skipped":1384,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:04:36.877: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5228
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:04:44.042: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "resourcequota-5228" for this suite.

• [SLOW TEST:7.180 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":280,"completed":95,"skipped":1434,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:04:44.057: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2803
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 08:04:44.206: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:04:44.746: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2803" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":280,"completed":96,"skipped":1461,"failed":0}
SS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:04:44.763: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-589
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 08:04:44.969: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jun 11 08:04:44.979: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 11 08:04:49.984: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 11 08:04:49.984: INFO: Creating deployment "test-rolling-update-deployment"
Jun 11 08:04:49.990: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun 11 08:04:49.999: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jun 11 08:04:52.008: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun 11 08:04:52.011: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jun 11 08:04:52.023: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-589 /apis/apps/v1/namespaces/deployment-589/deployments/test-rolling-update-deployment c28058f9-1e61-4a05-ad77-be32b1e5e120 113358 1 2020-06-11 08:04:49 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc007cc42d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-06-11 08:04:50 +0000 UTC,LastTransitionTime:2020-06-11 08:04:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67cf4f6444" has successfully progressed.,LastUpdateTime:2020-06-11 08:04:51 +0000 UTC,LastTransitionTime:2020-06-11 08:04:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 11 08:04:52.027: INFO: New ReplicaSet "test-rolling-update-deployment-67cf4f6444" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67cf4f6444  deployment-589 /apis/apps/v1/namespaces/deployment-589/replicasets/test-rolling-update-deployment-67cf4f6444 94bb2096-9704-4079-8bb4-eff1f7603aae 113347 1 2020-06-11 08:04:50 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment c28058f9-1e61-4a05-ad77-be32b1e5e120 0xc007cc47a7 0xc007cc47a8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67cf4f6444,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc007cc4818 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 11 08:04:52.027: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun 11 08:04:52.027: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-589 /apis/apps/v1/namespaces/deployment-589/replicasets/test-rolling-update-controller b181e617-1607-48e8-8d4e-26cd1458ef2a 113356 2 2020-06-11 08:04:44 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment c28058f9-1e61-4a05-ad77-be32b1e5e120 0xc007cc46d7 0xc007cc46d8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc007cc4738 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 11 08:04:52.031: INFO: Pod "test-rolling-update-deployment-67cf4f6444-nvpkl" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67cf4f6444-nvpkl test-rolling-update-deployment-67cf4f6444- deployment-589 /api/v1/namespaces/deployment-589/pods/test-rolling-update-deployment-67cf4f6444-nvpkl fb144b05-8e2b-428e-8374-65bf440479a0 113346 0 2020-06-11 08:04:50 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[cni.projectcalico.org/podIP:192.168.42.31/32 cni.projectcalico.org/podIPs:192.168.42.31/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-67cf4f6444 94bb2096-9704-4079-8bb4-eff1f7603aae 0xc007cc4cb7 0xc007cc4cb8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cdg22,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cdg22,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cdg22,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-136-38.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 08:04:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 08:04:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 08:04:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 08:04:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.136.38,PodIP:192.168.42.31,StartTime:2020-06-11 08:04:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-11 08:04:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:containerd://32af99a731d2329b1837d09892dd2d3d5916c4f1557e8550e183cc7e412e3ce5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.42.31,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:04:52.031: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "deployment-589" for this suite.

• [SLOW TEST:7.283 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":97,"skipped":1463,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:04:52.047: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1890
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jun 11 08:04:52.195: INFO: Waiting up to 1m0s for all (but 7) nodes to be ready
Jun 11 08:04:52.219: INFO: Waiting for terminating namespaces to be deleted...
Jun 11 08:04:52.224: INFO:
Logging pods the kubelet thinks is on node ip-10-0-128-119.us-west-2.compute.internal before test
Jun 11 08:04:52.244: INFO: ebs-csi-node-2w524 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:04:52.244: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:04:52.244: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:04:52.244: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:04:52.244: INFO: prometheus-kubeaddons-prometheus-node-exporter-fz2j2 from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.244: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:04:52.244: INFO: kubefed-controller-manager-78b769f688-8mjvz from kommander started at 2020-06-11 05:33:45 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.244: INFO: 	Container controller-manager ready: true, restart count 0
Jun 11 08:04:52.244: INFO: fluentbit-kubeaddons-fluent-bit-hfm9r from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.244: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:04:52.244: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-594c2 from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.244: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:04:52.244: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:04:52.244: INFO: kube-proxy-b8rs5 from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.244: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:04:52.245: INFO: calico-node-wxzzc from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.245: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:04:52.245: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:04:52.245: INFO: cert-manager-kubeaddons-cainjector-6dcd94769b-v8x5p from cert-manager started at 2020-06-11 05:29:09 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.245: INFO: 	Container cainjector ready: true, restart count 0
Jun 11 08:04:52.245: INFO: elasticsearch-kubeaddons-client-6c56cc5c7-kpxr5 from kubeaddons started at 2020-06-11 05:30:43 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.245: INFO: 	Container elasticsearch ready: true, restart count 0
Jun 11 08:04:52.245: INFO: yakcl-licensing-cm-7c5cc586b5-xgtnw from kommander started at 2020-06-11 05:33:34 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.245: INFO: 	Container controller-manager ready: true, restart count 0
Jun 11 08:04:52.245: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 11 08:04:52.245: INFO:
Logging pods the kubelet thinks is on node ip-10-0-129-105.us-west-2.compute.internal before test
Jun 11 08:04:52.264: INFO: calico-node-tnwfg from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.264: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:04:52.264: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:04:52.264: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-cxrgp from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.264: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:04:52.264: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:04:52.264: INFO: prometheusadapter-kubeaddons-prometheus-adapter-77bc665f9-sjxp8 from kubeaddons started at 2020-06-11 05:33:06 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.264: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 11 08:04:52.264: INFO: opsportal-kubeaddons-kommander-ui-689b97997f-x45gn from kubeaddons started at 2020-06-11 05:28:52 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.264: INFO: 	Container opsportal-kubeaddons-kommander-ui ready: true, restart count 0
Jun 11 08:04:52.264: INFO: kube-proxy-7vnlr from kube-system started at 2020-06-11 05:26:20 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.264: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:04:52.264: INFO: ebs-csi-node-jzjgw from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:04:52.264: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:04:52.264: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:04:52.264: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:04:52.264: INFO: kube-oidc-proxy-kubeaddons-545d6df8-w9v44 from kubeaddons started at 2020-06-11 05:31:42 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.264: INFO: 	Container kube-oidc-proxy ready: true, restart count 0
Jun 11 08:04:52.264: INFO: prometheus-kubeaddons-grafana-c8f7fcdb5-xjw4p from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.264: INFO: 	Container grafana ready: true, restart count 0
Jun 11 08:04:52.264: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Jun 11 08:04:52.264: INFO: prometheus-kubeaddons-prometheus-node-exporter-zl5mt from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.264: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:04:52.264: INFO: fluentbit-kubeaddons-fluent-bit-nbsn6 from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.264: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:04:52.264: INFO:
Logging pods the kubelet thinks is on node ip-10-0-129-30.us-west-2.compute.internal before test
Jun 11 08:04:52.284: INFO: dstorageclass-controller-manager-5c966c767f-h5rsz from kubeaddons started at 2020-06-11 05:29:47 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.284: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 11 08:04:52.284: INFO: 	Container manager ready: true, restart count 0
Jun 11 08:04:52.284: INFO: prometheus-kubeaddons-prometheus-node-exporter-dwrbk from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.284: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:04:52.284: INFO: kommander-kubeaddons-cost-analyzer-7cfd4cb9cd-6tfnq from kommander started at 2020-06-11 05:33:41 +0000 UTC (3 container statuses recorded)
Jun 11 08:04:52.284: INFO: 	Container cost-analyzer-frontend ready: true, restart count 0
Jun 11 08:04:52.284: INFO: 	Container cost-analyzer-server ready: true, restart count 0
Jun 11 08:04:52.284: INFO: 	Container cost-model ready: true, restart count 0
Jun 11 08:04:52.284: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-5wq4g from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.284: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:04:52.284: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:04:52.284: INFO: ebs-csi-node-s8248 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:04:52.284: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:04:52.284: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:04:52.284: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:04:52.284: INFO: traefik-forward-auth-kubeaddons-6675968b94-fjznl from kubeaddons started at 2020-06-11 05:37:16 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.284: INFO: 	Container traefik-forward-auth ready: true, restart count 1
Jun 11 08:04:52.284: INFO: kommander-kubeaddons-karma-8df6cfdb6-4jg59 from kommander started at 2020-06-11 05:33:33 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.284: INFO: 	Container karma ready: true, restart count 0
Jun 11 08:04:52.284: INFO: kubernetes-dashboard-549989bcdf-n9ncj from kubeaddons started at 2020-06-11 07:15:29 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.284: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jun 11 08:04:52.284: INFO: fluentbit-kubeaddons-fluent-bit-mpwz7 from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.284: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:04:52.284: INFO: kommander-kubeaddons-kommander-ui-86b4f5f88f-pvgsw from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.284: INFO: 	Container kommander-kubeaddons-kommander-ui ready: true, restart count 0
Jun 11 08:04:52.284: INFO: kube-proxy-td2b5 from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.284: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:04:52.284: INFO: calico-node-vpvmh from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.284: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:04:52.284: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:04:52.284: INFO: tiller-deploy-6cdf7f9d6f-d6b2p from kube-system started at 2020-06-11 05:28:13 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.284: INFO: 	Container tiller ready: true, restart count 0
Jun 11 08:04:52.284: INFO: minio-1 from velero started at 2020-06-11 05:32:15 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.284: INFO: 	Container minio ready: true, restart count 0
Jun 11 08:04:52.284: INFO: kommander-kubecost-thanos-query-79fc7d8747-fm6rq from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.284: INFO: 	Container thanos-query ready: true, restart count 0
Jun 11 08:04:52.284: INFO: kommander-kubeaddons-kube-state-metrics-6d77646c89-bvrqg from kommander started at 2020-06-11 05:33:33 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.284: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 11 08:04:52.284: INFO:
Logging pods the kubelet thinks is on node ip-10-0-130-174.us-west-2.compute.internal before test
Jun 11 08:04:52.304: INFO: kibana-kubeaddons-c8c7b687-4lqrx from kubeaddons started at 2020-06-11 07:15:29 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.304: INFO: 	Container initialize-kibana-index ready: true, restart count 0
Jun 11 08:04:52.304: INFO: 	Container kibana ready: true, restart count 0
Jun 11 08:04:52.304: INFO: calico-node-hrkxr from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.304: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:04:52.304: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:04:52.304: INFO: kubefed-admission-webhook-7b4997895b-x5h84 from kommander started at 2020-06-11 05:33:38 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.304: INFO: 	Container admission-webhook ready: true, restart count 0
Jun 11 08:04:52.304: INFO: kommander-kubeaddons-prometheus-alertmanager-7cdccf8c44-h9grs from kommander started at 2020-06-11 05:33:38 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.304: INFO: 	Container prometheus-alertmanager ready: true, restart count 0
Jun 11 08:04:52.304: INFO: 	Container prometheus-alertmanager-configmap-reload ready: true, restart count 0
Jun 11 08:04:52.304: INFO: fluentbit-kubeaddons-fluent-bit-xx74f from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.304: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:04:52.304: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-zvgtl from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.304: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:04:52.304: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:04:52.304: INFO: cert-manager-kubeaddons-webhook-77fbc6d59b-d67w5 from cert-manager started at 2020-06-11 05:29:09 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.304: INFO: 	Container cert-manager ready: true, restart count 1
Jun 11 08:04:52.304: INFO: ebs-csi-node-bfvm8 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:04:52.304: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:04:52.304: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:04:52.304: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:04:52.304: INFO: kommander-kubeaddons-grafana-66c558d6f5-ntspl from kommander started at 2020-06-11 05:33:32 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.304: INFO: 	Container grafana ready: true, restart count 0
Jun 11 08:04:52.304: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Jun 11 08:04:52.304: INFO: kube-proxy-6htkf from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.304: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:04:52.304: INFO: prometheus-kubeaddons-prometheus-node-exporter-mzcfk from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.304: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:04:52.304: INFO: yakcl-licensing-webhook-6d876f844d-mrvm5 from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.304: INFO: 	Container webhook ready: true, restart count 0
Jun 11 08:04:52.304: INFO: ebs-csi-controller-0 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (6 container statuses recorded)
Jun 11 08:04:52.304: INFO: 	Container csi-attacher ready: true, restart count 0
Jun 11 08:04:52.304: INFO: 	Container csi-provisioner ready: true, restart count 0
Jun 11 08:04:52.304: INFO: 	Container csi-resizer ready: true, restart count 0
Jun 11 08:04:52.304: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jun 11 08:04:52.304: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:04:52.304: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:04:52.304: INFO: kommander-kubeaddons-thanos-query-6cddb86b55-f4g4l from kommander started at 2020-06-11 05:33:35 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.304: INFO: 	Container thanos-query ready: true, restart count 0
Jun 11 08:04:52.304: INFO: elasticsearchexporter-kubeaddons-elasticsearch-exporter-84pdc2n from kubeaddons started at 2020-06-11 05:36:03 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.304: INFO: 	Container elasticsearch-exporter ready: true, restart count 0
Jun 11 08:04:52.304: INFO:
Logging pods the kubelet thinks is on node ip-10-0-132-48.us-west-2.compute.internal before test
Jun 11 08:04:52.319: INFO: ebs-csi-node-xzdsx from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:04:52.319: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:04:52.319: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:04:52.319: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:04:52.319: INFO: elasticsearch-kubeaddons-master-0 from kubeaddons started at 2020-06-11 05:31:03 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.319: INFO: 	Container elasticsearch ready: true, restart count 0
Jun 11 08:04:52.319: INFO: fluentbit-kubeaddons-fluent-bit-2wt5v from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.319: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:04:52.319: INFO: kube-proxy-q79z5 from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.319: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:04:52.319: INFO: cert-manager-kubeaddons-7d7f98fbc6-86r7x from cert-manager started at 2020-06-11 05:29:09 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.319: INFO: 	Container cert-manager ready: true, restart count 0
Jun 11 08:04:52.319: INFO: prometheus-kubeaddons-prometheus-node-exporter-cqrh7 from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.319: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:04:52.319: INFO: yakcl-federation-cm-55469d7b6b-f6xhf from kommander started at 2020-06-11 05:33:33 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.319: INFO: 	Container controller-manager ready: true, restart count 0
Jun 11 08:04:52.319: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 11 08:04:52.319: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-ft9g5 from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.319: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:04:52.319: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:04:52.319: INFO: calico-node-wr8kq from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.319: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:04:52.319: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:04:52.319: INFO:
Logging pods the kubelet thinks is on node ip-10-0-132-52.us-west-2.compute.internal before test
Jun 11 08:04:52.336: INFO: traefik-kubeaddons-1.72.19-zdq6l from kubeaddons started at 2020-06-11 05:29:49 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.336: INFO: 	Container traefik ready: false, restart count 0
Jun 11 08:04:52.336: INFO: ebs-csi-node-cf4v4 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:04:52.336: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:04:52.336: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:04:52.336: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:04:52.336: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-tssmj from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.336: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:04:52.336: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:04:52.336: INFO: kubeaddons-controller-manager-986f46689-sk6x8 from kubeaddons started at 2020-06-11 05:27:57 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.336: INFO: 	Container manager ready: true, restart count 0
Jun 11 08:04:52.336: INFO: prometheus-kubeaddons-prometheus-node-exporter-nff74 from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.336: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:04:52.336: INFO: elasticsearch-kubeaddons-client-6c56cc5c7-zp9fl from kubeaddons started at 2020-06-11 05:30:43 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.336: INFO: 	Container elasticsearch ready: true, restart count 0
Jun 11 08:04:52.336: INFO: fluentbit-kubeaddons-fluent-bit-78mbt from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.336: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:04:52.336: INFO: kube-proxy-w486c from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.336: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:04:52.336: INFO: calico-node-6mfwv from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.336: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:04:52.336: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:04:52.336: INFO: traefik-kubeaddons-64fbc79c9-54zfn from kubeaddons started at 2020-06-11 05:31:03 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.336: INFO: 	Container traefik-kubeaddons ready: true, restart count 0
Jun 11 08:04:52.336: INFO: kubefed-controller-manager-78b769f688-rmx64 from kommander started at 2020-06-11 05:33:38 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.336: INFO: 	Container controller-manager ready: true, restart count 0
Jun 11 08:04:52.336: INFO:
Logging pods the kubelet thinks is on node ip-10-0-135-119.us-west-2.compute.internal before test
Jun 11 08:04:52.355: INFO: dex-kubeaddons-dex-controller-7bd5fc575c-xmnlq from kubeaddons started at 2020-06-11 05:31:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.355: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 11 08:04:52.355: INFO: 	Container manager ready: true, restart count 0
Jun 11 08:04:52.356: INFO: velero-kubeaddons-5d85fcdcb9-gqb5c from velero started at 2020-06-11 05:32:08 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.356: INFO: 	Container velero ready: true, restart count 3
Jun 11 08:04:52.356: INFO: elasticsearch-kubeaddons-master-1 from kubeaddons started at 2020-06-11 05:32:29 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.356: INFO: 	Container elasticsearch ready: true, restart count 0
Jun 11 08:04:52.356: INFO: fluentbit-kubeaddons-fluent-bit-6lwhn from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.356: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:04:52.356: INFO: calico-node-sq7sb from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.356: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:04:52.356: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:04:52.356: INFO: external-dns-kubeaddons-765d55b455-vlxcv from kubeaddons started at 2020-06-11 05:28:25 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.356: INFO: 	Container external-dns ready: true, restart count 0
Jun 11 08:04:52.356: INFO: minio-2 from velero started at 2020-06-11 05:32:12 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.356: INFO: 	Container minio ready: true, restart count 0
Jun 11 08:04:52.356: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-b7drp from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.356: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:04:52.356: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:04:52.356: INFO: kube-proxy-xnz2c from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.356: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:04:52.356: INFO: prometheus-kubeaddons-prometheus-node-exporter-4fllq from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.356: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:04:52.356: INFO: alertmanager-prometheus-kubeaddons-prom-alertmanager-0 from kubeaddons started at 2020-06-11 05:32:06 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.356: INFO: 	Container alertmanager ready: true, restart count 0
Jun 11 08:04:52.356: INFO: 	Container config-reloader ready: true, restart count 0
Jun 11 08:04:52.356: INFO: ebs-csi-node-w9whr from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:04:52.356: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:04:52.356: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:04:52.356: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:04:52.356: INFO:
Logging pods the kubelet thinks is on node ip-10-0-136-38.us-west-2.compute.internal before test
Jun 11 08:04:52.370: INFO: fluentbit-kubeaddons-fluent-bit-ht88l from kubeaddons started at 2020-06-11 08:04:31 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.370: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:04:52.370: INFO: kube-proxy-dz75m from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.370: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:04:52.370: INFO: ebs-csi-node-8xl6p from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:04:52.370: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:04:52.370: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:04:52.370: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:04:52.370: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-lgn88 from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.370: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:04:52.370: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:04:52.370: INFO: cost-analyzer-checks-1591862400-tjx9g from kommander started at 2020-06-11 08:00:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.370: INFO: 	Container cost-analyzer-checks ready: false, restart count 0
Jun 11 08:04:52.370: INFO: prometheus-kubeaddons-prometheus-node-exporter-78vv8 from kubeaddons started at 2020-06-11 08:04:15 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.370: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:04:52.370: INFO: test-rolling-update-deployment-67cf4f6444-nvpkl from deployment-589 started at 2020-06-11 08:04:50 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.370: INFO: 	Container agnhost ready: true, restart count 0
Jun 11 08:04:52.370: INFO: calico-node-tzhxw from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.370: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:04:52.370: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:04:52.370: INFO: sonobuoy from sonobuoy started at 2020-06-11 07:40:59 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.370: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 11 08:04:52.370: INFO:
Logging pods the kubelet thinks is on node ip-10-0-137-210.us-west-2.compute.internal before test
Jun 11 08:04:52.387: INFO: ebs-csi-node-frsf2 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:04:52.387: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:04:52.387: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:04:52.387: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:04:52.387: INFO: prometheus-kubeaddons-prometheus-node-exporter-8z4lv from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.387: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:04:52.387: INFO: calico-node-vkrzv from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.387: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:04:52.387: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:04:52.387: INFO: gatekeeper-kubeaddons-776f4b5c96-jqsq8 from kubeaddons started at 2020-06-11 05:29:52 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.387: INFO: 	Container manager ready: true, restart count 0
Jun 11 08:04:52.387: INFO: elasticsearch-kubeaddons-master-2 from kubeaddons started at 2020-06-11 05:33:29 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.387: INFO: 	Container elasticsearch ready: true, restart count 0
Jun 11 08:04:52.387: INFO: fluentbit-kubeaddons-fluent-bit-hv9bs from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.387: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:04:52.387: INFO: dex-kubeaddons-696dbdb6c6-5prn9 from kubeaddons started at 2020-06-11 05:37:11 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.387: INFO: 	Container main ready: true, restart count 0
Jun 11 08:04:52.387: INFO: opsportal-landing-6f6865b688-6r5kn from kubeaddons started at 2020-06-11 05:28:52 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.387: INFO: 	Container opsportal-landing ready: true, restart count 0
Jun 11 08:04:52.387: INFO: minio-3 from velero started at 2020-06-11 05:32:15 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.387: INFO: 	Container minio ready: true, restart count 0
Jun 11 08:04:52.387: INFO: kube-proxy-qj7tx from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.387: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:04:52.387: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-6cvjn from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.387: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:04:52.387: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:04:52.387: INFO: prometheus-kubeaddons-kube-state-metrics-6599df558b-mmf9r from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.387: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 11 08:04:52.387: INFO:
Logging pods the kubelet thinks is on node ip-10-0-138-28.us-west-2.compute.internal before test
Jun 11 08:04:52.404: INFO: reloader-kubeaddons-reloader-7d4bd64cfb-4qjm4 from kubeaddons started at 2020-06-11 05:28:30 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.404: INFO: 	Container reloader-kubeaddons-reloader ready: true, restart count 0
Jun 11 08:04:52.404: INFO: kommander-kubeaddons-kubeaddons-catalog-6654f856df-kndt7 from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.404: INFO: 	Container kubeaddons-catalog ready: true, restart count 0
Jun 11 08:04:52.404: INFO: yakcl-federation-webhook-6fcc46596-smpfh from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.404: INFO: 	Container webhook ready: true, restart count 0
Jun 11 08:04:52.404: INFO: yakcl-federation-utility-apiserver-7d57699df9-wp7bs from kommander started at 2020-06-11 05:33:33 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.404: INFO: 	Container server ready: true, restart count 0
Jun 11 08:04:52.404: INFO: ebs-csi-snapshot-controller-0 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.404: INFO: 	Container ebs-csi-snapshot-controller ready: true, restart count 0
Jun 11 08:04:52.404: INFO: minio-0 from velero started at 2020-06-11 05:32:12 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.404: INFO: 	Container minio ready: true, restart count 0
Jun 11 08:04:52.404: INFO: yakcl-federation-authorizedlister-5db7b69bfd-zl4mt from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.404: INFO: 	Container authorizedlister ready: true, restart count 0
Jun 11 08:04:52.404: INFO: calico-node-fzn27 from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.404: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:04:52.404: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:04:52.404: INFO: prometheus-kubeaddons-prom-operator-767c8d59cb-44z76 from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.404: INFO: 	Container prometheus-operator ready: true, restart count 0
Jun 11 08:04:52.404: INFO: 	Container tls-proxy ready: true, restart count 0
Jun 11 08:04:52.404: INFO: dex-k8s-authenticator-kubeaddons-748fcc984d-vdpxg from kubeaddons started at 2020-06-11 05:31:50 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.404: INFO: 	Container dex-k8s-authenticator ready: true, restart count 5
Jun 11 08:04:52.404: INFO: elasticsearch-kubeaddons-client-6c56cc5c7-blbht from kubeaddons started at 2020-06-11 07:15:29 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.404: INFO: 	Container elasticsearch ready: true, restart count 0
Jun 11 08:04:52.404: INFO: kube-proxy-pp8td from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.404: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:04:52.404: INFO: ebs-csi-node-nbd7n from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:04:52.404: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:04:52.404: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:04:52.404: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:04:52.404: INFO: prometheus-kubeaddons-prometheus-node-exporter-4c6c2 from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.404: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:04:52.404: INFO: kommander-kubeaddons-prometheus-server-86d645cc98-5bbdz from kommander started at 2020-06-11 05:33:41 +0000 UTC (3 container statuses recorded)
Jun 11 08:04:52.404: INFO: 	Container prometheus-server ready: true, restart count 0
Jun 11 08:04:52.404: INFO: 	Container prometheus-server-configmap-reload ready: true, restart count 0
Jun 11 08:04:52.404: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 11 08:04:52.404: INFO: fluentbit-kubeaddons-fluent-bit-46z8w from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:04:52.404: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:04:52.404: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-lf27c from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:04:52.404: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:04:52.404: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-4ba819aa-a882-48a2-8131-1db0ca2e4836 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-4ba819aa-a882-48a2-8131-1db0ca2e4836 off the node ip-10-0-136-38.us-west-2.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-4ba819aa-a882-48a2-8131-1db0ca2e4836
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:04:56.489: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "sched-pred-1890" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":280,"completed":98,"skipped":1466,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:04:56.505: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6226
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 11 08:04:56.718: INFO: Waiting up to 5m0s for pod "pod-f2f06161-f335-4ce4-b9b7-d1fa03ed84a7" in namespace "emptydir-6226" to be "success or failure"
Jun 11 08:04:56.722: INFO: Pod "pod-f2f06161-f335-4ce4-b9b7-d1fa03ed84a7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.846573ms
Jun 11 08:04:58.726: INFO: Pod "pod-f2f06161-f335-4ce4-b9b7-d1fa03ed84a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008223997s
STEP: Saw pod success
Jun 11 08:04:58.726: INFO: Pod "pod-f2f06161-f335-4ce4-b9b7-d1fa03ed84a7" satisfied condition "success or failure"
Jun 11 08:04:58.739: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-f2f06161-f335-4ce4-b9b7-d1fa03ed84a7 container test-container: <nil>
STEP: delete the pod
Jun 11 08:04:58.764: INFO: Waiting for pod pod-f2f06161-f335-4ce4-b9b7-d1fa03ed84a7 to disappear
Jun 11 08:04:58.770: INFO: Pod pod-f2f06161-f335-4ce4-b9b7-d1fa03ed84a7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:04:58.770: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "emptydir-6226" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":99,"skipped":1552,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:04:58.787: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-504
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:04:58.940: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "services-504" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":280,"completed":100,"skipped":1580,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:04:58.955: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8531
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 11 08:05:03.157: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 11 08:05:03.161: INFO: Pod pod-with-prestop-http-hook still exists
Jun 11 08:05:05.161: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 11 08:05:05.166: INFO: Pod pod-with-prestop-http-hook still exists
Jun 11 08:05:07.161: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 11 08:05:07.166: INFO: Pod pod-with-prestop-http-hook still exists
Jun 11 08:05:09.161: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 11 08:05:09.166: INFO: Pod pod-with-prestop-http-hook still exists
Jun 11 08:05:11.161: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 11 08:05:11.166: INFO: Pod pod-with-prestop-http-hook still exists
Jun 11 08:05:13.162: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 11 08:05:13.166: INFO: Pod pod-with-prestop-http-hook still exists
Jun 11 08:05:15.162: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 11 08:05:15.166: INFO: Pod pod-with-prestop-http-hook still exists
Jun 11 08:05:17.162: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 11 08:05:17.167: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:05:17.175: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8531" for this suite.

• [SLOW TEST:18.235 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":280,"completed":101,"skipped":1595,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:05:17.190: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5903
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-835e41a0-ce3c-4e90-bb82-d9bb8171d1bb
STEP: Creating secret with name s-test-opt-upd-c7c853c4-ad21-4bef-b26e-d28ea487176f
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-835e41a0-ce3c-4e90-bb82-d9bb8171d1bb
STEP: Updating secret s-test-opt-upd-c7c853c4-ad21-4bef-b26e-d28ea487176f
STEP: Creating secret with name s-test-opt-create-20474701-2c6d-4a60-aff0-c0636df9246b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:06:29.765: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "projected-5903" for this suite.

• [SLOW TEST:72.590 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":102,"skipped":1605,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:06:29.781: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6889
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:06:45.981: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "resourcequota-6889" for this suite.

• [SLOW TEST:16.217 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":280,"completed":103,"skipped":1615,"failed":0}
SSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:06:45.997: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1281
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 11 08:06:48.181: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:06:48.202: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "container-runtime-1281" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":280,"completed":104,"skipped":1618,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:06:48.217: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3857
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:06:48.400: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "resourcequota-3857" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":280,"completed":105,"skipped":1621,"failed":0}
S
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:06:48.415: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6598
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 08:06:48.563: INFO: Creating deployment "test-recreate-deployment"
Jun 11 08:06:48.568: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun 11 08:06:48.577: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jun 11 08:06:50.585: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun 11 08:06:50.589: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jun 11 08:06:50.598: INFO: Updating deployment test-recreate-deployment
Jun 11 08:06:50.598: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jun 11 08:06:50.677: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-6598 /apis/apps/v1/namespaces/deployment-6598/deployments/test-recreate-deployment cb905282-7241-4fe7-bb88-e762fd73fb99 114705 2 2020-06-11 08:06:48 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004e6ea28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-06-11 08:06:50 +0000 UTC,LastTransitionTime:2020-06-11 08:06:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-06-11 08:06:50 +0000 UTC,LastTransitionTime:2020-06-11 08:06:48 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jun 11 08:06:50.681: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-6598 /apis/apps/v1/namespaces/deployment-6598/replicasets/test-recreate-deployment-5f94c574ff ec083197-6ff5-404d-8b14-0ec78ef44544 114703 1 2020-06-11 08:06:50 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment cb905282-7241-4fe7-bb88-e762fd73fb99 0xc004e6edd7 0xc004e6edd8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004e6ee38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 11 08:06:50.681: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun 11 08:06:50.681: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-799c574856  deployment-6598 /apis/apps/v1/namespaces/deployment-6598/replicasets/test-recreate-deployment-799c574856 b9b028d6-e08e-4895-a6fe-f146d45cd459 114693 2 2020-06-11 08:06:48 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment cb905282-7241-4fe7-bb88-e762fd73fb99 0xc004e6eea7 0xc004e6eea8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 799c574856,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004e6ef18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 11 08:06:50.685: INFO: Pod "test-recreate-deployment-5f94c574ff-rmzsc" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-rmzsc test-recreate-deployment-5f94c574ff- deployment-6598 /api/v1/namespaces/deployment-6598/pods/test-recreate-deployment-5f94c574ff-rmzsc 985f4c23-2850-40e2-ba24-cb0d697c09f9 114704 0 2020-06-11 08:06:50 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff ec083197-6ff5-404d-8b14-0ec78ef44544 0xc004e6f397 0xc004e6f398}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xs298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xs298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xs298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-136-38.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 08:06:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 08:06:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 08:06:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 08:06:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.136.38,PodIP:,StartTime:2020-06-11 08:06:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:06:50.685: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "deployment-6598" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":106,"skipped":1622,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:06:50.700: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-455
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 08:06:50.849: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 11 08:06:56.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 --namespace=crd-publish-openapi-455 create -f -'
Jun 11 08:06:56.810: INFO: stderr: ""
Jun 11 08:06:56.810: INFO: stdout: "e2e-test-crd-publish-openapi-4522-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 11 08:06:56.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 --namespace=crd-publish-openapi-455 delete e2e-test-crd-publish-openapi-4522-crds test-cr'
Jun 11 08:06:56.980: INFO: stderr: ""
Jun 11 08:06:56.980: INFO: stdout: "e2e-test-crd-publish-openapi-4522-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jun 11 08:06:56.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 --namespace=crd-publish-openapi-455 apply -f -'
Jun 11 08:06:57.300: INFO: stderr: ""
Jun 11 08:06:57.300: INFO: stdout: "e2e-test-crd-publish-openapi-4522-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 11 08:06:57.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 --namespace=crd-publish-openapi-455 delete e2e-test-crd-publish-openapi-4522-crds test-cr'
Jun 11 08:06:57.438: INFO: stderr: ""
Jun 11 08:06:57.438: INFO: stdout: "e2e-test-crd-publish-openapi-4522-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jun 11 08:06:57.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 explain e2e-test-crd-publish-openapi-4522-crds'
Jun 11 08:06:57.775: INFO: stderr: ""
Jun 11 08:06:57.775: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4522-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:07:03.196: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-455" for this suite.

• [SLOW TEST:12.513 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":280,"completed":107,"skipped":1626,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:07:03.213: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8408
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 11 08:07:03.377: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f7dd9380-8a36-44ea-9eda-c26ad964ed96" in namespace "downward-api-8408" to be "success or failure"
Jun 11 08:07:03.381: INFO: Pod "downwardapi-volume-f7dd9380-8a36-44ea-9eda-c26ad964ed96": Phase="Pending", Reason="", readiness=false. Elapsed: 3.936592ms
Jun 11 08:07:05.386: INFO: Pod "downwardapi-volume-f7dd9380-8a36-44ea-9eda-c26ad964ed96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00863193s
STEP: Saw pod success
Jun 11 08:07:05.386: INFO: Pod "downwardapi-volume-f7dd9380-8a36-44ea-9eda-c26ad964ed96" satisfied condition "success or failure"
Jun 11 08:07:05.390: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod downwardapi-volume-f7dd9380-8a36-44ea-9eda-c26ad964ed96 container client-container: <nil>
STEP: delete the pod
Jun 11 08:07:05.413: INFO: Waiting for pod downwardapi-volume-f7dd9380-8a36-44ea-9eda-c26ad964ed96 to disappear
Jun 11 08:07:05.418: INFO: Pod downwardapi-volume-f7dd9380-8a36-44ea-9eda-c26ad964ed96 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:07:05.418: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "downward-api-8408" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":108,"skipped":1637,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:07:05.434: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8229
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Jun 11 08:07:05.583: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:07:34.759: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8229" for this suite.

• [SLOW TEST:29.345 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":280,"completed":109,"skipped":1666,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:07:34.779: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9967
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Jun 11 08:07:34.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 create -f - --namespace=kubectl-9967'
Jun 11 08:07:35.234: INFO: stderr: ""
Jun 11 08:07:35.234: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 11 08:07:35.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9967'
Jun 11 08:07:35.372: INFO: stderr: ""
Jun 11 08:07:35.372: INFO: stdout: "update-demo-nautilus-gwgjp update-demo-nautilus-n5mtx "
Jun 11 08:07:35.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods update-demo-nautilus-gwgjp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9967'
Jun 11 08:07:35.502: INFO: stderr: ""
Jun 11 08:07:35.502: INFO: stdout: ""
Jun 11 08:07:35.502: INFO: update-demo-nautilus-gwgjp is created but not running
Jun 11 08:07:40.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9967'
Jun 11 08:07:40.635: INFO: stderr: ""
Jun 11 08:07:40.635: INFO: stdout: "update-demo-nautilus-gwgjp update-demo-nautilus-n5mtx "
Jun 11 08:07:40.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods update-demo-nautilus-gwgjp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9967'
Jun 11 08:07:40.768: INFO: stderr: ""
Jun 11 08:07:40.768: INFO: stdout: "true"
Jun 11 08:07:40.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods update-demo-nautilus-gwgjp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9967'
Jun 11 08:07:40.901: INFO: stderr: ""
Jun 11 08:07:40.901: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 11 08:07:40.901: INFO: validating pod update-demo-nautilus-gwgjp
Jun 11 08:07:40.907: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 11 08:07:40.907: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 11 08:07:40.907: INFO: update-demo-nautilus-gwgjp is verified up and running
Jun 11 08:07:40.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods update-demo-nautilus-n5mtx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9967'
Jun 11 08:07:41.039: INFO: stderr: ""
Jun 11 08:07:41.039: INFO: stdout: "true"
Jun 11 08:07:41.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods update-demo-nautilus-n5mtx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9967'
Jun 11 08:07:41.167: INFO: stderr: ""
Jun 11 08:07:41.167: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 11 08:07:41.167: INFO: validating pod update-demo-nautilus-n5mtx
Jun 11 08:07:41.173: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 11 08:07:41.173: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 11 08:07:41.173: INFO: update-demo-nautilus-n5mtx is verified up and running
STEP: using delete to clean up resources
Jun 11 08:07:41.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 delete --grace-period=0 --force -f - --namespace=kubectl-9967'
Jun 11 08:07:41.271: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 11 08:07:41.271: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 11 08:07:41.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9967'
Jun 11 08:07:41.446: INFO: stderr: "No resources found in kubectl-9967 namespace.\n"
Jun 11 08:07:41.446: INFO: stdout: ""
Jun 11 08:07:41.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods -l name=update-demo --namespace=kubectl-9967 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 11 08:07:41.575: INFO: stderr: ""
Jun 11 08:07:41.575: INFO: stdout: "update-demo-nautilus-gwgjp\nupdate-demo-nautilus-n5mtx\n"
Jun 11 08:07:42.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9967'
Jun 11 08:07:42.248: INFO: stderr: "No resources found in kubectl-9967 namespace.\n"
Jun 11 08:07:42.248: INFO: stdout: ""
Jun 11 08:07:42.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods -l name=update-demo --namespace=kubectl-9967 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 11 08:07:42.379: INFO: stderr: ""
Jun 11 08:07:42.379: INFO: stdout: "update-demo-nautilus-gwgjp\nupdate-demo-nautilus-n5mtx\n"
Jun 11 08:07:42.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9967'
Jun 11 08:07:42.749: INFO: stderr: "No resources found in kubectl-9967 namespace.\n"
Jun 11 08:07:42.749: INFO: stdout: ""
Jun 11 08:07:42.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods -l name=update-demo --namespace=kubectl-9967 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 11 08:07:42.884: INFO: stderr: ""
Jun 11 08:07:42.884: INFO: stdout: "update-demo-nautilus-gwgjp\nupdate-demo-nautilus-n5mtx\n"
Jun 11 08:07:43.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9967'
Jun 11 08:07:43.275: INFO: stderr: "No resources found in kubectl-9967 namespace.\n"
Jun 11 08:07:43.275: INFO: stdout: ""
Jun 11 08:07:43.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods -l name=update-demo --namespace=kubectl-9967 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 11 08:07:43.405: INFO: stderr: ""
Jun 11 08:07:43.405: INFO: stdout: "update-demo-nautilus-gwgjp\nupdate-demo-nautilus-n5mtx\n"
Jun 11 08:07:43.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9967'
Jun 11 08:07:43.747: INFO: stderr: "No resources found in kubectl-9967 namespace.\n"
Jun 11 08:07:43.747: INFO: stdout: ""
Jun 11 08:07:43.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods -l name=update-demo --namespace=kubectl-9967 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 11 08:07:43.877: INFO: stderr: ""
Jun 11 08:07:43.877: INFO: stdout: "update-demo-nautilus-gwgjp\nupdate-demo-nautilus-n5mtx\n"
Jun 11 08:07:44.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9967'
Jun 11 08:07:44.249: INFO: stderr: "No resources found in kubectl-9967 namespace.\n"
Jun 11 08:07:44.249: INFO: stdout: ""
Jun 11 08:07:44.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods -l name=update-demo --namespace=kubectl-9967 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 11 08:07:44.379: INFO: stderr: ""
Jun 11 08:07:44.379: INFO: stdout: "update-demo-nautilus-gwgjp\nupdate-demo-nautilus-n5mtx\n"
Jun 11 08:07:44.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9967'
Jun 11 08:07:44.754: INFO: stderr: "No resources found in kubectl-9967 namespace.\n"
Jun 11 08:07:44.754: INFO: stdout: ""
Jun 11 08:07:44.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods -l name=update-demo --namespace=kubectl-9967 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 11 08:07:44.888: INFO: stderr: ""
Jun 11 08:07:44.888: INFO: stdout: "update-demo-nautilus-gwgjp\nupdate-demo-nautilus-n5mtx\n"
Jun 11 08:07:45.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9967'
Jun 11 08:07:45.250: INFO: stderr: "No resources found in kubectl-9967 namespace.\n"
Jun 11 08:07:45.250: INFO: stdout: ""
Jun 11 08:07:45.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods -l name=update-demo --namespace=kubectl-9967 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 11 08:07:45.387: INFO: stderr: ""
Jun 11 08:07:45.387: INFO: stdout: "update-demo-nautilus-gwgjp\nupdate-demo-nautilus-n5mtx\n"
Jun 11 08:07:45.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9967'
Jun 11 08:07:45.746: INFO: stderr: "No resources found in kubectl-9967 namespace.\n"
Jun 11 08:07:45.746: INFO: stdout: ""
Jun 11 08:07:45.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods -l name=update-demo --namespace=kubectl-9967 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 11 08:07:45.876: INFO: stderr: ""
Jun 11 08:07:45.876: INFO: stdout: "update-demo-nautilus-gwgjp\nupdate-demo-nautilus-n5mtx\n"
Jun 11 08:07:46.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9967'
Jun 11 08:07:46.248: INFO: stderr: "No resources found in kubectl-9967 namespace.\n"
Jun 11 08:07:46.248: INFO: stdout: ""
Jun 11 08:07:46.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pods -l name=update-demo --namespace=kubectl-9967 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 11 08:07:46.380: INFO: stderr: ""
Jun 11 08:07:46.380: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:07:46.380: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "kubectl-9967" for this suite.

• [SLOW TEST:11.616 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":280,"completed":110,"skipped":1669,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:07:46.396: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2853
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on node default medium
Jun 11 08:07:46.556: INFO: Waiting up to 5m0s for pod "pod-c19a6fe9-c5d8-48d9-8362-7303022738d3" in namespace "emptydir-2853" to be "success or failure"
Jun 11 08:07:46.560: INFO: Pod "pod-c19a6fe9-c5d8-48d9-8362-7303022738d3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.972261ms
Jun 11 08:07:48.565: INFO: Pod "pod-c19a6fe9-c5d8-48d9-8362-7303022738d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008680312s
STEP: Saw pod success
Jun 11 08:07:48.565: INFO: Pod "pod-c19a6fe9-c5d8-48d9-8362-7303022738d3" satisfied condition "success or failure"
Jun 11 08:07:48.569: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-c19a6fe9-c5d8-48d9-8362-7303022738d3 container test-container: <nil>
STEP: delete the pod
Jun 11 08:07:48.592: INFO: Waiting for pod pod-c19a6fe9-c5d8-48d9-8362-7303022738d3 to disappear
Jun 11 08:07:48.598: INFO: Pod pod-c19a6fe9-c5d8-48d9-8362-7303022738d3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:07:48.598: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "emptydir-2853" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":111,"skipped":1703,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:07:48.613: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5524
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-5524
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 11 08:07:48.764: INFO: Waiting up to 10m0s for all (but 7) nodes to be schedulable
Jun 11 08:07:48.767: INFO: Unschedulable nodes:
Jun 11 08:07:48.767: INFO: -> ip-10-0-132-151.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated elasticsearch-data NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 08:07:48.767: INFO: -> ip-10-0-137-193.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated elasticsearch-data NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 08:07:48.767: INFO: -> ip-10-0-139-38.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated route-reflector NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 08:07:48.767: INFO: -> ip-10-0-130-153.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated monitoring NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 08:07:48.767: INFO: -> ip-10-0-129-22.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated route-reflector NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 08:07:48.767: INFO: -> ip-10-0-129-167.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated elasticsearch-data NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 08:07:48.767: INFO: -> ip-10-0-133-101.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated route-reflector NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 08:07:48.767: INFO: ================================
STEP: Creating test pods
Jun 11 08:08:11.037: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.42.44:8080/dial?request=hostname&protocol=udp&host=192.168.28.223&port=8081&tries=1'] Namespace:pod-network-test-5524 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:08:11.037: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:08:11.099: INFO: Waiting for responses: map[]
Jun 11 08:08:11.103: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.42.44:8080/dial?request=hostname&protocol=udp&host=192.168.168.165&port=8081&tries=1'] Namespace:pod-network-test-5524 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:08:11.103: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:08:11.171: INFO: Waiting for responses: map[]
Jun 11 08:08:11.176: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.42.44:8080/dial?request=hostname&protocol=udp&host=192.168.208.149&port=8081&tries=1'] Namespace:pod-network-test-5524 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:08:11.176: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:08:11.245: INFO: Waiting for responses: map[]
Jun 11 08:08:11.248: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.42.44:8080/dial?request=hostname&protocol=udp&host=192.168.74.248&port=8081&tries=1'] Namespace:pod-network-test-5524 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:08:11.248: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:08:11.310: INFO: Waiting for responses: map[]
Jun 11 08:08:11.314: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.42.44:8080/dial?request=hostname&protocol=udp&host=192.168.226.102&port=8081&tries=1'] Namespace:pod-network-test-5524 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:08:11.314: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:08:11.383: INFO: Waiting for responses: map[]
Jun 11 08:08:11.387: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.42.44:8080/dial?request=hostname&protocol=udp&host=192.168.23.90&port=8081&tries=1'] Namespace:pod-network-test-5524 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:08:11.387: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:08:11.445: INFO: Waiting for responses: map[]
Jun 11 08:08:11.449: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.42.44:8080/dial?request=hostname&protocol=udp&host=192.168.169.38&port=8081&tries=1'] Namespace:pod-network-test-5524 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:08:11.449: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:08:11.518: INFO: Waiting for responses: map[]
Jun 11 08:08:11.522: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.42.44:8080/dial?request=hostname&protocol=udp&host=192.168.42.36&port=8081&tries=1'] Namespace:pod-network-test-5524 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:08:11.522: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:08:11.591: INFO: Waiting for responses: map[]
Jun 11 08:08:11.595: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.42.44:8080/dial?request=hostname&protocol=udp&host=192.168.239.227&port=8081&tries=1'] Namespace:pod-network-test-5524 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:08:11.595: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:08:11.661: INFO: Waiting for responses: map[]
Jun 11 08:08:11.665: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.42.44:8080/dial?request=hostname&protocol=udp&host=192.168.162.227&port=8081&tries=1'] Namespace:pod-network-test-5524 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:08:11.665: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:08:11.723: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:08:11.723: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "pod-network-test-5524" for this suite.

• [SLOW TEST:23.125 seconds]
[sig-network] Networking
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":112,"skipped":1717,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:08:11.739: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1367
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-60e53258-4ed5-414e-92f6-8c95807d0a65
STEP: Creating a pod to test consume configMaps
Jun 11 08:08:11.903: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e11628a3-bd48-4af2-99c4-ceedd1946973" in namespace "projected-1367" to be "success or failure"
Jun 11 08:08:11.907: INFO: Pod "pod-projected-configmaps-e11628a3-bd48-4af2-99c4-ceedd1946973": Phase="Pending", Reason="", readiness=false. Elapsed: 3.958808ms
Jun 11 08:08:13.912: INFO: Pod "pod-projected-configmaps-e11628a3-bd48-4af2-99c4-ceedd1946973": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008659587s
STEP: Saw pod success
Jun 11 08:08:13.912: INFO: Pod "pod-projected-configmaps-e11628a3-bd48-4af2-99c4-ceedd1946973" satisfied condition "success or failure"
Jun 11 08:08:13.916: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-projected-configmaps-e11628a3-bd48-4af2-99c4-ceedd1946973 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 11 08:08:13.939: INFO: Waiting for pod pod-projected-configmaps-e11628a3-bd48-4af2-99c4-ceedd1946973 to disappear
Jun 11 08:08:13.943: INFO: Pod pod-projected-configmaps-e11628a3-bd48-4af2-99c4-ceedd1946973 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:08:13.943: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "projected-1367" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":113,"skipped":1737,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:08:13.959: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6824
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-c5a3b06f-f692-45d5-9437-caf4f62053f4
STEP: Creating a pod to test consume configMaps
Jun 11 08:08:14.124: INFO: Waiting up to 5m0s for pod "pod-configmaps-e5fbaf00-73b4-400e-a9cd-c6f09e8260f0" in namespace "configmap-6824" to be "success or failure"
Jun 11 08:08:14.128: INFO: Pod "pod-configmaps-e5fbaf00-73b4-400e-a9cd-c6f09e8260f0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.997698ms
Jun 11 08:08:16.132: INFO: Pod "pod-configmaps-e5fbaf00-73b4-400e-a9cd-c6f09e8260f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008195719s
STEP: Saw pod success
Jun 11 08:08:16.132: INFO: Pod "pod-configmaps-e5fbaf00-73b4-400e-a9cd-c6f09e8260f0" satisfied condition "success or failure"
Jun 11 08:08:16.136: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-configmaps-e5fbaf00-73b4-400e-a9cd-c6f09e8260f0 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 11 08:08:16.159: INFO: Waiting for pod pod-configmaps-e5fbaf00-73b4-400e-a9cd-c6f09e8260f0 to disappear
Jun 11 08:08:16.164: INFO: Pod pod-configmaps-e5fbaf00-73b4-400e-a9cd-c6f09e8260f0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:08:16.164: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "configmap-6824" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":114,"skipped":1767,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:08:16.179: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-150
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 08:08:16.328: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jun 11 08:08:18.364: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:08:19.372: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "replication-controller-150" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":280,"completed":115,"skipped":1776,"failed":0}
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:08:19.388: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6998
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:08:21.567: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "kubelet-test-6998" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":280,"completed":116,"skipped":1782,"failed":0}

------------------------------
[k8s.io] Container Runtime blackbox test on terminated container
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:08:21.582: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1516
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 11 08:08:24.767: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:08:24.787: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "container-runtime-1516" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":117,"skipped":1782,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:08:24.803: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8615
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting the proxy server
Jun 11 08:08:24.953: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-515000098 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:08:25.016: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "kubectl-8615" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":280,"completed":118,"skipped":1816,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:08:25.032: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9354
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-9354
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 11 08:08:25.183: INFO: Waiting up to 10m0s for all (but 7) nodes to be schedulable
Jun 11 08:08:25.186: INFO: Unschedulable nodes:
Jun 11 08:08:25.186: INFO: -> ip-10-0-139-38.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated route-reflector NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 08:08:25.186: INFO: -> ip-10-0-132-151.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated elasticsearch-data NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 08:08:25.186: INFO: -> ip-10-0-137-193.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated elasticsearch-data NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 08:08:25.186: INFO: -> ip-10-0-129-167.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated elasticsearch-data NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 08:08:25.186: INFO: -> ip-10-0-130-153.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated monitoring NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 08:08:25.186: INFO: -> ip-10-0-129-22.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated route-reflector NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 08:08:25.186: INFO: -> ip-10-0-133-101.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated route-reflector NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 08:08:25.186: INFO: ================================
STEP: Creating test pods
Jun 11 08:08:49.404: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.28.224:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9354 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:08:49.404: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:08:49.469: INFO: Found all expected endpoints: [netserver-0]
Jun 11 08:08:49.472: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.168.166:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9354 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:08:49.472: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:08:49.543: INFO: Found all expected endpoints: [netserver-1]
Jun 11 08:08:49.547: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.208.148:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9354 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:08:49.547: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:08:49.616: INFO: Found all expected endpoints: [netserver-2]
Jun 11 08:08:49.620: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.74.249:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9354 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:08:49.620: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:08:49.680: INFO: Found all expected endpoints: [netserver-3]
Jun 11 08:08:49.684: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.226.104:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9354 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:08:49.684: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:08:49.747: INFO: Found all expected endpoints: [netserver-4]
Jun 11 08:08:49.751: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.23.91:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9354 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:08:49.751: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:08:49.820: INFO: Found all expected endpoints: [netserver-5]
Jun 11 08:08:49.824: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.169.39:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9354 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:08:49.824: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:08:49.883: INFO: Found all expected endpoints: [netserver-6]
Jun 11 08:08:49.887: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.42.43:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9354 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:08:49.887: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:08:49.954: INFO: Found all expected endpoints: [netserver-7]
Jun 11 08:08:49.958: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.239.228:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9354 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:08:49.958: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:08:50.016: INFO: Found all expected endpoints: [netserver-8]
Jun 11 08:08:50.020: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.162.228:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9354 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:08:50.020: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:08:50.092: INFO: Found all expected endpoints: [netserver-9]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:08:50.092: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "pod-network-test-9354" for this suite.

• [SLOW TEST:25.075 seconds]
[sig-network] Networking
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":119,"skipped":1828,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:08:50.107: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4395
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 08:08:50.315: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jun 11 08:08:50.325: INFO: Number of nodes with available pods: 0
Jun 11 08:08:50.325: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jun 11 08:08:50.347: INFO: Number of nodes with available pods: 0
Jun 11 08:08:50.347: INFO: Node ip-10-0-132-52.us-west-2.compute.internal is running more than one daemon pod
Jun 11 08:08:51.352: INFO: Number of nodes with available pods: 0
Jun 11 08:08:51.352: INFO: Node ip-10-0-132-52.us-west-2.compute.internal is running more than one daemon pod
Jun 11 08:08:52.352: INFO: Number of nodes with available pods: 0
Jun 11 08:08:52.352: INFO: Node ip-10-0-132-52.us-west-2.compute.internal is running more than one daemon pod
Jun 11 08:08:53.351: INFO: Number of nodes with available pods: 1
Jun 11 08:08:53.351: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jun 11 08:08:53.372: INFO: Number of nodes with available pods: 1
Jun 11 08:08:53.372: INFO: Number of running nodes: 0, number of available pods: 1
Jun 11 08:08:54.377: INFO: Number of nodes with available pods: 0
Jun 11 08:08:54.377: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jun 11 08:08:54.387: INFO: Number of nodes with available pods: 0
Jun 11 08:08:54.387: INFO: Node ip-10-0-132-52.us-west-2.compute.internal is running more than one daemon pod
Jun 11 08:08:55.392: INFO: Number of nodes with available pods: 0
Jun 11 08:08:55.392: INFO: Node ip-10-0-132-52.us-west-2.compute.internal is running more than one daemon pod
Jun 11 08:08:56.391: INFO: Number of nodes with available pods: 0
Jun 11 08:08:56.391: INFO: Node ip-10-0-132-52.us-west-2.compute.internal is running more than one daemon pod
Jun 11 08:08:57.392: INFO: Number of nodes with available pods: 0
Jun 11 08:08:57.392: INFO: Node ip-10-0-132-52.us-west-2.compute.internal is running more than one daemon pod
Jun 11 08:08:58.393: INFO: Number of nodes with available pods: 0
Jun 11 08:08:58.393: INFO: Node ip-10-0-132-52.us-west-2.compute.internal is running more than one daemon pod
Jun 11 08:08:59.391: INFO: Number of nodes with available pods: 0
Jun 11 08:08:59.391: INFO: Node ip-10-0-132-52.us-west-2.compute.internal is running more than one daemon pod
Jun 11 08:09:00.392: INFO: Number of nodes with available pods: 0
Jun 11 08:09:00.392: INFO: Node ip-10-0-132-52.us-west-2.compute.internal is running more than one daemon pod
Jun 11 08:09:01.392: INFO: Number of nodes with available pods: 0
Jun 11 08:09:01.392: INFO: Node ip-10-0-132-52.us-west-2.compute.internal is running more than one daemon pod
Jun 11 08:09:02.392: INFO: Number of nodes with available pods: 0
Jun 11 08:09:02.392: INFO: Node ip-10-0-132-52.us-west-2.compute.internal is running more than one daemon pod
Jun 11 08:09:03.392: INFO: Number of nodes with available pods: 0
Jun 11 08:09:03.392: INFO: Node ip-10-0-132-52.us-west-2.compute.internal is running more than one daemon pod
Jun 11 08:09:04.392: INFO: Number of nodes with available pods: 0
Jun 11 08:09:04.392: INFO: Node ip-10-0-132-52.us-west-2.compute.internal is running more than one daemon pod
Jun 11 08:09:05.392: INFO: Number of nodes with available pods: 0
Jun 11 08:09:05.392: INFO: Node ip-10-0-132-52.us-west-2.compute.internal is running more than one daemon pod
Jun 11 08:09:06.391: INFO: Number of nodes with available pods: 0
Jun 11 08:09:06.391: INFO: Node ip-10-0-132-52.us-west-2.compute.internal is running more than one daemon pod
Jun 11 08:09:07.391: INFO: Number of nodes with available pods: 0
Jun 11 08:09:07.391: INFO: Node ip-10-0-132-52.us-west-2.compute.internal is running more than one daemon pod
Jun 11 08:09:08.392: INFO: Number of nodes with available pods: 0
Jun 11 08:09:08.392: INFO: Node ip-10-0-132-52.us-west-2.compute.internal is running more than one daemon pod
Jun 11 08:09:09.394: INFO: Number of nodes with available pods: 1
Jun 11 08:09:09.394: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4395, will wait for the garbage collector to delete the pods
Jun 11 08:09:09.467: INFO: Deleting DaemonSet.extensions daemon-set took: 10.47929ms
Jun 11 08:09:11.068: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.600300323s
Jun 11 08:09:16.673: INFO: Number of nodes with available pods: 0
Jun 11 08:09:16.673: INFO: Number of running nodes: 0, number of available pods: 0
Jun 11 08:09:16.677: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4395/daemonsets","resourceVersion":"117030"},"items":null}

Jun 11 08:09:16.681: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4395/pods","resourceVersion":"117030"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:09:16.735: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "daemonsets-4395" for this suite.

• [SLOW TEST:26.642 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":280,"completed":120,"skipped":1851,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:09:16.750: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7158
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-projected-cr2t
STEP: Creating a pod to test atomic-volume-subpath
Jun 11 08:09:16.921: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-cr2t" in namespace "subpath-7158" to be "success or failure"
Jun 11 08:09:16.925: INFO: Pod "pod-subpath-test-projected-cr2t": Phase="Pending", Reason="", readiness=false. Elapsed: 4.717321ms
Jun 11 08:09:18.930: INFO: Pod "pod-subpath-test-projected-cr2t": Phase="Running", Reason="", readiness=true. Elapsed: 2.009141064s
Jun 11 08:09:20.934: INFO: Pod "pod-subpath-test-projected-cr2t": Phase="Running", Reason="", readiness=true. Elapsed: 4.013771819s
Jun 11 08:09:22.939: INFO: Pod "pod-subpath-test-projected-cr2t": Phase="Running", Reason="", readiness=true. Elapsed: 6.017931989s
Jun 11 08:09:24.943: INFO: Pod "pod-subpath-test-projected-cr2t": Phase="Running", Reason="", readiness=true. Elapsed: 8.022596982s
Jun 11 08:09:26.948: INFO: Pod "pod-subpath-test-projected-cr2t": Phase="Running", Reason="", readiness=true. Elapsed: 10.027320671s
Jun 11 08:09:28.953: INFO: Pod "pod-subpath-test-projected-cr2t": Phase="Running", Reason="", readiness=true. Elapsed: 12.031860054s
Jun 11 08:09:30.957: INFO: Pod "pod-subpath-test-projected-cr2t": Phase="Running", Reason="", readiness=true. Elapsed: 14.036219065s
Jun 11 08:09:32.962: INFO: Pod "pod-subpath-test-projected-cr2t": Phase="Running", Reason="", readiness=true. Elapsed: 16.040940482s
Jun 11 08:09:34.967: INFO: Pod "pod-subpath-test-projected-cr2t": Phase="Running", Reason="", readiness=true. Elapsed: 18.045978295s
Jun 11 08:09:36.972: INFO: Pod "pod-subpath-test-projected-cr2t": Phase="Running", Reason="", readiness=true. Elapsed: 20.050895867s
Jun 11 08:09:38.976: INFO: Pod "pod-subpath-test-projected-cr2t": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.055623273s
STEP: Saw pod success
Jun 11 08:09:38.976: INFO: Pod "pod-subpath-test-projected-cr2t" satisfied condition "success or failure"
Jun 11 08:09:38.980: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-subpath-test-projected-cr2t container test-container-subpath-projected-cr2t: <nil>
STEP: delete the pod
Jun 11 08:09:39.004: INFO: Waiting for pod pod-subpath-test-projected-cr2t to disappear
Jun 11 08:09:39.008: INFO: Pod pod-subpath-test-projected-cr2t no longer exists
STEP: Deleting pod pod-subpath-test-projected-cr2t
Jun 11 08:09:39.008: INFO: Deleting pod "pod-subpath-test-projected-cr2t" in namespace "subpath-7158"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:09:39.012: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "subpath-7158" for this suite.

• [SLOW TEST:22.276 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":280,"completed":121,"skipped":1857,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:09:39.026: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5821
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 08:09:39.177: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:09:40.318: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5821" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":280,"completed":122,"skipped":1860,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:09:40.335: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1069
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service nodeport-test with type=NodePort in namespace services-1069
STEP: creating replication controller nodeport-test in namespace services-1069
I0611 08:09:40.514415      23 runners.go:189] Created replication controller with name: nodeport-test, namespace: services-1069, replica count: 2
Jun 11 08:09:43.564: INFO: Creating new exec pod
I0611 08:09:43.564859      23 runners.go:189] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady
Jun 11 08:09:46.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=services-1069 execpod28hb2 -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Jun 11 08:09:46.764: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 11 08:09:46.764: INFO: stdout: ""
Jun 11 08:09:46.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=services-1069 execpod28hb2 -- /bin/sh -x -c nc -zv -t -w 2 10.0.1.144 80'
Jun 11 08:09:46.941: INFO: stderr: "+ nc -zv -t -w 2 10.0.1.144 80\nConnection to 10.0.1.144 80 port [tcp/http] succeeded!\n"
Jun 11 08:09:46.941: INFO: stdout: ""
Jun 11 08:09:46.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=services-1069 execpod28hb2 -- /bin/sh -x -c nc -zv -t -w 2 10.0.135.119 31847'
Jun 11 08:09:47.126: INFO: stderr: "+ nc -zv -t -w 2 10.0.135.119 31847\nConnection to 10.0.135.119 31847 port [tcp/31847] succeeded!\n"
Jun 11 08:09:47.127: INFO: stdout: ""
Jun 11 08:09:47.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=services-1069 execpod28hb2 -- /bin/sh -x -c nc -zv -t -w 2 10.0.129.105 31847'
Jun 11 08:09:47.302: INFO: stderr: "+ nc -zv -t -w 2 10.0.129.105 31847\nConnection to 10.0.129.105 31847 port [tcp/31847] succeeded!\n"
Jun 11 08:09:47.302: INFO: stdout: ""
Jun 11 08:09:47.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=services-1069 execpod28hb2 -- /bin/sh -x -c nc -zv -t -w 2 34.220.175.65 31847'
Jun 11 08:09:47.477: INFO: stderr: "+ nc -zv -t -w 2 34.220.175.65 31847\nConnection to 34.220.175.65 31847 port [tcp/31847] succeeded!\n"
Jun 11 08:09:47.477: INFO: stdout: ""
Jun 11 08:09:47.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=services-1069 execpod28hb2 -- /bin/sh -x -c nc -zv -t -w 2 34.221.100.73 31847'
Jun 11 08:09:47.656: INFO: stderr: "+ nc -zv -t -w 2 34.221.100.73 31847\nConnection to 34.221.100.73 31847 port [tcp/31847] succeeded!\n"
Jun 11 08:09:47.656: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:09:47.656: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "services-1069" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:7.337 seconds]
[sig-network] Services
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":280,"completed":123,"skipped":1900,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:09:47.672: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4432
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jun 11 08:09:47.823: INFO: Waiting up to 1m0s for all (but 7) nodes to be ready
Jun 11 08:09:47.844: INFO: Waiting for terminating namespaces to be deleted...
Jun 11 08:09:47.849: INFO:
Logging pods the kubelet thinks is on node ip-10-0-128-119.us-west-2.compute.internal before test
Jun 11 08:09:47.864: INFO: kubefed-controller-manager-78b769f688-8mjvz from kommander started at 2020-06-11 05:33:45 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.864: INFO: 	Container controller-manager ready: true, restart count 0
Jun 11 08:09:47.864: INFO: fluentbit-kubeaddons-fluent-bit-hfm9r from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.864: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:09:47.864: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-594c2 from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.864: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:09:47.864: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:09:47.864: INFO: ebs-csi-node-2w524 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:09:47.864: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:09:47.864: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:09:47.864: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:09:47.864: INFO: prometheus-kubeaddons-prometheus-node-exporter-fz2j2 from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.864: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:09:47.864: INFO: cert-manager-kubeaddons-cainjector-6dcd94769b-v8x5p from cert-manager started at 2020-06-11 05:29:09 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.864: INFO: 	Container cainjector ready: true, restart count 0
Jun 11 08:09:47.864: INFO: elasticsearch-kubeaddons-client-6c56cc5c7-kpxr5 from kubeaddons started at 2020-06-11 05:30:43 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.864: INFO: 	Container elasticsearch ready: true, restart count 0
Jun 11 08:09:47.864: INFO: yakcl-licensing-cm-7c5cc586b5-xgtnw from kommander started at 2020-06-11 05:33:34 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.864: INFO: 	Container controller-manager ready: true, restart count 0
Jun 11 08:09:47.864: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 11 08:09:47.864: INFO: kube-proxy-b8rs5 from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.864: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:09:47.864: INFO: calico-node-wxzzc from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.864: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:09:47.864: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:09:47.864: INFO:
Logging pods the kubelet thinks is on node ip-10-0-129-105.us-west-2.compute.internal before test
Jun 11 08:09:47.880: INFO: opsportal-kubeaddons-kommander-ui-689b97997f-x45gn from kubeaddons started at 2020-06-11 05:28:52 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.880: INFO: 	Container opsportal-kubeaddons-kommander-ui ready: true, restart count 0
Jun 11 08:09:47.880: INFO: prometheusadapter-kubeaddons-prometheus-adapter-77bc665f9-sjxp8 from kubeaddons started at 2020-06-11 05:33:06 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.880: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 11 08:09:47.880: INFO: kube-proxy-7vnlr from kube-system started at 2020-06-11 05:26:20 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.880: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:09:47.880: INFO: ebs-csi-node-jzjgw from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:09:47.880: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:09:47.880: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:09:47.880: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:09:47.880: INFO: fluentbit-kubeaddons-fluent-bit-nbsn6 from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.880: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:09:47.880: INFO: kube-oidc-proxy-kubeaddons-545d6df8-w9v44 from kubeaddons started at 2020-06-11 05:31:42 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.880: INFO: 	Container kube-oidc-proxy ready: true, restart count 0
Jun 11 08:09:47.880: INFO: prometheus-kubeaddons-grafana-c8f7fcdb5-xjw4p from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.880: INFO: 	Container grafana ready: true, restart count 0
Jun 11 08:09:47.880: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Jun 11 08:09:47.880: INFO: prometheus-kubeaddons-prometheus-node-exporter-zl5mt from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.880: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:09:47.880: INFO: calico-node-tnwfg from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.880: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:09:47.880: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:09:47.880: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-cxrgp from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.880: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:09:47.880: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:09:47.880: INFO:
Logging pods the kubelet thinks is on node ip-10-0-129-30.us-west-2.compute.internal before test
Jun 11 08:09:47.898: INFO: prometheus-kubeaddons-prometheus-node-exporter-dwrbk from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.898: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:09:47.898: INFO: kommander-kubeaddons-cost-analyzer-7cfd4cb9cd-6tfnq from kommander started at 2020-06-11 05:33:41 +0000 UTC (3 container statuses recorded)
Jun 11 08:09:47.898: INFO: 	Container cost-analyzer-frontend ready: true, restart count 0
Jun 11 08:09:47.898: INFO: 	Container cost-analyzer-server ready: true, restart count 0
Jun 11 08:09:47.898: INFO: 	Container cost-model ready: true, restart count 0
Jun 11 08:09:47.898: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-5wq4g from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.898: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:09:47.898: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:09:47.898: INFO: dstorageclass-controller-manager-5c966c767f-h5rsz from kubeaddons started at 2020-06-11 05:29:47 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.898: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 11 08:09:47.898: INFO: 	Container manager ready: true, restart count 0
Jun 11 08:09:47.898: INFO: traefik-forward-auth-kubeaddons-6675968b94-fjznl from kubeaddons started at 2020-06-11 05:37:16 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.898: INFO: 	Container traefik-forward-auth ready: true, restart count 1
Jun 11 08:09:47.898: INFO: kommander-kubeaddons-karma-8df6cfdb6-4jg59 from kommander started at 2020-06-11 05:33:33 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.898: INFO: 	Container karma ready: true, restart count 0
Jun 11 08:09:47.898: INFO: kubernetes-dashboard-549989bcdf-n9ncj from kubeaddons started at 2020-06-11 07:15:29 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.898: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jun 11 08:09:47.898: INFO: ebs-csi-node-s8248 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:09:47.898: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:09:47.898: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:09:47.898: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:09:47.898: INFO: kommander-kubeaddons-kommander-ui-86b4f5f88f-pvgsw from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.898: INFO: 	Container kommander-kubeaddons-kommander-ui ready: true, restart count 0
Jun 11 08:09:47.898: INFO: fluentbit-kubeaddons-fluent-bit-mpwz7 from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.898: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:09:47.898: INFO: calico-node-vpvmh from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.898: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:09:47.898: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:09:47.898: INFO: tiller-deploy-6cdf7f9d6f-d6b2p from kube-system started at 2020-06-11 05:28:13 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.898: INFO: 	Container tiller ready: true, restart count 0
Jun 11 08:09:47.898: INFO: minio-1 from velero started at 2020-06-11 05:32:15 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.898: INFO: 	Container minio ready: true, restart count 0
Jun 11 08:09:47.898: INFO: kommander-kubecost-thanos-query-79fc7d8747-fm6rq from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.898: INFO: 	Container thanos-query ready: true, restart count 0
Jun 11 08:09:47.898: INFO: kommander-kubeaddons-kube-state-metrics-6d77646c89-bvrqg from kommander started at 2020-06-11 05:33:33 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.898: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 11 08:09:47.898: INFO: kube-proxy-td2b5 from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.898: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:09:47.898: INFO:
Logging pods the kubelet thinks is on node ip-10-0-130-174.us-west-2.compute.internal before test
Jun 11 08:09:47.917: INFO: calico-node-hrkxr from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.917: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:09:47.917: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:09:47.917: INFO: kubefed-admission-webhook-7b4997895b-x5h84 from kommander started at 2020-06-11 05:33:38 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.917: INFO: 	Container admission-webhook ready: true, restart count 0
Jun 11 08:09:47.917: INFO: kibana-kubeaddons-c8c7b687-4lqrx from kubeaddons started at 2020-06-11 07:15:29 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.917: INFO: 	Container initialize-kibana-index ready: true, restart count 0
Jun 11 08:09:47.917: INFO: 	Container kibana ready: true, restart count 0
Jun 11 08:09:47.917: INFO: cert-manager-kubeaddons-webhook-77fbc6d59b-d67w5 from cert-manager started at 2020-06-11 05:29:09 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.917: INFO: 	Container cert-manager ready: true, restart count 1
Jun 11 08:09:47.917: INFO: ebs-csi-node-bfvm8 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:09:47.917: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:09:47.917: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:09:47.917: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:09:47.917: INFO: kommander-kubeaddons-grafana-66c558d6f5-ntspl from kommander started at 2020-06-11 05:33:32 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.917: INFO: 	Container grafana ready: true, restart count 0
Jun 11 08:09:47.917: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Jun 11 08:09:47.917: INFO: kommander-kubeaddons-prometheus-alertmanager-7cdccf8c44-h9grs from kommander started at 2020-06-11 05:33:38 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.917: INFO: 	Container prometheus-alertmanager ready: true, restart count 0
Jun 11 08:09:47.917: INFO: 	Container prometheus-alertmanager-configmap-reload ready: true, restart count 0
Jun 11 08:09:47.917: INFO: fluentbit-kubeaddons-fluent-bit-xx74f from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.917: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:09:47.917: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-zvgtl from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.917: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:09:47.917: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:09:47.917: INFO: kube-proxy-6htkf from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.917: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:09:47.917: INFO: prometheus-kubeaddons-prometheus-node-exporter-mzcfk from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.917: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:09:47.917: INFO: yakcl-licensing-webhook-6d876f844d-mrvm5 from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.917: INFO: 	Container webhook ready: true, restart count 0
Jun 11 08:09:47.917: INFO: ebs-csi-controller-0 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (6 container statuses recorded)
Jun 11 08:09:47.917: INFO: 	Container csi-attacher ready: true, restart count 0
Jun 11 08:09:47.917: INFO: 	Container csi-provisioner ready: true, restart count 0
Jun 11 08:09:47.917: INFO: 	Container csi-resizer ready: true, restart count 0
Jun 11 08:09:47.917: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jun 11 08:09:47.917: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:09:47.917: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:09:47.917: INFO: kommander-kubeaddons-thanos-query-6cddb86b55-f4g4l from kommander started at 2020-06-11 05:33:35 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.917: INFO: 	Container thanos-query ready: true, restart count 0
Jun 11 08:09:47.917: INFO: elasticsearchexporter-kubeaddons-elasticsearch-exporter-84pdc2n from kubeaddons started at 2020-06-11 05:36:03 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.917: INFO: 	Container elasticsearch-exporter ready: true, restart count 0
Jun 11 08:09:47.917: INFO:
Logging pods the kubelet thinks is on node ip-10-0-132-48.us-west-2.compute.internal before test
Jun 11 08:09:47.931: INFO: calico-node-wr8kq from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.931: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:09:47.931: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:09:47.931: INFO: cert-manager-kubeaddons-7d7f98fbc6-86r7x from cert-manager started at 2020-06-11 05:29:09 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.931: INFO: 	Container cert-manager ready: true, restart count 0
Jun 11 08:09:47.931: INFO: prometheus-kubeaddons-prometheus-node-exporter-cqrh7 from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.931: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:09:47.931: INFO: yakcl-federation-cm-55469d7b6b-f6xhf from kommander started at 2020-06-11 05:33:33 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.931: INFO: 	Container controller-manager ready: true, restart count 0
Jun 11 08:09:47.931: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 11 08:09:47.931: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-ft9g5 from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.931: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:09:47.931: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:09:47.931: INFO: kube-proxy-q79z5 from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.931: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:09:47.931: INFO: ebs-csi-node-xzdsx from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:09:47.931: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:09:47.931: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:09:47.931: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:09:47.931: INFO: elasticsearch-kubeaddons-master-0 from kubeaddons started at 2020-06-11 05:31:03 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.931: INFO: 	Container elasticsearch ready: true, restart count 0
Jun 11 08:09:47.931: INFO: fluentbit-kubeaddons-fluent-bit-2wt5v from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.931: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:09:47.931: INFO:
Logging pods the kubelet thinks is on node ip-10-0-132-52.us-west-2.compute.internal before test
Jun 11 08:09:47.944: INFO: elasticsearch-kubeaddons-client-6c56cc5c7-zp9fl from kubeaddons started at 2020-06-11 05:30:43 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.945: INFO: 	Container elasticsearch ready: true, restart count 0
Jun 11 08:09:47.945: INFO: fluentbit-kubeaddons-fluent-bit-78mbt from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.945: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:09:47.945: INFO: kube-proxy-w486c from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.945: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:09:47.945: INFO: calico-node-6mfwv from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.945: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:09:47.945: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:09:47.945: INFO: traefik-kubeaddons-64fbc79c9-54zfn from kubeaddons started at 2020-06-11 05:31:03 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.945: INFO: 	Container traefik-kubeaddons ready: true, restart count 0
Jun 11 08:09:47.945: INFO: kubefed-controller-manager-78b769f688-rmx64 from kommander started at 2020-06-11 05:33:38 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.945: INFO: 	Container controller-manager ready: true, restart count 0
Jun 11 08:09:47.945: INFO: traefik-kubeaddons-1.72.19-zdq6l from kubeaddons started at 2020-06-11 05:29:49 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.945: INFO: 	Container traefik ready: false, restart count 0
Jun 11 08:09:47.945: INFO: ebs-csi-node-cf4v4 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:09:47.945: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:09:47.945: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:09:47.945: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:09:47.945: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-tssmj from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.945: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:09:47.945: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:09:47.945: INFO: kubeaddons-controller-manager-986f46689-sk6x8 from kubeaddons started at 2020-06-11 05:27:57 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.945: INFO: 	Container manager ready: true, restart count 0
Jun 11 08:09:47.945: INFO: prometheus-kubeaddons-prometheus-node-exporter-nff74 from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.945: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:09:47.945: INFO:
Logging pods the kubelet thinks is on node ip-10-0-135-119.us-west-2.compute.internal before test
Jun 11 08:09:47.959: INFO: ebs-csi-node-w9whr from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:09:47.959: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:09:47.959: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:09:47.959: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:09:47.959: INFO: nodeport-test-pdjbt from services-1069 started at 2020-06-11 08:09:40 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.959: INFO: 	Container nodeport-test ready: true, restart count 0
Jun 11 08:09:47.959: INFO: dex-kubeaddons-dex-controller-7bd5fc575c-xmnlq from kubeaddons started at 2020-06-11 05:31:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.959: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 11 08:09:47.959: INFO: 	Container manager ready: true, restart count 0
Jun 11 08:09:47.959: INFO: velero-kubeaddons-5d85fcdcb9-gqb5c from velero started at 2020-06-11 05:32:08 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.959: INFO: 	Container velero ready: true, restart count 3
Jun 11 08:09:47.959: INFO: elasticsearch-kubeaddons-master-1 from kubeaddons started at 2020-06-11 05:32:29 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.959: INFO: 	Container elasticsearch ready: true, restart count 0
Jun 11 08:09:47.960: INFO: fluentbit-kubeaddons-fluent-bit-6lwhn from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.960: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:09:47.960: INFO: calico-node-sq7sb from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.960: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:09:47.960: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:09:47.960: INFO: external-dns-kubeaddons-765d55b455-vlxcv from kubeaddons started at 2020-06-11 05:28:25 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.960: INFO: 	Container external-dns ready: true, restart count 0
Jun 11 08:09:47.960: INFO: minio-2 from velero started at 2020-06-11 05:32:12 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.960: INFO: 	Container minio ready: true, restart count 0
Jun 11 08:09:47.960: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-b7drp from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.960: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:09:47.960: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:09:47.960: INFO: kube-proxy-xnz2c from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.960: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:09:47.960: INFO: prometheus-kubeaddons-prometheus-node-exporter-4fllq from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.960: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:09:47.960: INFO: alertmanager-prometheus-kubeaddons-prom-alertmanager-0 from kubeaddons started at 2020-06-11 05:32:06 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.960: INFO: 	Container alertmanager ready: true, restart count 0
Jun 11 08:09:47.960: INFO: 	Container config-reloader ready: true, restart count 0
Jun 11 08:09:47.960: INFO:
Logging pods the kubelet thinks is on node ip-10-0-136-38.us-west-2.compute.internal before test
Jun 11 08:09:47.967: INFO: fluentbit-kubeaddons-fluent-bit-ht88l from kubeaddons started at 2020-06-11 08:04:31 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.967: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:09:47.967: INFO: execpod28hb2 from services-1069 started at 2020-06-11 08:09:43 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.967: INFO: 	Container agnhost-pause ready: true, restart count 0
Jun 11 08:09:47.967: INFO: kube-proxy-dz75m from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.967: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:09:47.967: INFO: ebs-csi-node-8xl6p from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:09:47.967: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:09:47.967: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:09:47.967: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:09:47.967: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-lgn88 from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.967: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:09:47.967: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:09:47.967: INFO: cost-analyzer-checks-1591862400-tjx9g from kommander started at 2020-06-11 08:00:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.967: INFO: 	Container cost-analyzer-checks ready: false, restart count 0
Jun 11 08:09:47.967: INFO: prometheus-kubeaddons-prometheus-node-exporter-78vv8 from kubeaddons started at 2020-06-11 08:04:15 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.967: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:09:47.967: INFO: calico-node-tzhxw from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.967: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:09:47.967: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:09:47.967: INFO: sonobuoy from sonobuoy started at 2020-06-11 07:40:59 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.967: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 11 08:09:47.967: INFO: nodeport-test-pxc4f from services-1069 started at 2020-06-11 08:09:40 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.967: INFO: 	Container nodeport-test ready: true, restart count 0
Jun 11 08:09:47.967: INFO:
Logging pods the kubelet thinks is on node ip-10-0-137-210.us-west-2.compute.internal before test
Jun 11 08:09:47.981: INFO: kube-proxy-qj7tx from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.981: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:09:47.981: INFO: minio-3 from velero started at 2020-06-11 05:32:15 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.981: INFO: 	Container minio ready: true, restart count 0
Jun 11 08:09:47.981: INFO: prometheus-kubeaddons-kube-state-metrics-6599df558b-mmf9r from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.981: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 11 08:09:47.981: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-6cvjn from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.981: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:09:47.981: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:09:47.981: INFO: calico-node-vkrzv from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.981: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:09:47.981: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:09:47.981: INFO: ebs-csi-node-frsf2 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:09:47.981: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:09:47.981: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:09:47.981: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:09:47.981: INFO: prometheus-kubeaddons-prometheus-node-exporter-8z4lv from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.981: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:09:47.981: INFO: opsportal-landing-6f6865b688-6r5kn from kubeaddons started at 2020-06-11 05:28:52 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.981: INFO: 	Container opsportal-landing ready: true, restart count 0
Jun 11 08:09:47.981: INFO: gatekeeper-kubeaddons-776f4b5c96-jqsq8 from kubeaddons started at 2020-06-11 05:29:52 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.981: INFO: 	Container manager ready: true, restart count 0
Jun 11 08:09:47.981: INFO: elasticsearch-kubeaddons-master-2 from kubeaddons started at 2020-06-11 05:33:29 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.981: INFO: 	Container elasticsearch ready: true, restart count 0
Jun 11 08:09:47.981: INFO: fluentbit-kubeaddons-fluent-bit-hv9bs from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.981: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:09:47.981: INFO: dex-kubeaddons-696dbdb6c6-5prn9 from kubeaddons started at 2020-06-11 05:37:11 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.981: INFO: 	Container main ready: true, restart count 0
Jun 11 08:09:47.981: INFO:
Logging pods the kubelet thinks is on node ip-10-0-138-28.us-west-2.compute.internal before test
Jun 11 08:09:47.996: INFO: fluentbit-kubeaddons-fluent-bit-46z8w from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.996: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:09:47.996: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-lf27c from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.996: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:09:47.996: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:09:47.996: INFO: kube-proxy-pp8td from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.996: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:09:47.996: INFO: ebs-csi-node-nbd7n from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:09:47.996: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:09:47.996: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:09:47.996: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:09:47.996: INFO: prometheus-kubeaddons-prometheus-node-exporter-4c6c2 from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.996: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:09:47.996: INFO: kommander-kubeaddons-prometheus-server-86d645cc98-5bbdz from kommander started at 2020-06-11 05:33:41 +0000 UTC (3 container statuses recorded)
Jun 11 08:09:47.996: INFO: 	Container prometheus-server ready: true, restart count 0
Jun 11 08:09:47.996: INFO: 	Container prometheus-server-configmap-reload ready: true, restart count 0
Jun 11 08:09:47.996: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 11 08:09:47.996: INFO: reloader-kubeaddons-reloader-7d4bd64cfb-4qjm4 from kubeaddons started at 2020-06-11 05:28:30 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.996: INFO: 	Container reloader-kubeaddons-reloader ready: true, restart count 0
Jun 11 08:09:47.996: INFO: kommander-kubeaddons-kubeaddons-catalog-6654f856df-kndt7 from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.996: INFO: 	Container kubeaddons-catalog ready: true, restart count 0
Jun 11 08:09:47.996: INFO: yakcl-federation-webhook-6fcc46596-smpfh from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.996: INFO: 	Container webhook ready: true, restart count 0
Jun 11 08:09:47.996: INFO: yakcl-federation-utility-apiserver-7d57699df9-wp7bs from kommander started at 2020-06-11 05:33:33 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.996: INFO: 	Container server ready: true, restart count 0
Jun 11 08:09:47.996: INFO: ebs-csi-snapshot-controller-0 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.996: INFO: 	Container ebs-csi-snapshot-controller ready: true, restart count 0
Jun 11 08:09:47.996: INFO: minio-0 from velero started at 2020-06-11 05:32:12 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.996: INFO: 	Container minio ready: true, restart count 0
Jun 11 08:09:47.996: INFO: yakcl-federation-authorizedlister-5db7b69bfd-zl4mt from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.996: INFO: 	Container authorizedlister ready: true, restart count 0
Jun 11 08:09:47.996: INFO: calico-node-fzn27 from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.996: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:09:47.996: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:09:47.996: INFO: prometheus-kubeaddons-prom-operator-767c8d59cb-44z76 from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (2 container statuses recorded)
Jun 11 08:09:47.996: INFO: 	Container prometheus-operator ready: true, restart count 0
Jun 11 08:09:47.996: INFO: 	Container tls-proxy ready: true, restart count 0
Jun 11 08:09:47.996: INFO: dex-k8s-authenticator-kubeaddons-748fcc984d-vdpxg from kubeaddons started at 2020-06-11 05:31:50 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.996: INFO: 	Container dex-k8s-authenticator ready: true, restart count 5
Jun 11 08:09:47.996: INFO: elasticsearch-kubeaddons-client-6c56cc5c7-blbht from kubeaddons started at 2020-06-11 07:15:29 +0000 UTC (1 container statuses recorded)
Jun 11 08:09:47.997: INFO: 	Container elasticsearch ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: verifying the node has the label node ip-10-0-128-119.us-west-2.compute.internal
STEP: verifying the node has the label node ip-10-0-129-105.us-west-2.compute.internal
STEP: verifying the node has the label node ip-10-0-129-30.us-west-2.compute.internal
STEP: verifying the node has the label node ip-10-0-130-174.us-west-2.compute.internal
STEP: verifying the node has the label node ip-10-0-132-48.us-west-2.compute.internal
STEP: verifying the node has the label node ip-10-0-132-52.us-west-2.compute.internal
STEP: verifying the node has the label node ip-10-0-135-119.us-west-2.compute.internal
STEP: verifying the node has the label node ip-10-0-136-38.us-west-2.compute.internal
STEP: verifying the node has the label node ip-10-0-137-210.us-west-2.compute.internal
STEP: verifying the node has the label node ip-10-0-138-28.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod cert-manager-kubeaddons-7d7f98fbc6-86r7x requesting resource cpu=0m on Node ip-10-0-132-48.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod cert-manager-kubeaddons-cainjector-6dcd94769b-v8x5p requesting resource cpu=0m on Node ip-10-0-128-119.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod cert-manager-kubeaddons-webhook-77fbc6d59b-d67w5 requesting resource cpu=0m on Node ip-10-0-130-174.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod kommander-kubeaddons-cost-analyzer-7cfd4cb9cd-6tfnq requesting resource cpu=310m on Node ip-10-0-129-30.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod kommander-kubeaddons-grafana-66c558d6f5-ntspl requesting resource cpu=0m on Node ip-10-0-130-174.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod kommander-kubeaddons-karma-8df6cfdb6-4jg59 requesting resource cpu=0m on Node ip-10-0-129-30.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod kommander-kubeaddons-kommander-ui-86b4f5f88f-pvgsw requesting resource cpu=100m on Node ip-10-0-129-30.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod kommander-kubeaddons-kube-state-metrics-6d77646c89-bvrqg requesting resource cpu=0m on Node ip-10-0-129-30.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod kommander-kubeaddons-kubeaddons-catalog-6654f856df-kndt7 requesting resource cpu=100m on Node ip-10-0-138-28.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod kommander-kubeaddons-prometheus-alertmanager-7cdccf8c44-h9grs requesting resource cpu=0m on Node ip-10-0-130-174.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod kommander-kubeaddons-prometheus-server-86d645cc98-5bbdz requesting resource cpu=0m on Node ip-10-0-138-28.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod kommander-kubeaddons-thanos-query-6cddb86b55-f4g4l requesting resource cpu=0m on Node ip-10-0-130-174.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod kommander-kubecost-thanos-query-79fc7d8747-fm6rq requesting resource cpu=0m on Node ip-10-0-129-30.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod kubefed-admission-webhook-7b4997895b-x5h84 requesting resource cpu=0m on Node ip-10-0-130-174.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod kubefed-controller-manager-78b769f688-8mjvz requesting resource cpu=100m on Node ip-10-0-128-119.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod kubefed-controller-manager-78b769f688-rmx64 requesting resource cpu=100m on Node ip-10-0-132-52.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod yakcl-federation-authorizedlister-5db7b69bfd-zl4mt requesting resource cpu=100m on Node ip-10-0-138-28.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod yakcl-federation-cm-55469d7b6b-f6xhf requesting resource cpu=500m on Node ip-10-0-132-48.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod yakcl-federation-utility-apiserver-7d57699df9-wp7bs requesting resource cpu=100m on Node ip-10-0-138-28.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod yakcl-federation-webhook-6fcc46596-smpfh requesting resource cpu=100m on Node ip-10-0-138-28.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod yakcl-licensing-cm-7c5cc586b5-xgtnw requesting resource cpu=500m on Node ip-10-0-128-119.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod yakcl-licensing-webhook-6d876f844d-mrvm5 requesting resource cpu=100m on Node ip-10-0-130-174.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod calico-node-6mfwv requesting resource cpu=300m on Node ip-10-0-132-52.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod calico-node-fzn27 requesting resource cpu=300m on Node ip-10-0-138-28.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod calico-node-hrkxr requesting resource cpu=300m on Node ip-10-0-130-174.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod calico-node-sq7sb requesting resource cpu=300m on Node ip-10-0-135-119.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod calico-node-tnwfg requesting resource cpu=300m on Node ip-10-0-129-105.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod calico-node-tzhxw requesting resource cpu=300m on Node ip-10-0-136-38.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod calico-node-vkrzv requesting resource cpu=300m on Node ip-10-0-137-210.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod calico-node-vpvmh requesting resource cpu=300m on Node ip-10-0-129-30.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod calico-node-wr8kq requesting resource cpu=300m on Node ip-10-0-132-48.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod calico-node-wxzzc requesting resource cpu=300m on Node ip-10-0-128-119.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod ebs-csi-controller-0 requesting resource cpu=0m on Node ip-10-0-130-174.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod ebs-csi-node-2w524 requesting resource cpu=0m on Node ip-10-0-128-119.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod ebs-csi-node-8xl6p requesting resource cpu=0m on Node ip-10-0-136-38.us-west-2.compute.internal
Jun 11 08:09:48.191: INFO: Pod ebs-csi-node-bfvm8 requesting resource cpu=0m on Node ip-10-0-130-174.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod ebs-csi-node-cf4v4 requesting resource cpu=0m on Node ip-10-0-132-52.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod ebs-csi-node-frsf2 requesting resource cpu=0m on Node ip-10-0-137-210.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod ebs-csi-node-jzjgw requesting resource cpu=0m on Node ip-10-0-129-105.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod ebs-csi-node-nbd7n requesting resource cpu=0m on Node ip-10-0-138-28.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod ebs-csi-node-s8248 requesting resource cpu=0m on Node ip-10-0-129-30.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod ebs-csi-node-w9whr requesting resource cpu=0m on Node ip-10-0-135-119.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod ebs-csi-node-xzdsx requesting resource cpu=0m on Node ip-10-0-132-48.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod ebs-csi-snapshot-controller-0 requesting resource cpu=0m on Node ip-10-0-138-28.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod kube-proxy-6htkf requesting resource cpu=0m on Node ip-10-0-130-174.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod kube-proxy-7vnlr requesting resource cpu=0m on Node ip-10-0-129-105.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod kube-proxy-b8rs5 requesting resource cpu=0m on Node ip-10-0-128-119.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod kube-proxy-dz75m requesting resource cpu=0m on Node ip-10-0-136-38.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod kube-proxy-pp8td requesting resource cpu=0m on Node ip-10-0-138-28.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod kube-proxy-q79z5 requesting resource cpu=0m on Node ip-10-0-132-48.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod kube-proxy-qj7tx requesting resource cpu=0m on Node ip-10-0-137-210.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod kube-proxy-td2b5 requesting resource cpu=0m on Node ip-10-0-129-30.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod kube-proxy-w486c requesting resource cpu=0m on Node ip-10-0-132-52.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod kube-proxy-xnz2c requesting resource cpu=0m on Node ip-10-0-135-119.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod tiller-deploy-6cdf7f9d6f-d6b2p requesting resource cpu=0m on Node ip-10-0-129-30.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod alertmanager-prometheus-kubeaddons-prom-alertmanager-0 requesting resource cpu=110m on Node ip-10-0-135-119.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod dex-k8s-authenticator-kubeaddons-748fcc984d-vdpxg requesting resource cpu=100m on Node ip-10-0-138-28.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod dex-kubeaddons-696dbdb6c6-5prn9 requesting resource cpu=100m on Node ip-10-0-137-210.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod dex-kubeaddons-dex-controller-7bd5fc575c-xmnlq requesting resource cpu=100m on Node ip-10-0-135-119.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod dstorageclass-controller-manager-5c966c767f-h5rsz requesting resource cpu=0m on Node ip-10-0-129-30.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod elasticsearch-kubeaddons-client-6c56cc5c7-blbht requesting resource cpu=1500m on Node ip-10-0-138-28.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod elasticsearch-kubeaddons-client-6c56cc5c7-kpxr5 requesting resource cpu=1500m on Node ip-10-0-128-119.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod elasticsearch-kubeaddons-client-6c56cc5c7-zp9fl requesting resource cpu=1500m on Node ip-10-0-132-52.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod elasticsearch-kubeaddons-master-0 requesting resource cpu=1500m on Node ip-10-0-132-48.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod elasticsearch-kubeaddons-master-1 requesting resource cpu=1500m on Node ip-10-0-135-119.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod elasticsearch-kubeaddons-master-2 requesting resource cpu=1500m on Node ip-10-0-137-210.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod elasticsearchexporter-kubeaddons-elasticsearch-exporter-84pdc2n requesting resource cpu=0m on Node ip-10-0-130-174.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod external-dns-kubeaddons-765d55b455-vlxcv requesting resource cpu=0m on Node ip-10-0-135-119.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod fluentbit-kubeaddons-fluent-bit-2wt5v requesting resource cpu=200m on Node ip-10-0-132-48.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod fluentbit-kubeaddons-fluent-bit-46z8w requesting resource cpu=200m on Node ip-10-0-138-28.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod fluentbit-kubeaddons-fluent-bit-6lwhn requesting resource cpu=200m on Node ip-10-0-135-119.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod fluentbit-kubeaddons-fluent-bit-78mbt requesting resource cpu=200m on Node ip-10-0-132-52.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod fluentbit-kubeaddons-fluent-bit-hfm9r requesting resource cpu=200m on Node ip-10-0-128-119.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod fluentbit-kubeaddons-fluent-bit-ht88l requesting resource cpu=200m on Node ip-10-0-136-38.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod fluentbit-kubeaddons-fluent-bit-hv9bs requesting resource cpu=200m on Node ip-10-0-137-210.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod fluentbit-kubeaddons-fluent-bit-mpwz7 requesting resource cpu=200m on Node ip-10-0-129-30.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod fluentbit-kubeaddons-fluent-bit-nbsn6 requesting resource cpu=200m on Node ip-10-0-129-105.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod fluentbit-kubeaddons-fluent-bit-xx74f requesting resource cpu=200m on Node ip-10-0-130-174.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod gatekeeper-kubeaddons-776f4b5c96-jqsq8 requesting resource cpu=200m on Node ip-10-0-137-210.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod kibana-kubeaddons-c8c7b687-4lqrx requesting resource cpu=100m on Node ip-10-0-130-174.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod kube-oidc-proxy-kubeaddons-545d6df8-w9v44 requesting resource cpu=0m on Node ip-10-0-129-105.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod kubeaddons-controller-manager-986f46689-sk6x8 requesting resource cpu=100m on Node ip-10-0-132-52.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod kubernetes-dashboard-549989bcdf-n9ncj requesting resource cpu=250m on Node ip-10-0-129-30.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod opsportal-kubeaddons-kommander-ui-689b97997f-x45gn requesting resource cpu=100m on Node ip-10-0-129-105.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod opsportal-landing-6f6865b688-6r5kn requesting resource cpu=100m on Node ip-10-0-137-210.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod prometheus-kubeaddons-grafana-c8f7fcdb5-xjw4p requesting resource cpu=200m on Node ip-10-0-129-105.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod prometheus-kubeaddons-kube-state-metrics-6599df558b-mmf9r requesting resource cpu=0m on Node ip-10-0-137-210.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod prometheus-kubeaddons-prom-operator-767c8d59cb-44z76 requesting resource cpu=0m on Node ip-10-0-138-28.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod prometheus-kubeaddons-prometheus-node-exporter-4c6c2 requesting resource cpu=0m on Node ip-10-0-138-28.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod prometheus-kubeaddons-prometheus-node-exporter-4fllq requesting resource cpu=0m on Node ip-10-0-135-119.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod prometheus-kubeaddons-prometheus-node-exporter-78vv8 requesting resource cpu=0m on Node ip-10-0-136-38.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod prometheus-kubeaddons-prometheus-node-exporter-8z4lv requesting resource cpu=0m on Node ip-10-0-137-210.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod prometheus-kubeaddons-prometheus-node-exporter-cqrh7 requesting resource cpu=0m on Node ip-10-0-132-48.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod prometheus-kubeaddons-prometheus-node-exporter-dwrbk requesting resource cpu=0m on Node ip-10-0-129-30.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod prometheus-kubeaddons-prometheus-node-exporter-fz2j2 requesting resource cpu=0m on Node ip-10-0-128-119.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod prometheus-kubeaddons-prometheus-node-exporter-mzcfk requesting resource cpu=0m on Node ip-10-0-130-174.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod prometheus-kubeaddons-prometheus-node-exporter-nff74 requesting resource cpu=0m on Node ip-10-0-132-52.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod prometheus-kubeaddons-prometheus-node-exporter-zl5mt requesting resource cpu=0m on Node ip-10-0-129-105.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod prometheusadapter-kubeaddons-prometheus-adapter-77bc665f9-sjxp8 requesting resource cpu=2000m on Node ip-10-0-129-105.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod reloader-kubeaddons-reloader-7d4bd64cfb-4qjm4 requesting resource cpu=100m on Node ip-10-0-138-28.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod traefik-forward-auth-kubeaddons-6675968b94-fjznl requesting resource cpu=100m on Node ip-10-0-129-30.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod traefik-kubeaddons-64fbc79c9-54zfn requesting resource cpu=500m on Node ip-10-0-132-52.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod execpod28hb2 requesting resource cpu=0m on Node ip-10-0-136-38.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod nodeport-test-pdjbt requesting resource cpu=0m on Node ip-10-0-135-119.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod nodeport-test-pxc4f requesting resource cpu=0m on Node ip-10-0-136-38.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-0-136-38.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-594c2 requesting resource cpu=0m on Node ip-10-0-128-119.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-5wq4g requesting resource cpu=0m on Node ip-10-0-129-30.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-6cvjn requesting resource cpu=0m on Node ip-10-0-137-210.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-b7drp requesting resource cpu=0m on Node ip-10-0-135-119.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-cxrgp requesting resource cpu=0m on Node ip-10-0-129-105.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-ft9g5 requesting resource cpu=0m on Node ip-10-0-132-48.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-lf27c requesting resource cpu=0m on Node ip-10-0-138-28.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-lgn88 requesting resource cpu=0m on Node ip-10-0-136-38.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-tssmj requesting resource cpu=0m on Node ip-10-0-132-52.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-zvgtl requesting resource cpu=0m on Node ip-10-0-130-174.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod minio-0 requesting resource cpu=250m on Node ip-10-0-138-28.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod minio-1 requesting resource cpu=250m on Node ip-10-0-129-30.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod minio-2 requesting resource cpu=250m on Node ip-10-0-135-119.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod minio-3 requesting resource cpu=250m on Node ip-10-0-137-210.us-west-2.compute.internal
Jun 11 08:09:48.192: INFO: Pod velero-kubeaddons-5d85fcdcb9-gqb5c requesting resource cpu=0m on Node ip-10-0-135-119.us-west-2.compute.internal
STEP: Starting Pods to consume most of the cluster CPU.
Jun 11 08:09:48.192: INFO: Creating a pod which consumes cpu=805m on Node ip-10-0-138-28.us-west-2.compute.internal
Jun 11 08:09:48.205: INFO: Creating a pod which consumes cpu=980m on Node ip-10-0-128-119.us-west-2.compute.internal
Jun 11 08:09:48.214: INFO: Creating a pod which consumes cpu=840m on Node ip-10-0-129-105.us-west-2.compute.internal
Jun 11 08:09:48.221: INFO: Creating a pod which consumes cpu=1743m on Node ip-10-0-129-30.us-west-2.compute.internal
Jun 11 08:09:48.228: INFO: Creating a pod which consumes cpu=1078m on Node ip-10-0-135-119.us-west-2.compute.internal
Jun 11 08:09:48.237: INFO: Creating a pod which consumes cpu=2450m on Node ip-10-0-136-38.us-west-2.compute.internal
Jun 11 08:09:48.248: INFO: Creating a pod which consumes cpu=945m on Node ip-10-0-137-210.us-west-2.compute.internal
Jun 11 08:09:48.255: INFO: Creating a pod which consumes cpu=2310m on Node ip-10-0-130-174.us-west-2.compute.internal
Jun 11 08:09:48.263: INFO: Creating a pod which consumes cpu=1050m on Node ip-10-0-132-48.us-west-2.compute.internal
Jun 11 08:09:48.271: INFO: Creating a pod which consumes cpu=910m on Node ip-10-0-132-52.us-west-2.compute.internal
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event:
Type = [Normal], Name = [filler-pod-0936c6e3-d4c9-43c0-bc83-074efd833448.16176ef42b7abe68], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4432/filler-pod-0936c6e3-d4c9-43c0-bc83-074efd833448 to ip-10-0-132-48.us-west-2.compute.internal]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-0936c6e3-d4c9-43c0-bc83-074efd833448.16176ef44df33040], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-0936c6e3-d4c9-43c0-bc83-074efd833448.16176ef4595806ed], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-0936c6e3-d4c9-43c0-bc83-074efd833448.16176ef45ae48570], Reason = [Created], Message = [Created container filler-pod-0936c6e3-d4c9-43c0-bc83-074efd833448]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-0936c6e3-d4c9-43c0-bc83-074efd833448.16176ef45fb10f7f], Reason = [Started], Message = [Started container filler-pod-0936c6e3-d4c9-43c0-bc83-074efd833448]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-34abe830-b8c2-49a2-8d01-8ba02649dcf1.16176ef42a67f886], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4432/filler-pod-34abe830-b8c2-49a2-8d01-8ba02649dcf1 to ip-10-0-137-210.us-west-2.compute.internal]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-34abe830-b8c2-49a2-8d01-8ba02649dcf1.16176ef44db59f53], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-34abe830-b8c2-49a2-8d01-8ba02649dcf1.16176ef45843a87f], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-34abe830-b8c2-49a2-8d01-8ba02649dcf1.16176ef459c85df4], Reason = [Created], Message = [Created container filler-pod-34abe830-b8c2-49a2-8d01-8ba02649dcf1]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-34abe830-b8c2-49a2-8d01-8ba02649dcf1.16176ef45f2a6a3b], Reason = [Started], Message = [Started container filler-pod-34abe830-b8c2-49a2-8d01-8ba02649dcf1]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-3690cea0-427a-4ea3-ae33-767fb410ffe6.16176ef427ed4a0f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4432/filler-pod-3690cea0-427a-4ea3-ae33-767fb410ffe6 to ip-10-0-128-119.us-west-2.compute.internal]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-3690cea0-427a-4ea3-ae33-767fb410ffe6.16176ef44c1b6dc7], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-3690cea0-427a-4ea3-ae33-767fb410ffe6.16176ef45847e5bc], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-3690cea0-427a-4ea3-ae33-767fb410ffe6.16176ef45a18edc2], Reason = [Created], Message = [Created container filler-pod-3690cea0-427a-4ea3-ae33-767fb410ffe6]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-3690cea0-427a-4ea3-ae33-767fb410ffe6.16176ef45ec8101a], Reason = [Started], Message = [Started container filler-pod-3690cea0-427a-4ea3-ae33-767fb410ffe6]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-7af55591-58e5-48f7-aa37-ee07d7b0ee78.16176ef428610d0d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4432/filler-pod-7af55591-58e5-48f7-aa37-ee07d7b0ee78 to ip-10-0-129-105.us-west-2.compute.internal]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-7af55591-58e5-48f7-aa37-ee07d7b0ee78.16176ef44d01d060], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-7af55591-58e5-48f7-aa37-ee07d7b0ee78.16176ef4590713ef], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-7af55591-58e5-48f7-aa37-ee07d7b0ee78.16176ef45ab71db1], Reason = [Created], Message = [Created container filler-pod-7af55591-58e5-48f7-aa37-ee07d7b0ee78]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-7af55591-58e5-48f7-aa37-ee07d7b0ee78.16176ef45fec803a], Reason = [Started], Message = [Started container filler-pod-7af55591-58e5-48f7-aa37-ee07d7b0ee78]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-7b902bf9-23b2-4a06-a8cc-759a2f094852.16176ef429553fa8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4432/filler-pod-7b902bf9-23b2-4a06-a8cc-759a2f094852 to ip-10-0-135-119.us-west-2.compute.internal]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-7b902bf9-23b2-4a06-a8cc-759a2f094852.16176ef44cc4adde], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-7b902bf9-23b2-4a06-a8cc-759a2f094852.16176ef457ba13c1], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-7b902bf9-23b2-4a06-a8cc-759a2f094852.16176ef45945d8d4], Reason = [Created], Message = [Created container filler-pod-7b902bf9-23b2-4a06-a8cc-759a2f094852]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-7b902bf9-23b2-4a06-a8cc-759a2f094852.16176ef45e2d510c], Reason = [Started], Message = [Started container filler-pod-7b902bf9-23b2-4a06-a8cc-759a2f094852]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-995bd142-99bd-4ed0-8515-018984f8d9ff.16176ef42a07cae8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4432/filler-pod-995bd142-99bd-4ed0-8515-018984f8d9ff to ip-10-0-136-38.us-west-2.compute.internal]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-995bd142-99bd-4ed0-8515-018984f8d9ff.16176ef44ec9f381], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-995bd142-99bd-4ed0-8515-018984f8d9ff.16176ef45aee6bd8], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-995bd142-99bd-4ed0-8515-018984f8d9ff.16176ef45c87e67b], Reason = [Created], Message = [Created container filler-pod-995bd142-99bd-4ed0-8515-018984f8d9ff]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-995bd142-99bd-4ed0-8515-018984f8d9ff.16176ef461810227], Reason = [Started], Message = [Started container filler-pod-995bd142-99bd-4ed0-8515-018984f8d9ff]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-9f4aa45d-1690-40de-a1b0-ba78bdbb5967.16176ef42be6a19f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4432/filler-pod-9f4aa45d-1690-40de-a1b0-ba78bdbb5967 to ip-10-0-132-52.us-west-2.compute.internal]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-9f4aa45d-1690-40de-a1b0-ba78bdbb5967.16176ef450093db8], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-9f4aa45d-1690-40de-a1b0-ba78bdbb5967.16176ef45afb681e], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-9f4aa45d-1690-40de-a1b0-ba78bdbb5967.16176ef45cf767ed], Reason = [Created], Message = [Created container filler-pod-9f4aa45d-1690-40de-a1b0-ba78bdbb5967]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-9f4aa45d-1690-40de-a1b0-ba78bdbb5967.16176ef461e8b7cd], Reason = [Started], Message = [Started container filler-pod-9f4aa45d-1690-40de-a1b0-ba78bdbb5967]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-a7b24032-5328-4eea-93be-63dcc2b02993.16176ef428d38d0f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4432/filler-pod-a7b24032-5328-4eea-93be-63dcc2b02993 to ip-10-0-129-30.us-west-2.compute.internal]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-a7b24032-5328-4eea-93be-63dcc2b02993.16176ef44d294674], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-a7b24032-5328-4eea-93be-63dcc2b02993.16176ef459b944bf], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-a7b24032-5328-4eea-93be-63dcc2b02993.16176ef45b4fcb64], Reason = [Created], Message = [Created container filler-pod-a7b24032-5328-4eea-93be-63dcc2b02993]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-a7b24032-5328-4eea-93be-63dcc2b02993.16176ef460b1c2fa], Reason = [Started], Message = [Started container filler-pod-a7b24032-5328-4eea-93be-63dcc2b02993]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-ad9095c3-50f2-4a1b-9e67-d89f6e11e2d9.16176ef42aeef609], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4432/filler-pod-ad9095c3-50f2-4a1b-9e67-d89f6e11e2d9 to ip-10-0-130-174.us-west-2.compute.internal]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-ad9095c3-50f2-4a1b-9e67-d89f6e11e2d9.16176ef44fff7e90], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-ad9095c3-50f2-4a1b-9e67-d89f6e11e2d9.16176ef45b26b348], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-ad9095c3-50f2-4a1b-9e67-d89f6e11e2d9.16176ef45c937bfe], Reason = [Created], Message = [Created container filler-pod-ad9095c3-50f2-4a1b-9e67-d89f6e11e2d9]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-ad9095c3-50f2-4a1b-9e67-d89f6e11e2d9.16176ef4617f2992], Reason = [Started], Message = [Started container filler-pod-ad9095c3-50f2-4a1b-9e67-d89f6e11e2d9]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-b8c68492-dd3e-4e47-97d2-d8934462fdbe.16176ef42771a811], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4432/filler-pod-b8c68492-dd3e-4e47-97d2-d8934462fdbe to ip-10-0-138-28.us-west-2.compute.internal]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-b8c68492-dd3e-4e47-97d2-d8934462fdbe.16176ef44abef845], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-b8c68492-dd3e-4e47-97d2-d8934462fdbe.16176ef456a86391], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-b8c68492-dd3e-4e47-97d2-d8934462fdbe.16176ef458588abc], Reason = [Created], Message = [Created container filler-pod-b8c68492-dd3e-4e47-97d2-d8934462fdbe]
STEP: Considering event:
Type = [Normal], Name = [filler-pod-b8c68492-dd3e-4e47-97d2-d8934462fdbe.16176ef45dae652c], Reason = [Started], Message = [Started container filler-pod-b8c68492-dd3e-4e47-97d2-d8934462fdbe]
STEP: Considering event:
Type = [Warning], Name = [additional-pod.16176ef4a61a1a80], Reason = [FailedScheduling], Message = [0/20 nodes are available: 10 Insufficient cpu, 10 node(s) had taints that the pod didn't tolerate.]
STEP: removing the label node off the node ip-10-0-130-174.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-132-48.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-132-52.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-136-38.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-137-210.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-138-28.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-128-119.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-129-105.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-129-30.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-135-119.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:09:51.505: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "sched-pred-4432" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":280,"completed":124,"skipped":1917,"failed":0}
SS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:09:51.519: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2090
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-3363
STEP: Creating secret with name secret-test-4a1184fb-e800-404c-b169-f511c39bfa94
STEP: Creating a pod to test consume secrets
Jun 11 08:09:51.834: INFO: Waiting up to 5m0s for pod "pod-secrets-fc8b7ed0-e8df-440b-8e5d-b722266cc279" in namespace "secrets-2090" to be "success or failure"
Jun 11 08:09:51.838: INFO: Pod "pod-secrets-fc8b7ed0-e8df-440b-8e5d-b722266cc279": Phase="Pending", Reason="", readiness=false. Elapsed: 4.097788ms
Jun 11 08:09:53.843: INFO: Pod "pod-secrets-fc8b7ed0-e8df-440b-8e5d-b722266cc279": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009030278s
STEP: Saw pod success
Jun 11 08:09:53.843: INFO: Pod "pod-secrets-fc8b7ed0-e8df-440b-8e5d-b722266cc279" satisfied condition "success or failure"
Jun 11 08:09:53.847: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-secrets-fc8b7ed0-e8df-440b-8e5d-b722266cc279 container secret-volume-test: <nil>
STEP: delete the pod
Jun 11 08:09:53.872: INFO: Waiting for pod pod-secrets-fc8b7ed0-e8df-440b-8e5d-b722266cc279 to disappear
Jun 11 08:09:53.875: INFO: Pod pod-secrets-fc8b7ed0-e8df-440b-8e5d-b722266cc279 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:09:53.875: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "secrets-2090" for this suite.
STEP: Destroying namespace "secret-namespace-3363" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":280,"completed":125,"skipped":1919,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:09:53.897: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2432
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 08:09:54.046: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jun 11 08:09:59.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 --namespace=crd-publish-openapi-2432 create -f -'
Jun 11 08:10:00.525: INFO: stderr: ""
Jun 11 08:10:00.525: INFO: stdout: "e2e-test-crd-publish-openapi-2285-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 11 08:10:00.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 --namespace=crd-publish-openapi-2432 delete e2e-test-crd-publish-openapi-2285-crds test-foo'
Jun 11 08:10:00.699: INFO: stderr: ""
Jun 11 08:10:00.699: INFO: stdout: "e2e-test-crd-publish-openapi-2285-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jun 11 08:10:00.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 --namespace=crd-publish-openapi-2432 apply -f -'
Jun 11 08:10:00.913: INFO: stderr: ""
Jun 11 08:10:00.913: INFO: stdout: "e2e-test-crd-publish-openapi-2285-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 11 08:10:00.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 --namespace=crd-publish-openapi-2432 delete e2e-test-crd-publish-openapi-2285-crds test-foo'
Jun 11 08:10:01.054: INFO: stderr: ""
Jun 11 08:10:01.054: INFO: stdout: "e2e-test-crd-publish-openapi-2285-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jun 11 08:10:01.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 --namespace=crd-publish-openapi-2432 create -f -'
Jun 11 08:10:01.231: INFO: rc: 1
Jun 11 08:10:01.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 --namespace=crd-publish-openapi-2432 apply -f -'
Jun 11 08:10:01.510: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jun 11 08:10:01.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 --namespace=crd-publish-openapi-2432 create -f -'
Jun 11 08:10:01.783: INFO: rc: 1
Jun 11 08:10:01.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 --namespace=crd-publish-openapi-2432 apply -f -'
Jun 11 08:10:01.956: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jun 11 08:10:01.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 explain e2e-test-crd-publish-openapi-2285-crds'
Jun 11 08:10:02.305: INFO: stderr: ""
Jun 11 08:10:02.305: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2285-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jun 11 08:10:02.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 explain e2e-test-crd-publish-openapi-2285-crds.metadata'
Jun 11 08:10:02.548: INFO: stderr: ""
Jun 11 08:10:02.548: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2285-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jun 11 08:10:02.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 explain e2e-test-crd-publish-openapi-2285-crds.spec'
Jun 11 08:10:02.892: INFO: stderr: ""
Jun 11 08:10:02.892: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2285-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jun 11 08:10:02.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 explain e2e-test-crd-publish-openapi-2285-crds.spec.bars'
Jun 11 08:10:03.247: INFO: stderr: ""
Jun 11 08:10:03.247: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2285-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jun 11 08:10:03.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 explain e2e-test-crd-publish-openapi-2285-crds.spec.bars2'
Jun 11 08:10:03.478: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:10:08.888: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2432" for this suite.

• [SLOW TEST:15.006 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":280,"completed":126,"skipped":1924,"failed":0}
SSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:10:08.903: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7114
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-93df97f7-2505-4e19-9847-7617266ba057
STEP: Creating a pod to test consume secrets
Jun 11 08:10:09.071: INFO: Waiting up to 5m0s for pod "pod-secrets-0470016c-aaba-411a-85c1-8b98a8c6bb61" in namespace "secrets-7114" to be "success or failure"
Jun 11 08:10:09.077: INFO: Pod "pod-secrets-0470016c-aaba-411a-85c1-8b98a8c6bb61": Phase="Pending", Reason="", readiness=false. Elapsed: 6.443477ms
Jun 11 08:10:11.082: INFO: Pod "pod-secrets-0470016c-aaba-411a-85c1-8b98a8c6bb61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011054049s
STEP: Saw pod success
Jun 11 08:10:11.082: INFO: Pod "pod-secrets-0470016c-aaba-411a-85c1-8b98a8c6bb61" satisfied condition "success or failure"
Jun 11 08:10:11.086: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-secrets-0470016c-aaba-411a-85c1-8b98a8c6bb61 container secret-volume-test: <nil>
STEP: delete the pod
Jun 11 08:10:11.109: INFO: Waiting for pod pod-secrets-0470016c-aaba-411a-85c1-8b98a8c6bb61 to disappear
Jun 11 08:10:11.114: INFO: Pod pod-secrets-0470016c-aaba-411a-85c1-8b98a8c6bb61 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:10:11.114: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "secrets-7114" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":127,"skipped":1927,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:10:11.129: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4687
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override command
Jun 11 08:10:11.290: INFO: Waiting up to 5m0s for pod "client-containers-263a1ff9-2d4c-4b3e-8ea2-ee9626897e8c" in namespace "containers-4687" to be "success or failure"
Jun 11 08:10:11.294: INFO: Pod "client-containers-263a1ff9-2d4c-4b3e-8ea2-ee9626897e8c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.749471ms
Jun 11 08:10:13.298: INFO: Pod "client-containers-263a1ff9-2d4c-4b3e-8ea2-ee9626897e8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008337059s
STEP: Saw pod success
Jun 11 08:10:13.298: INFO: Pod "client-containers-263a1ff9-2d4c-4b3e-8ea2-ee9626897e8c" satisfied condition "success or failure"
Jun 11 08:10:13.302: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod client-containers-263a1ff9-2d4c-4b3e-8ea2-ee9626897e8c container test-container: <nil>
STEP: delete the pod
Jun 11 08:10:13.325: INFO: Waiting for pod client-containers-263a1ff9-2d4c-4b3e-8ea2-ee9626897e8c to disappear
Jun 11 08:10:13.331: INFO: Pod client-containers-263a1ff9-2d4c-4b3e-8ea2-ee9626897e8c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:10:13.331: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "containers-4687" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":280,"completed":128,"skipped":1944,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:10:13.346: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5297
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 08:10:14.187: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 08:10:17.212: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jun 11 08:10:19.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 attach --namespace=webhook-5297 to-be-attached-pod -i -c=container1'
Jun 11 08:10:19.381: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:10:19.393: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "webhook-5297" for this suite.
STEP: Destroying namespace "webhook-5297-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.134 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":280,"completed":129,"skipped":1953,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:10:19.480: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3030
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Starting the proxy
Jun 11 08:10:19.629: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-515000098 proxy --unix-socket=/tmp/kubectl-proxy-unix848705733/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:10:19.682: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "kubectl-3030" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":280,"completed":130,"skipped":1966,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:10:19.697: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4822
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 11 08:10:19.860: INFO: Waiting up to 5m0s for pod "pod-36e2c1b4-7671-4a83-8ab5-f6941d0c6c54" in namespace "emptydir-4822" to be "success or failure"
Jun 11 08:10:19.864: INFO: Pod "pod-36e2c1b4-7671-4a83-8ab5-f6941d0c6c54": Phase="Pending", Reason="", readiness=false. Elapsed: 3.856885ms
Jun 11 08:10:21.868: INFO: Pod "pod-36e2c1b4-7671-4a83-8ab5-f6941d0c6c54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008132621s
STEP: Saw pod success
Jun 11 08:10:21.868: INFO: Pod "pod-36e2c1b4-7671-4a83-8ab5-f6941d0c6c54" satisfied condition "success or failure"
Jun 11 08:10:21.872: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-36e2c1b4-7671-4a83-8ab5-f6941d0c6c54 container test-container: <nil>
STEP: delete the pod
Jun 11 08:10:21.895: INFO: Waiting for pod pod-36e2c1b4-7671-4a83-8ab5-f6941d0c6c54 to disappear
Jun 11 08:10:21.901: INFO: Pod pod-36e2c1b4-7671-4a83-8ab5-f6941d0c6c54 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:10:21.901: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "emptydir-4822" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":131,"skipped":1970,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:10:21.917: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7018
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 11 08:10:22.120: INFO: Waiting up to 5m0s for pod "downwardapi-volume-27d81be6-ac22-460f-8e7c-2a45189dbc9f" in namespace "projected-7018" to be "success or failure"
Jun 11 08:10:22.125: INFO: Pod "downwardapi-volume-27d81be6-ac22-460f-8e7c-2a45189dbc9f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.375984ms
Jun 11 08:10:24.129: INFO: Pod "downwardapi-volume-27d81be6-ac22-460f-8e7c-2a45189dbc9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008654522s
STEP: Saw pod success
Jun 11 08:10:24.129: INFO: Pod "downwardapi-volume-27d81be6-ac22-460f-8e7c-2a45189dbc9f" satisfied condition "success or failure"
Jun 11 08:10:24.133: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod downwardapi-volume-27d81be6-ac22-460f-8e7c-2a45189dbc9f container client-container: <nil>
STEP: delete the pod
Jun 11 08:10:24.156: INFO: Waiting for pod downwardapi-volume-27d81be6-ac22-460f-8e7c-2a45189dbc9f to disappear
Jun 11 08:10:24.162: INFO: Pod downwardapi-volume-27d81be6-ac22-460f-8e7c-2a45189dbc9f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:10:24.162: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "projected-7018" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":132,"skipped":1970,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:10:24.177: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2007
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-e85b5fb9-a20d-4e8c-9d68-8d757e566950
STEP: Creating secret with name s-test-opt-upd-7ee5f892-346f-4df9-9888-e45af6c21afa
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-e85b5fb9-a20d-4e8c-9d68-8d757e566950
STEP: Updating secret s-test-opt-upd-7ee5f892-346f-4df9-9888-e45af6c21afa
STEP: Creating secret with name s-test-opt-create-4804ebbb-6494-455d-b293-fa17b9788dc5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:11:42.773: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "secrets-2007" for this suite.

• [SLOW TEST:78.611 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":133,"skipped":2011,"failed":0}
SSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:11:42.789: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-7649
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 08:11:42.953: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-4e81d074-f879-4215-80e9-127fe6a68e0a" in namespace "security-context-test-7649" to be "success or failure"
Jun 11 08:11:42.957: INFO: Pod "busybox-readonly-false-4e81d074-f879-4215-80e9-127fe6a68e0a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.158482ms
Jun 11 08:11:44.962: INFO: Pod "busybox-readonly-false-4e81d074-f879-4215-80e9-127fe6a68e0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008554724s
Jun 11 08:11:44.962: INFO: Pod "busybox-readonly-false-4e81d074-f879-4215-80e9-127fe6a68e0a" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:11:44.962: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "security-context-test-7649" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":280,"completed":134,"skipped":2015,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:11:44.977: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-9237
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:11:47.260: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9237" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":280,"completed":135,"skipped":2039,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:11:47.277: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-300
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Jun 11 08:11:47.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 create -f - --namespace=kubectl-300'
Jun 11 08:11:47.742: INFO: stderr: ""
Jun 11 08:11:47.742: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Jun 11 08:11:48.746: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 11 08:11:48.746: INFO: Found 0 / 1
Jun 11 08:11:49.746: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 11 08:11:49.746: INFO: Found 1 / 1
Jun 11 08:11:49.746: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jun 11 08:11:49.751: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 11 08:11:49.751: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 11 08:11:49.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 patch pod agnhost-master-xlzd9 --namespace=kubectl-300 -p {"metadata":{"annotations":{"x":"y"}}}'
Jun 11 08:11:49.888: INFO: stderr: ""
Jun 11 08:11:49.888: INFO: stdout: "pod/agnhost-master-xlzd9 patched\n"
STEP: checking annotations
Jun 11 08:11:49.892: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 11 08:11:49.892: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:11:49.892: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "kubectl-300" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":280,"completed":136,"skipped":2039,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:11:49.908: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4914
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jun 11 08:11:52.604: INFO: Successfully updated pod "labelsupdatec235e364-67ec-4b7d-90e4-615e948940f4"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:11:54.623: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "downward-api-4914" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":137,"skipped":2055,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:11:54.639: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1299
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-b52abb3e-0c27-44f0-ac11-c02ad7e26292
STEP: Creating a pod to test consume secrets
Jun 11 08:11:54.804: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0ad466ef-efa2-469a-8f33-44295041b38c" in namespace "projected-1299" to be "success or failure"
Jun 11 08:11:54.808: INFO: Pod "pod-projected-secrets-0ad466ef-efa2-469a-8f33-44295041b38c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.908056ms
Jun 11 08:11:56.812: INFO: Pod "pod-projected-secrets-0ad466ef-efa2-469a-8f33-44295041b38c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007916307s
STEP: Saw pod success
Jun 11 08:11:56.812: INFO: Pod "pod-projected-secrets-0ad466ef-efa2-469a-8f33-44295041b38c" satisfied condition "success or failure"
Jun 11 08:11:56.815: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-projected-secrets-0ad466ef-efa2-469a-8f33-44295041b38c container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 11 08:11:56.840: INFO: Waiting for pod pod-projected-secrets-0ad466ef-efa2-469a-8f33-44295041b38c to disappear
Jun 11 08:11:56.843: INFO: Pod pod-projected-secrets-0ad466ef-efa2-469a-8f33-44295041b38c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:11:56.844: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "projected-1299" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":138,"skipped":2075,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:11:56.860: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1100
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 11 08:11:57.026: INFO: Waiting up to 5m0s for pod "downwardapi-volume-216cd2d7-2022-429a-8f7d-62f48d41e63b" in namespace "projected-1100" to be "success or failure"
Jun 11 08:11:57.030: INFO: Pod "downwardapi-volume-216cd2d7-2022-429a-8f7d-62f48d41e63b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.436337ms
Jun 11 08:11:59.034: INFO: Pod "downwardapi-volume-216cd2d7-2022-429a-8f7d-62f48d41e63b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008702814s
STEP: Saw pod success
Jun 11 08:11:59.034: INFO: Pod "downwardapi-volume-216cd2d7-2022-429a-8f7d-62f48d41e63b" satisfied condition "success or failure"
Jun 11 08:11:59.038: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod downwardapi-volume-216cd2d7-2022-429a-8f7d-62f48d41e63b container client-container: <nil>
STEP: delete the pod
Jun 11 08:11:59.061: INFO: Waiting for pod downwardapi-volume-216cd2d7-2022-429a-8f7d-62f48d41e63b to disappear
Jun 11 08:11:59.065: INFO: Pod downwardapi-volume-216cd2d7-2022-429a-8f7d-62f48d41e63b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:11:59.065: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "projected-1100" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":139,"skipped":2077,"failed":0}

------------------------------
[k8s.io] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:11:59.081: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5320
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-a0897021-002b-4ab4-b0a9-960d24898cc6 in namespace container-probe-5320
Jun 11 08:12:01.249: INFO: Started pod liveness-a0897021-002b-4ab4-b0a9-960d24898cc6 in namespace container-probe-5320
STEP: checking the pod's current state and verifying that restartCount is present
Jun 11 08:12:01.252: INFO: Initial restart count of pod liveness-a0897021-002b-4ab4-b0a9-960d24898cc6 is 0
Jun 11 08:12:25.318: INFO: Restart count of pod container-probe-5320/liveness-a0897021-002b-4ab4-b0a9-960d24898cc6 is now 1 (24.065077095s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:12:25.334: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "container-probe-5320" for this suite.

• [SLOW TEST:26.269 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":140,"skipped":2077,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:12:25.350: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3310
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-3310
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 11 08:12:25.501: INFO: Waiting up to 10m0s for all (but 7) nodes to be schedulable
Jun 11 08:12:25.504: INFO: Unschedulable nodes:
Jun 11 08:12:25.504: INFO: -> ip-10-0-129-22.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated route-reflector NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 08:12:25.504: INFO: -> ip-10-0-129-167.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated elasticsearch-data NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 08:12:25.504: INFO: -> ip-10-0-130-153.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated monitoring NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 08:12:25.504: INFO: -> ip-10-0-133-101.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated route-reflector NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 08:12:25.504: INFO: -> ip-10-0-132-151.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated elasticsearch-data NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 08:12:25.505: INFO: -> ip-10-0-137-193.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated elasticsearch-data NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 08:12:25.505: INFO: -> ip-10-0-139-38.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated route-reflector NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 08:12:25.505: INFO: ================================
STEP: Creating test pods
Jun 11 08:12:49.735: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.42.3:8080/dial?request=hostname&protocol=http&host=192.168.28.226&port=8080&tries=1'] Namespace:pod-network-test-3310 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:12:49.735: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:12:49.802: INFO: Waiting for responses: map[]
Jun 11 08:12:49.806: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.42.3:8080/dial?request=hostname&protocol=http&host=192.168.168.168&port=8080&tries=1'] Namespace:pod-network-test-3310 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:12:49.806: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:12:49.871: INFO: Waiting for responses: map[]
Jun 11 08:12:49.875: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.42.3:8080/dial?request=hostname&protocol=http&host=192.168.208.152&port=8080&tries=1'] Namespace:pod-network-test-3310 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:12:49.875: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:12:49.941: INFO: Waiting for responses: map[]
Jun 11 08:12:49.945: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.42.3:8080/dial?request=hostname&protocol=http&host=192.168.74.251&port=8080&tries=1'] Namespace:pod-network-test-3310 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:12:49.945: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:12:50.008: INFO: Waiting for responses: map[]
Jun 11 08:12:50.012: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.42.3:8080/dial?request=hostname&protocol=http&host=192.168.226.106&port=8080&tries=1'] Namespace:pod-network-test-3310 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:12:50.012: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:12:50.079: INFO: Waiting for responses: map[]
Jun 11 08:12:50.084: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.42.3:8080/dial?request=hostname&protocol=http&host=192.168.23.95&port=8080&tries=1'] Namespace:pod-network-test-3310 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:12:50.084: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:12:50.153: INFO: Waiting for responses: map[]
Jun 11 08:12:50.157: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.42.3:8080/dial?request=hostname&protocol=http&host=192.168.169.42&port=8080&tries=1'] Namespace:pod-network-test-3310 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:12:50.157: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:12:50.218: INFO: Waiting for responses: map[]
Jun 11 08:12:50.222: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.42.3:8080/dial?request=hostname&protocol=http&host=192.168.42.13&port=8080&tries=1'] Namespace:pod-network-test-3310 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:12:50.222: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:12:50.282: INFO: Waiting for responses: map[]
Jun 11 08:12:50.286: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.42.3:8080/dial?request=hostname&protocol=http&host=192.168.239.230&port=8080&tries=1'] Namespace:pod-network-test-3310 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:12:50.286: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:12:50.349: INFO: Waiting for responses: map[]
Jun 11 08:12:50.353: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.42.3:8080/dial?request=hostname&protocol=http&host=192.168.162.230&port=8080&tries=1'] Namespace:pod-network-test-3310 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:12:50.353: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:12:50.421: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:12:50.421: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "pod-network-test-3310" for this suite.

• [SLOW TEST:25.086 seconds]
[sig-network] Networking
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":141,"skipped":2093,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:12:50.436: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5146
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 11 08:12:50.598: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f62f2d11-f451-4e5a-bfc3-09e15babecf2" in namespace "downward-api-5146" to be "success or failure"
Jun 11 08:12:50.602: INFO: Pod "downwardapi-volume-f62f2d11-f451-4e5a-bfc3-09e15babecf2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.86733ms
Jun 11 08:12:52.606: INFO: Pod "downwardapi-volume-f62f2d11-f451-4e5a-bfc3-09e15babecf2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008378707s
STEP: Saw pod success
Jun 11 08:12:52.606: INFO: Pod "downwardapi-volume-f62f2d11-f451-4e5a-bfc3-09e15babecf2" satisfied condition "success or failure"
Jun 11 08:12:52.610: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod downwardapi-volume-f62f2d11-f451-4e5a-bfc3-09e15babecf2 container client-container: <nil>
STEP: delete the pod
Jun 11 08:12:52.634: INFO: Waiting for pod downwardapi-volume-f62f2d11-f451-4e5a-bfc3-09e15babecf2 to disappear
Jun 11 08:12:52.639: INFO: Pod downwardapi-volume-f62f2d11-f451-4e5a-bfc3-09e15babecf2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:12:52.639: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "downward-api-5146" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":142,"skipped":2095,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:12:52.656: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7355
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-64381d4d-722b-4467-99cb-48c6e902b5d8
STEP: Creating a pod to test consume configMaps
Jun 11 08:12:52.821: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e713394c-4dc8-40ab-912e-2d7372507bc4" in namespace "projected-7355" to be "success or failure"
Jun 11 08:12:52.825: INFO: Pod "pod-projected-configmaps-e713394c-4dc8-40ab-912e-2d7372507bc4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.085232ms
Jun 11 08:12:54.830: INFO: Pod "pod-projected-configmaps-e713394c-4dc8-40ab-912e-2d7372507bc4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008877082s
STEP: Saw pod success
Jun 11 08:12:54.830: INFO: Pod "pod-projected-configmaps-e713394c-4dc8-40ab-912e-2d7372507bc4" satisfied condition "success or failure"
Jun 11 08:12:54.834: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-projected-configmaps-e713394c-4dc8-40ab-912e-2d7372507bc4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 11 08:12:54.857: INFO: Waiting for pod pod-projected-configmaps-e713394c-4dc8-40ab-912e-2d7372507bc4 to disappear
Jun 11 08:12:54.862: INFO: Pod pod-projected-configmaps-e713394c-4dc8-40ab-912e-2d7372507bc4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:12:54.862: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "projected-7355" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":280,"completed":143,"skipped":2291,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:12:54.897: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-1996
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jun 11 08:12:59.602: INFO: Successfully updated pod "adopt-release-hcwhr"
STEP: Checking that the Job readopts the Pod
Jun 11 08:12:59.602: INFO: Waiting up to 15m0s for pod "adopt-release-hcwhr" in namespace "job-1996" to be "adopted"
Jun 11 08:12:59.606: INFO: Pod "adopt-release-hcwhr": Phase="Running", Reason="", readiness=true. Elapsed: 3.83738ms
Jun 11 08:13:01.611: INFO: Pod "adopt-release-hcwhr": Phase="Running", Reason="", readiness=true. Elapsed: 2.008728662s
Jun 11 08:13:01.611: INFO: Pod "adopt-release-hcwhr" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jun 11 08:13:02.126: INFO: Successfully updated pod "adopt-release-hcwhr"
STEP: Checking that the Job releases the Pod
Jun 11 08:13:02.126: INFO: Waiting up to 15m0s for pod "adopt-release-hcwhr" in namespace "job-1996" to be "released"
Jun 11 08:13:02.134: INFO: Pod "adopt-release-hcwhr": Phase="Running", Reason="", readiness=true. Elapsed: 7.749098ms
Jun 11 08:13:04.139: INFO: Pod "adopt-release-hcwhr": Phase="Running", Reason="", readiness=true. Elapsed: 2.012343634s
Jun 11 08:13:04.139: INFO: Pod "adopt-release-hcwhr" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:13:04.139: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "job-1996" for this suite.

• [SLOW TEST:9.257 seconds]
[sig-apps] Job
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":280,"completed":144,"skipped":2303,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:13:04.154: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9245
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 08:13:04.349: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"209a7b67-dcf8-4060-ad33-5a6ad579f903", Controller:(*bool)(0xc007d7b72a), BlockOwnerDeletion:(*bool)(0xc007d7b72b)}}
Jun 11 08:13:04.358: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"d9fed809-839d-47b3-91fe-fa1cf6b34248", Controller:(*bool)(0xc007e4ba42), BlockOwnerDeletion:(*bool)(0xc007e4ba43)}}
Jun 11 08:13:04.373: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"5418a6c6-7121-4624-9319-bf55d4e3cec1", Controller:(*bool)(0xc004013a72), BlockOwnerDeletion:(*bool)(0xc004013a73)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:13:09.386: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "gc-9245" for this suite.

• [SLOW TEST:5.246 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":280,"completed":145,"skipped":2319,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:13:09.400: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8561
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-secret-r887
STEP: Creating a pod to test atomic-volume-subpath
Jun 11 08:13:09.580: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-r887" in namespace "subpath-8561" to be "success or failure"
Jun 11 08:13:09.587: INFO: Pod "pod-subpath-test-secret-r887": Phase="Pending", Reason="", readiness=false. Elapsed: 7.204228ms
Jun 11 08:13:11.592: INFO: Pod "pod-subpath-test-secret-r887": Phase="Running", Reason="", readiness=true. Elapsed: 2.011682438s
Jun 11 08:13:13.596: INFO: Pod "pod-subpath-test-secret-r887": Phase="Running", Reason="", readiness=true. Elapsed: 4.016066611s
Jun 11 08:13:15.600: INFO: Pod "pod-subpath-test-secret-r887": Phase="Running", Reason="", readiness=true. Elapsed: 6.020386054s
Jun 11 08:13:17.605: INFO: Pod "pod-subpath-test-secret-r887": Phase="Running", Reason="", readiness=true. Elapsed: 8.024698653s
Jun 11 08:13:19.609: INFO: Pod "pod-subpath-test-secret-r887": Phase="Running", Reason="", readiness=true. Elapsed: 10.029485655s
Jun 11 08:13:21.614: INFO: Pod "pod-subpath-test-secret-r887": Phase="Running", Reason="", readiness=true. Elapsed: 12.034165442s
Jun 11 08:13:23.619: INFO: Pod "pod-subpath-test-secret-r887": Phase="Running", Reason="", readiness=true. Elapsed: 14.039143494s
Jun 11 08:13:25.624: INFO: Pod "pod-subpath-test-secret-r887": Phase="Running", Reason="", readiness=true. Elapsed: 16.043773157s
Jun 11 08:13:27.629: INFO: Pod "pod-subpath-test-secret-r887": Phase="Running", Reason="", readiness=true. Elapsed: 18.048612074s
Jun 11 08:13:29.633: INFO: Pod "pod-subpath-test-secret-r887": Phase="Running", Reason="", readiness=true. Elapsed: 20.053254115s
Jun 11 08:13:31.638: INFO: Pod "pod-subpath-test-secret-r887": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.057730617s
STEP: Saw pod success
Jun 11 08:13:31.638: INFO: Pod "pod-subpath-test-secret-r887" satisfied condition "success or failure"
Jun 11 08:13:31.642: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-subpath-test-secret-r887 container test-container-subpath-secret-r887: <nil>
STEP: delete the pod
Jun 11 08:13:31.666: INFO: Waiting for pod pod-subpath-test-secret-r887 to disappear
Jun 11 08:13:31.669: INFO: Pod pod-subpath-test-secret-r887 no longer exists
STEP: Deleting pod pod-subpath-test-secret-r887
Jun 11 08:13:31.669: INFO: Deleting pod "pod-subpath-test-secret-r887" in namespace "subpath-8561"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:13:31.674: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "subpath-8561" for this suite.

• [SLOW TEST:22.288 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":280,"completed":146,"skipped":2329,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:13:31.689: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8404
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8404
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8404
STEP: creating replication controller externalsvc in namespace services-8404
I0611 08:13:31.869715      23 runners.go:189] Created replication controller with name: externalsvc, namespace: services-8404, replica count: 2
I0611 08:13:34.920196      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady
STEP: changing the ClusterIP service to type=ExternalName
Jun 11 08:13:34.944: INFO: Creating new exec pod
Jun 11 08:13:36.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=services-8404 execpod4l488 -- /bin/sh -x -c nslookup clusterip-service'
Jun 11 08:13:37.153: INFO: stderr: "+ nslookup clusterip-service\n"
Jun 11 08:13:37.153: INFO: stdout: "Server:\t\t10.0.0.10\nAddress:\t10.0.0.10#53\n\nclusterip-service.services-8404.svc.cluster.local\tcanonical name = externalsvc.services-8404.svc.cluster.local.\nName:\texternalsvc.services-8404.svc.cluster.local\nAddress: 10.0.51.71\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8404, will wait for the garbage collector to delete the pods
Jun 11 08:13:37.218: INFO: Deleting ReplicationController externalsvc took: 11.100593ms
Jun 11 08:13:38.818: INFO: Terminating ReplicationController externalsvc pods took: 1.600249696s
Jun 11 08:13:47.048: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:13:47.069: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "services-8404" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:15.395 seconds]
[sig-network] Services
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":280,"completed":147,"skipped":2422,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:13:47.085: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8545
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jun 11 08:13:47.250: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8545 /api/v1/namespaces/watch-8545/configmaps/e2e-watch-test-watch-closed 42cd12fd-b455-4897-b2bc-44afe00734fe 121002 0 2020-06-11 08:13:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 11 08:13:47.250: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8545 /api/v1/namespaces/watch-8545/configmaps/e2e-watch-test-watch-closed 42cd12fd-b455-4897-b2bc-44afe00734fe 121003 0 2020-06-11 08:13:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jun 11 08:13:47.270: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8545 /api/v1/namespaces/watch-8545/configmaps/e2e-watch-test-watch-closed 42cd12fd-b455-4897-b2bc-44afe00734fe 121004 0 2020-06-11 08:13:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 11 08:13:47.270: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8545 /api/v1/namespaces/watch-8545/configmaps/e2e-watch-test-watch-closed 42cd12fd-b455-4897-b2bc-44afe00734fe 121005 0 2020-06-11 08:13:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:13:47.271: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "watch-8545" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":280,"completed":148,"skipped":2427,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:13:47.285: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4163
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-d3d01f70-948f-47bf-904b-2be3a82d9ce2
STEP: Creating a pod to test consume secrets
Jun 11 08:13:47.456: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bf74daf5-50cf-4e4a-b7b7-87595297193c" in namespace "projected-4163" to be "success or failure"
Jun 11 08:13:47.460: INFO: Pod "pod-projected-secrets-bf74daf5-50cf-4e4a-b7b7-87595297193c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.957034ms
Jun 11 08:13:49.464: INFO: Pod "pod-projected-secrets-bf74daf5-50cf-4e4a-b7b7-87595297193c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008292757s
STEP: Saw pod success
Jun 11 08:13:49.464: INFO: Pod "pod-projected-secrets-bf74daf5-50cf-4e4a-b7b7-87595297193c" satisfied condition "success or failure"
Jun 11 08:13:49.468: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-projected-secrets-bf74daf5-50cf-4e4a-b7b7-87595297193c container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 11 08:13:49.491: INFO: Waiting for pod pod-projected-secrets-bf74daf5-50cf-4e4a-b7b7-87595297193c to disappear
Jun 11 08:13:49.496: INFO: Pod pod-projected-secrets-bf74daf5-50cf-4e4a-b7b7-87595297193c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:13:49.496: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "projected-4163" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":149,"skipped":2449,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:13:49.512: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7374
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 11 08:13:49.674: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c5d8b5a5-ddc6-4238-a857-f0d228b60075" in namespace "projected-7374" to be "success or failure"
Jun 11 08:13:49.679: INFO: Pod "downwardapi-volume-c5d8b5a5-ddc6-4238-a857-f0d228b60075": Phase="Pending", Reason="", readiness=false. Elapsed: 4.872138ms
Jun 11 08:13:51.684: INFO: Pod "downwardapi-volume-c5d8b5a5-ddc6-4238-a857-f0d228b60075": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009716298s
STEP: Saw pod success
Jun 11 08:13:51.684: INFO: Pod "downwardapi-volume-c5d8b5a5-ddc6-4238-a857-f0d228b60075" satisfied condition "success or failure"
Jun 11 08:13:51.688: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod downwardapi-volume-c5d8b5a5-ddc6-4238-a857-f0d228b60075 container client-container: <nil>
STEP: delete the pod
Jun 11 08:13:51.713: INFO: Waiting for pod downwardapi-volume-c5d8b5a5-ddc6-4238-a857-f0d228b60075 to disappear
Jun 11 08:13:51.722: INFO: Pod downwardapi-volume-c5d8b5a5-ddc6-4238-a857-f0d228b60075 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:13:51.722: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "projected-7374" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":150,"skipped":2485,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:13:51.738: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-997
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jun 11 08:13:51.887: INFO: Waiting up to 1m0s for all (but 7) nodes to be ready
Jun 11 08:13:51.917: INFO: Waiting for terminating namespaces to be deleted...
Jun 11 08:13:51.921: INFO:
Logging pods the kubelet thinks is on node ip-10-0-128-119.us-west-2.compute.internal before test
Jun 11 08:13:51.941: INFO: kube-proxy-b8rs5 from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:51.941: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:13:51.941: INFO: calico-node-wxzzc from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:51.941: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:13:51.941: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:13:51.941: INFO: cert-manager-kubeaddons-cainjector-6dcd94769b-v8x5p from cert-manager started at 2020-06-11 05:29:09 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:51.941: INFO: 	Container cainjector ready: true, restart count 0
Jun 11 08:13:51.941: INFO: elasticsearch-kubeaddons-client-6c56cc5c7-kpxr5 from kubeaddons started at 2020-06-11 05:30:43 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:51.941: INFO: 	Container elasticsearch ready: true, restart count 0
Jun 11 08:13:51.941: INFO: yakcl-licensing-cm-7c5cc586b5-xgtnw from kommander started at 2020-06-11 05:33:34 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:51.941: INFO: 	Container controller-manager ready: true, restart count 0
Jun 11 08:13:51.941: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 11 08:13:51.941: INFO: ebs-csi-node-2w524 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:13:51.941: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:13:51.941: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:13:51.941: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:13:51.941: INFO: prometheus-kubeaddons-prometheus-node-exporter-fz2j2 from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:51.941: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:13:51.941: INFO: kubefed-controller-manager-78b769f688-8mjvz from kommander started at 2020-06-11 05:33:45 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:51.941: INFO: 	Container controller-manager ready: true, restart count 0
Jun 11 08:13:51.941: INFO: fluentbit-kubeaddons-fluent-bit-hfm9r from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:51.941: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:13:51.941: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-594c2 from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:51.941: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:13:51.941: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:13:51.941: INFO:
Logging pods the kubelet thinks is on node ip-10-0-129-105.us-west-2.compute.internal before test
Jun 11 08:13:51.962: INFO: kube-proxy-7vnlr from kube-system started at 2020-06-11 05:26:20 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:51.963: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:13:51.963: INFO: ebs-csi-node-jzjgw from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:13:51.963: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:13:51.963: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:13:51.963: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:13:51.963: INFO: kube-oidc-proxy-kubeaddons-545d6df8-w9v44 from kubeaddons started at 2020-06-11 05:31:42 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:51.963: INFO: 	Container kube-oidc-proxy ready: true, restart count 0
Jun 11 08:13:51.963: INFO: prometheus-kubeaddons-grafana-c8f7fcdb5-xjw4p from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:51.963: INFO: 	Container grafana ready: true, restart count 0
Jun 11 08:13:51.963: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Jun 11 08:13:51.963: INFO: prometheus-kubeaddons-prometheus-node-exporter-zl5mt from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:51.963: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:13:51.963: INFO: fluentbit-kubeaddons-fluent-bit-nbsn6 from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:51.963: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:13:51.963: INFO: calico-node-tnwfg from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:51.963: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:13:51.963: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:13:51.963: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-cxrgp from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:51.963: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:13:51.963: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:13:51.963: INFO: prometheusadapter-kubeaddons-prometheus-adapter-77bc665f9-sjxp8 from kubeaddons started at 2020-06-11 05:33:06 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:51.963: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 11 08:13:51.963: INFO: opsportal-kubeaddons-kommander-ui-689b97997f-x45gn from kubeaddons started at 2020-06-11 05:28:52 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:51.963: INFO: 	Container opsportal-kubeaddons-kommander-ui ready: true, restart count 0
Jun 11 08:13:51.963: INFO:
Logging pods the kubelet thinks is on node ip-10-0-129-30.us-west-2.compute.internal before test
Jun 11 08:13:51.983: INFO: fluentbit-kubeaddons-fluent-bit-mpwz7 from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:51.983: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:13:51.983: INFO: kommander-kubeaddons-kommander-ui-86b4f5f88f-pvgsw from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:51.983: INFO: 	Container kommander-kubeaddons-kommander-ui ready: true, restart count 0
Jun 11 08:13:51.983: INFO: tiller-deploy-6cdf7f9d6f-d6b2p from kube-system started at 2020-06-11 05:28:13 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:51.983: INFO: 	Container tiller ready: true, restart count 0
Jun 11 08:13:51.983: INFO: minio-1 from velero started at 2020-06-11 05:32:15 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:51.983: INFO: 	Container minio ready: true, restart count 0
Jun 11 08:13:51.983: INFO: kommander-kubecost-thanos-query-79fc7d8747-fm6rq from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:51.983: INFO: 	Container thanos-query ready: true, restart count 0
Jun 11 08:13:51.983: INFO: kommander-kubeaddons-kube-state-metrics-6d77646c89-bvrqg from kommander started at 2020-06-11 05:33:33 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:51.983: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 11 08:13:51.983: INFO: kube-proxy-td2b5 from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:51.983: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:13:51.983: INFO: calico-node-vpvmh from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:51.983: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:13:51.983: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:13:51.983: INFO: kommander-kubeaddons-cost-analyzer-7cfd4cb9cd-6tfnq from kommander started at 2020-06-11 05:33:41 +0000 UTC (3 container statuses recorded)
Jun 11 08:13:51.983: INFO: 	Container cost-analyzer-frontend ready: true, restart count 0
Jun 11 08:13:51.983: INFO: 	Container cost-analyzer-server ready: true, restart count 0
Jun 11 08:13:51.983: INFO: 	Container cost-model ready: true, restart count 0
Jun 11 08:13:51.983: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-5wq4g from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:51.983: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:13:51.983: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:13:51.983: INFO: dstorageclass-controller-manager-5c966c767f-h5rsz from kubeaddons started at 2020-06-11 05:29:47 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:51.983: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 11 08:13:51.983: INFO: 	Container manager ready: true, restart count 0
Jun 11 08:13:51.983: INFO: prometheus-kubeaddons-prometheus-node-exporter-dwrbk from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:51.983: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:13:51.983: INFO: kommander-kubeaddons-karma-8df6cfdb6-4jg59 from kommander started at 2020-06-11 05:33:33 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:51.983: INFO: 	Container karma ready: true, restart count 0
Jun 11 08:13:51.983: INFO: kubernetes-dashboard-549989bcdf-n9ncj from kubeaddons started at 2020-06-11 07:15:29 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:51.983: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jun 11 08:13:51.983: INFO: ebs-csi-node-s8248 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:13:51.983: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:13:51.983: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:13:51.983: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:13:51.983: INFO: traefik-forward-auth-kubeaddons-6675968b94-fjznl from kubeaddons started at 2020-06-11 05:37:16 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:51.983: INFO: 	Container traefik-forward-auth ready: true, restart count 1
Jun 11 08:13:51.983: INFO:
Logging pods the kubelet thinks is on node ip-10-0-130-174.us-west-2.compute.internal before test
Jun 11 08:13:52.003: INFO: ebs-csi-controller-0 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (6 container statuses recorded)
Jun 11 08:13:52.003: INFO: 	Container csi-attacher ready: true, restart count 0
Jun 11 08:13:52.003: INFO: 	Container csi-provisioner ready: true, restart count 0
Jun 11 08:13:52.003: INFO: 	Container csi-resizer ready: true, restart count 0
Jun 11 08:13:52.003: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jun 11 08:13:52.003: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:13:52.003: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:13:52.003: INFO: kommander-kubeaddons-thanos-query-6cddb86b55-f4g4l from kommander started at 2020-06-11 05:33:35 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.003: INFO: 	Container thanos-query ready: true, restart count 0
Jun 11 08:13:52.003: INFO: elasticsearchexporter-kubeaddons-elasticsearch-exporter-84pdc2n from kubeaddons started at 2020-06-11 05:36:03 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.003: INFO: 	Container elasticsearch-exporter ready: true, restart count 0
Jun 11 08:13:52.003: INFO: calico-node-hrkxr from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:52.003: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:13:52.003: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:13:52.003: INFO: kubefed-admission-webhook-7b4997895b-x5h84 from kommander started at 2020-06-11 05:33:38 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.003: INFO: 	Container admission-webhook ready: true, restart count 0
Jun 11 08:13:52.003: INFO: kibana-kubeaddons-c8c7b687-4lqrx from kubeaddons started at 2020-06-11 07:15:29 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:52.003: INFO: 	Container initialize-kibana-index ready: true, restart count 0
Jun 11 08:13:52.003: INFO: 	Container kibana ready: true, restart count 0
Jun 11 08:13:52.003: INFO: fluentbit-kubeaddons-fluent-bit-xx74f from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.003: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:13:52.003: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-zvgtl from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:52.003: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:13:52.003: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:13:52.003: INFO: cert-manager-kubeaddons-webhook-77fbc6d59b-d67w5 from cert-manager started at 2020-06-11 05:29:09 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.003: INFO: 	Container cert-manager ready: true, restart count 1
Jun 11 08:13:52.003: INFO: ebs-csi-node-bfvm8 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:13:52.003: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:13:52.003: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:13:52.003: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:13:52.003: INFO: kommander-kubeaddons-grafana-66c558d6f5-ntspl from kommander started at 2020-06-11 05:33:32 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:52.003: INFO: 	Container grafana ready: true, restart count 0
Jun 11 08:13:52.003: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Jun 11 08:13:52.003: INFO: kommander-kubeaddons-prometheus-alertmanager-7cdccf8c44-h9grs from kommander started at 2020-06-11 05:33:38 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:52.003: INFO: 	Container prometheus-alertmanager ready: true, restart count 0
Jun 11 08:13:52.003: INFO: 	Container prometheus-alertmanager-configmap-reload ready: true, restart count 0
Jun 11 08:13:52.003: INFO: kube-proxy-6htkf from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.003: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:13:52.003: INFO: prometheus-kubeaddons-prometheus-node-exporter-mzcfk from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.003: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:13:52.003: INFO: yakcl-licensing-webhook-6d876f844d-mrvm5 from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.003: INFO: 	Container webhook ready: true, restart count 0
Jun 11 08:13:52.003: INFO:
Logging pods the kubelet thinks is on node ip-10-0-132-48.us-west-2.compute.internal before test
Jun 11 08:13:52.019: INFO: kube-proxy-q79z5 from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.019: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:13:52.019: INFO: ebs-csi-node-xzdsx from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:13:52.019: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:13:52.019: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:13:52.019: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:13:52.019: INFO: elasticsearch-kubeaddons-master-0 from kubeaddons started at 2020-06-11 05:31:03 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.019: INFO: 	Container elasticsearch ready: true, restart count 0
Jun 11 08:13:52.019: INFO: fluentbit-kubeaddons-fluent-bit-2wt5v from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.019: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:13:52.019: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-ft9g5 from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:52.019: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:13:52.019: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:13:52.019: INFO: calico-node-wr8kq from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:52.019: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:13:52.019: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:13:52.019: INFO: cert-manager-kubeaddons-7d7f98fbc6-86r7x from cert-manager started at 2020-06-11 05:29:09 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.019: INFO: 	Container cert-manager ready: true, restart count 0
Jun 11 08:13:52.019: INFO: prometheus-kubeaddons-prometheus-node-exporter-cqrh7 from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.019: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:13:52.019: INFO: yakcl-federation-cm-55469d7b6b-f6xhf from kommander started at 2020-06-11 05:33:33 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:52.019: INFO: 	Container controller-manager ready: true, restart count 0
Jun 11 08:13:52.019: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 11 08:13:52.019: INFO:
Logging pods the kubelet thinks is on node ip-10-0-132-52.us-west-2.compute.internal before test
Jun 11 08:13:52.038: INFO: kube-proxy-w486c from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.038: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:13:52.038: INFO: calico-node-6mfwv from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:52.038: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:13:52.038: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:13:52.038: INFO: traefik-kubeaddons-64fbc79c9-54zfn from kubeaddons started at 2020-06-11 05:31:03 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.038: INFO: 	Container traefik-kubeaddons ready: true, restart count 0
Jun 11 08:13:52.038: INFO: kubefed-controller-manager-78b769f688-rmx64 from kommander started at 2020-06-11 05:33:38 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.038: INFO: 	Container controller-manager ready: true, restart count 0
Jun 11 08:13:52.038: INFO: traefik-kubeaddons-1.72.19-zdq6l from kubeaddons started at 2020-06-11 05:29:49 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.038: INFO: 	Container traefik ready: false, restart count 0
Jun 11 08:13:52.038: INFO: ebs-csi-node-cf4v4 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:13:52.038: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:13:52.038: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:13:52.038: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:13:52.038: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-tssmj from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:52.038: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:13:52.038: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:13:52.038: INFO: kubeaddons-controller-manager-986f46689-sk6x8 from kubeaddons started at 2020-06-11 05:27:57 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.038: INFO: 	Container manager ready: true, restart count 0
Jun 11 08:13:52.038: INFO: prometheus-kubeaddons-prometheus-node-exporter-nff74 from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.038: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:13:52.038: INFO: elasticsearch-kubeaddons-client-6c56cc5c7-zp9fl from kubeaddons started at 2020-06-11 05:30:43 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.038: INFO: 	Container elasticsearch ready: true, restart count 0
Jun 11 08:13:52.038: INFO: fluentbit-kubeaddons-fluent-bit-78mbt from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.038: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:13:52.038: INFO:
Logging pods the kubelet thinks is on node ip-10-0-135-119.us-west-2.compute.internal before test
Jun 11 08:13:52.056: INFO: calico-node-sq7sb from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:52.056: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:13:52.056: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:13:52.056: INFO: external-dns-kubeaddons-765d55b455-vlxcv from kubeaddons started at 2020-06-11 05:28:25 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.056: INFO: 	Container external-dns ready: true, restart count 0
Jun 11 08:13:52.056: INFO: minio-2 from velero started at 2020-06-11 05:32:12 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.056: INFO: 	Container minio ready: true, restart count 0
Jun 11 08:13:52.056: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-b7drp from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:52.056: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:13:52.056: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:13:52.056: INFO: kube-proxy-xnz2c from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.056: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:13:52.056: INFO: prometheus-kubeaddons-prometheus-node-exporter-4fllq from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.056: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:13:52.056: INFO: alertmanager-prometheus-kubeaddons-prom-alertmanager-0 from kubeaddons started at 2020-06-11 05:32:06 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:52.056: INFO: 	Container alertmanager ready: true, restart count 0
Jun 11 08:13:52.056: INFO: 	Container config-reloader ready: true, restart count 0
Jun 11 08:13:52.056: INFO: ebs-csi-node-w9whr from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:13:52.056: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:13:52.056: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:13:52.056: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:13:52.056: INFO: dex-kubeaddons-dex-controller-7bd5fc575c-xmnlq from kubeaddons started at 2020-06-11 05:31:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:52.056: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 11 08:13:52.056: INFO: 	Container manager ready: true, restart count 0
Jun 11 08:13:52.056: INFO: velero-kubeaddons-5d85fcdcb9-gqb5c from velero started at 2020-06-11 05:32:08 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.056: INFO: 	Container velero ready: true, restart count 3
Jun 11 08:13:52.056: INFO: elasticsearch-kubeaddons-master-1 from kubeaddons started at 2020-06-11 05:32:29 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.056: INFO: 	Container elasticsearch ready: true, restart count 0
Jun 11 08:13:52.056: INFO: fluentbit-kubeaddons-fluent-bit-6lwhn from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.056: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:13:52.056: INFO:
Logging pods the kubelet thinks is on node ip-10-0-136-38.us-west-2.compute.internal before test
Jun 11 08:13:52.064: INFO: calico-node-tzhxw from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:52.064: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:13:52.064: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:13:52.064: INFO: sonobuoy from sonobuoy started at 2020-06-11 07:40:59 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.064: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 11 08:13:52.064: INFO: fluentbit-kubeaddons-fluent-bit-ht88l from kubeaddons started at 2020-06-11 08:04:31 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.064: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:13:52.064: INFO: cost-analyzer-checks-1591862400-tjx9g from kommander started at 2020-06-11 08:00:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.064: INFO: 	Container cost-analyzer-checks ready: false, restart count 0
Jun 11 08:13:52.064: INFO: execpod4l488 from services-8404 started at 2020-06-11 08:13:34 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.064: INFO: 	Container agnhost-pause ready: true, restart count 0
Jun 11 08:13:52.064: INFO: kube-proxy-dz75m from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.064: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:13:52.064: INFO: ebs-csi-node-8xl6p from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:13:52.064: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:13:52.064: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:13:52.064: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:13:52.064: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-lgn88 from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:52.064: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:13:52.064: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:13:52.064: INFO: prometheus-kubeaddons-prometheus-node-exporter-78vv8 from kubeaddons started at 2020-06-11 08:04:15 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.064: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:13:52.064: INFO: cost-analyzer-checks-1591863000-vsn7c from kommander started at 2020-06-11 08:10:03 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.064: INFO: 	Container cost-analyzer-checks ready: false, restart count 0
Jun 11 08:13:52.064: INFO:
Logging pods the kubelet thinks is on node ip-10-0-137-210.us-west-2.compute.internal before test
Jun 11 08:13:52.080: INFO: prometheus-kubeaddons-kube-state-metrics-6599df558b-mmf9r from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.080: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 11 08:13:52.080: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-6cvjn from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:52.080: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:13:52.080: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:13:52.080: INFO: prometheus-kubeaddons-prometheus-node-exporter-8z4lv from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.080: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:13:52.080: INFO: calico-node-vkrzv from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:52.080: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:13:52.080: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:13:52.080: INFO: ebs-csi-node-frsf2 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:13:52.080: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:13:52.080: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:13:52.080: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:13:52.080: INFO: elasticsearch-kubeaddons-master-2 from kubeaddons started at 2020-06-11 05:33:29 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.080: INFO: 	Container elasticsearch ready: true, restart count 0
Jun 11 08:13:52.080: INFO: fluentbit-kubeaddons-fluent-bit-hv9bs from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.080: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:13:52.080: INFO: dex-kubeaddons-696dbdb6c6-5prn9 from kubeaddons started at 2020-06-11 05:37:11 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.080: INFO: 	Container main ready: true, restart count 0
Jun 11 08:13:52.080: INFO: opsportal-landing-6f6865b688-6r5kn from kubeaddons started at 2020-06-11 05:28:52 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.080: INFO: 	Container opsportal-landing ready: true, restart count 0
Jun 11 08:13:52.080: INFO: gatekeeper-kubeaddons-776f4b5c96-jqsq8 from kubeaddons started at 2020-06-11 05:29:52 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.080: INFO: 	Container manager ready: true, restart count 0
Jun 11 08:13:52.080: INFO: kube-proxy-qj7tx from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.080: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:13:52.080: INFO: minio-3 from velero started at 2020-06-11 05:32:15 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.080: INFO: 	Container minio ready: true, restart count 0
Jun 11 08:13:52.080: INFO:
Logging pods the kubelet thinks is on node ip-10-0-138-28.us-west-2.compute.internal before test
Jun 11 08:13:52.100: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-lf27c from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:52.100: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:13:52.100: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:13:52.100: INFO: kube-proxy-pp8td from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.100: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:13:52.100: INFO: ebs-csi-node-nbd7n from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:13:52.100: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:13:52.100: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:13:52.100: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:13:52.100: INFO: prometheus-kubeaddons-prometheus-node-exporter-4c6c2 from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.100: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:13:52.100: INFO: kommander-kubeaddons-prometheus-server-86d645cc98-5bbdz from kommander started at 2020-06-11 05:33:41 +0000 UTC (3 container statuses recorded)
Jun 11 08:13:52.100: INFO: 	Container prometheus-server ready: true, restart count 0
Jun 11 08:13:52.100: INFO: 	Container prometheus-server-configmap-reload ready: true, restart count 0
Jun 11 08:13:52.100: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 11 08:13:52.100: INFO: fluentbit-kubeaddons-fluent-bit-46z8w from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.100: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:13:52.100: INFO: reloader-kubeaddons-reloader-7d4bd64cfb-4qjm4 from kubeaddons started at 2020-06-11 05:28:30 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.100: INFO: 	Container reloader-kubeaddons-reloader ready: true, restart count 0
Jun 11 08:13:52.100: INFO: kommander-kubeaddons-kubeaddons-catalog-6654f856df-kndt7 from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.100: INFO: 	Container kubeaddons-catalog ready: true, restart count 0
Jun 11 08:13:52.100: INFO: yakcl-federation-webhook-6fcc46596-smpfh from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.100: INFO: 	Container webhook ready: true, restart count 0
Jun 11 08:13:52.100: INFO: yakcl-federation-utility-apiserver-7d57699df9-wp7bs from kommander started at 2020-06-11 05:33:33 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.100: INFO: 	Container server ready: true, restart count 0
Jun 11 08:13:52.100: INFO: ebs-csi-snapshot-controller-0 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.100: INFO: 	Container ebs-csi-snapshot-controller ready: true, restart count 0
Jun 11 08:13:52.100: INFO: minio-0 from velero started at 2020-06-11 05:32:12 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.100: INFO: 	Container minio ready: true, restart count 0
Jun 11 08:13:52.100: INFO: yakcl-federation-authorizedlister-5db7b69bfd-zl4mt from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.100: INFO: 	Container authorizedlister ready: true, restart count 0
Jun 11 08:13:52.100: INFO: calico-node-fzn27 from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:52.100: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:13:52.100: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:13:52.100: INFO: prometheus-kubeaddons-prom-operator-767c8d59cb-44z76 from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (2 container statuses recorded)
Jun 11 08:13:52.100: INFO: 	Container prometheus-operator ready: true, restart count 0
Jun 11 08:13:52.100: INFO: 	Container tls-proxy ready: true, restart count 0
Jun 11 08:13:52.100: INFO: dex-k8s-authenticator-kubeaddons-748fcc984d-vdpxg from kubeaddons started at 2020-06-11 05:31:50 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.100: INFO: 	Container dex-k8s-authenticator ready: true, restart count 5
Jun 11 08:13:52.100: INFO: elasticsearch-kubeaddons-client-6c56cc5c7-blbht from kubeaddons started at 2020-06-11 07:15:29 +0000 UTC (1 container statuses recorded)
Jun 11 08:13:52.100: INFO: 	Container elasticsearch ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event:
Type = [Warning], Name = [restricted-pod.16176f2cf534a2ab], Reason = [FailedScheduling], Message = [0/20 nodes are available: 20 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:13:53.193: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "sched-pred-997" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":280,"completed":151,"skipped":2511,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:13:53.210: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5559
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-8ac7c31f-93ee-414a-b29c-7b86e7cb8fb9
STEP: Creating a pod to test consume configMaps
Jun 11 08:13:53.386: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d42f651d-a2a9-4834-9e94-d1c7824f6b65" in namespace "projected-5559" to be "success or failure"
Jun 11 08:13:53.392: INFO: Pod "pod-projected-configmaps-d42f651d-a2a9-4834-9e94-d1c7824f6b65": Phase="Pending", Reason="", readiness=false. Elapsed: 5.588988ms
Jun 11 08:13:55.399: INFO: Pod "pod-projected-configmaps-d42f651d-a2a9-4834-9e94-d1c7824f6b65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012168097s
STEP: Saw pod success
Jun 11 08:13:55.399: INFO: Pod "pod-projected-configmaps-d42f651d-a2a9-4834-9e94-d1c7824f6b65" satisfied condition "success or failure"
Jun 11 08:13:55.404: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-projected-configmaps-d42f651d-a2a9-4834-9e94-d1c7824f6b65 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 11 08:13:55.427: INFO: Waiting for pod pod-projected-configmaps-d42f651d-a2a9-4834-9e94-d1c7824f6b65 to disappear
Jun 11 08:13:55.433: INFO: Pod pod-projected-configmaps-d42f651d-a2a9-4834-9e94-d1c7824f6b65 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:13:55.433: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "projected-5559" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":280,"completed":152,"skipped":2529,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:13:55.449: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7046
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-db76c630-03be-4fb4-a062-b97fd6780dbf
STEP: Creating a pod to test consume secrets
Jun 11 08:13:55.619: INFO: Waiting up to 5m0s for pod "pod-secrets-5230484f-9c40-471e-a94b-9dafd6a8242b" in namespace "secrets-7046" to be "success or failure"
Jun 11 08:13:55.623: INFO: Pod "pod-secrets-5230484f-9c40-471e-a94b-9dafd6a8242b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063278ms
Jun 11 08:13:57.627: INFO: Pod "pod-secrets-5230484f-9c40-471e-a94b-9dafd6a8242b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008695799s
STEP: Saw pod success
Jun 11 08:13:57.627: INFO: Pod "pod-secrets-5230484f-9c40-471e-a94b-9dafd6a8242b" satisfied condition "success or failure"
Jun 11 08:13:57.631: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-secrets-5230484f-9c40-471e-a94b-9dafd6a8242b container secret-volume-test: <nil>
STEP: delete the pod
Jun 11 08:13:57.655: INFO: Waiting for pod pod-secrets-5230484f-9c40-471e-a94b-9dafd6a8242b to disappear
Jun 11 08:13:57.665: INFO: Pod pod-secrets-5230484f-9c40-471e-a94b-9dafd6a8242b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:13:57.665: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "secrets-7046" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":153,"skipped":2549,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:13:57.682: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6716
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-8a9a5dc1-0eb6-4cec-a721-ee6bb61e083f
STEP: Creating a pod to test consume configMaps
Jun 11 08:13:57.852: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-13b20a32-c909-45f2-93fa-eddae2de6557" in namespace "projected-6716" to be "success or failure"
Jun 11 08:13:57.856: INFO: Pod "pod-projected-configmaps-13b20a32-c909-45f2-93fa-eddae2de6557": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02692ms
Jun 11 08:13:59.861: INFO: Pod "pod-projected-configmaps-13b20a32-c909-45f2-93fa-eddae2de6557": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008495129s
STEP: Saw pod success
Jun 11 08:13:59.861: INFO: Pod "pod-projected-configmaps-13b20a32-c909-45f2-93fa-eddae2de6557" satisfied condition "success or failure"
Jun 11 08:13:59.865: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-projected-configmaps-13b20a32-c909-45f2-93fa-eddae2de6557 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 11 08:13:59.888: INFO: Waiting for pod pod-projected-configmaps-13b20a32-c909-45f2-93fa-eddae2de6557 to disappear
Jun 11 08:13:59.893: INFO: Pod pod-projected-configmaps-13b20a32-c909-45f2-93fa-eddae2de6557 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:13:59.893: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "projected-6716" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":154,"skipped":2568,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:13:59.910: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3040
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
STEP: reading a file in the container
Jun 11 08:14:02.592: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3040 pod-service-account-01a0d93e-00c4-48f4-aabb-079439a0dcaf -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jun 11 08:14:02.774: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3040 pod-service-account-01a0d93e-00c4-48f4-aabb-079439a0dcaf -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jun 11 08:14:02.948: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3040 pod-service-account-01a0d93e-00c4-48f4-aabb-079439a0dcaf -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:14:03.129: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "svcaccounts-3040" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":280,"completed":155,"skipped":2627,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:14:03.145: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2721
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-fe17586a-a034-4119-9fb0-347e3fa1ee6e
STEP: Creating a pod to test consume configMaps
Jun 11 08:14:03.309: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bc359f75-4508-4a27-8a24-6be3517ab740" in namespace "projected-2721" to be "success or failure"
Jun 11 08:14:03.313: INFO: Pod "pod-projected-configmaps-bc359f75-4508-4a27-8a24-6be3517ab740": Phase="Pending", Reason="", readiness=false. Elapsed: 3.724471ms
Jun 11 08:14:05.317: INFO: Pod "pod-projected-configmaps-bc359f75-4508-4a27-8a24-6be3517ab740": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008046081s
STEP: Saw pod success
Jun 11 08:14:05.317: INFO: Pod "pod-projected-configmaps-bc359f75-4508-4a27-8a24-6be3517ab740" satisfied condition "success or failure"
Jun 11 08:14:05.321: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-projected-configmaps-bc359f75-4508-4a27-8a24-6be3517ab740 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 11 08:14:05.352: INFO: Waiting for pod pod-projected-configmaps-bc359f75-4508-4a27-8a24-6be3517ab740 to disappear
Jun 11 08:14:05.356: INFO: Pod pod-projected-configmaps-bc359f75-4508-4a27-8a24-6be3517ab740 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:14:05.356: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "projected-2721" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":156,"skipped":2637,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:14:05.372: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-652
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:14:22.579: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "resourcequota-652" for this suite.

• [SLOW TEST:17.222 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":280,"completed":157,"skipped":2653,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:14:22.595: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5685
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 08:14:23.621: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 08:14:26.644: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jun 11 08:14:26.662: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:14:26.677: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "webhook-5685" for this suite.
STEP: Destroying namespace "webhook-5685-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":280,"completed":158,"skipped":2761,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:14:26.772: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3012
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1357
STEP: creating an pod
Jun 11 08:14:26.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.8 --namespace=kubectl-3012 -- logs-generator --log-lines-total 100 --run-duration 20s'
Jun 11 08:14:27.040: INFO: stderr: ""
Jun 11 08:14:27.040: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Waiting for log generator to start.
Jun 11 08:14:27.040: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jun 11 08:14:27.040: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3012" to be "running and ready, or succeeded"
Jun 11 08:14:27.045: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.430514ms
Jun 11 08:14:29.049: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.009156098s
Jun 11 08:14:29.049: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jun 11 08:14:29.049: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jun 11 08:14:29.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 logs logs-generator logs-generator --namespace=kubectl-3012'
Jun 11 08:14:29.171: INFO: stderr: ""
Jun 11 08:14:29.171: INFO: stdout: "I0611 08:14:27.971465       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/w46 380\nI0611 08:14:28.171595       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/tf4q 219\nI0611 08:14:28.371588       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/sxk 446\nI0611 08:14:28.571577       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/72mx 371\nI0611 08:14:28.771596       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/swl2 591\nI0611 08:14:28.971570       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/2ww 314\nI0611 08:14:29.171600       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/ztt 408\n"
STEP: limiting log lines
Jun 11 08:14:29.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 logs logs-generator logs-generator --namespace=kubectl-3012 --tail=1'
Jun 11 08:14:29.297: INFO: stderr: ""
Jun 11 08:14:29.297: INFO: stdout: "I0611 08:14:29.171600       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/ztt 408\n"
Jun 11 08:14:29.297: INFO: got output "I0611 08:14:29.171600       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/ztt 408\n"
STEP: limiting log bytes
Jun 11 08:14:29.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 logs logs-generator logs-generator --namespace=kubectl-3012 --limit-bytes=1'
Jun 11 08:14:29.433: INFO: stderr: ""
Jun 11 08:14:29.434: INFO: stdout: "I"
Jun 11 08:14:29.434: INFO: got output "I"
STEP: exposing timestamps
Jun 11 08:14:29.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 logs logs-generator logs-generator --namespace=kubectl-3012 --tail=1 --timestamps'
Jun 11 08:14:29.559: INFO: stderr: ""
Jun 11 08:14:29.559: INFO: stdout: "2020-06-11T08:14:29.371655138Z I0611 08:14:29.371584       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/z6q 206\n"
Jun 11 08:14:29.559: INFO: got output "2020-06-11T08:14:29.371655138Z I0611 08:14:29.371584       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/z6q 206\n"
STEP: restricting to a time range
Jun 11 08:14:32.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 logs logs-generator logs-generator --namespace=kubectl-3012 --since=1s'
Jun 11 08:14:32.193: INFO: stderr: ""
Jun 11 08:14:32.193: INFO: stdout: "I0611 08:14:31.371582       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/6gl 504\nI0611 08:14:31.571587       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/nqj 441\nI0611 08:14:31.771583       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/m24j 496\nI0611 08:14:31.971582       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/qcdd 303\nI0611 08:14:32.171577       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/52nb 316\n"
Jun 11 08:14:32.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 logs logs-generator logs-generator --namespace=kubectl-3012 --since=24h'
Jun 11 08:14:32.319: INFO: stderr: ""
Jun 11 08:14:32.319: INFO: stdout: "I0611 08:14:27.971465       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/w46 380\nI0611 08:14:28.171595       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/tf4q 219\nI0611 08:14:28.371588       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/sxk 446\nI0611 08:14:28.571577       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/72mx 371\nI0611 08:14:28.771596       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/swl2 591\nI0611 08:14:28.971570       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/2ww 314\nI0611 08:14:29.171600       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/ztt 408\nI0611 08:14:29.371584       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/z6q 206\nI0611 08:14:29.571589       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/ld5 270\nI0611 08:14:29.771584       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/7wrv 390\nI0611 08:14:29.971575       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/2pl 364\nI0611 08:14:30.171582       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/nzxm 337\nI0611 08:14:30.371573       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/ptz 484\nI0611 08:14:30.571584       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/rb6 414\nI0611 08:14:30.771583       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/pjzd 520\nI0611 08:14:30.971586       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/hbz 384\nI0611 08:14:31.171592       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/95f 522\nI0611 08:14:31.371582       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/6gl 504\nI0611 08:14:31.571587       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/nqj 441\nI0611 08:14:31.771583       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/m24j 496\nI0611 08:14:31.971582       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/qcdd 303\nI0611 08:14:32.171577       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/52nb 316\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1363
Jun 11 08:14:32.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 delete pod logs-generator --namespace=kubectl-3012'
Jun 11 08:14:36.864: INFO: stderr: ""
Jun 11 08:14:36.864: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:14:36.864: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "kubectl-3012" for this suite.

• [SLOW TEST:10.109 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1353
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":280,"completed":159,"skipped":2766,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:14:36.881: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4602
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service endpoint-test2 in namespace services-4602
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4602 to expose endpoints map[]
Jun 11 08:14:37.046: INFO: Get endpoints failed (4.009276ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Jun 11 08:14:38.050: INFO: successfully validated that service endpoint-test2 in namespace services-4602 exposes endpoints map[] (1.008368324s elapsed)
STEP: Creating pod pod1 in namespace services-4602
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4602 to expose endpoints map[pod1:[80]]
Jun 11 08:14:40.085: INFO: successfully validated that service endpoint-test2 in namespace services-4602 exposes endpoints map[pod1:[80]] (2.023749248s elapsed)
STEP: Creating pod pod2 in namespace services-4602
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4602 to expose endpoints map[pod1:[80] pod2:[80]]
Jun 11 08:14:41.116: INFO: successfully validated that service endpoint-test2 in namespace services-4602 exposes endpoints map[pod1:[80] pod2:[80]] (1.023612223s elapsed)
STEP: Deleting pod pod1 in namespace services-4602
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4602 to expose endpoints map[pod2:[80]]
Jun 11 08:14:42.142: INFO: successfully validated that service endpoint-test2 in namespace services-4602 exposes endpoints map[pod2:[80]] (1.017312266s elapsed)
STEP: Deleting pod pod2 in namespace services-4602
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4602 to expose endpoints map[]
Jun 11 08:14:43.160: INFO: successfully validated that service endpoint-test2 in namespace services-4602 exposes endpoints map[] (1.008804531s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:14:43.187: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "services-4602" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:6.322 seconds]
[sig-network] Services
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":280,"completed":160,"skipped":2782,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:14:43.204: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4487
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 11 08:14:43.370: INFO: Waiting up to 5m0s for pod "downwardapi-volume-19640257-f8f1-4142-b674-0d1500c0c28b" in namespace "projected-4487" to be "success or failure"
Jun 11 08:14:43.374: INFO: Pod "downwardapi-volume-19640257-f8f1-4142-b674-0d1500c0c28b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.960337ms
Jun 11 08:14:45.379: INFO: Pod "downwardapi-volume-19640257-f8f1-4142-b674-0d1500c0c28b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009279907s
STEP: Saw pod success
Jun 11 08:14:45.379: INFO: Pod "downwardapi-volume-19640257-f8f1-4142-b674-0d1500c0c28b" satisfied condition "success or failure"
Jun 11 08:14:45.383: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod downwardapi-volume-19640257-f8f1-4142-b674-0d1500c0c28b container client-container: <nil>
STEP: delete the pod
Jun 11 08:14:45.407: INFO: Waiting for pod downwardapi-volume-19640257-f8f1-4142-b674-0d1500c0c28b to disappear
Jun 11 08:14:45.412: INFO: Pod downwardapi-volume-19640257-f8f1-4142-b674-0d1500c0c28b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:14:45.412: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "projected-4487" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":161,"skipped":2791,"failed":0}
SS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:14:45.427: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2788
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2788.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2788.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2788.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2788.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2788.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2788.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 11 08:14:47.641: INFO: DNS probes using dns-2788/dns-test-bde67d2d-b4d2-4fa6-86f4-ca90025d4b66 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:14:47.683: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "dns-2788" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":280,"completed":162,"skipped":2793,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:14:47.698: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7512
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-33f1e3ce-984a-48ee-a8bb-5b352a5f6045
STEP: Creating a pod to test consume secrets
Jun 11 08:14:47.863: INFO: Waiting up to 5m0s for pod "pod-secrets-b5b61c74-9610-44f9-aa17-e7bd1b49b28c" in namespace "secrets-7512" to be "success or failure"
Jun 11 08:14:47.867: INFO: Pod "pod-secrets-b5b61c74-9610-44f9-aa17-e7bd1b49b28c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.8193ms
Jun 11 08:14:49.871: INFO: Pod "pod-secrets-b5b61c74-9610-44f9-aa17-e7bd1b49b28c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008449348s
STEP: Saw pod success
Jun 11 08:14:49.871: INFO: Pod "pod-secrets-b5b61c74-9610-44f9-aa17-e7bd1b49b28c" satisfied condition "success or failure"
Jun 11 08:14:49.875: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-secrets-b5b61c74-9610-44f9-aa17-e7bd1b49b28c container secret-volume-test: <nil>
STEP: delete the pod
Jun 11 08:14:49.898: INFO: Waiting for pod pod-secrets-b5b61c74-9610-44f9-aa17-e7bd1b49b28c to disappear
Jun 11 08:14:49.904: INFO: Pod pod-secrets-b5b61c74-9610-44f9-aa17-e7bd1b49b28c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:14:49.904: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "secrets-7512" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":163,"skipped":2801,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease
  lease API should be available [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:14:49.919: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-7238
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:14:50.135: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "lease-test-7238" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":280,"completed":164,"skipped":2849,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:14:50.150: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5061
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 11 08:14:50.312: INFO: Waiting up to 5m0s for pod "pod-1a2f3317-febd-46d3-a568-b3d42e12e8f6" in namespace "emptydir-5061" to be "success or failure"
Jun 11 08:14:50.316: INFO: Pod "pod-1a2f3317-febd-46d3-a568-b3d42e12e8f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.156115ms
Jun 11 08:14:52.320: INFO: Pod "pod-1a2f3317-febd-46d3-a568-b3d42e12e8f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008603398s
STEP: Saw pod success
Jun 11 08:14:52.320: INFO: Pod "pod-1a2f3317-febd-46d3-a568-b3d42e12e8f6" satisfied condition "success or failure"
Jun 11 08:14:52.324: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-1a2f3317-febd-46d3-a568-b3d42e12e8f6 container test-container: <nil>
STEP: delete the pod
Jun 11 08:14:52.346: INFO: Waiting for pod pod-1a2f3317-febd-46d3-a568-b3d42e12e8f6 to disappear
Jun 11 08:14:52.353: INFO: Pod pod-1a2f3317-febd-46d3-a568-b3d42e12e8f6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:14:52.353: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "emptydir-5061" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":165,"skipped":2849,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:14:52.372: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5708
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 08:14:52.968: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 08:14:55.993: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:14:55.999: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "webhook-5708" for this suite.
STEP: Destroying namespace "webhook-5708-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":280,"completed":166,"skipped":2853,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:14:56.086: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-2090
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-2090, will wait for the garbage collector to delete the pods
Jun 11 08:15:00.316: INFO: Deleting Job.batch foo took: 10.936716ms
Jun 11 08:15:00.416: INFO: Terminating Job.batch foo pods took: 100.243323ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:15:36.921: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "job-2090" for this suite.

• [SLOW TEST:40.850 seconds]
[sig-apps] Job
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":280,"completed":167,"skipped":2868,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:15:36.936: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3773
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 11 08:15:41.166: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 11 08:15:41.171: INFO: Pod pod-with-poststart-http-hook still exists
Jun 11 08:15:43.171: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 11 08:15:43.176: INFO: Pod pod-with-poststart-http-hook still exists
Jun 11 08:15:45.171: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 11 08:15:45.176: INFO: Pod pod-with-poststart-http-hook still exists
Jun 11 08:15:47.171: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 11 08:15:47.175: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:15:47.176: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3773" for this suite.

• [SLOW TEST:10.254 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":280,"completed":168,"skipped":2893,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:15:47.191: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3481
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 11 08:15:47.352: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a02c5cd8-f756-4bdc-9ae8-a2da7bfbac5d" in namespace "downward-api-3481" to be "success or failure"
Jun 11 08:15:47.356: INFO: Pod "downwardapi-volume-a02c5cd8-f756-4bdc-9ae8-a2da7bfbac5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.877958ms
Jun 11 08:15:49.361: INFO: Pod "downwardapi-volume-a02c5cd8-f756-4bdc-9ae8-a2da7bfbac5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008497429s
STEP: Saw pod success
Jun 11 08:15:49.361: INFO: Pod "downwardapi-volume-a02c5cd8-f756-4bdc-9ae8-a2da7bfbac5d" satisfied condition "success or failure"
Jun 11 08:15:49.364: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod downwardapi-volume-a02c5cd8-f756-4bdc-9ae8-a2da7bfbac5d container client-container: <nil>
STEP: delete the pod
Jun 11 08:15:49.388: INFO: Waiting for pod downwardapi-volume-a02c5cd8-f756-4bdc-9ae8-a2da7bfbac5d to disappear
Jun 11 08:15:49.392: INFO: Pod downwardapi-volume-a02c5cd8-f756-4bdc-9ae8-a2da7bfbac5d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:15:49.392: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "downward-api-3481" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":169,"skipped":2897,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:15:49.408: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8578
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Jun 11 08:15:55.592: INFO: 10 pods remaining
Jun 11 08:15:55.592: INFO: 10 pods has nil DeletionTimestamp
Jun 11 08:15:55.592: INFO:
STEP: Gathering metrics
Jun 11 08:15:56.599: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W0611 08:15:56.599606      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 11 08:15:56.599: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "gc-8578" for this suite.

• [SLOW TEST:7.209 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":280,"completed":170,"skipped":2920,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:15:56.617: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-4627
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 08:15:56.784: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-e939be5e-c9bc-4847-a138-3272a5375382" in namespace "security-context-test-4627" to be "success or failure"
Jun 11 08:15:56.795: INFO: Pod "alpine-nnp-false-e939be5e-c9bc-4847-a138-3272a5375382": Phase="Pending", Reason="", readiness=false. Elapsed: 10.536032ms
Jun 11 08:15:58.800: INFO: Pod "alpine-nnp-false-e939be5e-c9bc-4847-a138-3272a5375382": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01523002s
Jun 11 08:15:58.800: INFO: Pod "alpine-nnp-false-e939be5e-c9bc-4847-a138-3272a5375382" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:15:58.808: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "security-context-test-4627" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":171,"skipped":2941,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:15:58.825: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6122
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's command
Jun 11 08:15:58.988: INFO: Waiting up to 5m0s for pod "var-expansion-08f62e3c-52f0-4253-ad9f-61622f94f32a" in namespace "var-expansion-6122" to be "success or failure"
Jun 11 08:15:58.995: INFO: Pod "var-expansion-08f62e3c-52f0-4253-ad9f-61622f94f32a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.405134ms
Jun 11 08:16:01.000: INFO: Pod "var-expansion-08f62e3c-52f0-4253-ad9f-61622f94f32a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01149867s
Jun 11 08:16:03.005: INFO: Pod "var-expansion-08f62e3c-52f0-4253-ad9f-61622f94f32a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016483437s
STEP: Saw pod success
Jun 11 08:16:03.005: INFO: Pod "var-expansion-08f62e3c-52f0-4253-ad9f-61622f94f32a" satisfied condition "success or failure"
Jun 11 08:16:03.010: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod var-expansion-08f62e3c-52f0-4253-ad9f-61622f94f32a container dapi-container: <nil>
STEP: delete the pod
Jun 11 08:16:03.047: INFO: Waiting for pod var-expansion-08f62e3c-52f0-4253-ad9f-61622f94f32a to disappear
Jun 11 08:16:03.053: INFO: Pod var-expansion-08f62e3c-52f0-4253-ad9f-61622f94f32a no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:16:03.053: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "var-expansion-6122" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":280,"completed":172,"skipped":2955,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:16:03.069: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9779
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test env composition
Jun 11 08:16:03.232: INFO: Waiting up to 5m0s for pod "var-expansion-756186e0-c2a9-4d91-b21b-edd40a214aa9" in namespace "var-expansion-9779" to be "success or failure"
Jun 11 08:16:03.237: INFO: Pod "var-expansion-756186e0-c2a9-4d91-b21b-edd40a214aa9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.0893ms
Jun 11 08:16:05.242: INFO: Pod "var-expansion-756186e0-c2a9-4d91-b21b-edd40a214aa9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009728541s
Jun 11 08:16:07.246: INFO: Pod "var-expansion-756186e0-c2a9-4d91-b21b-edd40a214aa9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014082243s
STEP: Saw pod success
Jun 11 08:16:07.246: INFO: Pod "var-expansion-756186e0-c2a9-4d91-b21b-edd40a214aa9" satisfied condition "success or failure"
Jun 11 08:16:07.250: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod var-expansion-756186e0-c2a9-4d91-b21b-edd40a214aa9 container dapi-container: <nil>
STEP: delete the pod
Jun 11 08:16:07.275: INFO: Waiting for pod var-expansion-756186e0-c2a9-4d91-b21b-edd40a214aa9 to disappear
Jun 11 08:16:07.279: INFO: Pod var-expansion-756186e0-c2a9-4d91-b21b-edd40a214aa9 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:16:07.279: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "var-expansion-9779" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":280,"completed":173,"skipped":2968,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:16:07.294: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1143
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1143.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1143.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1143.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1143.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 11 08:16:11.487: INFO: DNS probes using dns-test-f4d1968f-a6ee-4d27-a1ce-96026fb6ce60 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1143.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1143.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1143.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1143.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 11 08:16:15.549: INFO: DNS probes using dns-test-fc465521-2b73-4fe7-9b99-e9970ec16182 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1143.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1143.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1143.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1143.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 11 08:16:19.621: INFO: DNS probes using dns-test-3867f999-cee0-4150-ac25-8c7801d0ac25 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:16:19.662: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "dns-1143" for this suite.

• [SLOW TEST:12.385 seconds]
[sig-network] DNS
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":280,"completed":174,"skipped":2984,"failed":0}
SSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:16:19.680: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1151
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jun 11 08:16:19.845: INFO: Waiting up to 5m0s for pod "downward-api-f358df5f-9b9f-4624-bf1f-272a54a5c9bb" in namespace "downward-api-1151" to be "success or failure"
Jun 11 08:16:19.850: INFO: Pod "downward-api-f358df5f-9b9f-4624-bf1f-272a54a5c9bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.113489ms
Jun 11 08:16:21.854: INFO: Pod "downward-api-f358df5f-9b9f-4624-bf1f-272a54a5c9bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008807957s
Jun 11 08:16:23.859: INFO: Pod "downward-api-f358df5f-9b9f-4624-bf1f-272a54a5c9bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013632308s
STEP: Saw pod success
Jun 11 08:16:23.859: INFO: Pod "downward-api-f358df5f-9b9f-4624-bf1f-272a54a5c9bb" satisfied condition "success or failure"
Jun 11 08:16:23.863: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod downward-api-f358df5f-9b9f-4624-bf1f-272a54a5c9bb container dapi-container: <nil>
STEP: delete the pod
Jun 11 08:16:23.887: INFO: Waiting for pod downward-api-f358df5f-9b9f-4624-bf1f-272a54a5c9bb to disappear
Jun 11 08:16:23.893: INFO: Pod downward-api-f358df5f-9b9f-4624-bf1f-272a54a5c9bb no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:16:23.893: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "downward-api-1151" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":280,"completed":175,"skipped":2989,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:16:23.908: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-506
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-506.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-506.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-506.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-506.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-506.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-506.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 11 08:16:26.118: INFO: DNS probes using dns-506/dns-test-def0ec4b-57c2-4ec3-ad5b-9b79e04c5dff succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:16:26.135: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "dns-506" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":280,"completed":176,"skipped":3012,"failed":0}
SS
------------------------------
[k8s.io] Docker Containers
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:16:26.151: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5982
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override arguments
Jun 11 08:16:26.310: INFO: Waiting up to 5m0s for pod "client-containers-315e9c7a-2425-4d23-bffd-d2dc6d816641" in namespace "containers-5982" to be "success or failure"
Jun 11 08:16:26.314: INFO: Pod "client-containers-315e9c7a-2425-4d23-bffd-d2dc6d816641": Phase="Pending", Reason="", readiness=false. Elapsed: 3.724858ms
Jun 11 08:16:28.319: INFO: Pod "client-containers-315e9c7a-2425-4d23-bffd-d2dc6d816641": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008354942s
STEP: Saw pod success
Jun 11 08:16:28.319: INFO: Pod "client-containers-315e9c7a-2425-4d23-bffd-d2dc6d816641" satisfied condition "success or failure"
Jun 11 08:16:28.323: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod client-containers-315e9c7a-2425-4d23-bffd-d2dc6d816641 container test-container: <nil>
STEP: delete the pod
Jun 11 08:16:28.346: INFO: Waiting for pod client-containers-315e9c7a-2425-4d23-bffd-d2dc6d816641 to disappear
Jun 11 08:16:28.352: INFO: Pod client-containers-315e9c7a-2425-4d23-bffd-d2dc6d816641 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:16:28.352: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "containers-5982" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":280,"completed":177,"skipped":3014,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:16:28.373: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5295
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1754
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 11 08:16:28.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-5295'
Jun 11 08:16:28.640: INFO: stderr: ""
Jun 11 08:16:28.641: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1759
Jun 11 08:16:28.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 delete pods e2e-test-httpd-pod --namespace=kubectl-5295'
Jun 11 08:16:36.865: INFO: stderr: ""
Jun 11 08:16:36.865: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:16:36.865: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "kubectl-5295" for this suite.

• [SLOW TEST:8.510 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1750
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":280,"completed":178,"skipped":3017,"failed":0}
[k8s.io] Docker Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:16:36.883: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1363
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override all
Jun 11 08:16:37.046: INFO: Waiting up to 5m0s for pod "client-containers-0c9a9bef-cbc8-4ab1-a022-73e1caec86b5" in namespace "containers-1363" to be "success or failure"
Jun 11 08:16:37.051: INFO: Pod "client-containers-0c9a9bef-cbc8-4ab1-a022-73e1caec86b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.204016ms
Jun 11 08:16:39.055: INFO: Pod "client-containers-0c9a9bef-cbc8-4ab1-a022-73e1caec86b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009054178s
STEP: Saw pod success
Jun 11 08:16:39.055: INFO: Pod "client-containers-0c9a9bef-cbc8-4ab1-a022-73e1caec86b5" satisfied condition "success or failure"
Jun 11 08:16:39.059: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod client-containers-0c9a9bef-cbc8-4ab1-a022-73e1caec86b5 container test-container: <nil>
STEP: delete the pod
Jun 11 08:16:39.083: INFO: Waiting for pod client-containers-0c9a9bef-cbc8-4ab1-a022-73e1caec86b5 to disappear
Jun 11 08:16:39.089: INFO: Pod client-containers-0c9a9bef-cbc8-4ab1-a022-73e1caec86b5 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:16:39.089: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "containers-1363" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":280,"completed":179,"skipped":3017,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:16:39.105: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3867
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-a3536366-8d56-42d2-a51e-c6d3d06ca7d6
STEP: Creating a pod to test consume secrets
Jun 11 08:16:39.272: INFO: Waiting up to 5m0s for pod "pod-secrets-d137159a-cb2f-4907-a90b-21f72beb837c" in namespace "secrets-3867" to be "success or failure"
Jun 11 08:16:39.276: INFO: Pod "pod-secrets-d137159a-cb2f-4907-a90b-21f72beb837c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.084852ms
Jun 11 08:16:41.281: INFO: Pod "pod-secrets-d137159a-cb2f-4907-a90b-21f72beb837c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008869706s
STEP: Saw pod success
Jun 11 08:16:41.281: INFO: Pod "pod-secrets-d137159a-cb2f-4907-a90b-21f72beb837c" satisfied condition "success or failure"
Jun 11 08:16:41.285: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-secrets-d137159a-cb2f-4907-a90b-21f72beb837c container secret-volume-test: <nil>
STEP: delete the pod
Jun 11 08:16:41.308: INFO: Waiting for pod pod-secrets-d137159a-cb2f-4907-a90b-21f72beb837c to disappear
Jun 11 08:16:41.314: INFO: Pod pod-secrets-d137159a-cb2f-4907-a90b-21f72beb837c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:16:41.314: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "secrets-3867" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":180,"skipped":3022,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:16:41.330: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5023
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jun 11 08:16:41.482: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:16:45.796: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "init-container-5023" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":280,"completed":181,"skipped":3031,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:16:45.813: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-662
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 11 08:16:45.979: INFO: Waiting up to 5m0s for pod "pod-939d6fa2-eb3f-4667-aa70-2009a3d553b9" in namespace "emptydir-662" to be "success or failure"
Jun 11 08:16:45.983: INFO: Pod "pod-939d6fa2-eb3f-4667-aa70-2009a3d553b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.478102ms
Jun 11 08:16:47.988: INFO: Pod "pod-939d6fa2-eb3f-4667-aa70-2009a3d553b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009096854s
STEP: Saw pod success
Jun 11 08:16:47.988: INFO: Pod "pod-939d6fa2-eb3f-4667-aa70-2009a3d553b9" satisfied condition "success or failure"
Jun 11 08:16:47.992: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-939d6fa2-eb3f-4667-aa70-2009a3d553b9 container test-container: <nil>
STEP: delete the pod
Jun 11 08:16:48.016: INFO: Waiting for pod pod-939d6fa2-eb3f-4667-aa70-2009a3d553b9 to disappear
Jun 11 08:16:48.020: INFO: Pod pod-939d6fa2-eb3f-4667-aa70-2009a3d553b9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:16:48.020: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "emptydir-662" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":182,"skipped":3031,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:16:48.036: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2317
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 08:16:48.187: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:16:48.715: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2317" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":280,"completed":183,"skipped":3045,"failed":0}
S
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:16:48.731: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-8378
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:46
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:16:48.883: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "tables-8378" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":280,"completed":184,"skipped":3046,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:16:48.898: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2893
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 11 08:16:49.061: INFO: Waiting up to 5m0s for pod "pod-eab2cab5-4867-42fe-968e-a9ed49f56623" in namespace "emptydir-2893" to be "success or failure"
Jun 11 08:16:49.066: INFO: Pod "pod-eab2cab5-4867-42fe-968e-a9ed49f56623": Phase="Pending", Reason="", readiness=false. Elapsed: 4.135679ms
Jun 11 08:16:51.070: INFO: Pod "pod-eab2cab5-4867-42fe-968e-a9ed49f56623": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008919463s
STEP: Saw pod success
Jun 11 08:16:51.070: INFO: Pod "pod-eab2cab5-4867-42fe-968e-a9ed49f56623" satisfied condition "success or failure"
Jun 11 08:16:51.080: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-eab2cab5-4867-42fe-968e-a9ed49f56623 container test-container: <nil>
STEP: delete the pod
Jun 11 08:16:51.111: INFO: Waiting for pod pod-eab2cab5-4867-42fe-968e-a9ed49f56623 to disappear
Jun 11 08:16:51.117: INFO: Pod pod-eab2cab5-4867-42fe-968e-a9ed49f56623 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:16:51.117: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "emptydir-2893" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":185,"skipped":3052,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:16:51.135: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6282
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating cluster-info
Jun 11 08:16:51.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 cluster-info'
Jun 11 08:16:51.422: INFO: stderr: ""
Jun 11 08:16:51.422: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:16:51.422: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "kubectl-6282" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":280,"completed":186,"skipped":3056,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:16:51.439: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8535
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with configMap that has name projected-configmap-test-upd-8ba914ce-c431-4f9d-8cb8-d29195360e49
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-8ba914ce-c431-4f9d-8cb8-d29195360e49
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:16:55.657: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "projected-8535" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":187,"skipped":3089,"failed":0}
SSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:16:55.672: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-6807
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-khrlv in namespace proxy-6807
I0611 08:16:55.843087      23 runners.go:189] Created replication controller with name: proxy-service-khrlv, namespace: proxy-6807, replica count: 1
I0611 08:16:56.893589      23 runners.go:189] proxy-service-khrlv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady
I0611 08:16:57.893846      23 runners.go:189] proxy-service-khrlv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady
I0611 08:16:58.894076      23 runners.go:189] proxy-service-khrlv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady
I0611 08:16:59.894317      23 runners.go:189] proxy-service-khrlv Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady
Jun 11 08:16:59.899: INFO: setup took 4.074924999s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jun 11 08:16:59.907: INFO: (0) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/rewriteme">... (200; 8.310748ms)
Jun 11 08:16:59.907: INFO: (0) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:162/proxy/: bar (200; 8.398939ms)
Jun 11 08:16:59.907: INFO: (0) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:162/proxy/: bar (200; 8.495149ms)
Jun 11 08:16:59.907: INFO: (0) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/rewriteme">test<... (200; 8.520955ms)
Jun 11 08:16:59.908: INFO: (0) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:160/proxy/: foo (200; 8.740286ms)
Jun 11 08:16:59.908: INFO: (0) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:160/proxy/: foo (200; 8.876353ms)
Jun 11 08:16:59.909: INFO: (0) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname1/proxy/: foo (200; 9.906434ms)
Jun 11 08:16:59.909: INFO: (0) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/rewriteme">test</a> (200; 9.714219ms)
Jun 11 08:16:59.911: INFO: (0) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:462/proxy/: tls qux (200; 12.414823ms)
Jun 11 08:16:59.911: INFO: (0) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:460/proxy/: tls baz (200; 12.431368ms)
Jun 11 08:16:59.911: INFO: (0) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/tlsrewritem... (200; 12.478932ms)
Jun 11 08:16:59.912: INFO: (0) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname1/proxy/: tls baz (200; 12.656474ms)
Jun 11 08:16:59.912: INFO: (0) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname2/proxy/: tls qux (200; 12.775818ms)
Jun 11 08:16:59.912: INFO: (0) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname2/proxy/: bar (200; 12.630396ms)
Jun 11 08:16:59.912: INFO: (0) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname1/proxy/: foo (200; 12.932145ms)
Jun 11 08:16:59.912: INFO: (0) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname2/proxy/: bar (200; 12.698288ms)
Jun 11 08:16:59.920: INFO: (1) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:160/proxy/: foo (200; 7.719398ms)
Jun 11 08:16:59.922: INFO: (1) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:162/proxy/: bar (200; 9.297311ms)
Jun 11 08:16:59.922: INFO: (1) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:462/proxy/: tls qux (200; 9.132073ms)
Jun 11 08:16:59.923: INFO: (1) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:160/proxy/: foo (200; 10.748529ms)
Jun 11 08:16:59.923: INFO: (1) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:460/proxy/: tls baz (200; 10.691134ms)
Jun 11 08:16:59.923: INFO: (1) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/rewriteme">... (200; 10.542286ms)
Jun 11 08:16:59.924: INFO: (1) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/tlsrewritem... (200; 11.835381ms)
Jun 11 08:16:59.925: INFO: (1) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/rewriteme">test<... (200; 10.515485ms)
Jun 11 08:16:59.925: INFO: (1) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/rewriteme">test</a> (200; 11.059765ms)
Jun 11 08:16:59.928: INFO: (1) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:162/proxy/: bar (200; 13.874961ms)
Jun 11 08:16:59.928: INFO: (1) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname1/proxy/: foo (200; 16.20594ms)
Jun 11 08:16:59.929: INFO: (1) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname2/proxy/: tls qux (200; 16.621386ms)
Jun 11 08:16:59.937: INFO: (1) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname2/proxy/: bar (200; 24.572046ms)
Jun 11 08:16:59.939: INFO: (1) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname2/proxy/: bar (200; 27.125057ms)
Jun 11 08:16:59.940: INFO: (1) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname1/proxy/: foo (200; 27.351409ms)
Jun 11 08:16:59.940: INFO: (1) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname1/proxy/: tls baz (200; 26.696181ms)
Jun 11 08:16:59.948: INFO: (2) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:162/proxy/: bar (200; 7.464039ms)
Jun 11 08:16:59.948: INFO: (2) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/tlsrewritem... (200; 7.89757ms)
Jun 11 08:16:59.948: INFO: (2) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:162/proxy/: bar (200; 8.00409ms)
Jun 11 08:16:59.948: INFO: (2) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/rewriteme">... (200; 7.203581ms)
Jun 11 08:16:59.948: INFO: (2) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:460/proxy/: tls baz (200; 7.735751ms)
Jun 11 08:16:59.948: INFO: (2) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/rewriteme">test<... (200; 8.207778ms)
Jun 11 08:16:59.948: INFO: (2) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/rewriteme">test</a> (200; 8.374911ms)
Jun 11 08:16:59.948: INFO: (2) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:160/proxy/: foo (200; 7.202658ms)
Jun 11 08:16:59.948: INFO: (2) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:160/proxy/: foo (200; 7.903283ms)
Jun 11 08:16:59.948: INFO: (2) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:462/proxy/: tls qux (200; 8.690094ms)
Jun 11 08:16:59.950: INFO: (2) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname1/proxy/: tls baz (200; 10.361509ms)
Jun 11 08:16:59.953: INFO: (2) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname2/proxy/: tls qux (200; 11.440518ms)
Jun 11 08:16:59.953: INFO: (2) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname2/proxy/: bar (200; 11.661155ms)
Jun 11 08:16:59.953: INFO: (2) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname1/proxy/: foo (200; 11.925493ms)
Jun 11 08:16:59.953: INFO: (2) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname2/proxy/: bar (200; 11.849386ms)
Jun 11 08:16:59.953: INFO: (2) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname1/proxy/: foo (200; 12.384467ms)
Jun 11 08:16:59.957: INFO: (3) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:462/proxy/: tls qux (200; 4.380916ms)
Jun 11 08:16:59.960: INFO: (3) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:160/proxy/: foo (200; 7.200639ms)
Jun 11 08:16:59.960: INFO: (3) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:162/proxy/: bar (200; 7.189172ms)
Jun 11 08:16:59.960: INFO: (3) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:162/proxy/: bar (200; 6.906274ms)
Jun 11 08:16:59.961: INFO: (3) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/rewriteme">test</a> (200; 7.356701ms)
Jun 11 08:16:59.961: INFO: (3) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:160/proxy/: foo (200; 7.859099ms)
Jun 11 08:16:59.961: INFO: (3) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/tlsrewritem... (200; 7.955355ms)
Jun 11 08:16:59.961: INFO: (3) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/rewriteme">... (200; 8.384804ms)
Jun 11 08:16:59.961: INFO: (3) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/rewriteme">test<... (200; 8.034109ms)
Jun 11 08:16:59.961: INFO: (3) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:460/proxy/: tls baz (200; 8.44605ms)
Jun 11 08:16:59.963: INFO: (3) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname2/proxy/: tls qux (200; 10.002456ms)
Jun 11 08:16:59.966: INFO: (3) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname2/proxy/: bar (200; 12.807284ms)
Jun 11 08:16:59.966: INFO: (3) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname1/proxy/: tls baz (200; 12.657437ms)
Jun 11 08:16:59.966: INFO: (3) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname1/proxy/: foo (200; 12.503763ms)
Jun 11 08:16:59.966: INFO: (3) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname1/proxy/: foo (200; 12.882766ms)
Jun 11 08:16:59.966: INFO: (3) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname2/proxy/: bar (200; 12.781774ms)
Jun 11 08:16:59.971: INFO: (4) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:462/proxy/: tls qux (200; 5.135816ms)
Jun 11 08:16:59.974: INFO: (4) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/rewriteme">... (200; 7.65852ms)
Jun 11 08:16:59.974: INFO: (4) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:160/proxy/: foo (200; 7.845407ms)
Jun 11 08:16:59.974: INFO: (4) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:460/proxy/: tls baz (200; 8.380937ms)
Jun 11 08:16:59.974: INFO: (4) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/rewriteme">test<... (200; 8.252148ms)
Jun 11 08:16:59.974: INFO: (4) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/rewriteme">test</a> (200; 8.324798ms)
Jun 11 08:16:59.974: INFO: (4) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:162/proxy/: bar (200; 8.12075ms)
Jun 11 08:16:59.974: INFO: (4) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:162/proxy/: bar (200; 8.271584ms)
Jun 11 08:16:59.974: INFO: (4) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/tlsrewritem... (200; 8.229942ms)
Jun 11 08:16:59.974: INFO: (4) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:160/proxy/: foo (200; 8.174812ms)
Jun 11 08:16:59.977: INFO: (4) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname1/proxy/: tls baz (200; 10.92642ms)
Jun 11 08:16:59.980: INFO: (4) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname2/proxy/: bar (200; 13.286529ms)
Jun 11 08:16:59.980: INFO: (4) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname2/proxy/: tls qux (200; 13.31005ms)
Jun 11 08:16:59.980: INFO: (4) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname1/proxy/: foo (200; 13.407354ms)
Jun 11 08:16:59.980: INFO: (4) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname2/proxy/: bar (200; 13.410851ms)
Jun 11 08:16:59.980: INFO: (4) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname1/proxy/: foo (200; 13.508427ms)
Jun 11 08:16:59.988: INFO: (5) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:462/proxy/: tls qux (200; 8.482122ms)
Jun 11 08:16:59.988: INFO: (5) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:160/proxy/: foo (200; 7.675114ms)
Jun 11 08:16:59.988: INFO: (5) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/rewriteme">test</a> (200; 8.264356ms)
Jun 11 08:16:59.988: INFO: (5) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/rewriteme">... (200; 8.07129ms)
Jun 11 08:16:59.988: INFO: (5) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/tlsrewritem... (200; 7.840683ms)
Jun 11 08:16:59.988: INFO: (5) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:162/proxy/: bar (200; 7.907059ms)
Jun 11 08:16:59.988: INFO: (5) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:160/proxy/: foo (200; 7.732352ms)
Jun 11 08:16:59.988: INFO: (5) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:162/proxy/: bar (200; 7.838147ms)
Jun 11 08:16:59.988: INFO: (5) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/rewriteme">test<... (200; 8.251013ms)
Jun 11 08:16:59.988: INFO: (5) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:460/proxy/: tls baz (200; 7.721054ms)
Jun 11 08:16:59.989: INFO: (5) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname1/proxy/: tls baz (200; 9.507091ms)
Jun 11 08:16:59.992: INFO: (5) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname1/proxy/: foo (200; 11.840614ms)
Jun 11 08:16:59.992: INFO: (5) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname2/proxy/: bar (200; 11.690956ms)
Jun 11 08:16:59.992: INFO: (5) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname2/proxy/: tls qux (200; 11.363389ms)
Jun 11 08:16:59.992: INFO: (5) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname2/proxy/: bar (200; 11.77026ms)
Jun 11 08:16:59.992: INFO: (5) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname1/proxy/: foo (200; 11.688022ms)
Jun 11 08:16:59.997: INFO: (6) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:462/proxy/: tls qux (200; 4.74489ms)
Jun 11 08:16:59.999: INFO: (6) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:160/proxy/: foo (200; 7.368529ms)
Jun 11 08:17:00.000: INFO: (6) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:162/proxy/: bar (200; 7.334298ms)
Jun 11 08:17:00.000: INFO: (6) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/rewriteme">... (200; 7.492432ms)
Jun 11 08:17:00.000: INFO: (6) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/rewriteme">test<... (200; 7.532448ms)
Jun 11 08:17:00.000: INFO: (6) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/tlsrewritem... (200; 7.592513ms)
Jun 11 08:17:00.000: INFO: (6) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/rewriteme">test</a> (200; 7.525562ms)
Jun 11 08:17:00.000: INFO: (6) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:162/proxy/: bar (200; 7.541123ms)
Jun 11 08:17:00.000: INFO: (6) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:160/proxy/: foo (200; 7.747752ms)
Jun 11 08:17:00.001: INFO: (6) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:460/proxy/: tls baz (200; 8.252811ms)
Jun 11 08:17:00.003: INFO: (6) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname2/proxy/: bar (200; 10.60794ms)
Jun 11 08:17:00.006: INFO: (6) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname1/proxy/: foo (200; 13.411174ms)
Jun 11 08:17:00.006: INFO: (6) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname2/proxy/: tls qux (200; 13.395211ms)
Jun 11 08:17:00.006: INFO: (6) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname1/proxy/: foo (200; 13.352892ms)
Jun 11 08:17:00.006: INFO: (6) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname1/proxy/: tls baz (200; 13.560567ms)
Jun 11 08:17:00.006: INFO: (6) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname2/proxy/: bar (200; 13.466765ms)
Jun 11 08:17:00.011: INFO: (7) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:162/proxy/: bar (200; 5.424799ms)
Jun 11 08:17:00.014: INFO: (7) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:160/proxy/: foo (200; 8.208437ms)
Jun 11 08:17:00.014: INFO: (7) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:460/proxy/: tls baz (200; 8.278108ms)
Jun 11 08:17:00.014: INFO: (7) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/rewriteme">test<... (200; 8.058961ms)
Jun 11 08:17:00.014: INFO: (7) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:162/proxy/: bar (200; 8.21602ms)
Jun 11 08:17:00.014: INFO: (7) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:462/proxy/: tls qux (200; 8.625845ms)
Jun 11 08:17:00.015: INFO: (7) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/rewriteme">... (200; 8.760331ms)
Jun 11 08:17:00.015: INFO: (7) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/rewriteme">test</a> (200; 8.51528ms)
Jun 11 08:17:00.015: INFO: (7) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:160/proxy/: foo (200; 8.349307ms)
Jun 11 08:17:00.015: INFO: (7) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/tlsrewritem... (200; 8.643719ms)
Jun 11 08:17:00.017: INFO: (7) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname2/proxy/: bar (200; 10.464563ms)
Jun 11 08:17:00.019: INFO: (7) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname1/proxy/: foo (200; 13.030872ms)
Jun 11 08:17:00.019: INFO: (7) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname2/proxy/: tls qux (200; 13.551534ms)
Jun 11 08:17:00.019: INFO: (7) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname1/proxy/: foo (200; 13.321333ms)
Jun 11 08:17:00.019: INFO: (7) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname1/proxy/: tls baz (200; 13.486348ms)
Jun 11 08:17:00.019: INFO: (7) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname2/proxy/: bar (200; 13.065106ms)
Jun 11 08:17:00.024: INFO: (8) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/tlsrewritem... (200; 4.544958ms)
Jun 11 08:17:00.027: INFO: (8) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:160/proxy/: foo (200; 7.269648ms)
Jun 11 08:17:00.028: INFO: (8) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/rewriteme">test<... (200; 7.914906ms)
Jun 11 08:17:00.028: INFO: (8) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:162/proxy/: bar (200; 7.913228ms)
Jun 11 08:17:00.028: INFO: (8) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:160/proxy/: foo (200; 7.997501ms)
Jun 11 08:17:00.028: INFO: (8) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/rewriteme">test</a> (200; 7.943887ms)
Jun 11 08:17:00.028: INFO: (8) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:162/proxy/: bar (200; 8.272796ms)
Jun 11 08:17:00.028: INFO: (8) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/rewriteme">... (200; 8.315822ms)
Jun 11 08:17:00.028: INFO: (8) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:462/proxy/: tls qux (200; 8.143808ms)
Jun 11 08:17:00.028: INFO: (8) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:460/proxy/: tls baz (200; 8.189389ms)
Jun 11 08:17:00.030: INFO: (8) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname1/proxy/: tls baz (200; 10.90292ms)
Jun 11 08:17:00.033: INFO: (8) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname1/proxy/: foo (200; 13.112158ms)
Jun 11 08:17:00.033: INFO: (8) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname1/proxy/: foo (200; 13.505623ms)
Jun 11 08:17:00.033: INFO: (8) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname2/proxy/: bar (200; 13.520774ms)
Jun 11 08:17:00.033: INFO: (8) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname2/proxy/: tls qux (200; 13.546785ms)
Jun 11 08:17:00.033: INFO: (8) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname2/proxy/: bar (200; 13.616647ms)
Jun 11 08:17:00.041: INFO: (9) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:162/proxy/: bar (200; 6.853456ms)
Jun 11 08:17:00.041: INFO: (9) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/rewriteme">test<... (200; 7.82759ms)
Jun 11 08:17:00.041: INFO: (9) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/tlsrewritem... (200; 7.599609ms)
Jun 11 08:17:00.041: INFO: (9) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/rewriteme">... (200; 7.727788ms)
Jun 11 08:17:00.041: INFO: (9) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:162/proxy/: bar (200; 7.685311ms)
Jun 11 08:17:00.041: INFO: (9) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:160/proxy/: foo (200; 7.730426ms)
Jun 11 08:17:00.041: INFO: (9) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:462/proxy/: tls qux (200; 7.604806ms)
Jun 11 08:17:00.041: INFO: (9) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:160/proxy/: foo (200; 7.884506ms)
Jun 11 08:17:00.041: INFO: (9) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:460/proxy/: tls baz (200; 7.839931ms)
Jun 11 08:17:00.041: INFO: (9) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/rewriteme">test</a> (200; 7.645567ms)
Jun 11 08:17:00.043: INFO: (9) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname2/proxy/: bar (200; 9.440596ms)
Jun 11 08:17:00.045: INFO: (9) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname2/proxy/: bar (200; 11.51272ms)
Jun 11 08:17:00.045: INFO: (9) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname2/proxy/: tls qux (200; 11.634945ms)
Jun 11 08:17:00.045: INFO: (9) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname1/proxy/: foo (200; 11.937578ms)
Jun 11 08:17:00.045: INFO: (9) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname1/proxy/: foo (200; 11.699662ms)
Jun 11 08:17:00.045: INFO: (9) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname1/proxy/: tls baz (200; 11.567268ms)
Jun 11 08:17:00.051: INFO: (10) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:162/proxy/: bar (200; 5.776428ms)
Jun 11 08:17:00.055: INFO: (10) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:462/proxy/: tls qux (200; 8.911391ms)
Jun 11 08:17:00.055: INFO: (10) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:160/proxy/: foo (200; 9.126283ms)
Jun 11 08:17:00.055: INFO: (10) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/rewriteme">test<... (200; 9.197209ms)
Jun 11 08:17:00.055: INFO: (10) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/rewriteme">... (200; 9.146933ms)
Jun 11 08:17:00.055: INFO: (10) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:460/proxy/: tls baz (200; 9.075055ms)
Jun 11 08:17:00.055: INFO: (10) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/rewriteme">test</a> (200; 9.321711ms)
Jun 11 08:17:00.055: INFO: (10) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:160/proxy/: foo (200; 9.162365ms)
Jun 11 08:17:00.055: INFO: (10) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/tlsrewritem... (200; 9.268481ms)
Jun 11 08:17:00.055: INFO: (10) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:162/proxy/: bar (200; 9.194231ms)
Jun 11 08:17:00.057: INFO: (10) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname1/proxy/: tls baz (200; 11.251724ms)
Jun 11 08:17:00.059: INFO: (10) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname1/proxy/: foo (200; 13.422439ms)
Jun 11 08:17:00.059: INFO: (10) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname2/proxy/: bar (200; 13.702094ms)
Jun 11 08:17:00.059: INFO: (10) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname2/proxy/: bar (200; 13.807447ms)
Jun 11 08:17:00.059: INFO: (10) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname2/proxy/: tls qux (200; 14.071765ms)
Jun 11 08:17:00.060: INFO: (10) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname1/proxy/: foo (200; 13.864241ms)
Jun 11 08:17:00.067: INFO: (11) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:160/proxy/: foo (200; 7.740577ms)
Jun 11 08:17:00.067: INFO: (11) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:460/proxy/: tls baz (200; 7.927797ms)
Jun 11 08:17:00.067: INFO: (11) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:462/proxy/: tls qux (200; 7.059263ms)
Jun 11 08:17:00.068: INFO: (11) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:160/proxy/: foo (200; 5.916523ms)
Jun 11 08:17:00.068: INFO: (11) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/rewriteme">... (200; 7.210058ms)
Jun 11 08:17:00.068: INFO: (11) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/rewriteme">test</a> (200; 6.841435ms)
Jun 11 08:17:00.068: INFO: (11) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:162/proxy/: bar (200; 7.777914ms)
Jun 11 08:17:00.068: INFO: (11) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/rewriteme">test<... (200; 6.812061ms)
Jun 11 08:17:00.068: INFO: (11) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/tlsrewritem... (200; 7.176783ms)
Jun 11 08:17:00.068: INFO: (11) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:162/proxy/: bar (200; 6.753134ms)
Jun 11 08:17:00.069: INFO: (11) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname1/proxy/: foo (200; 9.301911ms)
Jun 11 08:17:00.072: INFO: (11) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname2/proxy/: bar (200; 11.503274ms)
Jun 11 08:17:00.072: INFO: (11) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname1/proxy/: foo (200; 10.32526ms)
Jun 11 08:17:00.072: INFO: (11) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname2/proxy/: tls qux (200; 11.490598ms)
Jun 11 08:17:00.072: INFO: (11) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname2/proxy/: bar (200; 11.850843ms)
Jun 11 08:17:00.072: INFO: (11) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname1/proxy/: tls baz (200; 11.555071ms)
Jun 11 08:17:00.077: INFO: (12) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:460/proxy/: tls baz (200; 4.581406ms)
Jun 11 08:17:00.080: INFO: (12) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:162/proxy/: bar (200; 6.341837ms)
Jun 11 08:17:00.080: INFO: (12) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/rewriteme">test<... (200; 6.441922ms)
Jun 11 08:17:00.080: INFO: (12) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:160/proxy/: foo (200; 6.121515ms)
Jun 11 08:17:00.080: INFO: (12) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/tlsrewritem... (200; 6.31904ms)
Jun 11 08:17:00.080: INFO: (12) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/rewriteme">test</a> (200; 6.620559ms)
Jun 11 08:17:00.080: INFO: (12) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/rewriteme">... (200; 7.796338ms)
Jun 11 08:17:00.081: INFO: (12) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:162/proxy/: bar (200; 7.611727ms)
Jun 11 08:17:00.081: INFO: (12) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:462/proxy/: tls qux (200; 7.42219ms)
Jun 11 08:17:00.081: INFO: (12) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:160/proxy/: foo (200; 7.745586ms)
Jun 11 08:17:00.082: INFO: (12) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname1/proxy/: foo (200; 10.082633ms)
Jun 11 08:17:00.085: INFO: (12) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname2/proxy/: bar (200; 12.37682ms)
Jun 11 08:17:00.085: INFO: (12) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname1/proxy/: foo (200; 11.636207ms)
Jun 11 08:17:00.085: INFO: (12) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname2/proxy/: tls qux (200; 12.336815ms)
Jun 11 08:17:00.085: INFO: (12) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname2/proxy/: bar (200; 12.938652ms)
Jun 11 08:17:00.085: INFO: (12) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname1/proxy/: tls baz (200; 12.202792ms)
Jun 11 08:17:00.091: INFO: (13) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/rewriteme">... (200; 5.843641ms)
Jun 11 08:17:00.095: INFO: (13) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:160/proxy/: foo (200; 8.995691ms)
Jun 11 08:17:00.095: INFO: (13) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:162/proxy/: bar (200; 9.17115ms)
Jun 11 08:17:00.095: INFO: (13) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/rewriteme">test<... (200; 9.189811ms)
Jun 11 08:17:00.095: INFO: (13) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:160/proxy/: foo (200; 9.275823ms)
Jun 11 08:17:00.095: INFO: (13) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/rewriteme">test</a> (200; 9.285636ms)
Jun 11 08:17:00.095: INFO: (13) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:462/proxy/: tls qux (200; 9.403203ms)
Jun 11 08:17:00.095: INFO: (13) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/tlsrewritem... (200; 9.497323ms)
Jun 11 08:17:00.095: INFO: (13) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:162/proxy/: bar (200; 9.325613ms)
Jun 11 08:17:00.095: INFO: (13) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:460/proxy/: tls baz (200; 9.460317ms)
Jun 11 08:17:00.097: INFO: (13) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname2/proxy/: bar (200; 11.800502ms)
Jun 11 08:17:00.100: INFO: (13) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname2/proxy/: bar (200; 14.447012ms)
Jun 11 08:17:00.100: INFO: (13) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname1/proxy/: foo (200; 14.478669ms)
Jun 11 08:17:00.100: INFO: (13) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname2/proxy/: tls qux (200; 14.418101ms)
Jun 11 08:17:00.100: INFO: (13) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname1/proxy/: foo (200; 14.562667ms)
Jun 11 08:17:00.100: INFO: (13) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname1/proxy/: tls baz (200; 14.543084ms)
Jun 11 08:17:00.108: INFO: (14) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/rewriteme">... (200; 6.404178ms)
Jun 11 08:17:00.108: INFO: (14) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:162/proxy/: bar (200; 7.179094ms)
Jun 11 08:17:00.108: INFO: (14) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:460/proxy/: tls baz (200; 6.827475ms)
Jun 11 08:17:00.108: INFO: (14) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:160/proxy/: foo (200; 6.961135ms)
Jun 11 08:17:00.108: INFO: (14) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/rewriteme">test<... (200; 7.760796ms)
Jun 11 08:17:00.108: INFO: (14) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/tlsrewritem... (200; 7.621654ms)
Jun 11 08:17:00.108: INFO: (14) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:160/proxy/: foo (200; 6.890705ms)
Jun 11 08:17:00.109: INFO: (14) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:462/proxy/: tls qux (200; 8.302235ms)
Jun 11 08:17:00.109: INFO: (14) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:162/proxy/: bar (200; 6.944139ms)
Jun 11 08:17:00.109: INFO: (14) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/rewriteme">test</a> (200; 8.375283ms)
Jun 11 08:17:00.110: INFO: (14) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname1/proxy/: tls baz (200; 9.334435ms)
Jun 11 08:17:00.113: INFO: (14) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname1/proxy/: foo (200; 11.534857ms)
Jun 11 08:17:00.113: INFO: (14) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname2/proxy/: bar (200; 11.361689ms)
Jun 11 08:17:00.113: INFO: (14) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname1/proxy/: foo (200; 11.843782ms)
Jun 11 08:17:00.113: INFO: (14) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname2/proxy/: bar (200; 11.48199ms)
Jun 11 08:17:00.113: INFO: (14) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname2/proxy/: tls qux (200; 10.992027ms)
Jun 11 08:17:00.118: INFO: (15) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:162/proxy/: bar (200; 5.022162ms)
Jun 11 08:17:00.121: INFO: (15) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:160/proxy/: foo (200; 8.229551ms)
Jun 11 08:17:00.121: INFO: (15) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/rewriteme">test</a> (200; 8.168478ms)
Jun 11 08:17:00.121: INFO: (15) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:162/proxy/: bar (200; 8.270948ms)
Jun 11 08:17:00.121: INFO: (15) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/rewriteme">... (200; 8.161127ms)
Jun 11 08:17:00.121: INFO: (15) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:462/proxy/: tls qux (200; 8.526571ms)
Jun 11 08:17:00.121: INFO: (15) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:160/proxy/: foo (200; 8.223776ms)
Jun 11 08:17:00.121: INFO: (15) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/tlsrewritem... (200; 8.317257ms)
Jun 11 08:17:00.121: INFO: (15) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/rewriteme">test<... (200; 8.284615ms)
Jun 11 08:17:00.121: INFO: (15) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:460/proxy/: tls baz (200; 8.186804ms)
Jun 11 08:17:00.124: INFO: (15) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname1/proxy/: tls baz (200; 10.713696ms)
Jun 11 08:17:00.126: INFO: (15) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname2/proxy/: bar (200; 12.813218ms)
Jun 11 08:17:00.126: INFO: (15) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname1/proxy/: foo (200; 13.076845ms)
Jun 11 08:17:00.126: INFO: (15) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname2/proxy/: tls qux (200; 13.371292ms)
Jun 11 08:17:00.126: INFO: (15) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname1/proxy/: foo (200; 13.140506ms)
Jun 11 08:17:00.127: INFO: (15) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname2/proxy/: bar (200; 13.399173ms)
Jun 11 08:17:00.132: INFO: (16) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:460/proxy/: tls baz (200; 5.028039ms)
Jun 11 08:17:00.134: INFO: (16) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/rewriteme">test<... (200; 7.452663ms)
Jun 11 08:17:00.134: INFO: (16) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:162/proxy/: bar (200; 7.455416ms)
Jun 11 08:17:00.135: INFO: (16) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/rewriteme">test</a> (200; 7.533332ms)
Jun 11 08:17:00.135: INFO: (16) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/rewriteme">... (200; 7.719617ms)
Jun 11 08:17:00.135: INFO: (16) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:160/proxy/: foo (200; 7.518075ms)
Jun 11 08:17:00.135: INFO: (16) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:160/proxy/: foo (200; 7.798047ms)
Jun 11 08:17:00.135: INFO: (16) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:162/proxy/: bar (200; 7.776383ms)
Jun 11 08:17:00.135: INFO: (16) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/tlsrewritem... (200; 7.701014ms)
Jun 11 08:17:00.135: INFO: (16) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:462/proxy/: tls qux (200; 8.040761ms)
Jun 11 08:17:00.138: INFO: (16) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname1/proxy/: tls baz (200; 11.130448ms)
Jun 11 08:17:00.140: INFO: (16) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname2/proxy/: tls qux (200; 13.455435ms)
Jun 11 08:17:00.141: INFO: (16) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname1/proxy/: foo (200; 13.547748ms)
Jun 11 08:17:00.141: INFO: (16) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname1/proxy/: foo (200; 13.956836ms)
Jun 11 08:17:00.141: INFO: (16) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname2/proxy/: bar (200; 13.883353ms)
Jun 11 08:17:00.141: INFO: (16) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname2/proxy/: bar (200; 13.935228ms)
Jun 11 08:17:00.150: INFO: (17) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:460/proxy/: tls baz (200; 9.219614ms)
Jun 11 08:17:00.150: INFO: (17) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/rewriteme">test<... (200; 8.703979ms)
Jun 11 08:17:00.150: INFO: (17) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/tlsrewritem... (200; 8.796743ms)
Jun 11 08:17:00.150: INFO: (17) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:162/proxy/: bar (200; 8.733745ms)
Jun 11 08:17:00.150: INFO: (17) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:162/proxy/: bar (200; 9.426687ms)
Jun 11 08:17:00.150: INFO: (17) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:160/proxy/: foo (200; 9.218776ms)
Jun 11 08:17:00.150: INFO: (17) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:462/proxy/: tls qux (200; 8.918934ms)
Jun 11 08:17:00.150: INFO: (17) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/rewriteme">test</a> (200; 9.466104ms)
Jun 11 08:17:00.150: INFO: (17) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/rewriteme">... (200; 9.300149ms)
Jun 11 08:17:00.150: INFO: (17) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:160/proxy/: foo (200; 9.368565ms)
Jun 11 08:17:00.151: INFO: (17) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname1/proxy/: foo (200; 10.424895ms)
Jun 11 08:17:00.154: INFO: (17) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname2/proxy/: tls qux (200; 13.025846ms)
Jun 11 08:17:00.154: INFO: (17) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname1/proxy/: tls baz (200; 12.763844ms)
Jun 11 08:17:00.154: INFO: (17) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname2/proxy/: bar (200; 13.194673ms)
Jun 11 08:17:00.154: INFO: (17) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname2/proxy/: bar (200; 13.086814ms)
Jun 11 08:17:00.154: INFO: (17) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname1/proxy/: foo (200; 13.296937ms)
Jun 11 08:17:00.159: INFO: (18) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:160/proxy/: foo (200; 4.496807ms)
Jun 11 08:17:00.162: INFO: (18) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:460/proxy/: tls baz (200; 7.182342ms)
Jun 11 08:17:00.162: INFO: (18) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/tlsrewritem... (200; 7.110928ms)
Jun 11 08:17:00.162: INFO: (18) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:160/proxy/: foo (200; 7.688424ms)
Jun 11 08:17:00.162: INFO: (18) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:162/proxy/: bar (200; 7.795107ms)
Jun 11 08:17:00.163: INFO: (18) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/rewriteme">test</a> (200; 8.161194ms)
Jun 11 08:17:00.163: INFO: (18) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:462/proxy/: tls qux (200; 8.275675ms)
Jun 11 08:17:00.163: INFO: (18) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/rewriteme">test<... (200; 8.194837ms)
Jun 11 08:17:00.163: INFO: (18) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/rewriteme">... (200; 8.097902ms)
Jun 11 08:17:00.163: INFO: (18) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:162/proxy/: bar (200; 8.422841ms)
Jun 11 08:17:00.165: INFO: (18) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname2/proxy/: bar (200; 10.022156ms)
Jun 11 08:17:00.167: INFO: (18) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname1/proxy/: foo (200; 12.715768ms)
Jun 11 08:17:00.167: INFO: (18) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname1/proxy/: tls baz (200; 12.870731ms)
Jun 11 08:17:00.167: INFO: (18) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname2/proxy/: bar (200; 12.944445ms)
Jun 11 08:17:00.167: INFO: (18) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname2/proxy/: tls qux (200; 12.778038ms)
Jun 11 08:17:00.167: INFO: (18) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname1/proxy/: foo (200; 12.910817ms)
Jun 11 08:17:00.176: INFO: (19) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2/proxy/rewriteme">test</a> (200; 8.71548ms)
Jun 11 08:17:00.176: INFO: (19) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:462/proxy/: tls qux (200; 8.644104ms)
Jun 11 08:17:00.176: INFO: (19) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:162/proxy/: bar (200; 8.49821ms)
Jun 11 08:17:00.176: INFO: (19) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:160/proxy/: foo (200; 8.496741ms)
Jun 11 08:17:00.176: INFO: (19) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:443/proxy/tlsrewritem... (200; 8.682931ms)
Jun 11 08:17:00.176: INFO: (19) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:162/proxy/: bar (200; 8.72811ms)
Jun 11 08:17:00.176: INFO: (19) /api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/http:proxy-service-khrlv-nrml2:1080/proxy/rewriteme">... (200; 8.883806ms)
Jun 11 08:17:00.176: INFO: (19) /api/v1/namespaces/proxy-6807/pods/https:proxy-service-khrlv-nrml2:460/proxy/: tls baz (200; 8.891386ms)
Jun 11 08:17:00.176: INFO: (19) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:160/proxy/: foo (200; 8.858421ms)
Jun 11 08:17:00.176: INFO: (19) /api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6807/pods/proxy-service-khrlv-nrml2:1080/proxy/rewriteme">test<... (200; 8.697288ms)
Jun 11 08:17:00.178: INFO: (19) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname1/proxy/: foo (200; 10.261696ms)
Jun 11 08:17:00.180: INFO: (19) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname1/proxy/: foo (200; 12.418075ms)
Jun 11 08:17:00.180: INFO: (19) /api/v1/namespaces/proxy-6807/services/proxy-service-khrlv:portname2/proxy/: bar (200; 12.497418ms)
Jun 11 08:17:00.180: INFO: (19) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname1/proxy/: tls baz (200; 12.57679ms)
Jun 11 08:17:00.180: INFO: (19) /api/v1/namespaces/proxy-6807/services/https:proxy-service-khrlv:tlsportname2/proxy/: tls qux (200; 12.728823ms)
Jun 11 08:17:00.180: INFO: (19) /api/v1/namespaces/proxy-6807/services/http:proxy-service-khrlv:portname2/proxy/: bar (200; 12.593799ms)
STEP: deleting ReplicationController proxy-service-khrlv in namespace proxy-6807, will wait for the garbage collector to delete the pods
Jun 11 08:17:00.246: INFO: Deleting ReplicationController proxy-service-khrlv took: 11.56949ms
Jun 11 08:17:01.846: INFO: Terminating ReplicationController proxy-service-khrlv pods took: 1.600283117s
[AfterEach] version v1
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:17:06.947: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "proxy-6807" for this suite.

• [SLOW TEST:11.325 seconds]
[sig-network] Proxy
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":280,"completed":188,"skipped":3092,"failed":0}
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:17:06.998: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7987
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:17:23.277: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "resourcequota-7987" for this suite.

• [SLOW TEST:16.296 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":280,"completed":189,"skipped":3092,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:17:23.294: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6842
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 08:17:23.932: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 08:17:26.956: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:17:27.006: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "webhook-6842" for this suite.
STEP: Destroying namespace "webhook-6842-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":280,"completed":190,"skipped":3106,"failed":0}

------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:17:27.106: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5846
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jun 11 08:17:27.273: INFO: Waiting up to 5m0s for pod "downward-api-b48f9f5d-96cf-4109-92a4-613e62cd0c6a" in namespace "downward-api-5846" to be "success or failure"
Jun 11 08:17:27.277: INFO: Pod "downward-api-b48f9f5d-96cf-4109-92a4-613e62cd0c6a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.940503ms
Jun 11 08:17:29.282: INFO: Pod "downward-api-b48f9f5d-96cf-4109-92a4-613e62cd0c6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008363876s
Jun 11 08:17:31.286: INFO: Pod "downward-api-b48f9f5d-96cf-4109-92a4-613e62cd0c6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013063876s
STEP: Saw pod success
Jun 11 08:17:31.286: INFO: Pod "downward-api-b48f9f5d-96cf-4109-92a4-613e62cd0c6a" satisfied condition "success or failure"
Jun 11 08:17:31.291: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod downward-api-b48f9f5d-96cf-4109-92a4-613e62cd0c6a container dapi-container: <nil>
STEP: delete the pod
Jun 11 08:17:31.315: INFO: Waiting for pod downward-api-b48f9f5d-96cf-4109-92a4-613e62cd0c6a to disappear
Jun 11 08:17:31.321: INFO: Pod downward-api-b48f9f5d-96cf-4109-92a4-613e62cd0c6a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:17:31.321: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "downward-api-5846" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":280,"completed":191,"skipped":3106,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:17:31.335: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7476
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 11 08:17:31.495: INFO: Waiting up to 5m0s for pod "pod-404fcd8a-a026-4201-a430-f9776297eb66" in namespace "emptydir-7476" to be "success or failure"
Jun 11 08:17:31.499: INFO: Pod "pod-404fcd8a-a026-4201-a430-f9776297eb66": Phase="Pending", Reason="", readiness=false. Elapsed: 3.999288ms
Jun 11 08:17:33.504: INFO: Pod "pod-404fcd8a-a026-4201-a430-f9776297eb66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008626284s
STEP: Saw pod success
Jun 11 08:17:33.504: INFO: Pod "pod-404fcd8a-a026-4201-a430-f9776297eb66" satisfied condition "success or failure"
Jun 11 08:17:33.508: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-404fcd8a-a026-4201-a430-f9776297eb66 container test-container: <nil>
STEP: delete the pod
Jun 11 08:17:33.532: INFO: Waiting for pod pod-404fcd8a-a026-4201-a430-f9776297eb66 to disappear
Jun 11 08:17:33.538: INFO: Pod pod-404fcd8a-a026-4201-a430-f9776297eb66 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:17:33.538: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "emptydir-7476" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":192,"skipped":3111,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:17:33.553: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2406
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-2406
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Jun 11 08:17:33.720: INFO: Found 0 stateful pods, waiting for 3
Jun 11 08:17:43.725: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 11 08:17:43.725: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 11 08:17:43.725: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jun 11 08:17:43.758: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jun 11 08:17:53.796: INFO: Updating stateful set ss2
Jun 11 08:17:53.806: INFO: Waiting for Pod statefulset-2406/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 11 08:18:03.815: INFO: Waiting for Pod statefulset-2406/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Jun 11 08:18:13.855: INFO: Found 2 stateful pods, waiting for 3
Jun 11 08:18:23.861: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 11 08:18:23.861: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 11 08:18:23.861: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jun 11 08:18:23.889: INFO: Updating stateful set ss2
Jun 11 08:18:23.898: INFO: Waiting for Pod statefulset-2406/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 11 08:18:33.925: INFO: Updating stateful set ss2
Jun 11 08:18:33.935: INFO: Waiting for StatefulSet statefulset-2406/ss2 to complete update
Jun 11 08:18:33.935: INFO: Waiting for Pod statefulset-2406/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 11 08:18:43.944: INFO: Waiting for StatefulSet statefulset-2406/ss2 to complete update
Jun 11 08:18:43.944: INFO: Waiting for Pod statefulset-2406/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jun 11 08:18:53.944: INFO: Deleting all statefulset in ns statefulset-2406
Jun 11 08:18:53.948: INFO: Scaling statefulset ss2 to 0
Jun 11 08:19:33.964: INFO: Waiting for statefulset status.replicas updated to 0
Jun 11 08:19:33.969: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:19:33.988: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "statefulset-2406" for this suite.

• [SLOW TEST:120.450 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":280,"completed":193,"skipped":3115,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:19:34.004: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8076
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 08:19:34.157: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:19:38.203: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "pods-8076" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":280,"completed":194,"skipped":3149,"failed":0}

------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:19:38.218: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-4300
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6903
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5987
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:19:52.698: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "namespaces-4300" for this suite.
STEP: Destroying namespace "nsdeletetest-6903" for this suite.
Jun 11 08:19:52.717: INFO: Namespace nsdeletetest-6903 was already deleted
STEP: Destroying namespace "nsdeletetest-5987" for this suite.

• [SLOW TEST:14.507 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":280,"completed":195,"skipped":3149,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:19:52.725: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5827
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5827.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5827.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5827.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5827.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5827.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5827.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5827.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5827.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5827.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5827.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5827.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5827.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5827.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5827.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5827.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5827.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5827.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5827.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 11 08:19:56.960: INFO: DNS probes using dns-5827/dns-test-0c205770-d2a3-40c2-a1fa-550020749cd6 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:19:56.999: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "dns-5827" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":280,"completed":196,"skipped":3158,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:19:57.015: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2596
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jun 11 08:19:59.722: INFO: Successfully updated pod "annotationupdate16e30f51-6763-49c0-ac82-6ad50dcff75d"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:20:03.747: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "downward-api-2596" for this suite.

• [SLOW TEST:6.747 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":197,"skipped":3171,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:20:03.762: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8818
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jun 11 08:20:06.454: INFO: Successfully updated pod "labelsupdate3d3cbb7f-e1e0-4495-9a30-3aad96b846dc"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:20:10.479: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "projected-8818" for this suite.

• [SLOW TEST:6.731 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":198,"skipped":3220,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:20:10.493: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-904
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-904
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 11 08:20:10.641: INFO: Waiting up to 10m0s for all (but 7) nodes to be schedulable
Jun 11 08:20:10.643: INFO: Unschedulable nodes:
Jun 11 08:20:10.643: INFO: -> ip-10-0-129-22.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated route-reflector NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 08:20:10.643: INFO: -> ip-10-0-129-167.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated elasticsearch-data NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 08:20:10.643: INFO: -> ip-10-0-130-153.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated monitoring NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 08:20:10.643: INFO: -> ip-10-0-133-101.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated route-reflector NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 08:20:10.643: INFO: -> ip-10-0-132-151.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated elasticsearch-data NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 08:20:10.643: INFO: -> ip-10-0-137-193.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated elasticsearch-data NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 08:20:10.644: INFO: -> ip-10-0-139-38.us-west-2.compute.internal Ready=true Network=true Taints=[{dedicated route-reflector NoExecute <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Jun 11 08:20:10.644: INFO: ================================
STEP: Creating test pods
Jun 11 08:20:30.858: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.28.230 8081 | grep -v '^\s*$'] Namespace:pod-network-test-904 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:20:30.859: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:20:31.919: INFO: Found all expected endpoints: [netserver-0]
Jun 11 08:20:31.924: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.168.169 8081 | grep -v '^\s*$'] Namespace:pod-network-test-904 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:20:31.924: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:20:32.981: INFO: Found all expected endpoints: [netserver-1]
Jun 11 08:20:32.986: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.208.155 8081 | grep -v '^\s*$'] Namespace:pod-network-test-904 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:20:32.986: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:20:34.045: INFO: Found all expected endpoints: [netserver-2]
Jun 11 08:20:34.050: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.74.253 8081 | grep -v '^\s*$'] Namespace:pod-network-test-904 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:20:34.050: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:20:35.108: INFO: Found all expected endpoints: [netserver-3]
Jun 11 08:20:35.113: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.226.108 8081 | grep -v '^\s*$'] Namespace:pod-network-test-904 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:20:35.113: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:20:36.172: INFO: Found all expected endpoints: [netserver-4]
Jun 11 08:20:36.176: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.23.97 8081 | grep -v '^\s*$'] Namespace:pod-network-test-904 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:20:36.177: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:20:37.238: INFO: Found all expected endpoints: [netserver-5]
Jun 11 08:20:37.243: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.169.48 8081 | grep -v '^\s*$'] Namespace:pod-network-test-904 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:20:37.243: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:20:38.307: INFO: Found all expected endpoints: [netserver-6]
Jun 11 08:20:38.311: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.42.14 8081 | grep -v '^\s*$'] Namespace:pod-network-test-904 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:20:38.311: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:20:39.366: INFO: Found all expected endpoints: [netserver-7]
Jun 11 08:20:39.370: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.239.232 8081 | grep -v '^\s*$'] Namespace:pod-network-test-904 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:20:39.371: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:20:40.440: INFO: Found all expected endpoints: [netserver-8]
Jun 11 08:20:40.445: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.162.231 8081 | grep -v '^\s*$'] Namespace:pod-network-test-904 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:20:40.445: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:20:41.500: INFO: Found all expected endpoints: [netserver-9]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:20:41.500: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "pod-network-test-904" for this suite.

• [SLOW TEST:31.022 seconds]
[sig-network] Networking
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":199,"skipped":3232,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:20:41.516: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1815
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 08:20:41.665: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:20:43.748: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "pods-1815" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":280,"completed":200,"skipped":3286,"failed":0}
SSSS
------------------------------
[k8s.io] [sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:20:43.763: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-1005
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:172
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating server pod server in namespace prestop-1005
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1005
STEP: Deleting pre-stop pod
Jun 11 08:20:52.968: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:20:52.976: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "prestop-1005" for this suite.

• [SLOW TEST:9.230 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":280,"completed":201,"skipped":3290,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:20:52.994: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8996
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jun 11 08:20:53.179: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8996 /api/v1/namespaces/watch-8996/configmaps/e2e-watch-test-resource-version 1f0585af-af28-4bd1-8f9c-3a30bf818c44 127678 0 2020-06-11 08:20:53 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 11 08:20:53.179: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8996 /api/v1/namespaces/watch-8996/configmaps/e2e-watch-test-resource-version 1f0585af-af28-4bd1-8f9c-3a30bf818c44 127679 0 2020-06-11 08:20:53 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:20:53.179: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "watch-8996" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":280,"completed":202,"skipped":3294,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:20:53.194: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6626
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-6626
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-6626
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6626
Jun 11 08:20:53.368: INFO: Found 0 stateful pods, waiting for 1
Jun 11 08:21:03.373: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jun 11 08:21:03.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=statefulset-6626 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 11 08:21:03.729: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 11 08:21:03.729: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 11 08:21:03.729: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 11 08:21:03.734: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 11 08:21:13.738: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 11 08:21:13.738: INFO: Waiting for statefulset status.replicas updated to 0
Jun 11 08:21:13.755: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998413s
Jun 11 08:21:14.759: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995748202s
Jun 11 08:21:15.764: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.991126904s
Jun 11 08:21:16.769: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.986288023s
Jun 11 08:21:17.773: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.981797311s
Jun 11 08:21:18.778: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.977349589s
Jun 11 08:21:19.782: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.972447764s
Jun 11 08:21:20.787: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.968141285s
Jun 11 08:21:21.792: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.963608039s
Jun 11 08:21:22.798: INFO: Verifying statefulset ss doesn't scale past 1 for another 958.632445ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6626
Jun 11 08:21:23.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=statefulset-6626 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 11 08:21:23.977: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 11 08:21:23.977: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 11 08:21:23.977: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 11 08:21:23.982: INFO: Found 1 stateful pods, waiting for 3
Jun 11 08:21:33.987: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 11 08:21:33.987: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 11 08:21:33.987: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jun 11 08:21:33.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=statefulset-6626 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 11 08:21:34.169: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 11 08:21:34.169: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 11 08:21:34.169: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 11 08:21:34.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=statefulset-6626 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 11 08:21:34.350: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 11 08:21:34.350: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 11 08:21:34.350: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 11 08:21:34.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=statefulset-6626 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 11 08:21:34.537: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 11 08:21:34.537: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 11 08:21:34.537: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 11 08:21:34.537: INFO: Waiting for statefulset status.replicas updated to 0
Jun 11 08:21:34.542: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jun 11 08:21:44.552: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 11 08:21:44.552: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 11 08:21:44.552: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 11 08:21:44.566: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998357s
Jun 11 08:21:45.571: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995639546s
Jun 11 08:21:46.576: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990634368s
Jun 11 08:21:47.580: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985673379s
Jun 11 08:21:48.585: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.980981817s
Jun 11 08:21:49.591: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.975994505s
Jun 11 08:21:50.596: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.970768352s
Jun 11 08:21:51.601: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.965658923s
Jun 11 08:21:52.606: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.960394622s
Jun 11 08:21:53.611: INFO: Verifying statefulset ss doesn't scale past 3 for another 955.275659ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6626
Jun 11 08:21:54.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=statefulset-6626 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 11 08:21:54.785: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 11 08:21:54.785: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 11 08:21:54.785: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 11 08:21:54.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=statefulset-6626 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 11 08:21:54.962: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 11 08:21:54.962: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 11 08:21:54.962: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 11 08:21:54.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=statefulset-6626 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 11 08:21:55.137: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 11 08:21:55.137: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 11 08:21:55.137: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 11 08:21:55.137: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jun 11 08:22:25.156: INFO: Deleting all statefulset in ns statefulset-6626
Jun 11 08:22:25.160: INFO: Scaling statefulset ss to 0
Jun 11 08:22:25.172: INFO: Waiting for statefulset status.replicas updated to 0
Jun 11 08:22:25.176: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:22:25.195: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "statefulset-6626" for this suite.

• [SLOW TEST:92.015 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":280,"completed":203,"skipped":3306,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:22:25.210: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7653
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-8b29f40b-9559-44cb-8339-c0e0222cf54e
STEP: Creating configMap with name cm-test-opt-upd-3ce3c323-018d-48e9-821a-703db0cfb472
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-8b29f40b-9559-44cb-8339-c0e0222cf54e
STEP: Updating configmap cm-test-opt-upd-3ce3c323-018d-48e9-821a-703db0cfb472
STEP: Creating configMap with name cm-test-opt-create-118f9347-0d15-4b68-93c2-69a8572184c0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:23:59.887: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "configmap-7653" for this suite.

• [SLOW TEST:94.693 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":204,"skipped":3328,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:23:59.903: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-468
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 08:24:01.034: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 11 08:24:03.046: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727460641, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727460641, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727460641, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727460641, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 08:24:06.063: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:24:18.214: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "webhook-468" for this suite.
STEP: Destroying namespace "webhook-468-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.402 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":280,"completed":205,"skipped":3330,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:24:18.306: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1816
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:24:18.465: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1816" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":280,"completed":206,"skipped":3370,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:24:18.481: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2401
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jun 11 08:24:18.639: INFO: Pod name pod-release: Found 0 pods out of 1
Jun 11 08:24:23.644: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:24:24.670: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "replication-controller-2401" for this suite.

• [SLOW TEST:6.209 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":280,"completed":207,"skipped":3390,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:24:24.691: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-8672
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:24:34.848: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "job-8672" for this suite.

• [SLOW TEST:10.173 seconds]
[sig-apps] Job
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":280,"completed":208,"skipped":3418,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:24:34.864: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-354
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:344
Jun 11 08:24:35.013: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 11 08:25:35.104: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 08:25:35.108: INFO: Starting informer...
STEP: Starting pods...
Jun 11 08:25:35.337: INFO: Pod1 is running on ip-10-0-136-38.us-west-2.compute.internal. Tainting Node
Jun 11 08:25:37.561: INFO: Pod2 is running on ip-10-0-136-38.us-west-2.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jun 11 08:25:46.889: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jun 11 08:26:06.861: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:26:06.876: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-354" for this suite.

• [SLOW TEST:92.027 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":280,"completed":209,"skipped":3438,"failed":0}
S
------------------------------
[k8s.io] Docker Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:26:06.891: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1889
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:26:11.084: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "containers-1889" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":280,"completed":210,"skipped":3439,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:26:11.099: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4108
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 08:26:11.316: INFO: Create a RollingUpdate DaemonSet
Jun 11 08:26:11.321: INFO: Check that daemon pods launch on every node of the cluster
Jun 11 08:26:11.328: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:11.329: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:11.329: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:11.329: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:11.329: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:11.329: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:11.329: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:11.329: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:11.329: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:11.329: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:11.333: INFO: Number of nodes with available pods: 0
Jun 11 08:26:11.333: INFO: Node ip-10-0-128-119.us-west-2.compute.internal is running more than one daemon pod
Jun 11 08:26:12.340: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:12.340: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:12.340: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:12.340: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:12.340: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:12.340: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:12.340: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:12.340: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:12.340: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:12.340: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:12.345: INFO: Number of nodes with available pods: 0
Jun 11 08:26:12.345: INFO: Node ip-10-0-128-119.us-west-2.compute.internal is running more than one daemon pod
Jun 11 08:26:13.341: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:13.341: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:13.341: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:13.341: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:13.341: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:13.341: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:13.341: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:13.341: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:13.341: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:13.341: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:13.346: INFO: Number of nodes with available pods: 5
Jun 11 08:26:13.346: INFO: Node ip-10-0-129-30.us-west-2.compute.internal is running more than one daemon pod
Jun 11 08:26:14.342: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:14.342: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:14.342: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:14.342: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:14.342: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:14.342: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:14.343: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:14.343: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:14.343: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:14.343: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:14.348: INFO: Number of nodes with available pods: 10
Jun 11 08:26:14.348: INFO: Number of running nodes: 10, number of available pods: 10
Jun 11 08:26:14.348: INFO: Update the DaemonSet to trigger a rollout
Jun 11 08:26:14.357: INFO: Updating DaemonSet daemon-set
Jun 11 08:26:18.394: INFO: Roll back the DaemonSet before rollout is complete
Jun 11 08:26:18.434: INFO: Updating DaemonSet daemon-set
Jun 11 08:26:18.434: INFO: Make sure DaemonSet rollback is complete
Jun 11 08:26:18.445: INFO: Wrong image for pod: daemon-set-56zk8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 11 08:26:18.445: INFO: Pod daemon-set-56zk8 is not available
Jun 11 08:26:18.456: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:18.456: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:18.456: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:18.456: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:18.456: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:18.456: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:18.456: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:18.456: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:18.456: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:18.457: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:19.462: INFO: Pod daemon-set-vztz8 is not available
Jun 11 08:26:19.469: INFO: DaemonSet pods can't tolerate node ip-10-0-129-167.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:19.469: INFO: DaemonSet pods can't tolerate node ip-10-0-129-22.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:19.469: INFO: DaemonSet pods can't tolerate node ip-10-0-130-153.us-west-2.compute.internal with taints [{Key:dedicated Value:monitoring Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:19.469: INFO: DaemonSet pods can't tolerate node ip-10-0-132-151.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:19.469: INFO: DaemonSet pods can't tolerate node ip-10-0-133-101.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:19.469: INFO: DaemonSet pods can't tolerate node ip-10-0-137-193.us-west-2.compute.internal with taints [{Key:dedicated Value:elasticsearch-data Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:19.469: INFO: DaemonSet pods can't tolerate node ip-10-0-139-38.us-west-2.compute.internal with taints [{Key:dedicated Value:route-reflector Effect:NoExecute TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:19.470: INFO: DaemonSet pods can't tolerate node ip-10-0-193-249.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:19.470: INFO: DaemonSet pods can't tolerate node ip-10-0-197-158.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 08:26:19.470: INFO: DaemonSet pods can't tolerate node ip-10-0-203-191.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4108, will wait for the garbage collector to delete the pods
Jun 11 08:26:19.542: INFO: Deleting DaemonSet.extensions daemon-set took: 10.13811ms
Jun 11 08:26:21.143: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.60024103s
Jun 11 08:27:46.947: INFO: Number of nodes with available pods: 0
Jun 11 08:27:46.947: INFO: Number of running nodes: 0, number of available pods: 0
Jun 11 08:27:46.953: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4108/daemonsets","resourceVersion":"132233"},"items":null}

Jun 11 08:27:46.956: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4108/pods","resourceVersion":"132233"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:27:47.003: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "daemonsets-4108" for this suite.

• [SLOW TEST:95.919 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":280,"completed":211,"skipped":3450,"failed":0}
S
------------------------------
[k8s.io] [sig-node] Events
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:27:47.018: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-4506
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jun 11 08:27:49.217: INFO: &Pod{ObjectMeta:{send-events-e08b6a79-7349-466d-95f5-78a1de4e80f8  events-4506 /api/v1/namespaces/events-4506/pods/send-events-e08b6a79-7349-466d-95f5-78a1de4e80f8 4ff7063c-136c-4f35-bb83-a0aa8a94a0e9 132266 0 2020-06-11 08:27:47 +0000 UTC <nil> <nil> map[name:foo time:169693217] map[cni.projectcalico.org/podIP:192.168.42.38/32 cni.projectcalico.org/podIPs:192.168.42.38/32] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2hwpm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2hwpm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2hwpm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-136-38.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 08:27:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 08:27:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 08:27:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-11 08:27:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.136.38,PodIP:192.168.42.38,StartTime:2020-06-11 08:27:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-11 08:27:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:containerd://fd83a08796b2c0972ed31fdbdcc1b3fdaa66c0decb07e94ae2c2e02ef9c5ab06,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.42.38,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jun 11 08:27:51.221: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jun 11 08:27:53.225: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:27:53.234: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "events-4506" for this suite.

• [SLOW TEST:6.232 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":280,"completed":212,"skipped":3451,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:27:53.251: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-340
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jun 11 08:27:58.446: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:27:59.468: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "replicaset-340" for this suite.

• [SLOW TEST:6.233 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":280,"completed":213,"skipped":3467,"failed":0}
S
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:27:59.483: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3454
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap that has name configmap-test-emptyKey-c6dd6f97-33af-4ac2-afe1-50c4d9b3b3d3
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:27:59.635: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "configmap-3454" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":280,"completed":214,"skipped":3468,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:27:59.651: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1425
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 11 08:27:59.812: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5b83337b-9119-40b7-a2d9-e9f6505a04c1" in namespace "projected-1425" to be "success or failure"
Jun 11 08:27:59.816: INFO: Pod "downwardapi-volume-5b83337b-9119-40b7-a2d9-e9f6505a04c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.076938ms
Jun 11 08:28:01.820: INFO: Pod "downwardapi-volume-5b83337b-9119-40b7-a2d9-e9f6505a04c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008311731s
STEP: Saw pod success
Jun 11 08:28:01.820: INFO: Pod "downwardapi-volume-5b83337b-9119-40b7-a2d9-e9f6505a04c1" satisfied condition "success or failure"
Jun 11 08:28:01.824: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod downwardapi-volume-5b83337b-9119-40b7-a2d9-e9f6505a04c1 container client-container: <nil>
STEP: delete the pod
Jun 11 08:28:01.855: INFO: Waiting for pod downwardapi-volume-5b83337b-9119-40b7-a2d9-e9f6505a04c1 to disappear
Jun 11 08:28:01.860: INFO: Pod downwardapi-volume-5b83337b-9119-40b7-a2d9-e9f6505a04c1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:28:01.860: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "projected-1425" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":215,"skipped":3477,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:28:01.875: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1436
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jun 11 08:28:02.053: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1436 /api/v1/namespaces/watch-1436/configmaps/e2e-watch-test-label-changed 5854b222-df07-4205-8ff7-c963040e7735 132542 0 2020-06-11 08:28:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 11 08:28:02.053: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1436 /api/v1/namespaces/watch-1436/configmaps/e2e-watch-test-label-changed 5854b222-df07-4205-8ff7-c963040e7735 132543 0 2020-06-11 08:28:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jun 11 08:28:02.053: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1436 /api/v1/namespaces/watch-1436/configmaps/e2e-watch-test-label-changed 5854b222-df07-4205-8ff7-c963040e7735 132544 0 2020-06-11 08:28:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jun 11 08:28:12.091: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1436 /api/v1/namespaces/watch-1436/configmaps/e2e-watch-test-label-changed 5854b222-df07-4205-8ff7-c963040e7735 132669 0 2020-06-11 08:28:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 11 08:28:12.091: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1436 /api/v1/namespaces/watch-1436/configmaps/e2e-watch-test-label-changed 5854b222-df07-4205-8ff7-c963040e7735 132670 0 2020-06-11 08:28:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jun 11 08:28:12.091: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1436 /api/v1/namespaces/watch-1436/configmaps/e2e-watch-test-label-changed 5854b222-df07-4205-8ff7-c963040e7735 132671 0 2020-06-11 08:28:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:28:12.091: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "watch-1436" for this suite.

• [SLOW TEST:10.231 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":280,"completed":216,"skipped":3485,"failed":0}
SSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:28:12.106: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-2399
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jun 11 08:28:18.305: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2399 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:28:18.305: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:28:18.363: INFO: Exec stderr: ""
Jun 11 08:28:18.363: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2399 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:28:18.364: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:28:18.429: INFO: Exec stderr: ""
Jun 11 08:28:18.429: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2399 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:28:18.429: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:28:18.487: INFO: Exec stderr: ""
Jun 11 08:28:18.487: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2399 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:28:18.487: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:28:18.558: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jun 11 08:28:18.558: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2399 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:28:18.558: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:28:18.614: INFO: Exec stderr: ""
Jun 11 08:28:18.614: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2399 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:28:18.614: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:28:18.677: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jun 11 08:28:18.677: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2399 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:28:18.677: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:28:18.733: INFO: Exec stderr: ""
Jun 11 08:28:18.733: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2399 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:28:18.733: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:28:18.797: INFO: Exec stderr: ""
Jun 11 08:28:18.797: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2399 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:28:18.797: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:28:18.859: INFO: Exec stderr: ""
Jun 11 08:28:18.859: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2399 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 11 08:28:18.859: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:28:18.923: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:28:18.923: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2399" for this suite.

• [SLOW TEST:6.832 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":217,"skipped":3492,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:28:18.938: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3408
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jun 11 08:28:19.087: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:28:24.998: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:28:45.490: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3408" for this suite.

• [SLOW TEST:26.567 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":280,"completed":218,"skipped":3494,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:28:45.506: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-2399
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jun 11 08:28:45.654: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the sample API server.
Jun 11 08:28:46.060: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jun 11 08:28:48.110: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727460926, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727460926, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727460926, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727460926, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 11 08:28:51.344: INFO: Waited 1.222929454s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:28:53.107: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "aggregator-2399" for this suite.

• [SLOW TEST:7.708 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]","total":280,"completed":219,"skipped":3505,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:28:53.214: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8081
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 11 08:28:53.383: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8da7c908-644c-4206-a785-cdf4ae791e11" in namespace "projected-8081" to be "success or failure"
Jun 11 08:28:53.387: INFO: Pod "downwardapi-volume-8da7c908-644c-4206-a785-cdf4ae791e11": Phase="Pending", Reason="", readiness=false. Elapsed: 4.326348ms
Jun 11 08:28:55.393: INFO: Pod "downwardapi-volume-8da7c908-644c-4206-a785-cdf4ae791e11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009428508s
STEP: Saw pod success
Jun 11 08:28:55.393: INFO: Pod "downwardapi-volume-8da7c908-644c-4206-a785-cdf4ae791e11" satisfied condition "success or failure"
Jun 11 08:28:55.396: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod downwardapi-volume-8da7c908-644c-4206-a785-cdf4ae791e11 container client-container: <nil>
STEP: delete the pod
Jun 11 08:28:55.421: INFO: Waiting for pod downwardapi-volume-8da7c908-644c-4206-a785-cdf4ae791e11 to disappear
Jun 11 08:28:55.425: INFO: Pod downwardapi-volume-8da7c908-644c-4206-a785-cdf4ae791e11 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:28:55.425: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "projected-8081" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":220,"skipped":3537,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:28:55.442: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6520
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jun 11 08:28:55.591: INFO: Waiting up to 1m0s for all (but 7) nodes to be ready
Jun 11 08:28:55.612: INFO: Waiting for terminating namespaces to be deleted...
Jun 11 08:28:55.616: INFO:
Logging pods the kubelet thinks is on node ip-10-0-128-119.us-west-2.compute.internal before test
Jun 11 08:28:55.636: INFO: kubefed-controller-manager-78b769f688-8mjvz from kommander started at 2020-06-11 05:33:45 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.636: INFO: 	Container controller-manager ready: true, restart count 0
Jun 11 08:28:55.636: INFO: fluentbit-kubeaddons-fluent-bit-hfm9r from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.636: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:28:55.636: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-594c2 from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.636: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:28:55.636: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:28:55.636: INFO: ebs-csi-node-2w524 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:28:55.636: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:28:55.636: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:28:55.636: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:28:55.636: INFO: prometheus-kubeaddons-prometheus-node-exporter-fz2j2 from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.636: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:28:55.636: INFO: cert-manager-kubeaddons-cainjector-6dcd94769b-v8x5p from cert-manager started at 2020-06-11 05:29:09 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.636: INFO: 	Container cainjector ready: true, restart count 0
Jun 11 08:28:55.636: INFO: elasticsearch-kubeaddons-client-6c56cc5c7-kpxr5 from kubeaddons started at 2020-06-11 05:30:43 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.636: INFO: 	Container elasticsearch ready: true, restart count 0
Jun 11 08:28:55.636: INFO: yakcl-licensing-cm-7c5cc586b5-xgtnw from kommander started at 2020-06-11 05:33:34 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.636: INFO: 	Container controller-manager ready: true, restart count 0
Jun 11 08:28:55.636: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 11 08:28:55.636: INFO: kube-proxy-b8rs5 from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.636: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:28:55.636: INFO: calico-node-wxzzc from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.636: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:28:55.636: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:28:55.636: INFO:
Logging pods the kubelet thinks is on node ip-10-0-129-105.us-west-2.compute.internal before test
Jun 11 08:28:55.655: INFO: kube-proxy-7vnlr from kube-system started at 2020-06-11 05:26:20 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.655: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:28:55.655: INFO: ebs-csi-node-jzjgw from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:28:55.655: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:28:55.655: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:28:55.655: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:28:55.655: INFO: fluentbit-kubeaddons-fluent-bit-nbsn6 from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.655: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:28:55.655: INFO: kube-oidc-proxy-kubeaddons-545d6df8-w9v44 from kubeaddons started at 2020-06-11 05:31:42 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.655: INFO: 	Container kube-oidc-proxy ready: true, restart count 0
Jun 11 08:28:55.655: INFO: prometheus-kubeaddons-grafana-c8f7fcdb5-xjw4p from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.655: INFO: 	Container grafana ready: true, restart count 0
Jun 11 08:28:55.655: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Jun 11 08:28:55.655: INFO: prometheus-kubeaddons-prometheus-node-exporter-zl5mt from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.655: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:28:55.655: INFO: calico-node-tnwfg from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.655: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:28:55.655: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:28:55.655: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-cxrgp from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.655: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:28:55.655: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:28:55.655: INFO: opsportal-kubeaddons-kommander-ui-689b97997f-x45gn from kubeaddons started at 2020-06-11 05:28:52 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.655: INFO: 	Container opsportal-kubeaddons-kommander-ui ready: true, restart count 0
Jun 11 08:28:55.655: INFO: prometheusadapter-kubeaddons-prometheus-adapter-77bc665f9-sjxp8 from kubeaddons started at 2020-06-11 05:33:06 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.655: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 11 08:28:55.655: INFO:
Logging pods the kubelet thinks is on node ip-10-0-129-30.us-west-2.compute.internal before test
Jun 11 08:28:55.675: INFO: kommander-kubeaddons-kommander-ui-86b4f5f88f-pvgsw from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.676: INFO: 	Container kommander-kubeaddons-kommander-ui ready: true, restart count 0
Jun 11 08:28:55.676: INFO: fluentbit-kubeaddons-fluent-bit-mpwz7 from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.676: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:28:55.676: INFO: calico-node-vpvmh from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.676: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:28:55.676: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:28:55.676: INFO: tiller-deploy-6cdf7f9d6f-d6b2p from kube-system started at 2020-06-11 05:28:13 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.676: INFO: 	Container tiller ready: true, restart count 0
Jun 11 08:28:55.676: INFO: minio-1 from velero started at 2020-06-11 05:32:15 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.676: INFO: 	Container minio ready: true, restart count 0
Jun 11 08:28:55.676: INFO: kommander-kubecost-thanos-query-79fc7d8747-fm6rq from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.676: INFO: 	Container thanos-query ready: true, restart count 0
Jun 11 08:28:55.676: INFO: kommander-kubeaddons-kube-state-metrics-6d77646c89-bvrqg from kommander started at 2020-06-11 05:33:33 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.676: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 11 08:28:55.676: INFO: kube-proxy-td2b5 from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.676: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:28:55.676: INFO: prometheus-kubeaddons-prometheus-node-exporter-dwrbk from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.676: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:28:55.676: INFO: kommander-kubeaddons-cost-analyzer-7cfd4cb9cd-6tfnq from kommander started at 2020-06-11 05:33:41 +0000 UTC (3 container statuses recorded)
Jun 11 08:28:55.676: INFO: 	Container cost-analyzer-frontend ready: true, restart count 0
Jun 11 08:28:55.676: INFO: 	Container cost-analyzer-server ready: true, restart count 0
Jun 11 08:28:55.676: INFO: 	Container cost-model ready: true, restart count 0
Jun 11 08:28:55.676: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-5wq4g from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.676: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:28:55.676: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:28:55.676: INFO: dstorageclass-controller-manager-5c966c767f-h5rsz from kubeaddons started at 2020-06-11 05:29:47 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.676: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 11 08:28:55.676: INFO: 	Container manager ready: true, restart count 0
Jun 11 08:28:55.676: INFO: traefik-forward-auth-kubeaddons-6675968b94-fjznl from kubeaddons started at 2020-06-11 05:37:16 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.676: INFO: 	Container traefik-forward-auth ready: true, restart count 1
Jun 11 08:28:55.676: INFO: kommander-kubeaddons-karma-8df6cfdb6-4jg59 from kommander started at 2020-06-11 05:33:33 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.676: INFO: 	Container karma ready: true, restart count 0
Jun 11 08:28:55.676: INFO: kubernetes-dashboard-549989bcdf-n9ncj from kubeaddons started at 2020-06-11 07:15:29 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.676: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jun 11 08:28:55.676: INFO: ebs-csi-node-s8248 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:28:55.676: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:28:55.676: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:28:55.676: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:28:55.676: INFO:
Logging pods the kubelet thinks is on node ip-10-0-130-174.us-west-2.compute.internal before test
Jun 11 08:28:55.695: INFO: fluentbit-kubeaddons-fluent-bit-xx74f from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.695: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:28:55.695: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-zvgtl from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.695: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:28:55.695: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:28:55.695: INFO: cert-manager-kubeaddons-webhook-77fbc6d59b-d67w5 from cert-manager started at 2020-06-11 05:29:09 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.695: INFO: 	Container cert-manager ready: true, restart count 1
Jun 11 08:28:55.695: INFO: ebs-csi-node-bfvm8 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:28:55.695: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:28:55.695: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:28:55.695: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:28:55.695: INFO: kommander-kubeaddons-grafana-66c558d6f5-ntspl from kommander started at 2020-06-11 05:33:32 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.695: INFO: 	Container grafana ready: true, restart count 0
Jun 11 08:28:55.696: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Jun 11 08:28:55.696: INFO: kommander-kubeaddons-prometheus-alertmanager-7cdccf8c44-h9grs from kommander started at 2020-06-11 05:33:38 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.696: INFO: 	Container prometheus-alertmanager ready: true, restart count 0
Jun 11 08:28:55.696: INFO: 	Container prometheus-alertmanager-configmap-reload ready: true, restart count 0
Jun 11 08:28:55.696: INFO: kube-proxy-6htkf from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.696: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:28:55.696: INFO: prometheus-kubeaddons-prometheus-node-exporter-mzcfk from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.696: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:28:55.696: INFO: yakcl-licensing-webhook-6d876f844d-mrvm5 from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.696: INFO: 	Container webhook ready: true, restart count 0
Jun 11 08:28:55.696: INFO: ebs-csi-controller-0 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (6 container statuses recorded)
Jun 11 08:28:55.696: INFO: 	Container csi-attacher ready: true, restart count 0
Jun 11 08:28:55.696: INFO: 	Container csi-provisioner ready: true, restart count 0
Jun 11 08:28:55.696: INFO: 	Container csi-resizer ready: true, restart count 0
Jun 11 08:28:55.696: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jun 11 08:28:55.696: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:28:55.696: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:28:55.696: INFO: kommander-kubeaddons-thanos-query-6cddb86b55-f4g4l from kommander started at 2020-06-11 05:33:35 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.696: INFO: 	Container thanos-query ready: true, restart count 0
Jun 11 08:28:55.696: INFO: elasticsearchexporter-kubeaddons-elasticsearch-exporter-84pdc2n from kubeaddons started at 2020-06-11 05:36:03 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.696: INFO: 	Container elasticsearch-exporter ready: true, restart count 0
Jun 11 08:28:55.696: INFO: calico-node-hrkxr from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.696: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:28:55.696: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:28:55.696: INFO: kubefed-admission-webhook-7b4997895b-x5h84 from kommander started at 2020-06-11 05:33:38 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.696: INFO: 	Container admission-webhook ready: true, restart count 0
Jun 11 08:28:55.696: INFO: kibana-kubeaddons-c8c7b687-4lqrx from kubeaddons started at 2020-06-11 07:15:29 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.696: INFO: 	Container initialize-kibana-index ready: true, restart count 0
Jun 11 08:28:55.696: INFO: 	Container kibana ready: true, restart count 0
Jun 11 08:28:55.696: INFO:
Logging pods the kubelet thinks is on node ip-10-0-132-48.us-west-2.compute.internal before test
Jun 11 08:28:55.710: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-ft9g5 from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.710: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:28:55.710: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:28:55.710: INFO: calico-node-wr8kq from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.710: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:28:55.710: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:28:55.710: INFO: cert-manager-kubeaddons-7d7f98fbc6-86r7x from cert-manager started at 2020-06-11 05:29:09 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.710: INFO: 	Container cert-manager ready: true, restart count 0
Jun 11 08:28:55.710: INFO: prometheus-kubeaddons-prometheus-node-exporter-cqrh7 from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.710: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:28:55.710: INFO: yakcl-federation-cm-55469d7b6b-f6xhf from kommander started at 2020-06-11 05:33:33 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.710: INFO: 	Container controller-manager ready: true, restart count 0
Jun 11 08:28:55.710: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 11 08:28:55.710: INFO: kube-proxy-q79z5 from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.710: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:28:55.710: INFO: ebs-csi-node-xzdsx from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:28:55.711: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:28:55.711: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:28:55.711: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:28:55.711: INFO: elasticsearch-kubeaddons-master-0 from kubeaddons started at 2020-06-11 05:31:03 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.711: INFO: 	Container elasticsearch ready: true, restart count 0
Jun 11 08:28:55.711: INFO: fluentbit-kubeaddons-fluent-bit-2wt5v from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.711: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:28:55.711: INFO:
Logging pods the kubelet thinks is on node ip-10-0-132-52.us-west-2.compute.internal before test
Jun 11 08:28:55.731: INFO: calico-node-6mfwv from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.731: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:28:55.731: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:28:55.731: INFO: traefik-kubeaddons-64fbc79c9-54zfn from kubeaddons started at 2020-06-11 05:31:03 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.731: INFO: 	Container traefik-kubeaddons ready: true, restart count 0
Jun 11 08:28:55.731: INFO: kubefed-controller-manager-78b769f688-rmx64 from kommander started at 2020-06-11 05:33:38 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.731: INFO: 	Container controller-manager ready: true, restart count 0
Jun 11 08:28:55.731: INFO: kube-proxy-w486c from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.731: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:28:55.731: INFO: ebs-csi-node-cf4v4 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:28:55.731: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:28:55.731: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:28:55.731: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:28:55.731: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-tssmj from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.731: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:28:55.731: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:28:55.731: INFO: traefik-kubeaddons-1.72.19-zdq6l from kubeaddons started at 2020-06-11 05:29:49 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.731: INFO: 	Container traefik ready: false, restart count 0
Jun 11 08:28:55.731: INFO: prometheus-kubeaddons-prometheus-node-exporter-nff74 from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.731: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:28:55.731: INFO: kubeaddons-controller-manager-986f46689-sk6x8 from kubeaddons started at 2020-06-11 05:27:57 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.731: INFO: 	Container manager ready: true, restart count 0
Jun 11 08:28:55.731: INFO: fluentbit-kubeaddons-fluent-bit-78mbt from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.731: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:28:55.731: INFO: elasticsearch-kubeaddons-client-6c56cc5c7-zp9fl from kubeaddons started at 2020-06-11 05:30:43 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.731: INFO: 	Container elasticsearch ready: true, restart count 0
Jun 11 08:28:55.731: INFO:
Logging pods the kubelet thinks is on node ip-10-0-135-119.us-west-2.compute.internal before test
Jun 11 08:28:55.748: INFO: external-dns-kubeaddons-765d55b455-vlxcv from kubeaddons started at 2020-06-11 05:28:25 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.748: INFO: 	Container external-dns ready: true, restart count 0
Jun 11 08:28:55.748: INFO: minio-2 from velero started at 2020-06-11 05:32:12 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.748: INFO: 	Container minio ready: true, restart count 0
Jun 11 08:28:55.748: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-b7drp from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.748: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:28:55.748: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:28:55.748: INFO: calico-node-sq7sb from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.748: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:28:55.748: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:28:55.748: INFO: prometheus-kubeaddons-prometheus-node-exporter-4fllq from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.748: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:28:55.748: INFO: alertmanager-prometheus-kubeaddons-prom-alertmanager-0 from kubeaddons started at 2020-06-11 05:32:06 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.748: INFO: 	Container alertmanager ready: true, restart count 0
Jun 11 08:28:55.748: INFO: 	Container config-reloader ready: true, restart count 0
Jun 11 08:28:55.748: INFO: kube-proxy-xnz2c from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.748: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:28:55.748: INFO: ebs-csi-node-w9whr from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:28:55.748: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:28:55.748: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:28:55.748: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:28:55.748: INFO: velero-kubeaddons-5d85fcdcb9-gqb5c from velero started at 2020-06-11 05:32:08 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.748: INFO: 	Container velero ready: true, restart count 3
Jun 11 08:28:55.748: INFO: elasticsearch-kubeaddons-master-1 from kubeaddons started at 2020-06-11 05:32:29 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.748: INFO: 	Container elasticsearch ready: true, restart count 0
Jun 11 08:28:55.748: INFO: fluentbit-kubeaddons-fluent-bit-6lwhn from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.748: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:28:55.748: INFO: dex-kubeaddons-dex-controller-7bd5fc575c-xmnlq from kubeaddons started at 2020-06-11 05:31:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.748: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 11 08:28:55.748: INFO: 	Container manager ready: true, restart count 0
Jun 11 08:28:55.748: INFO:
Logging pods the kubelet thinks is on node ip-10-0-136-38.us-west-2.compute.internal before test
Jun 11 08:28:55.755: INFO: test-host-network-pod from e2e-kubelet-etc-hosts-2399 started at 2020-06-11 08:28:16 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.755: INFO: 	Container busybox-1 ready: false, restart count 0
Jun 11 08:28:55.755: INFO: 	Container busybox-2 ready: false, restart count 0
Jun 11 08:28:55.755: INFO: fluentbit-kubeaddons-fluent-bit-shb95 from kubeaddons started at 2020-06-11 08:26:06 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.755: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:28:55.755: INFO: ebs-csi-node-8xl6p from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:28:55.755: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:28:55.755: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:28:55.755: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:28:55.755: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-lgn88 from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.755: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:28:55.755: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:28:55.755: INFO: prometheus-kubeaddons-prometheus-node-exporter-wp8c6 from kubeaddons started at 2020-06-11 08:26:06 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.755: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:28:55.755: INFO: kube-proxy-dz75m from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.755: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:28:55.755: INFO: sonobuoy from sonobuoy started at 2020-06-11 07:40:59 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.755: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 11 08:28:55.755: INFO: test-pod from e2e-kubelet-etc-hosts-2399 started at 2020-06-11 08:28:12 +0000 UTC (3 container statuses recorded)
Jun 11 08:28:55.755: INFO: 	Container busybox-1 ready: true, restart count 0
Jun 11 08:28:55.755: INFO: 	Container busybox-2 ready: true, restart count 0
Jun 11 08:28:55.755: INFO: 	Container busybox-3 ready: true, restart count 0
Jun 11 08:28:55.755: INFO: calico-node-tzhxw from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.755: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:28:55.755: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:28:55.755: INFO:
Logging pods the kubelet thinks is on node ip-10-0-137-210.us-west-2.compute.internal before test
Jun 11 08:28:55.771: INFO: calico-node-vkrzv from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.771: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:28:55.771: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:28:55.771: INFO: ebs-csi-node-frsf2 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:28:55.771: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:28:55.771: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:28:55.771: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:28:55.771: INFO: prometheus-kubeaddons-prometheus-node-exporter-8z4lv from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.771: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:28:55.771: INFO: opsportal-landing-6f6865b688-6r5kn from kubeaddons started at 2020-06-11 05:28:52 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.771: INFO: 	Container opsportal-landing ready: true, restart count 0
Jun 11 08:28:55.771: INFO: gatekeeper-kubeaddons-776f4b5c96-jqsq8 from kubeaddons started at 2020-06-11 05:29:52 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.771: INFO: 	Container manager ready: true, restart count 0
Jun 11 08:28:55.771: INFO: elasticsearch-kubeaddons-master-2 from kubeaddons started at 2020-06-11 05:33:29 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.771: INFO: 	Container elasticsearch ready: true, restart count 0
Jun 11 08:28:55.771: INFO: fluentbit-kubeaddons-fluent-bit-hv9bs from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.771: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:28:55.771: INFO: dex-kubeaddons-696dbdb6c6-5prn9 from kubeaddons started at 2020-06-11 05:37:11 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.771: INFO: 	Container main ready: true, restart count 0
Jun 11 08:28:55.771: INFO: kube-proxy-qj7tx from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.771: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:28:55.771: INFO: minio-3 from velero started at 2020-06-11 05:32:15 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.771: INFO: 	Container minio ready: true, restart count 0
Jun 11 08:28:55.771: INFO: prometheus-kubeaddons-kube-state-metrics-6599df558b-mmf9r from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.771: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 11 08:28:55.771: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-6cvjn from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.771: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:28:55.771: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:28:55.771: INFO:
Logging pods the kubelet thinks is on node ip-10-0-138-28.us-west-2.compute.internal before test
Jun 11 08:28:55.789: INFO: reloader-kubeaddons-reloader-7d4bd64cfb-4qjm4 from kubeaddons started at 2020-06-11 05:28:30 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.789: INFO: 	Container reloader-kubeaddons-reloader ready: true, restart count 0
Jun 11 08:28:55.789: INFO: kommander-kubeaddons-kubeaddons-catalog-6654f856df-kndt7 from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.789: INFO: 	Container kubeaddons-catalog ready: true, restart count 0
Jun 11 08:28:55.789: INFO: yakcl-federation-webhook-6fcc46596-smpfh from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.789: INFO: 	Container webhook ready: true, restart count 0
Jun 11 08:28:55.789: INFO: yakcl-federation-utility-apiserver-7d57699df9-wp7bs from kommander started at 2020-06-11 05:33:33 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.789: INFO: 	Container server ready: true, restart count 0
Jun 11 08:28:55.789: INFO: ebs-csi-snapshot-controller-0 from kube-system started at 2020-06-11 05:30:40 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.789: INFO: 	Container ebs-csi-snapshot-controller ready: true, restart count 0
Jun 11 08:28:55.789: INFO: minio-0 from velero started at 2020-06-11 05:32:12 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.789: INFO: 	Container minio ready: true, restart count 0
Jun 11 08:28:55.789: INFO: yakcl-federation-authorizedlister-5db7b69bfd-zl4mt from kommander started at 2020-06-11 05:33:32 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.789: INFO: 	Container authorizedlister ready: true, restart count 0
Jun 11 08:28:55.789: INFO: calico-node-fzn27 from kube-system started at 2020-06-11 05:27:09 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.789: INFO: 	Container bird-metrics ready: true, restart count 0
Jun 11 08:28:55.789: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 08:28:55.789: INFO: prometheus-kubeaddons-prom-operator-767c8d59cb-44z76 from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.789: INFO: 	Container prometheus-operator ready: true, restart count 0
Jun 11 08:28:55.789: INFO: 	Container tls-proxy ready: true, restart count 0
Jun 11 08:28:55.789: INFO: dex-k8s-authenticator-kubeaddons-748fcc984d-vdpxg from kubeaddons started at 2020-06-11 05:31:50 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.789: INFO: 	Container dex-k8s-authenticator ready: true, restart count 5
Jun 11 08:28:55.789: INFO: elasticsearch-kubeaddons-client-6c56cc5c7-blbht from kubeaddons started at 2020-06-11 07:15:29 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.789: INFO: 	Container elasticsearch ready: true, restart count 0
Jun 11 08:28:55.789: INFO: fluentbit-kubeaddons-fluent-bit-46z8w from kubeaddons started at 2020-06-11 05:36:01 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.789: INFO: 	Container fluent-bit ready: true, restart count 0
Jun 11 08:28:55.789: INFO: sonobuoy-systemd-logs-daemon-set-619cde3bd01147f5-lf27c from sonobuoy started at 2020-06-11 07:41:05 +0000 UTC (2 container statuses recorded)
Jun 11 08:28:55.789: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 08:28:55.789: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 08:28:55.789: INFO: kube-proxy-pp8td from kube-system started at 2020-06-11 05:26:19 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.789: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 08:28:55.789: INFO: ebs-csi-node-nbd7n from kube-system started at 2020-06-11 05:30:40 +0000 UTC (3 container statuses recorded)
Jun 11 08:28:55.789: INFO: 	Container ebs-plugin ready: true, restart count 0
Jun 11 08:28:55.789: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 11 08:28:55.789: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 11 08:28:55.789: INFO: prometheus-kubeaddons-prometheus-node-exporter-4c6c2 from kubeaddons started at 2020-06-11 05:31:46 +0000 UTC (1 container statuses recorded)
Jun 11 08:28:55.789: INFO: 	Container node-exporter ready: true, restart count 0
Jun 11 08:28:55.789: INFO: kommander-kubeaddons-prometheus-server-86d645cc98-5bbdz from kommander started at 2020-06-11 05:33:41 +0000 UTC (3 container statuses recorded)
Jun 11 08:28:55.789: INFO: 	Container prometheus-server ready: true, restart count 0
Jun 11 08:28:55.789: INFO: 	Container prometheus-server-configmap-reload ready: true, restart count 0
Jun 11 08:28:55.789: INFO: 	Container thanos-sidecar ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-13db5022-6332-429b-9003-9e52b5cbc627 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-13db5022-6332-429b-9003-9e52b5cbc627 off the node ip-10-0-136-38.us-west-2.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-13db5022-6332-429b-9003-9e52b5cbc627
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:33:59.894: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "sched-pred-6520" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:304.468 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":280,"completed":221,"skipped":3571,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:33:59.910: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8974
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:34:11.106: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "resourcequota-8974" for this suite.

• [SLOW TEST:11.211 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":280,"completed":222,"skipped":3593,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:34:11.122: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5137
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 11 08:34:11.287: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4554cac6-3a71-4490-acf9-4047609cc653" in namespace "downward-api-5137" to be "success or failure"
Jun 11 08:34:11.293: INFO: Pod "downwardapi-volume-4554cac6-3a71-4490-acf9-4047609cc653": Phase="Pending", Reason="", readiness=false. Elapsed: 6.039958ms
Jun 11 08:34:13.298: INFO: Pod "downwardapi-volume-4554cac6-3a71-4490-acf9-4047609cc653": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011148077s
STEP: Saw pod success
Jun 11 08:34:13.298: INFO: Pod "downwardapi-volume-4554cac6-3a71-4490-acf9-4047609cc653" satisfied condition "success or failure"
Jun 11 08:34:13.302: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod downwardapi-volume-4554cac6-3a71-4490-acf9-4047609cc653 container client-container: <nil>
STEP: delete the pod
Jun 11 08:34:13.335: INFO: Waiting for pod downwardapi-volume-4554cac6-3a71-4490-acf9-4047609cc653 to disappear
Jun 11 08:34:13.340: INFO: Pod downwardapi-volume-4554cac6-3a71-4490-acf9-4047609cc653 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:34:13.340: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "downward-api-5137" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":223,"skipped":3596,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:34:13.358: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8573
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8573.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8573.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 11 08:34:17.572: INFO: DNS probes using dns-8573/dns-test-b9db1541-30a8-4f94-af40-b9570eacb312 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:34:17.589: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "dns-8573" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":280,"completed":224,"skipped":3615,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:34:17.605: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8438
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 11 08:34:19.786: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:34:19.825: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "container-runtime-8438" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":225,"skipped":3663,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:34:19.842: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6508
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 11 08:34:20.005: INFO: Waiting up to 5m0s for pod "downwardapi-volume-885d241d-55a3-4ee7-bc60-01ce6fac9047" in namespace "projected-6508" to be "success or failure"
Jun 11 08:34:20.010: INFO: Pod "downwardapi-volume-885d241d-55a3-4ee7-bc60-01ce6fac9047": Phase="Pending", Reason="", readiness=false. Elapsed: 4.266622ms
Jun 11 08:34:22.015: INFO: Pod "downwardapi-volume-885d241d-55a3-4ee7-bc60-01ce6fac9047": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009238162s
STEP: Saw pod success
Jun 11 08:34:22.015: INFO: Pod "downwardapi-volume-885d241d-55a3-4ee7-bc60-01ce6fac9047" satisfied condition "success or failure"
Jun 11 08:34:22.019: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod downwardapi-volume-885d241d-55a3-4ee7-bc60-01ce6fac9047 container client-container: <nil>
STEP: delete the pod
Jun 11 08:34:22.042: INFO: Waiting for pod downwardapi-volume-885d241d-55a3-4ee7-bc60-01ce6fac9047 to disappear
Jun 11 08:34:22.048: INFO: Pod downwardapi-volume-885d241d-55a3-4ee7-bc60-01ce6fac9047 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:34:22.048: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "projected-6508" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":226,"skipped":3683,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:34:22.065: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8405
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-8405/configmap-test-977834eb-eb03-459a-9204-8eeeace6ce48
STEP: Creating a pod to test consume configMaps
Jun 11 08:34:22.237: INFO: Waiting up to 5m0s for pod "pod-configmaps-390e8542-f5e6-4cd7-b53e-413ab0a1944c" in namespace "configmap-8405" to be "success or failure"
Jun 11 08:34:22.242: INFO: Pod "pod-configmaps-390e8542-f5e6-4cd7-b53e-413ab0a1944c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.2727ms
Jun 11 08:34:24.247: INFO: Pod "pod-configmaps-390e8542-f5e6-4cd7-b53e-413ab0a1944c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009218095s
Jun 11 08:34:26.251: INFO: Pod "pod-configmaps-390e8542-f5e6-4cd7-b53e-413ab0a1944c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013998806s
STEP: Saw pod success
Jun 11 08:34:26.251: INFO: Pod "pod-configmaps-390e8542-f5e6-4cd7-b53e-413ab0a1944c" satisfied condition "success or failure"
Jun 11 08:34:26.255: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-configmaps-390e8542-f5e6-4cd7-b53e-413ab0a1944c container env-test: <nil>
STEP: delete the pod
Jun 11 08:34:26.279: INFO: Waiting for pod pod-configmaps-390e8542-f5e6-4cd7-b53e-413ab0a1944c to disappear
Jun 11 08:34:26.285: INFO: Pod pod-configmaps-390e8542-f5e6-4cd7-b53e-413ab0a1944c no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:34:26.285: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "configmap-8405" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":227,"skipped":3766,"failed":0}
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:34:26.301: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1450
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:34:30.474: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "kubelet-test-1450" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":280,"completed":228,"skipped":3770,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:34:30.491: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3506
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-3506
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating statefulset ss in namespace statefulset-3506
Jun 11 08:34:30.659: INFO: Found 0 stateful pods, waiting for 1
Jun 11 08:34:40.666: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jun 11 08:34:40.691: INFO: Deleting all statefulset in ns statefulset-3506
Jun 11 08:34:40.696: INFO: Scaling statefulset ss to 0
Jun 11 08:35:00.725: INFO: Waiting for statefulset status.replicas updated to 0
Jun 11 08:35:00.729: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:35:00.749: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "statefulset-3506" for this suite.

• [SLOW TEST:30.274 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":280,"completed":229,"skipped":3813,"failed":0}
S
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:35:00.765: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5076
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 11 08:35:00.928: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5dd8af2d-2c4b-403f-a3c3-1f6e5b7a74de" in namespace "downward-api-5076" to be "success or failure"
Jun 11 08:35:00.932: INFO: Pod "downwardapi-volume-5dd8af2d-2c4b-403f-a3c3-1f6e5b7a74de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.185257ms
Jun 11 08:35:02.937: INFO: Pod "downwardapi-volume-5dd8af2d-2c4b-403f-a3c3-1f6e5b7a74de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009088965s
STEP: Saw pod success
Jun 11 08:35:02.937: INFO: Pod "downwardapi-volume-5dd8af2d-2c4b-403f-a3c3-1f6e5b7a74de" satisfied condition "success or failure"
Jun 11 08:35:02.941: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod downwardapi-volume-5dd8af2d-2c4b-403f-a3c3-1f6e5b7a74de container client-container: <nil>
STEP: delete the pod
Jun 11 08:35:02.969: INFO: Waiting for pod downwardapi-volume-5dd8af2d-2c4b-403f-a3c3-1f6e5b7a74de to disappear
Jun 11 08:35:02.974: INFO: Pod downwardapi-volume-5dd8af2d-2c4b-403f-a3c3-1f6e5b7a74de no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:35:02.974: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "downward-api-5076" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":230,"skipped":3814,"failed":0}
SS
------------------------------
[k8s.io] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:35:02.990: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6258
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-5989fb02-0381-4e03-abe1-9b30970858c2 in namespace container-probe-6258
Jun 11 08:35:05.165: INFO: Started pod busybox-5989fb02-0381-4e03-abe1-9b30970858c2 in namespace container-probe-6258
STEP: checking the pod's current state and verifying that restartCount is present
Jun 11 08:35:05.169: INFO: Initial restart count of pod busybox-5989fb02-0381-4e03-abe1-9b30970858c2 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:39:05.787: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "container-probe-6258" for this suite.

• [SLOW TEST:242.814 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":231,"skipped":3816,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:39:05.805: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8483
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 11 08:39:10.031: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 11 08:39:10.035: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 11 08:39:12.035: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 11 08:39:12.040: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 11 08:39:14.035: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 11 08:39:14.040: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:39:14.040: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8483" for this suite.

• [SLOW TEST:8.252 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":280,"completed":232,"skipped":3832,"failed":0}
SSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:39:14.057: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4885
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4885
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-4885
I0611 08:39:14.244536      23 runners.go:189] Created replication controller with name: externalname-service, namespace: services-4885, replica count: 2
Jun 11 08:39:17.295: INFO: Creating new exec pod
I0611 08:39:17.295034      23 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady
Jun 11 08:39:20.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=services-4885 execpodj8x2s -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jun 11 08:39:20.686: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 11 08:39:20.686: INFO: stdout: ""
Jun 11 08:39:20.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=services-4885 execpodj8x2s -- /bin/sh -x -c nc -zv -t -w 2 10.0.22.245 80'
Jun 11 08:39:20.868: INFO: stderr: "+ nc -zv -t -w 2 10.0.22.245 80\nConnection to 10.0.22.245 80 port [tcp/http] succeeded!\n"
Jun 11 08:39:20.868: INFO: stdout: ""
Jun 11 08:39:20.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=services-4885 execpodj8x2s -- /bin/sh -x -c nc -zv -t -w 2 10.0.129.30 31115'
Jun 11 08:39:21.041: INFO: stderr: "+ nc -zv -t -w 2 10.0.129.30 31115\nConnection to 10.0.129.30 31115 port [tcp/31115] succeeded!\n"
Jun 11 08:39:21.041: INFO: stdout: ""
Jun 11 08:39:21.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=services-4885 execpodj8x2s -- /bin/sh -x -c nc -zv -t -w 2 10.0.138.28 31115'
Jun 11 08:39:21.234: INFO: stderr: "+ nc -zv -t -w 2 10.0.138.28 31115\nConnection to 10.0.138.28 31115 port [tcp/31115] succeeded!\n"
Jun 11 08:39:21.234: INFO: stdout: ""
Jun 11 08:39:21.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=services-4885 execpodj8x2s -- /bin/sh -x -c nc -zv -t -w 2 35.166.241.112 31115'
Jun 11 08:39:21.416: INFO: stderr: "+ nc -zv -t -w 2 35.166.241.112 31115\nConnection to 35.166.241.112 31115 port [tcp/31115] succeeded!\n"
Jun 11 08:39:21.416: INFO: stdout: ""
Jun 11 08:39:21.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=services-4885 execpodj8x2s -- /bin/sh -x -c nc -zv -t -w 2 34.220.194.18 31115'
Jun 11 08:39:21.591: INFO: stderr: "+ nc -zv -t -w 2 34.220.194.18 31115\nConnection to 34.220.194.18 31115 port [tcp/31115] succeeded!\n"
Jun 11 08:39:21.591: INFO: stdout: ""
Jun 11 08:39:21.591: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:39:21.626: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "services-4885" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:7.588 seconds]
[sig-network] Services
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":280,"completed":233,"skipped":3836,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:39:21.645: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8463
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Jun 11 08:39:21.798: INFO: namespace kubectl-8463
Jun 11 08:39:21.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 create -f - --namespace=kubectl-8463'
Jun 11 08:39:22.124: INFO: stderr: ""
Jun 11 08:39:22.124: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Jun 11 08:39:23.129: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 11 08:39:23.129: INFO: Found 0 / 1
Jun 11 08:39:24.128: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 11 08:39:24.128: INFO: Found 1 / 1
Jun 11 08:39:24.128: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 11 08:39:24.133: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 11 08:39:24.133: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 11 08:39:24.133: INFO: wait on agnhost-master startup in kubectl-8463
Jun 11 08:39:24.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 logs agnhost-master-qtp74 agnhost-master --namespace=kubectl-8463'
Jun 11 08:39:24.260: INFO: stderr: ""
Jun 11 08:39:24.260: INFO: stdout: "Paused\n"
STEP: exposing RC
Jun 11 08:39:24.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-8463'
Jun 11 08:39:24.435: INFO: stderr: ""
Jun 11 08:39:24.435: INFO: stdout: "service/rm2 exposed\n"
Jun 11 08:39:24.439: INFO: Service rm2 in namespace kubectl-8463 found.
STEP: exposing service
Jun 11 08:39:26.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-8463'
Jun 11 08:39:26.616: INFO: stderr: ""
Jun 11 08:39:26.616: INFO: stdout: "service/rm3 exposed\n"
Jun 11 08:39:26.621: INFO: Service rm3 in namespace kubectl-8463 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:39:28.629: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "kubectl-8463" for this suite.

• [SLOW TEST:7.000 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1188
    should create services for rc  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":280,"completed":234,"skipped":3864,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:39:28.645: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9391
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-9391
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating stateful set ss in namespace statefulset-9391
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9391
Jun 11 08:39:28.816: INFO: Found 0 stateful pods, waiting for 1
Jun 11 08:39:38.821: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jun 11 08:39:38.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=statefulset-9391 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 11 08:39:39.004: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 11 08:39:39.004: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 11 08:39:39.004: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 11 08:39:39.008: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 11 08:39:49.014: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 11 08:39:49.014: INFO: Waiting for statefulset status.replicas updated to 0
Jun 11 08:39:49.031: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Jun 11 08:39:49.031: INFO: ss-0  ip-10-0-136-38.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:28 +0000 UTC  }]
Jun 11 08:39:49.031: INFO:
Jun 11 08:39:49.031: INFO: StatefulSet ss has not reached scale 3, at 1
Jun 11 08:39:50.036: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995710479s
Jun 11 08:39:51.041: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990556536s
Jun 11 08:39:52.047: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985592254s
Jun 11 08:39:53.052: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.980497386s
Jun 11 08:39:54.057: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.975440286s
Jun 11 08:39:55.063: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.970042357s
Jun 11 08:39:56.068: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.96451637s
Jun 11 08:39:57.073: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.959276453s
Jun 11 08:39:58.078: INFO: Verifying statefulset ss doesn't scale past 3 for another 954.277752ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9391
Jun 11 08:39:59.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=statefulset-9391 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 11 08:39:59.266: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 11 08:39:59.266: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 11 08:39:59.266: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 11 08:39:59.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=statefulset-9391 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 11 08:39:59.445: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 11 08:39:59.445: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 11 08:39:59.445: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 11 08:39:59.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=statefulset-9391 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 11 08:39:59.629: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 11 08:39:59.629: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 11 08:39:59.629: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 11 08:39:59.634: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jun 11 08:40:09.639: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 11 08:40:09.639: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 11 08:40:09.639: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jun 11 08:40:09.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=statefulset-9391 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 11 08:40:09.821: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 11 08:40:09.821: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 11 08:40:09.821: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 11 08:40:09.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=statefulset-9391 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 11 08:40:10.005: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 11 08:40:10.005: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 11 08:40:10.005: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 11 08:40:10.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 exec --namespace=statefulset-9391 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 11 08:40:10.190: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 11 08:40:10.190: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 11 08:40:10.190: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 11 08:40:10.190: INFO: Waiting for statefulset status.replicas updated to 0
Jun 11 08:40:10.195: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jun 11 08:40:20.204: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 11 08:40:20.204: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 11 08:40:20.204: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 11 08:40:20.218: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jun 11 08:40:20.218: INFO: ss-0  ip-10-0-136-38.us-west-2.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:28 +0000 UTC  }]
Jun 11 08:40:20.218: INFO: ss-1  ip-10-0-132-48.us-west-2.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:49 +0000 UTC  }]
Jun 11 08:40:20.218: INFO: ss-2  ip-10-0-128-119.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:49 +0000 UTC  }]
Jun 11 08:40:20.218: INFO:
Jun 11 08:40:20.218: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 11 08:40:21.223: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jun 11 08:40:21.223: INFO: ss-0  ip-10-0-136-38.us-west-2.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:28 +0000 UTC  }]
Jun 11 08:40:21.223: INFO: ss-1  ip-10-0-132-48.us-west-2.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:49 +0000 UTC  }]
Jun 11 08:40:21.223: INFO: ss-2  ip-10-0-128-119.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:49 +0000 UTC  }]
Jun 11 08:40:21.223: INFO:
Jun 11 08:40:21.223: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 11 08:40:22.229: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jun 11 08:40:22.229: INFO: ss-1  ip-10-0-132-48.us-west-2.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:49 +0000 UTC  }]
Jun 11 08:40:22.229: INFO: ss-2  ip-10-0-128-119.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:49 +0000 UTC  }]
Jun 11 08:40:22.229: INFO:
Jun 11 08:40:22.229: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 11 08:40:23.234: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jun 11 08:40:23.234: INFO: ss-1  ip-10-0-132-48.us-west-2.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:49 +0000 UTC  }]
Jun 11 08:40:23.234: INFO: ss-2  ip-10-0-128-119.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:49 +0000 UTC  }]
Jun 11 08:40:23.234: INFO:
Jun 11 08:40:23.234: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 11 08:40:24.239: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jun 11 08:40:24.239: INFO: ss-1  ip-10-0-132-48.us-west-2.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:49 +0000 UTC  }]
Jun 11 08:40:24.239: INFO: ss-2  ip-10-0-128-119.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:49 +0000 UTC  }]
Jun 11 08:40:24.239: INFO:
Jun 11 08:40:24.239: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 11 08:40:25.244: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jun 11 08:40:25.244: INFO: ss-1  ip-10-0-132-48.us-west-2.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:49 +0000 UTC  }]
Jun 11 08:40:25.244: INFO: ss-2  ip-10-0-128-119.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:49 +0000 UTC  }]
Jun 11 08:40:25.244: INFO:
Jun 11 08:40:25.244: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 11 08:40:26.250: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jun 11 08:40:26.250: INFO: ss-1  ip-10-0-132-48.us-west-2.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:49 +0000 UTC  }]
Jun 11 08:40:26.250: INFO: ss-2  ip-10-0-128-119.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:40:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-11 08:39:49 +0000 UTC  }]
Jun 11 08:40:26.250: INFO:
Jun 11 08:40:26.250: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 11 08:40:27.254: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.963744535s
Jun 11 08:40:28.259: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.959222657s
Jun 11 08:40:29.264: INFO: Verifying statefulset ss doesn't scale past 0 for another 954.362405ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9391
Jun 11 08:40:30.270: INFO: Scaling statefulset ss to 0
Jun 11 08:40:30.284: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jun 11 08:40:30.288: INFO: Deleting all statefulset in ns statefulset-9391
Jun 11 08:40:30.292: INFO: Scaling statefulset ss to 0
Jun 11 08:40:30.303: INFO: Waiting for statefulset status.replicas updated to 0
Jun 11 08:40:30.307: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:40:30.327: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "statefulset-9391" for this suite.

• [SLOW TEST:61.697 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":280,"completed":235,"skipped":3867,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:40:30.343: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6923
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-f19d9114-17e8-4d37-a07c-4b30b94856bd
STEP: Creating a pod to test consume secrets
Jun 11 08:40:30.512: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e74e8355-bbdf-4f4e-8eae-97b9ad611dd8" in namespace "projected-6923" to be "success or failure"
Jun 11 08:40:30.520: INFO: Pod "pod-projected-secrets-e74e8355-bbdf-4f4e-8eae-97b9ad611dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.528471ms
Jun 11 08:40:32.525: INFO: Pod "pod-projected-secrets-e74e8355-bbdf-4f4e-8eae-97b9ad611dd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013355173s
STEP: Saw pod success
Jun 11 08:40:32.525: INFO: Pod "pod-projected-secrets-e74e8355-bbdf-4f4e-8eae-97b9ad611dd8" satisfied condition "success or failure"
Jun 11 08:40:32.529: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-projected-secrets-e74e8355-bbdf-4f4e-8eae-97b9ad611dd8 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 11 08:40:32.553: INFO: Waiting for pod pod-projected-secrets-e74e8355-bbdf-4f4e-8eae-97b9ad611dd8 to disappear
Jun 11 08:40:32.558: INFO: Pod pod-projected-secrets-e74e8355-bbdf-4f4e-8eae-97b9ad611dd8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:40:32.558: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "projected-6923" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":236,"skipped":3878,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:40:32.575: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2497
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jun 11 08:40:32.726: INFO: PodSpec: initContainers in spec.initContainers
Jun 11 08:41:21.911: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-08619c9c-faa0-4a5d-a17e-a746f334ef11", GenerateName:"", Namespace:"init-container-2497", SelfLink:"/api/v1/namespaces/init-container-2497/pods/pod-init-08619c9c-faa0-4a5d-a17e-a746f334ef11", UID:"8a2cd6e0-cfdf-464c-88a5-89ba234493c3", ResourceVersion:"140616", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63727461632, loc:(*time.Location)(0x7925200)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"726625123"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"192.168.42.58/32", "cni.projectcalico.org/podIPs":"192.168.42.58/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-5fnnq", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc009860100), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-5fnnq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-5fnnq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-5fnnq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0050264e8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-136-38.us-west-2.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00a0b40c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0050265d0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005026630)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc005026638), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00502663c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727461632, loc:(*time.Location)(0x7925200)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727461632, loc:(*time.Location)(0x7925200)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727461632, loc:(*time.Location)(0x7925200)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727461632, loc:(*time.Location)(0x7925200)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.136.38", PodIP:"192.168.42.58", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.42.58"}}, StartTime:(*v1.Time)(0xc00403c140), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0023761c0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002376230)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://8e0b6cdb42d4d27a3e74a05dd7c3de1cc4816a2d5181ee0d86f6e73e9ff2077f", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00403c180), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00403c160), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc00502672f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:41:21.912: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "init-container-2497" for this suite.

• [SLOW TEST:49.353 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":280,"completed":237,"skipped":3917,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:41:21.928: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1734
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1626
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 11 08:41:22.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-1734'
Jun 11 08:41:22.191: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 11 08:41:22.191: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1631
Jun 11 08:41:24.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 delete deployment e2e-test-httpd-deployment --namespace=kubectl-1734'
Jun 11 08:41:24.352: INFO: stderr: ""
Jun 11 08:41:24.352: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:41:24.352: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "kubectl-1734" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run deployment should create a deployment from an image  [Conformance]","total":280,"completed":238,"skipped":3935,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:41:24.370: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6572
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-eef88866-db6f-4bd4-a237-45c88a354f6c
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:41:28.579: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "configmap-6572" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":239,"skipped":3956,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:41:28.595: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6647
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-8b3b34fb-a7bf-4000-8a0a-93dfc242841c
STEP: Creating configMap with name cm-test-opt-upd-eccf2196-e794-4358-a360-c7f73aeb0be0
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-8b3b34fb-a7bf-4000-8a0a-93dfc242841c
STEP: Updating configmap cm-test-opt-upd-eccf2196-e794-4358-a360-c7f73aeb0be0
STEP: Creating configMap with name cm-test-opt-create-418ec911-3a22-401e-9d71-f0c98e56cc67
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:42:47.210: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "projected-6647" for this suite.

• [SLOW TEST:78.631 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":240,"skipped":3968,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:42:47.226: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8837
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Jun 11 08:42:47.377: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:43:19.188: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8837" for this suite.

• [SLOW TEST:31.978 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":280,"completed":241,"skipped":3984,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:43:19.204: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4586
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-downwardapi-l7mw
STEP: Creating a pod to test atomic-volume-subpath
Jun 11 08:43:19.384: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-l7mw" in namespace "subpath-4586" to be "success or failure"
Jun 11 08:43:19.389: INFO: Pod "pod-subpath-test-downwardapi-l7mw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.465601ms
Jun 11 08:43:21.394: INFO: Pod "pod-subpath-test-downwardapi-l7mw": Phase="Running", Reason="", readiness=true. Elapsed: 2.00937671s
Jun 11 08:43:23.398: INFO: Pod "pod-subpath-test-downwardapi-l7mw": Phase="Running", Reason="", readiness=true. Elapsed: 4.014130623s
Jun 11 08:43:25.404: INFO: Pod "pod-subpath-test-downwardapi-l7mw": Phase="Running", Reason="", readiness=true. Elapsed: 6.019283876s
Jun 11 08:43:27.408: INFO: Pod "pod-subpath-test-downwardapi-l7mw": Phase="Running", Reason="", readiness=true. Elapsed: 8.02414093s
Jun 11 08:43:29.413: INFO: Pod "pod-subpath-test-downwardapi-l7mw": Phase="Running", Reason="", readiness=true. Elapsed: 10.029022547s
Jun 11 08:43:31.418: INFO: Pod "pod-subpath-test-downwardapi-l7mw": Phase="Running", Reason="", readiness=true. Elapsed: 12.033806374s
Jun 11 08:43:33.423: INFO: Pod "pod-subpath-test-downwardapi-l7mw": Phase="Running", Reason="", readiness=true. Elapsed: 14.038779543s
Jun 11 08:43:35.428: INFO: Pod "pod-subpath-test-downwardapi-l7mw": Phase="Running", Reason="", readiness=true. Elapsed: 16.043788308s
Jun 11 08:43:37.433: INFO: Pod "pod-subpath-test-downwardapi-l7mw": Phase="Running", Reason="", readiness=true. Elapsed: 18.048729265s
Jun 11 08:43:39.438: INFO: Pod "pod-subpath-test-downwardapi-l7mw": Phase="Running", Reason="", readiness=true. Elapsed: 20.053512079s
Jun 11 08:43:41.442: INFO: Pod "pod-subpath-test-downwardapi-l7mw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.058111377s
STEP: Saw pod success
Jun 11 08:43:41.442: INFO: Pod "pod-subpath-test-downwardapi-l7mw" satisfied condition "success or failure"
Jun 11 08:43:41.447: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-subpath-test-downwardapi-l7mw container test-container-subpath-downwardapi-l7mw: <nil>
STEP: delete the pod
Jun 11 08:43:41.471: INFO: Waiting for pod pod-subpath-test-downwardapi-l7mw to disappear
Jun 11 08:43:41.478: INFO: Pod pod-subpath-test-downwardapi-l7mw no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-l7mw
Jun 11 08:43:41.478: INFO: Deleting pod "pod-subpath-test-downwardapi-l7mw" in namespace "subpath-4586"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:43:41.482: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "subpath-4586" for this suite.

• [SLOW TEST:22.293 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":280,"completed":242,"skipped":3985,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:43:41.498: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4464
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:43:41.682: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "kubelet-test-4464" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":280,"completed":243,"skipped":4033,"failed":0}

------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:43:41.698: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6886
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jun 11 08:44:21.894: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0611 08:44:21.894020      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:44:21.894: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "gc-6886" for this suite.

• [SLOW TEST:40.211 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":280,"completed":244,"skipped":4033,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:44:21.910: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-4640
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jun 11 08:44:22.339: INFO: Pod name wrapped-volume-race-a8a35bd4-c0dc-4482-8d80-6962a00e500f: Found 0 pods out of 5
Jun 11 08:44:27.346: INFO: Pod name wrapped-volume-race-a8a35bd4-c0dc-4482-8d80-6962a00e500f: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-a8a35bd4-c0dc-4482-8d80-6962a00e500f in namespace emptydir-wrapper-4640, will wait for the garbage collector to delete the pods
Jun 11 08:44:27.441: INFO: Deleting ReplicationController wrapped-volume-race-a8a35bd4-c0dc-4482-8d80-6962a00e500f took: 16.843475ms
Jun 11 08:44:29.041: INFO: Terminating ReplicationController wrapped-volume-race-a8a35bd4-c0dc-4482-8d80-6962a00e500f pods took: 1.600284311s
STEP: Creating RC which spawns configmap-volume pods
Jun 11 08:44:37.064: INFO: Pod name wrapped-volume-race-d8509da3-24cd-4f1a-b603-4356da57b40e: Found 0 pods out of 5
Jun 11 08:44:42.071: INFO: Pod name wrapped-volume-race-d8509da3-24cd-4f1a-b603-4356da57b40e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d8509da3-24cd-4f1a-b603-4356da57b40e in namespace emptydir-wrapper-4640, will wait for the garbage collector to delete the pods
Jun 11 08:44:42.159: INFO: Deleting ReplicationController wrapped-volume-race-d8509da3-24cd-4f1a-b603-4356da57b40e took: 11.994448ms
Jun 11 08:44:42.259: INFO: Terminating ReplicationController wrapped-volume-race-d8509da3-24cd-4f1a-b603-4356da57b40e pods took: 100.254055ms
STEP: Creating RC which spawns configmap-volume pods
Jun 11 08:44:57.583: INFO: Pod name wrapped-volume-race-0822aae5-439e-48be-92cc-91f562a566c4: Found 0 pods out of 5
Jun 11 08:45:02.590: INFO: Pod name wrapped-volume-race-0822aae5-439e-48be-92cc-91f562a566c4: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-0822aae5-439e-48be-92cc-91f562a566c4 in namespace emptydir-wrapper-4640, will wait for the garbage collector to delete the pods
Jun 11 08:45:02.680: INFO: Deleting ReplicationController wrapped-volume-race-0822aae5-439e-48be-92cc-91f562a566c4 took: 12.302738ms
Jun 11 08:45:04.280: INFO: Terminating ReplicationController wrapped-volume-race-0822aae5-439e-48be-92cc-91f562a566c4 pods took: 1.600278876s
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:45:17.428: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4640" for this suite.

• [SLOW TEST:55.534 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":280,"completed":245,"skipped":4040,"failed":0}
SSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:45:17.444: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2577
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2577.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2577.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2577.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2577.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2577.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2577.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2577.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2577.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2577.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2577.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2577.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2577.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2577.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 165.5.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.5.165_udp@PTR;check="$$(dig +tcp +noall +answer +search 165.5.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.5.165_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2577.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2577.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2577.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2577.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2577.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2577.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2577.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2577.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2577.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2577.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2577.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2577.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2577.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 165.5.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.5.165_udp@PTR;check="$$(dig +tcp +noall +answer +search 165.5.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.5.165_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 11 08:45:19.653: INFO: Unable to read wheezy_udp@dns-test-service.dns-2577.svc.cluster.local from pod dns-2577/dns-test-ff04dd3f-0185-42c8-aa7a-053d78230794: the server could not find the requested resource (get pods dns-test-ff04dd3f-0185-42c8-aa7a-053d78230794)
Jun 11 08:45:19.657: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2577.svc.cluster.local from pod dns-2577/dns-test-ff04dd3f-0185-42c8-aa7a-053d78230794: the server could not find the requested resource (get pods dns-test-ff04dd3f-0185-42c8-aa7a-053d78230794)
Jun 11 08:45:19.662: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2577.svc.cluster.local from pod dns-2577/dns-test-ff04dd3f-0185-42c8-aa7a-053d78230794: the server could not find the requested resource (get pods dns-test-ff04dd3f-0185-42c8-aa7a-053d78230794)
Jun 11 08:45:19.666: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2577.svc.cluster.local from pod dns-2577/dns-test-ff04dd3f-0185-42c8-aa7a-053d78230794: the server could not find the requested resource (get pods dns-test-ff04dd3f-0185-42c8-aa7a-053d78230794)
Jun 11 08:45:19.697: INFO: Unable to read jessie_udp@dns-test-service.dns-2577.svc.cluster.local from pod dns-2577/dns-test-ff04dd3f-0185-42c8-aa7a-053d78230794: the server could not find the requested resource (get pods dns-test-ff04dd3f-0185-42c8-aa7a-053d78230794)
Jun 11 08:45:19.702: INFO: Unable to read jessie_tcp@dns-test-service.dns-2577.svc.cluster.local from pod dns-2577/dns-test-ff04dd3f-0185-42c8-aa7a-053d78230794: the server could not find the requested resource (get pods dns-test-ff04dd3f-0185-42c8-aa7a-053d78230794)
Jun 11 08:45:19.706: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2577.svc.cluster.local from pod dns-2577/dns-test-ff04dd3f-0185-42c8-aa7a-053d78230794: the server could not find the requested resource (get pods dns-test-ff04dd3f-0185-42c8-aa7a-053d78230794)
Jun 11 08:45:19.711: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2577.svc.cluster.local from pod dns-2577/dns-test-ff04dd3f-0185-42c8-aa7a-053d78230794: the server could not find the requested resource (get pods dns-test-ff04dd3f-0185-42c8-aa7a-053d78230794)
Jun 11 08:45:19.742: INFO: Lookups using dns-2577/dns-test-ff04dd3f-0185-42c8-aa7a-053d78230794 failed for: [wheezy_udp@dns-test-service.dns-2577.svc.cluster.local wheezy_tcp@dns-test-service.dns-2577.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2577.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2577.svc.cluster.local jessie_udp@dns-test-service.dns-2577.svc.cluster.local jessie_tcp@dns-test-service.dns-2577.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2577.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2577.svc.cluster.local]

Jun 11 08:45:24.831: INFO: DNS probes using dns-2577/dns-test-ff04dd3f-0185-42c8-aa7a-053d78230794 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:45:24.904: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "dns-2577" for this suite.

• [SLOW TEST:7.478 seconds]
[sig-network] DNS
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":280,"completed":246,"skipped":4045,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:45:24.922: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-3970
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun 11 08:45:25.680: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jun 11 08:45:27.693: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727461925, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727461925, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727461925, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727461925, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 08:45:30.711: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 08:45:30.715: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:45:31.882: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "crd-webhook-3970" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

• [SLOW TEST:7.114 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":280,"completed":247,"skipped":4061,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:45:32.037: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2823
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-bd722081-e1fc-4260-9695-3696e66ac052
STEP: Creating a pod to test consume secrets
Jun 11 08:45:32.240: INFO: Waiting up to 5m0s for pod "pod-secrets-17c9f991-7436-47ae-aff5-04f1d3c15040" in namespace "secrets-2823" to be "success or failure"
Jun 11 08:45:32.250: INFO: Pod "pod-secrets-17c9f991-7436-47ae-aff5-04f1d3c15040": Phase="Pending", Reason="", readiness=false. Elapsed: 10.511156ms
Jun 11 08:45:34.255: INFO: Pod "pod-secrets-17c9f991-7436-47ae-aff5-04f1d3c15040": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015003312s
STEP: Saw pod success
Jun 11 08:45:34.255: INFO: Pod "pod-secrets-17c9f991-7436-47ae-aff5-04f1d3c15040" satisfied condition "success or failure"
Jun 11 08:45:34.259: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-secrets-17c9f991-7436-47ae-aff5-04f1d3c15040 container secret-volume-test: <nil>
STEP: delete the pod
Jun 11 08:45:34.291: INFO: Waiting for pod pod-secrets-17c9f991-7436-47ae-aff5-04f1d3c15040 to disappear
Jun 11 08:45:34.295: INFO: Pod pod-secrets-17c9f991-7436-47ae-aff5-04f1d3c15040 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:45:34.295: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "secrets-2823" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":248,"skipped":4078,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:45:34.312: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5569
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 11 08:45:34.476: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ffafe24b-c37d-4182-b677-05a3e911bee5" in namespace "downward-api-5569" to be "success or failure"
Jun 11 08:45:34.480: INFO: Pod "downwardapi-volume-ffafe24b-c37d-4182-b677-05a3e911bee5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.359628ms
Jun 11 08:45:36.485: INFO: Pod "downwardapi-volume-ffafe24b-c37d-4182-b677-05a3e911bee5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009166184s
STEP: Saw pod success
Jun 11 08:45:36.485: INFO: Pod "downwardapi-volume-ffafe24b-c37d-4182-b677-05a3e911bee5" satisfied condition "success or failure"
Jun 11 08:45:36.489: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod downwardapi-volume-ffafe24b-c37d-4182-b677-05a3e911bee5 container client-container: <nil>
STEP: delete the pod
Jun 11 08:45:36.513: INFO: Waiting for pod downwardapi-volume-ffafe24b-c37d-4182-b677-05a3e911bee5 to disappear
Jun 11 08:45:36.518: INFO: Pod downwardapi-volume-ffafe24b-c37d-4182-b677-05a3e911bee5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:45:36.518: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "downward-api-5569" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":249,"skipped":4083,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:45:36.534: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-4958
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 08:45:36.706: INFO: (0) /api/v1/nodes/ip-10-0-135-119.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 15.423778ms)
Jun 11 08:45:36.711: INFO: (1) /api/v1/nodes/ip-10-0-135-119.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 5.21656ms)
Jun 11 08:45:36.716: INFO: (2) /api/v1/nodes/ip-10-0-135-119.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 5.039792ms)
Jun 11 08:45:36.721: INFO: (3) /api/v1/nodes/ip-10-0-135-119.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 4.985551ms)
Jun 11 08:45:36.726: INFO: (4) /api/v1/nodes/ip-10-0-135-119.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 4.731802ms)
Jun 11 08:45:36.730: INFO: (5) /api/v1/nodes/ip-10-0-135-119.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 4.767154ms)
Jun 11 08:45:36.735: INFO: (6) /api/v1/nodes/ip-10-0-135-119.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 4.826931ms)
Jun 11 08:45:36.740: INFO: (7) /api/v1/nodes/ip-10-0-135-119.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 4.942294ms)
Jun 11 08:45:36.745: INFO: (8) /api/v1/nodes/ip-10-0-135-119.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 4.718243ms)
Jun 11 08:45:36.750: INFO: (9) /api/v1/nodes/ip-10-0-135-119.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 4.815812ms)
Jun 11 08:45:36.755: INFO: (10) /api/v1/nodes/ip-10-0-135-119.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 4.753125ms)
Jun 11 08:45:36.759: INFO: (11) /api/v1/nodes/ip-10-0-135-119.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 4.715286ms)
Jun 11 08:45:36.764: INFO: (12) /api/v1/nodes/ip-10-0-135-119.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 4.920227ms)
Jun 11 08:45:36.769: INFO: (13) /api/v1/nodes/ip-10-0-135-119.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 4.915427ms)
Jun 11 08:45:36.774: INFO: (14) /api/v1/nodes/ip-10-0-135-119.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 4.723698ms)
Jun 11 08:45:36.779: INFO: (15) /api/v1/nodes/ip-10-0-135-119.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 5.319772ms)
Jun 11 08:45:36.784: INFO: (16) /api/v1/nodes/ip-10-0-135-119.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 4.877773ms)
Jun 11 08:45:36.789: INFO: (17) /api/v1/nodes/ip-10-0-135-119.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 5.005449ms)
Jun 11 08:45:36.794: INFO: (18) /api/v1/nodes/ip-10-0-135-119.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 4.830397ms)
Jun 11 08:45:36.799: INFO: (19) /api/v1/nodes/ip-10-0-135-119.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="audit.db">audit.db</... (200; 4.868584ms)
[AfterEach] version v1
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:45:36.799: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "proxy-4958" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":280,"completed":250,"skipped":4101,"failed":0}
SSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:45:36.815: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-7037
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 08:45:36.977: INFO: Waiting up to 5m0s for pod "busybox-user-65534-86eba046-ab1b-41f9-9468-913549d500eb" in namespace "security-context-test-7037" to be "success or failure"
Jun 11 08:45:36.982: INFO: Pod "busybox-user-65534-86eba046-ab1b-41f9-9468-913549d500eb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.323602ms
Jun 11 08:45:38.987: INFO: Pod "busybox-user-65534-86eba046-ab1b-41f9-9468-913549d500eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009392722s
Jun 11 08:45:38.987: INFO: Pod "busybox-user-65534-86eba046-ab1b-41f9-9468-913549d500eb" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:45:38.987: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "security-context-test-7037" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":251,"skipped":4104,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:45:39.002: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1688
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:45:52.234: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "resourcequota-1688" for this suite.

• [SLOW TEST:13.247 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":280,"completed":252,"skipped":4117,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:45:52.250: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-1510
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 08:45:52.400: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Creating first CR
Jun 11 08:45:52.975: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-11T08:45:52Z generation:1 name:name1 resourceVersion:144339 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:a47e32ba-a8e7-4688-a98c-5f07a2a2d4b9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jun 11 08:46:02.982: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-11T08:46:02Z generation:1 name:name2 resourceVersion:144429 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:19819e60-f234-412b-a72c-d08362ccd75b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jun 11 08:46:12.989: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-11T08:45:52Z generation:2 name:name1 resourceVersion:144513 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:a47e32ba-a8e7-4688-a98c-5f07a2a2d4b9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jun 11 08:46:22.996: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-11T08:46:02Z generation:2 name:name2 resourceVersion:144600 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:19819e60-f234-412b-a72c-d08362ccd75b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jun 11 08:46:33.009: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-11T08:45:52Z generation:2 name:name1 resourceVersion:144684 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:a47e32ba-a8e7-4688-a98c-5f07a2a2d4b9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jun 11 08:46:43.024: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-11T08:46:02Z generation:2 name:name2 resourceVersion:144764 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:19819e60-f234-412b-a72c-d08362ccd75b] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:46:53.536: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "crd-watch-1510" for this suite.

• [SLOW TEST:61.304 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:41
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":280,"completed":253,"skipped":4147,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:46:53.554: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1702
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Jun 11 08:46:56.269: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0611 08:46:56.269783      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:46:56.269: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "gc-1702" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":280,"completed":254,"skipped":4149,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:46:56.285: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6350
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-d0ac1a2a-d0dd-4e85-8386-affcb989b987
STEP: Creating a pod to test consume configMaps
Jun 11 08:46:56.458: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-98fe363a-bb9d-4858-b5cb-84749f3d9fed" in namespace "projected-6350" to be "success or failure"
Jun 11 08:46:56.463: INFO: Pod "pod-projected-configmaps-98fe363a-bb9d-4858-b5cb-84749f3d9fed": Phase="Pending", Reason="", readiness=false. Elapsed: 5.485336ms
Jun 11 08:46:58.468: INFO: Pod "pod-projected-configmaps-98fe363a-bb9d-4858-b5cb-84749f3d9fed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010140253s
STEP: Saw pod success
Jun 11 08:46:58.468: INFO: Pod "pod-projected-configmaps-98fe363a-bb9d-4858-b5cb-84749f3d9fed" satisfied condition "success or failure"
Jun 11 08:46:58.472: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-projected-configmaps-98fe363a-bb9d-4858-b5cb-84749f3d9fed container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 11 08:46:58.496: INFO: Waiting for pod pod-projected-configmaps-98fe363a-bb9d-4858-b5cb-84749f3d9fed to disappear
Jun 11 08:46:58.502: INFO: Pod pod-projected-configmaps-98fe363a-bb9d-4858-b5cb-84749f3d9fed no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:46:58.502: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "projected-6350" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":255,"skipped":4165,"failed":0}
S
------------------------------
[k8s.io] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:46:58.522: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5872
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-3d588336-c059-4843-bbc6-4388a71504ad in namespace container-probe-5872
Jun 11 08:47:00.757: INFO: Started pod liveness-3d588336-c059-4843-bbc6-4388a71504ad in namespace container-probe-5872
STEP: checking the pod's current state and verifying that restartCount is present
Jun 11 08:47:00.761: INFO: Initial restart count of pod liveness-3d588336-c059-4843-bbc6-4388a71504ad is 0
Jun 11 08:47:18.808: INFO: Restart count of pod container-probe-5872/liveness-3d588336-c059-4843-bbc6-4388a71504ad is now 1 (18.047733885s elapsed)
Jun 11 08:47:38.856: INFO: Restart count of pod container-probe-5872/liveness-3d588336-c059-4843-bbc6-4388a71504ad is now 2 (38.095154658s elapsed)
Jun 11 08:47:58.903: INFO: Restart count of pod container-probe-5872/liveness-3d588336-c059-4843-bbc6-4388a71504ad is now 3 (58.141978929s elapsed)
Jun 11 08:48:18.951: INFO: Restart count of pod container-probe-5872/liveness-3d588336-c059-4843-bbc6-4388a71504ad is now 4 (1m18.19056764s elapsed)
Jun 11 08:49:23.110: INFO: Restart count of pod container-probe-5872/liveness-3d588336-c059-4843-bbc6-4388a71504ad is now 5 (2m22.348913106s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:49:23.126: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "container-probe-5872" for this suite.

• [SLOW TEST:144.620 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":280,"completed":256,"skipped":4166,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:49:23.143: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8156
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's args
Jun 11 08:49:23.309: INFO: Waiting up to 5m0s for pod "var-expansion-6da7f22d-f6c0-4612-87be-e20bc927b28c" in namespace "var-expansion-8156" to be "success or failure"
Jun 11 08:49:23.316: INFO: Pod "var-expansion-6da7f22d-f6c0-4612-87be-e20bc927b28c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.81585ms
Jun 11 08:49:25.321: INFO: Pod "var-expansion-6da7f22d-f6c0-4612-87be-e20bc927b28c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011696718s
STEP: Saw pod success
Jun 11 08:49:25.321: INFO: Pod "var-expansion-6da7f22d-f6c0-4612-87be-e20bc927b28c" satisfied condition "success or failure"
Jun 11 08:49:25.325: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod var-expansion-6da7f22d-f6c0-4612-87be-e20bc927b28c container dapi-container: <nil>
STEP: delete the pod
Jun 11 08:49:25.348: INFO: Waiting for pod var-expansion-6da7f22d-f6c0-4612-87be-e20bc927b28c to disappear
Jun 11 08:49:25.355: INFO: Pod var-expansion-6da7f22d-f6c0-4612-87be-e20bc927b28c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:49:25.355: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "var-expansion-8156" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":280,"completed":257,"skipped":4237,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:49:25.371: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8420
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 11 08:49:28.069: INFO: Successfully updated pod "pod-update-3ca8bbe2-41cb-440a-bb0d-089da905bd79"
STEP: verifying the updated pod is in kubernetes
Jun 11 08:49:28.077: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:49:28.077: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "pods-8420" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":280,"completed":258,"skipped":4243,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:49:28.093: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8374
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 08:49:28.544: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 11 08:49:30.557: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727462168, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727462168, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727462168, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727462168, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 08:49:33.575: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 08:49:33.579: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5968-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:49:34.670: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "webhook-8374" for this suite.
STEP: Destroying namespace "webhook-8374-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.683 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":280,"completed":259,"skipped":4273,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:49:34.777: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6352
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 08:49:34.942: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:49:41.036: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6352" for this suite.

• [SLOW TEST:6.277 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:47
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":280,"completed":260,"skipped":4311,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:49:41.054: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6041
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jun 11 08:49:41.213: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jun 11 08:49:50.259: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:49:50.264: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "pods-6041" for this suite.

• [SLOW TEST:9.225 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":280,"completed":261,"skipped":4320,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:49:50.279: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7623
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-3b1bafd9-6886-43d3-913a-bc7c88344f24
STEP: Creating a pod to test consume configMaps
Jun 11 08:49:50.456: INFO: Waiting up to 5m0s for pod "pod-configmaps-39ad8907-b621-4d44-af8a-3e12ee12f841" in namespace "configmap-7623" to be "success or failure"
Jun 11 08:49:50.461: INFO: Pod "pod-configmaps-39ad8907-b621-4d44-af8a-3e12ee12f841": Phase="Pending", Reason="", readiness=false. Elapsed: 4.213079ms
Jun 11 08:49:52.466: INFO: Pod "pod-configmaps-39ad8907-b621-4d44-af8a-3e12ee12f841": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009722289s
STEP: Saw pod success
Jun 11 08:49:52.466: INFO: Pod "pod-configmaps-39ad8907-b621-4d44-af8a-3e12ee12f841" satisfied condition "success or failure"
Jun 11 08:49:52.471: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-configmaps-39ad8907-b621-4d44-af8a-3e12ee12f841 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 11 08:49:52.497: INFO: Waiting for pod pod-configmaps-39ad8907-b621-4d44-af8a-3e12ee12f841 to disappear
Jun 11 08:49:52.503: INFO: Pod pod-configmaps-39ad8907-b621-4d44-af8a-3e12ee12f841 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:49:52.503: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "configmap-7623" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":262,"skipped":4322,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:49:52.521: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9544
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-9544
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-9544
STEP: Creating statefulset with conflicting port in namespace statefulset-9544
STEP: Waiting until pod test-pod will start running in namespace statefulset-9544
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9544
Jun 11 08:49:56.715: INFO: Observed stateful pod in namespace: statefulset-9544, name: ss-0, uid: a11b7884-76e2-4670-9548-81e647ee1157, status phase: Failed. Waiting for statefulset controller to delete.
Jun 11 08:49:56.717: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9544
STEP: Removing pod with conflicting port in namespace statefulset-9544
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9544 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jun 11 08:50:00.749: INFO: Deleting all statefulset in ns statefulset-9544
Jun 11 08:50:00.753: INFO: Scaling statefulset ss to 0
Jun 11 08:50:10.772: INFO: Waiting for statefulset status.replicas updated to 0
Jun 11 08:50:10.776: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:50:10.794: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "statefulset-9544" for this suite.

• [SLOW TEST:18.290 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":280,"completed":263,"skipped":4340,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:50:10.811: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-108
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:50:14.995: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "kubelet-test-108" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":264,"skipped":4360,"failed":0}
SS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:50:15.012: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-794
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:50:19.192: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "kubelet-test-794" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":265,"skipped":4362,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:50:19.208: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1278
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-9519d660-82cf-48c0-b7fc-9c4b7ffe5951
STEP: Creating a pod to test consume secrets
Jun 11 08:50:19.376: INFO: Waiting up to 5m0s for pod "pod-secrets-2549c642-1da2-48cf-817a-a23dc738f673" in namespace "secrets-1278" to be "success or failure"
Jun 11 08:50:19.382: INFO: Pod "pod-secrets-2549c642-1da2-48cf-817a-a23dc738f673": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070965ms
Jun 11 08:50:21.387: INFO: Pod "pod-secrets-2549c642-1da2-48cf-817a-a23dc738f673": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011031213s
STEP: Saw pod success
Jun 11 08:50:21.387: INFO: Pod "pod-secrets-2549c642-1da2-48cf-817a-a23dc738f673" satisfied condition "success or failure"
Jun 11 08:50:21.393: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-secrets-2549c642-1da2-48cf-817a-a23dc738f673 container secret-volume-test: <nil>
STEP: delete the pod
Jun 11 08:50:21.418: INFO: Waiting for pod pod-secrets-2549c642-1da2-48cf-817a-a23dc738f673 to disappear
Jun 11 08:50:21.424: INFO: Pod pod-secrets-2549c642-1da2-48cf-817a-a23dc738f673 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:50:21.424: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "secrets-1278" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":266,"skipped":4369,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:50:21.441: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6051
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 11 08:50:21.606: INFO: Waiting up to 5m0s for pod "pod-945efcf6-2d60-43ef-a0ed-702e57ea8629" in namespace "emptydir-6051" to be "success or failure"
Jun 11 08:50:21.611: INFO: Pod "pod-945efcf6-2d60-43ef-a0ed-702e57ea8629": Phase="Pending", Reason="", readiness=false. Elapsed: 4.539582ms
Jun 11 08:50:23.616: INFO: Pod "pod-945efcf6-2d60-43ef-a0ed-702e57ea8629": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00995138s
STEP: Saw pod success
Jun 11 08:50:23.616: INFO: Pod "pod-945efcf6-2d60-43ef-a0ed-702e57ea8629" satisfied condition "success or failure"
Jun 11 08:50:23.620: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-945efcf6-2d60-43ef-a0ed-702e57ea8629 container test-container: <nil>
STEP: delete the pod
Jun 11 08:50:23.645: INFO: Waiting for pod pod-945efcf6-2d60-43ef-a0ed-702e57ea8629 to disappear
Jun 11 08:50:23.650: INFO: Pod pod-945efcf6-2d60-43ef-a0ed-702e57ea8629 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:50:23.650: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "emptydir-6051" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":267,"skipped":4384,"failed":0}
SSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:50:23.665: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-69
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-69/configmap-test-d59ffb21-44a7-4591-a524-ae562c22a906
STEP: Creating a pod to test consume configMaps
Jun 11 08:50:23.834: INFO: Waiting up to 5m0s for pod "pod-configmaps-89595d75-5517-4215-a20f-33b1ce2fe493" in namespace "configmap-69" to be "success or failure"
Jun 11 08:50:23.839: INFO: Pod "pod-configmaps-89595d75-5517-4215-a20f-33b1ce2fe493": Phase="Pending", Reason="", readiness=false. Elapsed: 4.774168ms
Jun 11 08:50:25.844: INFO: Pod "pod-configmaps-89595d75-5517-4215-a20f-33b1ce2fe493": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009385119s
STEP: Saw pod success
Jun 11 08:50:25.844: INFO: Pod "pod-configmaps-89595d75-5517-4215-a20f-33b1ce2fe493" satisfied condition "success or failure"
Jun 11 08:50:25.848: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-configmaps-89595d75-5517-4215-a20f-33b1ce2fe493 container env-test: <nil>
STEP: delete the pod
Jun 11 08:50:25.872: INFO: Waiting for pod pod-configmaps-89595d75-5517-4215-a20f-33b1ce2fe493 to disappear
Jun 11 08:50:25.876: INFO: Pod pod-configmaps-89595d75-5517-4215-a20f-33b1ce2fe493 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:50:25.876: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "configmap-69" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":280,"completed":268,"skipped":4389,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:50:25.895: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7034
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jun 11 08:50:26.046: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jun 11 08:50:47.009: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
Jun 11 08:50:52.454: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:51:13.029: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7034" for this suite.

• [SLOW TEST:47.149 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":280,"completed":269,"skipped":4396,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:51:13.045: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-139
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1790
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 11 08:51:13.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-139'
Jun 11 08:51:13.479: INFO: stderr: ""
Jun 11 08:51:13.479: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jun 11 08:51:18.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 get pod e2e-test-httpd-pod --namespace=kubectl-139 -o json'
Jun 11 08:51:18.666: INFO: stderr: ""
Jun 11 08:51:18.666: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"192.168.42.34/32\",\n            \"cni.projectcalico.org/podIPs\": \"192.168.42.34/32\"\n        },\n        \"creationTimestamp\": \"2020-06-11T08:51:13Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-139\",\n        \"resourceVersion\": \"148005\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-139/pods/e2e-test-httpd-pod\",\n        \"uid\": \"8ff133f4-a582-4c00-b99f-378f12516580\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"Always\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-6dr4c\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-0-136-38.us-west-2.compute.internal\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-6dr4c\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-6dr4c\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-06-11T08:51:13Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-06-11T08:51:15Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-06-11T08:51:15Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-06-11T08:51:13Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://94fdb3e37d3b7f9035f1f7d9633af117568823922814b154422817938ef1ec1e\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-06-11T08:51:14Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.136.38\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.42.34\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.42.34\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-06-11T08:51:13Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jun 11 08:51:18.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 replace -f - --namespace=kubectl-139'
Jun 11 08:51:18.995: INFO: stderr: ""
Jun 11 08:51:18.995: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1795
Jun 11 08:51:19.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-515000098 delete pods e2e-test-httpd-pod --namespace=kubectl-139'
Jun 11 08:51:21.617: INFO: stderr: ""
Jun 11 08:51:21.617: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:51:21.617: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "kubectl-139" for this suite.

• [SLOW TEST:8.589 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1786
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":280,"completed":270,"skipped":4402,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:51:21.634: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8251
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating secret secrets-8251/secret-test-4d798c9e-0431-464b-a1ab-fa777d3d2ce7
STEP: Creating a pod to test consume secrets
Jun 11 08:51:21.801: INFO: Waiting up to 5m0s for pod "pod-configmaps-697c6137-e8b4-4930-9ea8-4b1a414bec1b" in namespace "secrets-8251" to be "success or failure"
Jun 11 08:51:21.805: INFO: Pod "pod-configmaps-697c6137-e8b4-4930-9ea8-4b1a414bec1b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.096961ms
Jun 11 08:51:23.810: INFO: Pod "pod-configmaps-697c6137-e8b4-4930-9ea8-4b1a414bec1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008815458s
STEP: Saw pod success
Jun 11 08:51:23.810: INFO: Pod "pod-configmaps-697c6137-e8b4-4930-9ea8-4b1a414bec1b" satisfied condition "success or failure"
Jun 11 08:51:23.814: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-configmaps-697c6137-e8b4-4930-9ea8-4b1a414bec1b container env-test: <nil>
STEP: delete the pod
Jun 11 08:51:23.840: INFO: Waiting for pod pod-configmaps-697c6137-e8b4-4930-9ea8-4b1a414bec1b to disappear
Jun 11 08:51:23.844: INFO: Pod pod-configmaps-697c6137-e8b4-4930-9ea8-4b1a414bec1b no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:51:23.844: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "secrets-8251" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":271,"skipped":4408,"failed":0}

------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:51:23.861: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-6902
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 08:51:24.010: INFO: Creating ReplicaSet my-hostname-basic-ea2f9928-0966-4154-94a8-0a1fa60d3d1d
Jun 11 08:51:24.020: INFO: Pod name my-hostname-basic-ea2f9928-0966-4154-94a8-0a1fa60d3d1d: Found 0 pods out of 1
Jun 11 08:51:29.024: INFO: Pod name my-hostname-basic-ea2f9928-0966-4154-94a8-0a1fa60d3d1d: Found 1 pods out of 1
Jun 11 08:51:29.024: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-ea2f9928-0966-4154-94a8-0a1fa60d3d1d" is running
Jun 11 08:51:29.029: INFO: Pod "my-hostname-basic-ea2f9928-0966-4154-94a8-0a1fa60d3d1d-stsbw" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-11 08:51:24 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-11 08:51:25 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-11 08:51:25 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-11 08:51:24 +0000 UTC Reason: Message:}])
Jun 11 08:51:29.029: INFO: Trying to dial the pod
Jun 11 08:51:34.043: INFO: Controller my-hostname-basic-ea2f9928-0966-4154-94a8-0a1fa60d3d1d: Got expected result from replica 1 [my-hostname-basic-ea2f9928-0966-4154-94a8-0a1fa60d3d1d-stsbw]: "my-hostname-basic-ea2f9928-0966-4154-94a8-0a1fa60d3d1d-stsbw", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:51:34.043: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "replicaset-6902" for this suite.

• [SLOW TEST:10.199 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":272,"skipped":4408,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:51:34.060: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-335
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3354
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6734
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:51:41.536: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "namespaces-335" for this suite.
STEP: Destroying namespace "nsdeletetest-3354" for this suite.
Jun 11 08:51:41.556: INFO: Namespace nsdeletetest-3354 was already deleted
STEP: Destroying namespace "nsdeletetest-6734" for this suite.

• [SLOW TEST:7.504 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":280,"completed":273,"skipped":4412,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:51:41.564: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8510
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun 11 08:51:41.726: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fe11a488-d781-4b3e-8412-c62aec26f710" in namespace "downward-api-8510" to be "success or failure"
Jun 11 08:51:41.730: INFO: Pod "downwardapi-volume-fe11a488-d781-4b3e-8412-c62aec26f710": Phase="Pending", Reason="", readiness=false. Elapsed: 4.158018ms
Jun 11 08:51:43.735: INFO: Pod "downwardapi-volume-fe11a488-d781-4b3e-8412-c62aec26f710": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00895409s
STEP: Saw pod success
Jun 11 08:51:43.735: INFO: Pod "downwardapi-volume-fe11a488-d781-4b3e-8412-c62aec26f710" satisfied condition "success or failure"
Jun 11 08:51:43.739: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod downwardapi-volume-fe11a488-d781-4b3e-8412-c62aec26f710 container client-container: <nil>
STEP: delete the pod
Jun 11 08:51:43.767: INFO: Waiting for pod downwardapi-volume-fe11a488-d781-4b3e-8412-c62aec26f710 to disappear
Jun 11 08:51:43.795: INFO: Pod downwardapi-volume-fe11a488-d781-4b3e-8412-c62aec26f710 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:51:43.795: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "downward-api-8510" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":274,"skipped":4435,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:51:43.880: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-396
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 08:51:44.441: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 08:51:47.466: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:51:57.598: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "webhook-396" for this suite.
STEP: Destroying namespace "webhook-396-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.817 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":280,"completed":275,"skipped":4451,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:51:57.697: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4813
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 11 08:51:57.862: INFO: Waiting up to 5m0s for pod "pod-03a62a3e-d806-4f84-9b90-c47e5f428dda" in namespace "emptydir-4813" to be "success or failure"
Jun 11 08:51:57.874: INFO: Pod "pod-03a62a3e-d806-4f84-9b90-c47e5f428dda": Phase="Pending", Reason="", readiness=false. Elapsed: 11.923594ms
Jun 11 08:51:59.879: INFO: Pod "pod-03a62a3e-d806-4f84-9b90-c47e5f428dda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017384917s
STEP: Saw pod success
Jun 11 08:51:59.879: INFO: Pod "pod-03a62a3e-d806-4f84-9b90-c47e5f428dda" satisfied condition "success or failure"
Jun 11 08:51:59.883: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-03a62a3e-d806-4f84-9b90-c47e5f428dda container test-container: <nil>
STEP: delete the pod
Jun 11 08:51:59.907: INFO: Waiting for pod pod-03a62a3e-d806-4f84-9b90-c47e5f428dda to disappear
Jun 11 08:51:59.913: INFO: Pod pod-03a62a3e-d806-4f84-9b90-c47e5f428dda no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:51:59.913: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "emptydir-4813" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":276,"skipped":4459,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:51:59.929: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8525
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-6e98c1c6-cfcf-41d9-be4c-86ef04fa48f3
STEP: Creating a pod to test consume configMaps
Jun 11 08:52:00.096: INFO: Waiting up to 5m0s for pod "pod-configmaps-1173e186-4855-429c-8d52-a3c3a7e66990" in namespace "configmap-8525" to be "success or failure"
Jun 11 08:52:00.100: INFO: Pod "pod-configmaps-1173e186-4855-429c-8d52-a3c3a7e66990": Phase="Pending", Reason="", readiness=false. Elapsed: 4.22875ms
Jun 11 08:52:02.105: INFO: Pod "pod-configmaps-1173e186-4855-429c-8d52-a3c3a7e66990": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00897233s
STEP: Saw pod success
Jun 11 08:52:02.105: INFO: Pod "pod-configmaps-1173e186-4855-429c-8d52-a3c3a7e66990" satisfied condition "success or failure"
Jun 11 08:52:02.109: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-configmaps-1173e186-4855-429c-8d52-a3c3a7e66990 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 11 08:52:02.132: INFO: Waiting for pod pod-configmaps-1173e186-4855-429c-8d52-a3c3a7e66990 to disappear
Jun 11 08:52:02.135: INFO: Pod pod-configmaps-1173e186-4855-429c-8d52-a3c3a7e66990 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:52:02.135: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "configmap-8525" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":277,"skipped":4471,"failed":0}
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:52:02.151: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7145
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-1a8bb7bc-822e-43b2-a7fb-73c4d769291f
STEP: Creating a pod to test consume configMaps
Jun 11 08:52:02.318: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7006fcef-f30d-4d0d-8527-39c48098ec96" in namespace "projected-7145" to be "success or failure"
Jun 11 08:52:02.322: INFO: Pod "pod-projected-configmaps-7006fcef-f30d-4d0d-8527-39c48098ec96": Phase="Pending", Reason="", readiness=false. Elapsed: 3.960755ms
Jun 11 08:52:04.327: INFO: Pod "pod-projected-configmaps-7006fcef-f30d-4d0d-8527-39c48098ec96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008908335s
STEP: Saw pod success
Jun 11 08:52:04.327: INFO: Pod "pod-projected-configmaps-7006fcef-f30d-4d0d-8527-39c48098ec96" satisfied condition "success or failure"
Jun 11 08:52:04.331: INFO: Trying to get logs from node ip-10-0-136-38.us-west-2.compute.internal pod pod-projected-configmaps-7006fcef-f30d-4d0d-8527-39c48098ec96 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 11 08:52:04.355: INFO: Waiting for pod pod-projected-configmaps-7006fcef-f30d-4d0d-8527-39c48098ec96 to disappear
Jun 11 08:52:04.360: INFO: Pod pod-projected-configmaps-7006fcef-f30d-4d0d-8527-39c48098ec96 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:52:04.360: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "projected-7145" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":278,"skipped":4472,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:52:04.377: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1611
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jun 11 08:52:04.540: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1611 /api/v1/namespaces/watch-1611/configmaps/e2e-watch-test-configmap-a 88546ee6-6688-466d-b6e5-94cc933eba47 148769 0 2020-06-11 08:52:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 11 08:52:04.540: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1611 /api/v1/namespaces/watch-1611/configmaps/e2e-watch-test-configmap-a 88546ee6-6688-466d-b6e5-94cc933eba47 148769 0 2020-06-11 08:52:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jun 11 08:52:14.550: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1611 /api/v1/namespaces/watch-1611/configmaps/e2e-watch-test-configmap-a 88546ee6-6688-466d-b6e5-94cc933eba47 148887 0 2020-06-11 08:52:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jun 11 08:52:14.550: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1611 /api/v1/namespaces/watch-1611/configmaps/e2e-watch-test-configmap-a 88546ee6-6688-466d-b6e5-94cc933eba47 148887 0 2020-06-11 08:52:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jun 11 08:52:24.560: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1611 /api/v1/namespaces/watch-1611/configmaps/e2e-watch-test-configmap-a 88546ee6-6688-466d-b6e5-94cc933eba47 148971 0 2020-06-11 08:52:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 11 08:52:24.560: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1611 /api/v1/namespaces/watch-1611/configmaps/e2e-watch-test-configmap-a 88546ee6-6688-466d-b6e5-94cc933eba47 148971 0 2020-06-11 08:52:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jun 11 08:52:34.571: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1611 /api/v1/namespaces/watch-1611/configmaps/e2e-watch-test-configmap-a 88546ee6-6688-466d-b6e5-94cc933eba47 149051 0 2020-06-11 08:52:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 11 08:52:34.572: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1611 /api/v1/namespaces/watch-1611/configmaps/e2e-watch-test-configmap-a 88546ee6-6688-466d-b6e5-94cc933eba47 149051 0 2020-06-11 08:52:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jun 11 08:52:44.582: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1611 /api/v1/namespaces/watch-1611/configmaps/e2e-watch-test-configmap-b f05b6781-4cc8-4720-b858-ea72ca5bfdbe 149136 0 2020-06-11 08:52:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 11 08:52:44.582: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1611 /api/v1/namespaces/watch-1611/configmaps/e2e-watch-test-configmap-b f05b6781-4cc8-4720-b858-ea72ca5bfdbe 149136 0 2020-06-11 08:52:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jun 11 08:52:54.594: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1611 /api/v1/namespaces/watch-1611/configmaps/e2e-watch-test-configmap-b f05b6781-4cc8-4720-b858-ea72ca5bfdbe 149220 0 2020-06-11 08:52:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 11 08:52:54.594: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1611 /api/v1/namespaces/watch-1611/configmaps/e2e-watch-test-configmap-b f05b6781-4cc8-4720-b858-ea72ca5bfdbe 149220 0 2020-06-11 08:52:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:53:04.594: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "watch-1611" for this suite.

• [SLOW TEST:60.233 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":280,"completed":279,"skipped":4531,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 11 08:53:04.610: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6306
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 08:53:05.299: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 08:53:08.323: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun 11 08:53:08.327: INFO: >>> kubeConfig: /tmp/kubeconfig-515000098
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3073-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 11 08:53:09.439: INFO: Waiting up to 3m0s for all (but 7) nodes to be ready
STEP: Destroying namespace "webhook-6306" for this suite.
STEP: Destroying namespace "webhook-6306-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":280,"completed":280,"skipped":4559,"failed":0}
SSSSJun 11 08:53:09.537: INFO: Running AfterSuite actions on all nodes
Jun 11 08:53:09.537: INFO: Running AfterSuite actions on node 1
Jun 11 08:53:09.537: INFO: Skipping dumping logs from cluster
{"msg":"Test Suite completed","total":280,"completed":280,"skipped":4563,"failed":0}

Ran 280 of 4843 Specs in 4321.335 seconds
SUCCESS! -- 280 Passed | 0 Failed | 0 Pending | 4563 Skipped
PASS

Ginkgo ran 1 suite in 1h12m2.655630884s
Test Suite Passed
