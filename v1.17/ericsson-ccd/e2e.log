I0330 21:05:12.600149      20 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-408641372
I0330 21:05:12.600187      20 test_context.go:419] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0330 21:05:12.600487      20 e2e.go:109] Starting e2e run "cdbd2f2a-9a47-451c-bcee-d3eae1eea6f3" on Ginkgo node 1
{"msg":"Test Suite starting","total":278,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1585602310 - Will randomize all specs
Will run 278 of 4843 specs

Mar 30 21:05:12.663: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 21:05:12.666: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0330 21:05:12.667277      20 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post http://localhost:8099/progress: dial tcp 127.0.0.1:8099: connect: connection refused
Mar 30 21:05:12.695: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar 30 21:05:12.794: INFO: 54 / 54 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar 30 21:05:12.794: INFO: expected 10 pod replicas in namespace 'kube-system', 10 are Running and Ready.
Mar 30 21:05:12.794: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar 30 21:05:12.811: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Mar 30 21:05:12.811: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin' (0 seconds elapsed)
Mar 30 21:05:12.811: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'eric-tm-external-connectivity-frontend-speaker' (0 seconds elapsed)
Mar 30 21:05:12.811: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'kube-multus-ds-amd64' (0 seconds elapsed)
Mar 30 21:05:12.811: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Mar 30 21:05:12.811: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'openstack-cloud-controller-manager' (0 seconds elapsed)
Mar 30 21:05:12.811: INFO: e2e test version: v1.17.3
Mar 30 21:05:12.813: INFO: kube-apiserver version: v1.17.3
Mar 30 21:05:12.813: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 21:05:12.821: INFO: Cluster IP family: ipv4
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:05:12.822: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename kubectl
Mar 30 21:05:12.901: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Mar 30 21:05:12.932: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-97
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting the proxy server
Mar 30 21:05:13.075: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-408641372 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:05:13.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-97" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":278,"completed":1,"skipped":9,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:05:13.221: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename security-context-test
E0330 21:05:13.221465      20 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post http://localhost:8099/progress: dial tcp 127.0.0.1:8099: connect: connection refused
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-276
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 21:05:13.413: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-ea1d3733-515f-451f-b480-be15a98613c6" in namespace "security-context-test-276" to be "success or failure"
Mar 30 21:05:13.419: INFO: Pod "busybox-readonly-false-ea1d3733-515f-451f-b480-be15a98613c6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.563117ms
Mar 30 21:05:15.428: INFO: Pod "busybox-readonly-false-ea1d3733-515f-451f-b480-be15a98613c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014968592s
Mar 30 21:05:17.435: INFO: Pod "busybox-readonly-false-ea1d3733-515f-451f-b480-be15a98613c6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022791028s
Mar 30 21:05:19.443: INFO: Pod "busybox-readonly-false-ea1d3733-515f-451f-b480-be15a98613c6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02990697s
Mar 30 21:05:21.450: INFO: Pod "busybox-readonly-false-ea1d3733-515f-451f-b480-be15a98613c6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.037863973s
Mar 30 21:05:23.466: INFO: Pod "busybox-readonly-false-ea1d3733-515f-451f-b480-be15a98613c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.053306397s
Mar 30 21:05:23.466: INFO: Pod "busybox-readonly-false-ea1d3733-515f-451f-b480-be15a98613c6" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:05:23.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-276" for this suite.

• [SLOW TEST:10.266 seconds]
[k8s.io] Security Context
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:164
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":278,"completed":2,"skipped":22,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:05:23.488: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3381
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-3381
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating statefulset ss in namespace statefulset-3381
Mar 30 21:05:23.722: INFO: Found 0 stateful pods, waiting for 1
Mar 30 21:05:33.730: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Mar 30 21:05:43.730: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Mar 30 21:05:43.770: INFO: Deleting all statefulset in ns statefulset-3381
Mar 30 21:05:43.794: INFO: Scaling statefulset ss to 0
Mar 30 21:06:03.854: INFO: Waiting for statefulset status.replicas updated to 0
Mar 30 21:06:03.861: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:06:03.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3381" for this suite.

• [SLOW TEST:40.440 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":278,"completed":3,"skipped":57,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:06:03.929: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2841
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-e926df92-cae1-47ff-aa20-2603d06476e8 in namespace container-probe-2841
Mar 30 21:06:12.236: INFO: Started pod busybox-e926df92-cae1-47ff-aa20-2603d06476e8 in namespace container-probe-2841
STEP: checking the pod's current state and verifying that restartCount is present
Mar 30 21:06:12.243: INFO: Initial restart count of pod busybox-e926df92-cae1-47ff-aa20-2603d06476e8 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:10:13.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2841" for this suite.

• [SLOW TEST:249.230 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":278,"completed":4,"skipped":67,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:10:13.160: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5934
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 21:10:13.350: INFO: Creating deployment "test-recreate-deployment"
Mar 30 21:10:13.363: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar 30 21:10:13.374: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Mar 30 21:10:15.385: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar 30 21:10:15.389: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199413, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199413, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199413, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199413, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-799c574856\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:10:17.395: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199413, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199413, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199413, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199413, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-799c574856\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:10:19.393: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199413, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199413, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199413, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199413, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-799c574856\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:10:21.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199413, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199413, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199413, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199413, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-799c574856\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:10:23.396: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199413, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199413, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199413, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199413, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-799c574856\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:10:25.400: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar 30 21:10:25.414: INFO: Updating deployment test-recreate-deployment
Mar 30 21:10:25.414: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Mar 30 21:10:25.610: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5934 /apis/apps/v1/namespaces/deployment-5934/deployments/test-recreate-deployment 39435930-60a8-4988-bf67-b6ef7140252e 10274 2 2020-03-30 21:10:13 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0031e7d68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-03-30 21:10:25 +0000 UTC,LastTransitionTime:2020-03-30 21:10:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-03-30 21:10:25 +0000 UTC,LastTransitionTime:2020-03-30 21:10:13 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar 30 21:10:25.616: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-5934 /apis/apps/v1/namespaces/deployment-5934/replicasets/test-recreate-deployment-5f94c574ff 4a6d7f3d-420f-4e53-86f8-22a2f3ed69cb 10271 1 2020-03-30 21:10:25 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 39435930-60a8-4988-bf67-b6ef7140252e 0xc0033adf77 0xc0033adf78}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0033adfd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 30 21:10:25.616: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar 30 21:10:25.616: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-799c574856  deployment-5934 /apis/apps/v1/namespaces/deployment-5934/replicasets/test-recreate-deployment-799c574856 4c93a18f-7275-4e04-a861-bc318332863f 10262 2 2020-03-30 21:10:13 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 39435930-60a8-4988-bf67-b6ef7140252e 0xc002dde0a7 0xc002dde0a8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 799c574856,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002dde118 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 30 21:10:25.622: INFO: Pod "test-recreate-deployment-5f94c574ff-w26kj" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-w26kj test-recreate-deployment-5f94c574ff- deployment-5934 /api/v1/namespaces/deployment-5934/pods/test-recreate-deployment-5f94c574ff-w26kj b42a5789-6906-4364-aa54-e3cf4c5f7768 10273 0 2020-03-30 21:10:25 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 4a6d7f3d-420f-4e53-86f8-22a2f3ed69cb 0xc002dde5c7 0xc002dde5c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mvpz2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mvpz2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mvpz2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-tjim0te5-eccd-ci-os-12-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:10:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:10:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:10:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:10:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.5,PodIP:,StartTime:2020-03-30 21:10:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:10:25.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5934" for this suite.

• [SLOW TEST:12.479 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":278,"completed":5,"skipped":76,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:10:25.642: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5121
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating all guestbook components
Mar 30 21:10:25.826: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Mar 30 21:10:25.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 create -f - --namespace=kubectl-5121'
Mar 30 21:10:26.544: INFO: stderr: ""
Mar 30 21:10:26.545: INFO: stdout: "service/agnhost-slave created\n"
Mar 30 21:10:26.545: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Mar 30 21:10:26.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 create -f - --namespace=kubectl-5121'
Mar 30 21:10:27.101: INFO: stderr: ""
Mar 30 21:10:27.101: INFO: stdout: "service/agnhost-master created\n"
Mar 30 21:10:27.101: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar 30 21:10:27.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 create -f - --namespace=kubectl-5121'
Mar 30 21:10:27.464: INFO: stderr: ""
Mar 30 21:10:27.469: INFO: stdout: "service/frontend created\n"
Mar 30 21:10:27.470: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar 30 21:10:27.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 create -f - --namespace=kubectl-5121'
Mar 30 21:10:27.839: INFO: stderr: ""
Mar 30 21:10:27.839: INFO: stdout: "deployment.apps/frontend created\n"
Mar 30 21:10:27.839: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 30 21:10:27.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 create -f - --namespace=kubectl-5121'
Mar 30 21:10:28.079: INFO: stderr: ""
Mar 30 21:10:28.079: INFO: stdout: "deployment.apps/agnhost-master created\n"
Mar 30 21:10:28.080: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 30 21:10:28.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 create -f - --namespace=kubectl-5121'
Mar 30 21:10:28.364: INFO: stderr: ""
Mar 30 21:10:28.364: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Mar 30 21:10:28.364: INFO: Waiting for all frontend pods to be Running.
Mar 30 21:10:43.416: INFO: Waiting for frontend to serve content.
Mar 30 21:10:43.443: INFO: Trying to add a new entry to the guestbook.
Mar 30 21:10:43.463: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Mar 30 21:10:43.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 delete --grace-period=0 --force -f - --namespace=kubectl-5121'
Mar 30 21:10:43.657: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 30 21:10:43.657: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Mar 30 21:10:43.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 delete --grace-period=0 --force -f - --namespace=kubectl-5121'
Mar 30 21:10:43.808: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 30 21:10:43.808: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Mar 30 21:10:43.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 delete --grace-period=0 --force -f - --namespace=kubectl-5121'
Mar 30 21:10:43.963: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 30 21:10:43.963: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 30 21:10:43.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 delete --grace-period=0 --force -f - --namespace=kubectl-5121'
Mar 30 21:10:44.078: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 30 21:10:44.078: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 30 21:10:44.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 delete --grace-period=0 --force -f - --namespace=kubectl-5121'
Mar 30 21:10:44.204: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 30 21:10:44.204: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Mar 30 21:10:44.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 delete --grace-period=0 --force -f - --namespace=kubectl-5121'
Mar 30 21:10:44.297: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 30 21:10:44.297: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:10:44.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5121" for this suite.

• [SLOW TEST:18.678 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:386
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":278,"completed":6,"skipped":107,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:10:44.324: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9872
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 30 21:10:44.540: INFO: Waiting up to 5m0s for pod "pod-8fe62a92-1850-4d61-b576-ebcc2e18cbef" in namespace "emptydir-9872" to be "success or failure"
Mar 30 21:10:44.551: INFO: Pod "pod-8fe62a92-1850-4d61-b576-ebcc2e18cbef": Phase="Pending", Reason="", readiness=false. Elapsed: 10.638ms
Mar 30 21:10:46.565: INFO: Pod "pod-8fe62a92-1850-4d61-b576-ebcc2e18cbef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025363462s
Mar 30 21:10:48.573: INFO: Pod "pod-8fe62a92-1850-4d61-b576-ebcc2e18cbef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033180241s
Mar 30 21:10:50.580: INFO: Pod "pod-8fe62a92-1850-4d61-b576-ebcc2e18cbef": Phase="Pending", Reason="", readiness=false. Elapsed: 6.0400779s
Mar 30 21:10:52.592: INFO: Pod "pod-8fe62a92-1850-4d61-b576-ebcc2e18cbef": Phase="Pending", Reason="", readiness=false. Elapsed: 8.052278306s
Mar 30 21:10:54.599: INFO: Pod "pod-8fe62a92-1850-4d61-b576-ebcc2e18cbef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.058836741s
STEP: Saw pod success
Mar 30 21:10:54.599: INFO: Pod "pod-8fe62a92-1850-4d61-b576-ebcc2e18cbef" satisfied condition "success or failure"
Mar 30 21:10:54.606: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-8fe62a92-1850-4d61-b576-ebcc2e18cbef container test-container: <nil>
STEP: delete the pod
Mar 30 21:10:54.663: INFO: Waiting for pod pod-8fe62a92-1850-4d61-b576-ebcc2e18cbef to disappear
Mar 30 21:10:54.668: INFO: Pod pod-8fe62a92-1850-4d61-b576-ebcc2e18cbef no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:10:54.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9872" for this suite.

• [SLOW TEST:10.372 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":7,"skipped":121,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:10:54.697: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9183
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9183.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9183.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9183.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9183.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9183.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9183.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 30 21:11:17.012: INFO: DNS probes using dns-9183/dns-test-6f56de7c-43e0-4f49-a45e-4c22183c72ea succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:11:17.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9183" for this suite.

• [SLOW TEST:22.360 seconds]
[sig-network] DNS
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":278,"completed":8,"skipped":139,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:11:17.057: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3824
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 30 21:11:17.949: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 30 21:11:19.963: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199477, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199477, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199478, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199477, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:11:21.968: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199477, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199477, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199478, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199477, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:11:23.973: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199477, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199477, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199478, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199477, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 30 21:11:27.009: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:11:27.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3824" for this suite.
STEP: Destroying namespace "webhook-3824-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.411 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":278,"completed":9,"skipped":143,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:11:27.470: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7547
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-939d74ac-23cf-43d8-ac67-a8a8a22581ee
STEP: Creating a pod to test consume configMaps
Mar 30 21:11:27.729: INFO: Waiting up to 5m0s for pod "pod-configmaps-d8bfda13-1222-4567-b737-ec36c053e1d0" in namespace "configmap-7547" to be "success or failure"
Mar 30 21:11:27.733: INFO: Pod "pod-configmaps-d8bfda13-1222-4567-b737-ec36c053e1d0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.569491ms
Mar 30 21:11:29.740: INFO: Pod "pod-configmaps-d8bfda13-1222-4567-b737-ec36c053e1d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01054179s
Mar 30 21:11:31.747: INFO: Pod "pod-configmaps-d8bfda13-1222-4567-b737-ec36c053e1d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017519462s
Mar 30 21:11:33.752: INFO: Pod "pod-configmaps-d8bfda13-1222-4567-b737-ec36c053e1d0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022882074s
Mar 30 21:11:35.759: INFO: Pod "pod-configmaps-d8bfda13-1222-4567-b737-ec36c053e1d0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02963157s
Mar 30 21:11:37.765: INFO: Pod "pod-configmaps-d8bfda13-1222-4567-b737-ec36c053e1d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.036097436s
STEP: Saw pod success
Mar 30 21:11:37.765: INFO: Pod "pod-configmaps-d8bfda13-1222-4567-b737-ec36c053e1d0" satisfied condition "success or failure"
Mar 30 21:11:37.770: INFO: Trying to get logs from node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins pod pod-configmaps-d8bfda13-1222-4567-b737-ec36c053e1d0 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 30 21:11:37.834: INFO: Waiting for pod pod-configmaps-d8bfda13-1222-4567-b737-ec36c053e1d0 to disappear
Mar 30 21:11:37.838: INFO: Pod pod-configmaps-d8bfda13-1222-4567-b737-ec36c053e1d0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:11:37.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7547" for this suite.

• [SLOW TEST:10.386 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":10,"skipped":158,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:11:37.857: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-2655
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar 30 21:11:38.437: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Mar 30 21:11:40.451: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199498, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199498, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199498, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199498, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:11:42.459: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199498, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199498, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199498, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199498, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:11:44.457: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199498, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199498, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199498, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199498, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:11:46.458: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199498, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199498, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199498, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199498, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 30 21:11:49.487: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 21:11:49.497: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:11:50.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2655" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

• [SLOW TEST:12.631 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":278,"completed":11,"skipped":174,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:11:50.497: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7417
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service endpoint-test2 in namespace services-7417
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7417 to expose endpoints map[]
Mar 30 21:11:50.731: INFO: Get endpoints failed (5.076476ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Mar 30 21:11:51.739: INFO: successfully validated that service endpoint-test2 in namespace services-7417 exposes endpoints map[] (1.012673806s elapsed)
STEP: Creating pod pod1 in namespace services-7417
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7417 to expose endpoints map[pod1:[80]]
Mar 30 21:11:55.807: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (4.046780678s elapsed, will retry)
Mar 30 21:11:59.845: INFO: successfully validated that service endpoint-test2 in namespace services-7417 exposes endpoints map[pod1:[80]] (8.084045894s elapsed)
STEP: Creating pod pod2 in namespace services-7417
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7417 to expose endpoints map[pod1:[80] pod2:[80]]
Mar 30 21:12:03.937: INFO: Unexpected endpoints: found map[3ddf66df-f244-4808-94e8-34862935b616:[80]], expected map[pod1:[80] pod2:[80]] (4.083863473s elapsed, will retry)
Mar 30 21:12:07.986: INFO: successfully validated that service endpoint-test2 in namespace services-7417 exposes endpoints map[pod1:[80] pod2:[80]] (8.132891205s elapsed)
STEP: Deleting pod pod1 in namespace services-7417
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7417 to expose endpoints map[pod2:[80]]
Mar 30 21:12:09.013: INFO: successfully validated that service endpoint-test2 in namespace services-7417 exposes endpoints map[pod2:[80]] (1.015444571s elapsed)
STEP: Deleting pod pod2 in namespace services-7417
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7417 to expose endpoints map[]
Mar 30 21:12:10.034: INFO: successfully validated that service endpoint-test2 in namespace services-7417 exposes endpoints map[] (1.011134877s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:12:10.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7417" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:19.627 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":278,"completed":12,"skipped":190,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:12:10.143: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5487
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 30 21:12:24.948: INFO: Successfully updated pod "pod-update-activedeadlineseconds-afe9475a-a0b5-4f4c-a5b1-2e894b910049"
Mar 30 21:12:24.948: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-afe9475a-a0b5-4f4c-a5b1-2e894b910049" in namespace "pods-5487" to be "terminated due to deadline exceeded"
Mar 30 21:12:24.964: INFO: Pod "pod-update-activedeadlineseconds-afe9475a-a0b5-4f4c-a5b1-2e894b910049": Phase="Running", Reason="", readiness=true. Elapsed: 15.925566ms
Mar 30 21:12:26.976: INFO: Pod "pod-update-activedeadlineseconds-afe9475a-a0b5-4f4c-a5b1-2e894b910049": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.027943718s
Mar 30 21:12:26.976: INFO: Pod "pod-update-activedeadlineseconds-afe9475a-a0b5-4f4c-a5b1-2e894b910049" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:12:26.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5487" for this suite.

• [SLOW TEST:16.859 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":278,"completed":13,"skipped":246,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:12:27.003: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7651
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-4fe4a40b-8c3b-4ad6-955f-ede2a7c2689b
STEP: Creating a pod to test consume configMaps
Mar 30 21:12:27.206: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6a821411-3bd2-4d4f-b36a-97fceef36ec8" in namespace "projected-7651" to be "success or failure"
Mar 30 21:12:27.217: INFO: Pod "pod-projected-configmaps-6a821411-3bd2-4d4f-b36a-97fceef36ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.797616ms
Mar 30 21:12:29.223: INFO: Pod "pod-projected-configmaps-6a821411-3bd2-4d4f-b36a-97fceef36ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016915494s
Mar 30 21:12:31.233: INFO: Pod "pod-projected-configmaps-6a821411-3bd2-4d4f-b36a-97fceef36ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026251134s
Mar 30 21:12:33.242: INFO: Pod "pod-projected-configmaps-6a821411-3bd2-4d4f-b36a-97fceef36ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035385348s
Mar 30 21:12:35.247: INFO: Pod "pod-projected-configmaps-6a821411-3bd2-4d4f-b36a-97fceef36ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.040204584s
Mar 30 21:12:37.253: INFO: Pod "pod-projected-configmaps-6a821411-3bd2-4d4f-b36a-97fceef36ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.046213775s
STEP: Saw pod success
Mar 30 21:12:37.253: INFO: Pod "pod-projected-configmaps-6a821411-3bd2-4d4f-b36a-97fceef36ec8" satisfied condition "success or failure"
Mar 30 21:12:37.257: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-projected-configmaps-6a821411-3bd2-4d4f-b36a-97fceef36ec8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 30 21:12:37.319: INFO: Waiting for pod pod-projected-configmaps-6a821411-3bd2-4d4f-b36a-97fceef36ec8 to disappear
Mar 30 21:12:37.323: INFO: Pod pod-projected-configmaps-6a821411-3bd2-4d4f-b36a-97fceef36ec8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:12:37.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7651" for this suite.

• [SLOW TEST:10.339 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":14,"skipped":266,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:12:37.347: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6142
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:12:37.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6142" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":278,"completed":15,"skipped":303,"failed":0}
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:12:37.649: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3172
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:12:49.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3172" for this suite.

• [SLOW TEST:12.288 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":278,"completed":16,"skipped":307,"failed":0}
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:12:49.939: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2181
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Mar 30 21:12:50.110: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:13:00.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2181" for this suite.

• [SLOW TEST:10.614 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":278,"completed":17,"skipped":313,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:13:00.559: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8133
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 30 21:13:01.240: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 30 21:13:03.255: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199581, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199581, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199581, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199581, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:13:05.260: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199581, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199581, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199581, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199581, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:13:07.263: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199581, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199581, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199581, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199581, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:13:09.263: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199581, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199581, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199581, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199581, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 30 21:13:12.380: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:13:12.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8133" for this suite.
STEP: Destroying namespace "webhook-8133-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:12.332 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":278,"completed":18,"skipped":346,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:13:12.892: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7477
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 30 21:13:13.087: INFO: Waiting up to 5m0s for pod "pod-23497929-f16e-47e8-98e7-b28e14003da1" in namespace "emptydir-7477" to be "success or failure"
Mar 30 21:13:13.096: INFO: Pod "pod-23497929-f16e-47e8-98e7-b28e14003da1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.612432ms
Mar 30 21:13:15.102: INFO: Pod "pod-23497929-f16e-47e8-98e7-b28e14003da1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014900226s
Mar 30 21:13:17.119: INFO: Pod "pod-23497929-f16e-47e8-98e7-b28e14003da1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031499204s
Mar 30 21:13:19.134: INFO: Pod "pod-23497929-f16e-47e8-98e7-b28e14003da1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.046623753s
Mar 30 21:13:21.141: INFO: Pod "pod-23497929-f16e-47e8-98e7-b28e14003da1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.05320478s
Mar 30 21:13:23.148: INFO: Pod "pod-23497929-f16e-47e8-98e7-b28e14003da1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.060086713s
STEP: Saw pod success
Mar 30 21:13:23.148: INFO: Pod "pod-23497929-f16e-47e8-98e7-b28e14003da1" satisfied condition "success or failure"
Mar 30 21:13:23.157: INFO: Trying to get logs from node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins pod pod-23497929-f16e-47e8-98e7-b28e14003da1 container test-container: <nil>
STEP: delete the pod
Mar 30 21:13:23.209: INFO: Waiting for pod pod-23497929-f16e-47e8-98e7-b28e14003da1 to disappear
Mar 30 21:13:23.213: INFO: Pod pod-23497929-f16e-47e8-98e7-b28e14003da1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:13:23.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7477" for this suite.

• [SLOW TEST:10.339 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":19,"skipped":380,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:13:23.240: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4323
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0330 21:13:29.480507      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 30 21:13:29.481: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:13:29.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4323" for this suite.

• [SLOW TEST:6.261 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":278,"completed":20,"skipped":409,"failed":0}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:13:29.502: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-252
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-252
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 30 21:13:29.673: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 30 21:14:03.867: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.125.7 8081 | grep -v '^\s*$'] Namespace:pod-network-test-252 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 30 21:14:03.867: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 21:14:05.066: INFO: Found all expected endpoints: [netserver-0]
Mar 30 21:14:05.072: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.14.84 8081 | grep -v '^\s*$'] Namespace:pod-network-test-252 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 30 21:14:05.072: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 21:14:06.272: INFO: Found all expected endpoints: [netserver-1]
Mar 30 21:14:06.279: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.185.203 8081 | grep -v '^\s*$'] Namespace:pod-network-test-252 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 30 21:14:06.279: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 21:14:07.475: INFO: Found all expected endpoints: [netserver-2]
Mar 30 21:14:07.481: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.10.198 8081 | grep -v '^\s*$'] Namespace:pod-network-test-252 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 30 21:14:07.481: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 21:14:08.675: INFO: Found all expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:14:08.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-252" for this suite.

• [SLOW TEST:39.205 seconds]
[sig-network] Networking
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":21,"skipped":410,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:14:08.710: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1629
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 30 21:14:09.413: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 30 21:14:11.435: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199649, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199649, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199649, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199649, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:14:13.441: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199649, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199649, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199649, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199649, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:14:15.441: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199649, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199649, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199649, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199649, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:14:17.443: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199649, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199649, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199649, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199649, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 30 21:14:20.477: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:14:20.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1629" for this suite.
STEP: Destroying namespace "webhook-1629-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.993 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":278,"completed":22,"skipped":433,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:14:20.705: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7550
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Mar 30 21:14:20.923: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 21:14:24.916: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:14:40.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7550" for this suite.

• [SLOW TEST:19.932 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":278,"completed":23,"skipped":435,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:14:40.639: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2665
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 30 21:14:48.908: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:14:48.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2665" for this suite.

• [SLOW TEST:8.323 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:131
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":278,"completed":24,"skipped":451,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:14:48.966: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6430
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-e2a4faa7-e994-4c14-b4b1-e33c0a3d21e5
STEP: Creating a pod to test consume configMaps
Mar 30 21:14:49.176: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8999a001-da59-4114-b155-9ac5fdb007e3" in namespace "projected-6430" to be "success or failure"
Mar 30 21:14:49.180: INFO: Pod "pod-projected-configmaps-8999a001-da59-4114-b155-9ac5fdb007e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.300498ms
Mar 30 21:14:51.189: INFO: Pod "pod-projected-configmaps-8999a001-da59-4114-b155-9ac5fdb007e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013238105s
Mar 30 21:14:53.197: INFO: Pod "pod-projected-configmaps-8999a001-da59-4114-b155-9ac5fdb007e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020513867s
Mar 30 21:14:55.202: INFO: Pod "pod-projected-configmaps-8999a001-da59-4114-b155-9ac5fdb007e3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025822482s
Mar 30 21:14:57.208: INFO: Pod "pod-projected-configmaps-8999a001-da59-4114-b155-9ac5fdb007e3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.031657374s
Mar 30 21:14:59.215: INFO: Pod "pod-projected-configmaps-8999a001-da59-4114-b155-9ac5fdb007e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.038942248s
STEP: Saw pod success
Mar 30 21:14:59.215: INFO: Pod "pod-projected-configmaps-8999a001-da59-4114-b155-9ac5fdb007e3" satisfied condition "success or failure"
Mar 30 21:14:59.220: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-projected-configmaps-8999a001-da59-4114-b155-9ac5fdb007e3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 30 21:14:59.251: INFO: Waiting for pod pod-projected-configmaps-8999a001-da59-4114-b155-9ac5fdb007e3 to disappear
Mar 30 21:14:59.257: INFO: Pod pod-projected-configmaps-8999a001-da59-4114-b155-9ac5fdb007e3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:14:59.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6430" for this suite.

• [SLOW TEST:10.311 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":278,"completed":25,"skipped":470,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:14:59.279: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9590
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-fe3a3166-6d17-40c0-95e9-2eb49aa31ece
STEP: Creating a pod to test consume configMaps
Mar 30 21:14:59.486: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-382ff9f1-f262-4a7d-988a-32464065602c" in namespace "projected-9590" to be "success or failure"
Mar 30 21:14:59.493: INFO: Pod "pod-projected-configmaps-382ff9f1-f262-4a7d-988a-32464065602c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.029649ms
Mar 30 21:15:01.501: INFO: Pod "pod-projected-configmaps-382ff9f1-f262-4a7d-988a-32464065602c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015115996s
Mar 30 21:15:03.506: INFO: Pod "pod-projected-configmaps-382ff9f1-f262-4a7d-988a-32464065602c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019871194s
Mar 30 21:15:05.511: INFO: Pod "pod-projected-configmaps-382ff9f1-f262-4a7d-988a-32464065602c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024978637s
Mar 30 21:15:07.518: INFO: Pod "pod-projected-configmaps-382ff9f1-f262-4a7d-988a-32464065602c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.031677085s
Mar 30 21:15:09.525: INFO: Pod "pod-projected-configmaps-382ff9f1-f262-4a7d-988a-32464065602c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.038697958s
STEP: Saw pod success
Mar 30 21:15:09.525: INFO: Pod "pod-projected-configmaps-382ff9f1-f262-4a7d-988a-32464065602c" satisfied condition "success or failure"
Mar 30 21:15:09.528: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-projected-configmaps-382ff9f1-f262-4a7d-988a-32464065602c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 30 21:15:09.566: INFO: Waiting for pod pod-projected-configmaps-382ff9f1-f262-4a7d-988a-32464065602c to disappear
Mar 30 21:15:09.570: INFO: Pod pod-projected-configmaps-382ff9f1-f262-4a7d-988a-32464065602c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:15:09.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9590" for this suite.

• [SLOW TEST:10.312 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":278,"completed":26,"skipped":530,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:15:09.600: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9688
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-61c49a43-3be9-4fea-86d6-f6533e591e34
STEP: Creating a pod to test consume configMaps
Mar 30 21:15:09.817: INFO: Waiting up to 5m0s for pod "pod-configmaps-a27949d6-939f-4804-ab84-9c352a8108d6" in namespace "configmap-9688" to be "success or failure"
Mar 30 21:15:09.821: INFO: Pod "pod-configmaps-a27949d6-939f-4804-ab84-9c352a8108d6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.41124ms
Mar 30 21:15:11.827: INFO: Pod "pod-configmaps-a27949d6-939f-4804-ab84-9c352a8108d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01029436s
Mar 30 21:15:13.832: INFO: Pod "pod-configmaps-a27949d6-939f-4804-ab84-9c352a8108d6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015648946s
Mar 30 21:15:15.838: INFO: Pod "pod-configmaps-a27949d6-939f-4804-ab84-9c352a8108d6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021128634s
Mar 30 21:15:17.843: INFO: Pod "pod-configmaps-a27949d6-939f-4804-ab84-9c352a8108d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.025878958s
STEP: Saw pod success
Mar 30 21:15:17.843: INFO: Pod "pod-configmaps-a27949d6-939f-4804-ab84-9c352a8108d6" satisfied condition "success or failure"
Mar 30 21:15:17.846: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-configmaps-a27949d6-939f-4804-ab84-9c352a8108d6 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 30 21:15:17.877: INFO: Waiting for pod pod-configmaps-a27949d6-939f-4804-ab84-9c352a8108d6 to disappear
Mar 30 21:15:17.882: INFO: Pod pod-configmaps-a27949d6-939f-4804-ab84-9c352a8108d6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:15:17.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9688" for this suite.

• [SLOW TEST:8.299 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":278,"completed":27,"skipped":626,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:15:17.900: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3889
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-fa85d7a5-6713-4171-af37-74d94584c435
STEP: Creating a pod to test consume configMaps
Mar 30 21:15:18.102: INFO: Waiting up to 5m0s for pod "pod-configmaps-ae65e23e-52d0-4b26-81ec-5d20af9378fd" in namespace "configmap-3889" to be "success or failure"
Mar 30 21:15:18.106: INFO: Pod "pod-configmaps-ae65e23e-52d0-4b26-81ec-5d20af9378fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.726744ms
Mar 30 21:15:20.112: INFO: Pod "pod-configmaps-ae65e23e-52d0-4b26-81ec-5d20af9378fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009084367s
Mar 30 21:15:22.118: INFO: Pod "pod-configmaps-ae65e23e-52d0-4b26-81ec-5d20af9378fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015014253s
Mar 30 21:15:24.126: INFO: Pod "pod-configmaps-ae65e23e-52d0-4b26-81ec-5d20af9378fd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023414809s
Mar 30 21:15:26.135: INFO: Pod "pod-configmaps-ae65e23e-52d0-4b26-81ec-5d20af9378fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.032249257s
STEP: Saw pod success
Mar 30 21:15:26.135: INFO: Pod "pod-configmaps-ae65e23e-52d0-4b26-81ec-5d20af9378fd" satisfied condition "success or failure"
Mar 30 21:15:26.139: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-configmaps-ae65e23e-52d0-4b26-81ec-5d20af9378fd container configmap-volume-test: <nil>
STEP: delete the pod
Mar 30 21:15:26.169: INFO: Waiting for pod pod-configmaps-ae65e23e-52d0-4b26-81ec-5d20af9378fd to disappear
Mar 30 21:15:26.172: INFO: Pod pod-configmaps-ae65e23e-52d0-4b26-81ec-5d20af9378fd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:15:26.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3889" for this suite.

• [SLOW TEST:8.295 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":278,"completed":28,"skipped":642,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:15:26.196: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4119
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-projected-98cf
STEP: Creating a pod to test atomic-volume-subpath
Mar 30 21:15:26.441: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-98cf" in namespace "subpath-4119" to be "success or failure"
Mar 30 21:15:26.452: INFO: Pod "pod-subpath-test-projected-98cf": Phase="Pending", Reason="", readiness=false. Elapsed: 10.680228ms
Mar 30 21:15:28.457: INFO: Pod "pod-subpath-test-projected-98cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015516452s
Mar 30 21:15:30.462: INFO: Pod "pod-subpath-test-projected-98cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020662421s
Mar 30 21:15:32.469: INFO: Pod "pod-subpath-test-projected-98cf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027752649s
Mar 30 21:15:34.475: INFO: Pod "pod-subpath-test-projected-98cf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.033002117s
Mar 30 21:15:36.482: INFO: Pod "pod-subpath-test-projected-98cf": Phase="Running", Reason="", readiness=true. Elapsed: 10.040268453s
Mar 30 21:15:38.487: INFO: Pod "pod-subpath-test-projected-98cf": Phase="Running", Reason="", readiness=true. Elapsed: 12.04534224s
Mar 30 21:15:40.492: INFO: Pod "pod-subpath-test-projected-98cf": Phase="Running", Reason="", readiness=true. Elapsed: 14.050148251s
Mar 30 21:15:42.499: INFO: Pod "pod-subpath-test-projected-98cf": Phase="Running", Reason="", readiness=true. Elapsed: 16.05702879s
Mar 30 21:15:44.504: INFO: Pod "pod-subpath-test-projected-98cf": Phase="Running", Reason="", readiness=true. Elapsed: 18.062541166s
Mar 30 21:15:46.510: INFO: Pod "pod-subpath-test-projected-98cf": Phase="Running", Reason="", readiness=true. Elapsed: 20.068449475s
Mar 30 21:15:48.516: INFO: Pod "pod-subpath-test-projected-98cf": Phase="Running", Reason="", readiness=true. Elapsed: 22.074084592s
Mar 30 21:15:50.526: INFO: Pod "pod-subpath-test-projected-98cf": Phase="Running", Reason="", readiness=true. Elapsed: 24.084609732s
Mar 30 21:15:52.532: INFO: Pod "pod-subpath-test-projected-98cf": Phase="Running", Reason="", readiness=true. Elapsed: 26.090934973s
Mar 30 21:15:54.538: INFO: Pod "pod-subpath-test-projected-98cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.096946887s
STEP: Saw pod success
Mar 30 21:15:54.539: INFO: Pod "pod-subpath-test-projected-98cf" satisfied condition "success or failure"
Mar 30 21:15:54.542: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-subpath-test-projected-98cf container test-container-subpath-projected-98cf: <nil>
STEP: delete the pod
Mar 30 21:15:54.585: INFO: Waiting for pod pod-subpath-test-projected-98cf to disappear
Mar 30 21:15:54.589: INFO: Pod pod-subpath-test-projected-98cf no longer exists
STEP: Deleting pod pod-subpath-test-projected-98cf
Mar 30 21:15:54.589: INFO: Deleting pod "pod-subpath-test-projected-98cf" in namespace "subpath-4119"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:15:54.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4119" for this suite.

• [SLOW TEST:28.416 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":278,"completed":29,"skipped":666,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:15:54.615: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3611
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service multi-endpoint-test in namespace services-3611
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3611 to expose endpoints map[]
Mar 30 21:15:54.840: INFO: Get endpoints failed (6.365073ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Mar 30 21:15:55.846: INFO: successfully validated that service multi-endpoint-test in namespace services-3611 exposes endpoints map[] (1.012634481s elapsed)
STEP: Creating pod pod1 in namespace services-3611
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3611 to expose endpoints map[pod1:[100]]
Mar 30 21:15:59.923: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]] (4.062272179s elapsed, will retry)
Mar 30 21:16:03.965: INFO: successfully validated that service multi-endpoint-test in namespace services-3611 exposes endpoints map[pod1:[100]] (8.104448322s elapsed)
STEP: Creating pod pod2 in namespace services-3611
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3611 to expose endpoints map[pod1:[100] pod2:[101]]
Mar 30 21:16:08.060: INFO: Unexpected endpoints: found map[8be55132-1faf-4cdf-93d3-57c2366abb6c:[100]], expected map[pod1:[100] pod2:[101]] (4.082965116s elapsed, will retry)
Mar 30 21:16:12.111: INFO: successfully validated that service multi-endpoint-test in namespace services-3611 exposes endpoints map[pod1:[100] pod2:[101]] (8.134416233s elapsed)
STEP: Deleting pod pod1 in namespace services-3611
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3611 to expose endpoints map[pod2:[101]]
Mar 30 21:16:12.145: INFO: successfully validated that service multi-endpoint-test in namespace services-3611 exposes endpoints map[pod2:[101]] (23.822964ms elapsed)
STEP: Deleting pod pod2 in namespace services-3611
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3611 to expose endpoints map[]
Mar 30 21:16:13.178: INFO: successfully validated that service multi-endpoint-test in namespace services-3611 exposes endpoints map[] (1.012494549s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:16:13.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3611" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:18.715 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":278,"completed":30,"skipped":677,"failed":0}
SSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:16:13.330: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-7199
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-7199, will wait for the garbage collector to delete the pods
Mar 30 21:16:25.685: INFO: Deleting Job.batch foo took: 47.252587ms
Mar 30 21:16:26.286: INFO: Terminating Job.batch foo pods took: 600.430079ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:16:58.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7199" for this suite.

• [SLOW TEST:45.681 seconds]
[sig-apps] Job
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":278,"completed":31,"skipped":681,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:16:59.020: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6007
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's args
Mar 30 21:16:59.226: INFO: Waiting up to 5m0s for pod "var-expansion-75646f2e-9cad-41ca-b357-ab56980e9e3c" in namespace "var-expansion-6007" to be "success or failure"
Mar 30 21:16:59.232: INFO: Pod "var-expansion-75646f2e-9cad-41ca-b357-ab56980e9e3c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.459862ms
Mar 30 21:17:01.237: INFO: Pod "var-expansion-75646f2e-9cad-41ca-b357-ab56980e9e3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010355081s
Mar 30 21:17:03.243: INFO: Pod "var-expansion-75646f2e-9cad-41ca-b357-ab56980e9e3c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016843461s
Mar 30 21:17:05.250: INFO: Pod "var-expansion-75646f2e-9cad-41ca-b357-ab56980e9e3c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023362726s
Mar 30 21:17:07.254: INFO: Pod "var-expansion-75646f2e-9cad-41ca-b357-ab56980e9e3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.027997275s
STEP: Saw pod success
Mar 30 21:17:07.254: INFO: Pod "var-expansion-75646f2e-9cad-41ca-b357-ab56980e9e3c" satisfied condition "success or failure"
Mar 30 21:17:07.258: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod var-expansion-75646f2e-9cad-41ca-b357-ab56980e9e3c container dapi-container: <nil>
STEP: delete the pod
Mar 30 21:17:07.309: INFO: Waiting for pod var-expansion-75646f2e-9cad-41ca-b357-ab56980e9e3c to disappear
Mar 30 21:17:07.314: INFO: Pod var-expansion-75646f2e-9cad-41ca-b357-ab56980e9e3c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:17:07.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6007" for this suite.

• [SLOW TEST:8.313 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":278,"completed":32,"skipped":768,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:17:07.333: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-3978
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar 30 21:17:08.281: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Mar 30 21:17:10.303: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199828, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199828, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199828, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199828, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:17:12.309: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199828, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199828, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199828, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199828, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:17:14.307: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199828, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199828, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199828, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199828, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:17:16.317: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199828, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199828, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199828, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721199828, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 30 21:17:19.469: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 21:17:19.543: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:17:20.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3978" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

• [SLOW TEST:13.940 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":278,"completed":33,"skipped":786,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:17:21.277: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3221
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-downwardapi-45kp
STEP: Creating a pod to test atomic-volume-subpath
Mar 30 21:17:21.812: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-45kp" in namespace "subpath-3221" to be "success or failure"
Mar 30 21:17:21.817: INFO: Pod "pod-subpath-test-downwardapi-45kp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.29766ms
Mar 30 21:17:23.857: INFO: Pod "pod-subpath-test-downwardapi-45kp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045115676s
Mar 30 21:17:25.864: INFO: Pod "pod-subpath-test-downwardapi-45kp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051721801s
Mar 30 21:17:27.871: INFO: Pod "pod-subpath-test-downwardapi-45kp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.05923882s
Mar 30 21:17:29.877: INFO: Pod "pod-subpath-test-downwardapi-45kp": Phase="Pending", Reason="", readiness=false. Elapsed: 8.06455855s
Mar 30 21:17:31.889: INFO: Pod "pod-subpath-test-downwardapi-45kp": Phase="Running", Reason="", readiness=true. Elapsed: 10.07668667s
Mar 30 21:17:33.894: INFO: Pod "pod-subpath-test-downwardapi-45kp": Phase="Running", Reason="", readiness=true. Elapsed: 12.081980698s
Mar 30 21:17:35.899: INFO: Pod "pod-subpath-test-downwardapi-45kp": Phase="Running", Reason="", readiness=true. Elapsed: 14.086641973s
Mar 30 21:17:37.905: INFO: Pod "pod-subpath-test-downwardapi-45kp": Phase="Running", Reason="", readiness=true. Elapsed: 16.092795876s
Mar 30 21:17:39.910: INFO: Pod "pod-subpath-test-downwardapi-45kp": Phase="Running", Reason="", readiness=true. Elapsed: 18.098009071s
Mar 30 21:17:41.917: INFO: Pod "pod-subpath-test-downwardapi-45kp": Phase="Running", Reason="", readiness=true. Elapsed: 20.104776308s
Mar 30 21:17:43.925: INFO: Pod "pod-subpath-test-downwardapi-45kp": Phase="Running", Reason="", readiness=true. Elapsed: 22.113237532s
Mar 30 21:17:45.930: INFO: Pod "pod-subpath-test-downwardapi-45kp": Phase="Running", Reason="", readiness=true. Elapsed: 24.117956422s
Mar 30 21:17:47.935: INFO: Pod "pod-subpath-test-downwardapi-45kp": Phase="Running", Reason="", readiness=true. Elapsed: 26.122664933s
Mar 30 21:17:49.940: INFO: Pod "pod-subpath-test-downwardapi-45kp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.127791735s
STEP: Saw pod success
Mar 30 21:17:49.940: INFO: Pod "pod-subpath-test-downwardapi-45kp" satisfied condition "success or failure"
Mar 30 21:17:49.945: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-subpath-test-downwardapi-45kp container test-container-subpath-downwardapi-45kp: <nil>
STEP: delete the pod
Mar 30 21:17:50.008: INFO: Waiting for pod pod-subpath-test-downwardapi-45kp to disappear
Mar 30 21:17:50.041: INFO: Pod pod-subpath-test-downwardapi-45kp no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-45kp
Mar 30 21:17:50.041: INFO: Deleting pod "pod-subpath-test-downwardapi-45kp" in namespace "subpath-3221"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:17:50.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3221" for this suite.

• [SLOW TEST:28.789 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":278,"completed":34,"skipped":827,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:17:50.068: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8903
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-0259ac68-5cce-49c9-a903-0628a55e3f5e
STEP: Creating a pod to test consume secrets
Mar 30 21:17:50.277: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e882cc2d-7c43-4b2c-8eb3-6a2f5aaa1876" in namespace "projected-8903" to be "success or failure"
Mar 30 21:17:50.283: INFO: Pod "pod-projected-secrets-e882cc2d-7c43-4b2c-8eb3-6a2f5aaa1876": Phase="Pending", Reason="", readiness=false. Elapsed: 5.972814ms
Mar 30 21:17:52.294: INFO: Pod "pod-projected-secrets-e882cc2d-7c43-4b2c-8eb3-6a2f5aaa1876": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017288697s
Mar 30 21:17:54.300: INFO: Pod "pod-projected-secrets-e882cc2d-7c43-4b2c-8eb3-6a2f5aaa1876": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02362336s
Mar 30 21:17:56.306: INFO: Pod "pod-projected-secrets-e882cc2d-7c43-4b2c-8eb3-6a2f5aaa1876": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028856495s
Mar 30 21:17:58.312: INFO: Pod "pod-projected-secrets-e882cc2d-7c43-4b2c-8eb3-6a2f5aaa1876": Phase="Pending", Reason="", readiness=false. Elapsed: 8.034982984s
Mar 30 21:18:00.318: INFO: Pod "pod-projected-secrets-e882cc2d-7c43-4b2c-8eb3-6a2f5aaa1876": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.041066447s
STEP: Saw pod success
Mar 30 21:18:00.318: INFO: Pod "pod-projected-secrets-e882cc2d-7c43-4b2c-8eb3-6a2f5aaa1876" satisfied condition "success or failure"
Mar 30 21:18:00.322: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-projected-secrets-e882cc2d-7c43-4b2c-8eb3-6a2f5aaa1876 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 30 21:18:00.354: INFO: Waiting for pod pod-projected-secrets-e882cc2d-7c43-4b2c-8eb3-6a2f5aaa1876 to disappear
Mar 30 21:18:00.358: INFO: Pod pod-projected-secrets-e882cc2d-7c43-4b2c-8eb3-6a2f5aaa1876 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:18:00.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8903" for this suite.

• [SLOW TEST:10.306 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":35,"skipped":832,"failed":0}
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:18:00.375: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2237
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1733
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 30 21:18:00.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-2237'
Mar 30 21:18:00.698: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 30 21:18:00.698: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1738
Mar 30 21:18:02.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 delete deployment e2e-test-httpd-deployment --namespace=kubectl-2237'
Mar 30 21:18:02.860: INFO: stderr: ""
Mar 30 21:18:02.860: INFO: stdout: "deployment.extensions \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:18:02.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2237" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run deployment should create a deployment from an image  [Conformance]","total":278,"completed":36,"skipped":832,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:18:02.884: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-6832
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Mar 30 21:18:13.596: INFO: Successfully updated pod "adopt-release-dftvg"
STEP: Checking that the Job readopts the Pod
Mar 30 21:18:13.596: INFO: Waiting up to 15m0s for pod "adopt-release-dftvg" in namespace "job-6832" to be "adopted"
Mar 30 21:18:13.601: INFO: Pod "adopt-release-dftvg": Phase="Running", Reason="", readiness=true. Elapsed: 4.884881ms
Mar 30 21:18:15.609: INFO: Pod "adopt-release-dftvg": Phase="Running", Reason="", readiness=true. Elapsed: 2.012489818s
Mar 30 21:18:15.609: INFO: Pod "adopt-release-dftvg" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Mar 30 21:18:16.124: INFO: Successfully updated pod "adopt-release-dftvg"
STEP: Checking that the Job releases the Pod
Mar 30 21:18:16.124: INFO: Waiting up to 15m0s for pod "adopt-release-dftvg" in namespace "job-6832" to be "released"
Mar 30 21:18:16.128: INFO: Pod "adopt-release-dftvg": Phase="Running", Reason="", readiness=true. Elapsed: 3.672541ms
Mar 30 21:18:18.134: INFO: Pod "adopt-release-dftvg": Phase="Running", Reason="", readiness=true. Elapsed: 2.009821777s
Mar 30 21:18:18.134: INFO: Pod "adopt-release-dftvg" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:18:18.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6832" for this suite.

• [SLOW TEST:15.271 seconds]
[sig-apps] Job
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":278,"completed":37,"skipped":874,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:18:18.156: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2605
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Mar 30 21:18:28.918: INFO: Successfully updated pod "labelsupdate2b8bb3f1-3036-4bdb-a743-a5d5fcd18a6e"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:18:32.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2605" for this suite.

• [SLOW TEST:14.826 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":278,"completed":38,"skipped":880,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:18:32.986: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7074
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-7074/configmap-test-21ceea72-0abc-4fd4-81ab-34b5d6704056
STEP: Creating a pod to test consume configMaps
Mar 30 21:18:33.173: INFO: Waiting up to 5m0s for pod "pod-configmaps-392b90b6-4a16-4faf-ad24-9645c0fd88a2" in namespace "configmap-7074" to be "success or failure"
Mar 30 21:18:33.177: INFO: Pod "pod-configmaps-392b90b6-4a16-4faf-ad24-9645c0fd88a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.268717ms
Mar 30 21:18:35.183: INFO: Pod "pod-configmaps-392b90b6-4a16-4faf-ad24-9645c0fd88a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009708983s
Mar 30 21:18:37.190: INFO: Pod "pod-configmaps-392b90b6-4a16-4faf-ad24-9645c0fd88a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01675045s
Mar 30 21:18:39.196: INFO: Pod "pod-configmaps-392b90b6-4a16-4faf-ad24-9645c0fd88a2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022842088s
Mar 30 21:18:41.204: INFO: Pod "pod-configmaps-392b90b6-4a16-4faf-ad24-9645c0fd88a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.030862888s
STEP: Saw pod success
Mar 30 21:18:41.204: INFO: Pod "pod-configmaps-392b90b6-4a16-4faf-ad24-9645c0fd88a2" satisfied condition "success or failure"
Mar 30 21:18:41.208: INFO: Trying to get logs from node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins pod pod-configmaps-392b90b6-4a16-4faf-ad24-9645c0fd88a2 container env-test: <nil>
STEP: delete the pod
Mar 30 21:18:41.253: INFO: Waiting for pod pod-configmaps-392b90b6-4a16-4faf-ad24-9645c0fd88a2 to disappear
Mar 30 21:18:41.258: INFO: Pod pod-configmaps-392b90b6-4a16-4faf-ad24-9645c0fd88a2 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:18:41.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7074" for this suite.

• [SLOW TEST:8.286 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":278,"completed":39,"skipped":894,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:18:41.271: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-89
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-89
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Mar 30 21:18:41.466: INFO: Found 0 stateful pods, waiting for 3
Mar 30 21:18:51.479: INFO: Found 2 stateful pods, waiting for 3
Mar 30 21:19:01.473: INFO: Found 2 stateful pods, waiting for 3
Mar 30 21:19:11.478: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 21:19:11.479: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 21:19:11.479: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Mar 30 21:19:21.474: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 21:19:21.474: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 21:19:21.474: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar 30 21:19:21.513: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar 30 21:19:31.575: INFO: Updating stateful set ss2
Mar 30 21:19:31.590: INFO: Waiting for Pod statefulset-89/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Mar 30 21:19:41.699: INFO: Found 2 stateful pods, waiting for 3
Mar 30 21:19:51.916: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 21:19:51.916: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 21:19:51.916: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Mar 30 21:20:01.707: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 21:20:01.707: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 21:20:01.707: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar 30 21:20:01.745: INFO: Updating stateful set ss2
Mar 30 21:20:01.761: INFO: Waiting for Pod statefulset-89/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 30 21:20:11.805: INFO: Updating stateful set ss2
Mar 30 21:20:11.823: INFO: Waiting for StatefulSet statefulset-89/ss2 to complete update
Mar 30 21:20:11.823: INFO: Waiting for Pod statefulset-89/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 30 21:20:21.839: INFO: Waiting for StatefulSet statefulset-89/ss2 to complete update
Mar 30 21:20:21.839: INFO: Waiting for Pod statefulset-89/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 30 21:20:31.839: INFO: Waiting for StatefulSet statefulset-89/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Mar 30 21:20:41.836: INFO: Deleting all statefulset in ns statefulset-89
Mar 30 21:20:41.840: INFO: Scaling statefulset ss2 to 0
Mar 30 21:21:01.866: INFO: Waiting for statefulset status.replicas updated to 0
Mar 30 21:21:01.871: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:21:01.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-89" for this suite.

• [SLOW TEST:140.643 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":278,"completed":40,"skipped":896,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:21:01.916: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5461
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 30 21:21:02.579: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 30 21:21:04.594: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200062, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200062, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200062, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200062, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:21:06.600: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200062, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200062, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200062, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200062, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:21:08.601: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200062, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200062, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200062, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200062, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:21:10.608: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200062, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200062, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200062, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200062, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 30 21:21:13.632: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:21:13.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5461" for this suite.
STEP: Destroying namespace "webhook-5461-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.954 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":278,"completed":41,"skipped":899,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:21:13.871: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2382
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-be01f893-5f32-449e-a9eb-d3b821f97c69
STEP: Creating a pod to test consume configMaps
Mar 30 21:21:14.097: INFO: Waiting up to 5m0s for pod "pod-configmaps-974c647e-7fa6-49d5-b4f1-a4f578ff9eaf" in namespace "configmap-2382" to be "success or failure"
Mar 30 21:21:14.101: INFO: Pod "pod-configmaps-974c647e-7fa6-49d5-b4f1-a4f578ff9eaf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.807569ms
Mar 30 21:21:16.106: INFO: Pod "pod-configmaps-974c647e-7fa6-49d5-b4f1-a4f578ff9eaf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009184598s
Mar 30 21:21:18.111: INFO: Pod "pod-configmaps-974c647e-7fa6-49d5-b4f1-a4f578ff9eaf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014191827s
Mar 30 21:21:20.119: INFO: Pod "pod-configmaps-974c647e-7fa6-49d5-b4f1-a4f578ff9eaf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022189705s
Mar 30 21:21:22.125: INFO: Pod "pod-configmaps-974c647e-7fa6-49d5-b4f1-a4f578ff9eaf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.028708336s
Mar 30 21:21:24.132: INFO: Pod "pod-configmaps-974c647e-7fa6-49d5-b4f1-a4f578ff9eaf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.035201204s
STEP: Saw pod success
Mar 30 21:21:24.132: INFO: Pod "pod-configmaps-974c647e-7fa6-49d5-b4f1-a4f578ff9eaf" satisfied condition "success or failure"
Mar 30 21:21:24.143: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-configmaps-974c647e-7fa6-49d5-b4f1-a4f578ff9eaf container configmap-volume-test: <nil>
STEP: delete the pod
Mar 30 21:21:24.188: INFO: Waiting for pod pod-configmaps-974c647e-7fa6-49d5-b4f1-a4f578ff9eaf to disappear
Mar 30 21:21:24.195: INFO: Pod pod-configmaps-974c647e-7fa6-49d5-b4f1-a4f578ff9eaf no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:21:24.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2382" for this suite.

• [SLOW TEST:10.349 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":278,"completed":42,"skipped":917,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:21:24.229: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4132
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 30 21:21:25.128: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 30 21:21:27.158: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200085, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200085, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200085, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200085, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:21:29.163: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200085, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200085, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200085, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200085, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:21:31.169: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200085, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200085, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200085, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200085, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 30 21:21:34.194: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Mar 30 21:21:34.228: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:21:34.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4132" for this suite.
STEP: Destroying namespace "webhook-4132-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.220 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":278,"completed":43,"skipped":940,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:21:34.451: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6513
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 30 21:21:34.682: INFO: Waiting up to 5m0s for pod "pod-9a0601b0-8c90-46bb-9893-53683ce8bcd1" in namespace "emptydir-6513" to be "success or failure"
Mar 30 21:21:34.685: INFO: Pod "pod-9a0601b0-8c90-46bb-9893-53683ce8bcd1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.378354ms
Mar 30 21:21:36.691: INFO: Pod "pod-9a0601b0-8c90-46bb-9893-53683ce8bcd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009672227s
Mar 30 21:21:38.696: INFO: Pod "pod-9a0601b0-8c90-46bb-9893-53683ce8bcd1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014556012s
Mar 30 21:21:40.711: INFO: Pod "pod-9a0601b0-8c90-46bb-9893-53683ce8bcd1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028958475s
Mar 30 21:21:42.728: INFO: Pod "pod-9a0601b0-8c90-46bb-9893-53683ce8bcd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.046118758s
STEP: Saw pod success
Mar 30 21:21:42.728: INFO: Pod "pod-9a0601b0-8c90-46bb-9893-53683ce8bcd1" satisfied condition "success or failure"
Mar 30 21:21:42.732: INFO: Trying to get logs from node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins pod pod-9a0601b0-8c90-46bb-9893-53683ce8bcd1 container test-container: <nil>
STEP: delete the pod
Mar 30 21:21:42.768: INFO: Waiting for pod pod-9a0601b0-8c90-46bb-9893-53683ce8bcd1 to disappear
Mar 30 21:21:42.775: INFO: Pod pod-9a0601b0-8c90-46bb-9893-53683ce8bcd1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:21:42.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6513" for this suite.

• [SLOW TEST:8.343 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":44,"skipped":943,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:21:42.801: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9049
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-a666d50b-9fd2-4ae6-a8f7-9fd4f6682851
STEP: Creating a pod to test consume configMaps
Mar 30 21:21:43.020: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-31a59f43-835d-41c8-a5c5-6aa7b676054d" in namespace "projected-9049" to be "success or failure"
Mar 30 21:21:43.029: INFO: Pod "pod-projected-configmaps-31a59f43-835d-41c8-a5c5-6aa7b676054d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.921894ms
Mar 30 21:21:45.034: INFO: Pod "pod-projected-configmaps-31a59f43-835d-41c8-a5c5-6aa7b676054d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013561334s
Mar 30 21:21:47.039: INFO: Pod "pod-projected-configmaps-31a59f43-835d-41c8-a5c5-6aa7b676054d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0188953s
Mar 30 21:21:49.045: INFO: Pod "pod-projected-configmaps-31a59f43-835d-41c8-a5c5-6aa7b676054d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024264421s
Mar 30 21:21:51.049: INFO: Pod "pod-projected-configmaps-31a59f43-835d-41c8-a5c5-6aa7b676054d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.028918209s
Mar 30 21:21:53.056: INFO: Pod "pod-projected-configmaps-31a59f43-835d-41c8-a5c5-6aa7b676054d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.035914981s
STEP: Saw pod success
Mar 30 21:21:53.056: INFO: Pod "pod-projected-configmaps-31a59f43-835d-41c8-a5c5-6aa7b676054d" satisfied condition "success or failure"
Mar 30 21:21:53.061: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-projected-configmaps-31a59f43-835d-41c8-a5c5-6aa7b676054d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 30 21:21:53.100: INFO: Waiting for pod pod-projected-configmaps-31a59f43-835d-41c8-a5c5-6aa7b676054d to disappear
Mar 30 21:21:53.104: INFO: Pod pod-projected-configmaps-31a59f43-835d-41c8-a5c5-6aa7b676054d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:21:53.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9049" for this suite.

• [SLOW TEST:10.330 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":278,"completed":45,"skipped":953,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:21:53.134: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-387
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Mar 30 21:21:53.316: INFO: namespace kubectl-387
Mar 30 21:21:53.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 create -f - --namespace=kubectl-387'
Mar 30 21:21:54.270: INFO: stderr: ""
Mar 30 21:21:54.270: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Mar 30 21:21:55.277: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 21:21:55.277: INFO: Found 0 / 1
Mar 30 21:21:56.275: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 21:21:56.275: INFO: Found 0 / 1
Mar 30 21:21:57.275: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 21:21:57.275: INFO: Found 0 / 1
Mar 30 21:21:58.275: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 21:21:58.275: INFO: Found 0 / 1
Mar 30 21:21:59.276: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 21:21:59.276: INFO: Found 0 / 1
Mar 30 21:22:00.276: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 21:22:00.276: INFO: Found 0 / 1
Mar 30 21:22:01.278: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 21:22:01.278: INFO: Found 0 / 1
Mar 30 21:22:02.276: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 21:22:02.276: INFO: Found 1 / 1
Mar 30 21:22:02.276: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 30 21:22:02.282: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 21:22:02.282: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 30 21:22:02.282: INFO: wait on agnhost-master startup in kubectl-387 
Mar 30 21:22:02.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 logs agnhost-master-8ltl6 agnhost-master --namespace=kubectl-387'
Mar 30 21:22:02.402: INFO: stderr: ""
Mar 30 21:22:02.402: INFO: stdout: "Paused\n"
STEP: exposing RC
Mar 30 21:22:02.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-387'
Mar 30 21:22:02.532: INFO: stderr: ""
Mar 30 21:22:02.532: INFO: stdout: "service/rm2 exposed\n"
Mar 30 21:22:02.539: INFO: Service rm2 in namespace kubectl-387 found.
STEP: exposing service
Mar 30 21:22:04.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-387'
Mar 30 21:22:04.683: INFO: stderr: ""
Mar 30 21:22:04.683: INFO: stdout: "service/rm3 exposed\n"
Mar 30 21:22:04.692: INFO: Service rm3 in namespace kubectl-387 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:22:06.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-387" for this suite.

• [SLOW TEST:13.588 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1295
    should create services for rc  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":278,"completed":46,"skipped":961,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:22:06.723: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1707
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: executing a command with run --rm and attach with stdin
Mar 30 21:22:06.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 --namespace=kubectl-1707 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Mar 30 21:22:15.736: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Mar 30 21:22:15.736: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:22:17.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1707" for this suite.

• [SLOW TEST:11.043 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1944
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run --rm job should create a job from an image, then delete the job  [Conformance]","total":278,"completed":47,"skipped":974,"failed":0}
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:22:17.767: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6027
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap that has name configmap-test-emptyKey-5454f997-28f4-414c-bfec-ad9acb8af374
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:22:18.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6027" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":278,"completed":48,"skipped":981,"failed":0}
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:22:18.292: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3231
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:22:26.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3231" for this suite.

• [SLOW TEST:8.450 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when scheduling a busybox command in a pod
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":278,"completed":49,"skipped":984,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:22:26.743: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8502
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar 30 21:22:27.032: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8502 /api/v1/namespaces/watch-8502/configmaps/e2e-watch-test-resource-version 781934a4-5225-41e4-ab02-6240d63f3485 16276 0 2020-03-30 21:22:26 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 30 21:22:27.032: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8502 /api/v1/namespaces/watch-8502/configmaps/e2e-watch-test-resource-version 781934a4-5225-41e4-ab02-6240d63f3485 16277 0 2020-03-30 21:22:26 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:22:27.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8502" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":278,"completed":50,"skipped":1008,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:22:27.061: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1423
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Mar 30 21:22:27.281: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 30 21:22:27.302: INFO: Waiting for terminating namespaces to be deleted...
Mar 30 21:22:27.307: INFO: 
Logging pods the kubelet thinks is on node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins before test
Mar 30 21:22:27.319: INFO: eric-pm-server-eric-pm-server-7b4dd54bc5-m7fjg from monitoring started at 2020-03-30 20:51:51 +0000 UTC (2 container statuses recorded)
Mar 30 21:22:27.319: INFO: 	Container eric-pm-server-eric-pm-server ready: true, restart count 0
Mar 30 21:22:27.319: INFO: 	Container eric-pm-server-eric-pm-server-eric-pm-configmap-reload ready: true, restart count 0
Mar 30 21:22:27.319: INFO: eric-pm-server-node-exporter-m4kjc from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.319: INFO: 	Container eric-pm-server-node-exporter ready: true, restart count 0
Mar 30 21:22:27.319: INFO: sonobuoy from sonobuoy started at 2020-03-30 21:04:25 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.319: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 30 21:22:27.319: INFO: sonobuoy-systemd-logs-daemon-set-4495c60740044637-62zgj from sonobuoy started at 2020-03-30 21:04:37 +0000 UTC (2 container statuses recorded)
Mar 30 21:22:27.319: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 30 21:22:27.319: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 30 21:22:27.319: INFO: kube-multus-ds-amd64-vm7r6 from kube-system started at 2020-03-30 20:48:53 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.319: INFO: 	Container kube-multus ready: true, restart count 0
Mar 30 21:22:27.319: INFO: eric-pm-server-pushgateway-7798d479ff-fcnk9 from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.320: INFO: 	Container eric-pm-server-pushgateway ready: true, restart count 0
Mar 30 21:22:27.320: INFO: eric-tm-external-connectivity-frontend-speaker-vmndg from kube-system started at 2020-03-30 20:50:36 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.320: INFO: 	Container speaker ready: true, restart count 0
Mar 30 21:22:27.320: INFO: kube-proxy-746bb from kube-system started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.320: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 30 21:22:27.320: INFO: csi-cinder-nodeplugin-ftdf2 from kube-system started at 2020-03-30 20:48:53 +0000 UTC (2 container statuses recorded)
Mar 30 21:22:27.320: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 30 21:22:27.320: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 30 21:22:27.320: INFO: calico-node-kswgg from kube-system started at 2020-03-30 20:48:23 +0000 UTC (2 container statuses recorded)
Mar 30 21:22:27.320: INFO: 	Container calico-node ready: true, restart count 0
Mar 30 21:22:27.320: INFO: 	Container install-cni ready: true, restart count 0
Mar 30 21:22:27.320: INFO: ccd-license-consumer-5c7ff9fc96-b2hn2 from kube-system started at 2020-03-30 20:55:33 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.320: INFO: 	Container ccd-license-consumer ready: true, restart count 0
Mar 30 21:22:27.320: INFO: eric-lm-combined-server-license-consumer-handler-58597bc9679zxp from kube-system started at 2020-03-30 20:54:37 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.320: INFO: 	Container eric-lm-license-consumer-handler ready: true, restart count 0
Mar 30 21:22:27.321: INFO: 
Logging pods the kubelet thinks is on node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins before test
Mar 30 21:22:27.340: INFO: kube-multus-ds-amd64-q9869 from kube-system started at 2020-03-30 20:48:57 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.340: INFO: 	Container kube-multus ready: true, restart count 0
Mar 30 21:22:27.340: INFO: eric-pm-server-alertmanager-648979cdd-z57mc from monitoring started at 2020-03-30 20:51:51 +0000 UTC (2 container statuses recorded)
Mar 30 21:22:27.340: INFO: 	Container eric-pm-server-alertmanager ready: true, restart count 0
Mar 30 21:22:27.340: INFO: 	Container eric-pm-server-alertmanager-configmap-reload-for-alertmanager ready: true, restart count 0
Mar 30 21:22:27.340: INFO: sonobuoy-systemd-logs-daemon-set-4495c60740044637-gqxkr from sonobuoy started at 2020-03-30 21:04:37 +0000 UTC (2 container statuses recorded)
Mar 30 21:22:27.340: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 30 21:22:27.340: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 30 21:22:27.340: INFO: busybox-scheduling-abe42279-06c8-4b34-9210-d79c42bc004b from kubelet-test-3231 started at 2020-03-30 21:22:18 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.340: INFO: 	Container busybox-scheduling-abe42279-06c8-4b34-9210-d79c42bc004b ready: true, restart count 0
Mar 30 21:22:27.340: INFO: calico-node-lrw4b from kube-system started at 2020-03-30 20:48:27 +0000 UTC (2 container statuses recorded)
Mar 30 21:22:27.340: INFO: 	Container calico-node ready: true, restart count 0
Mar 30 21:22:27.340: INFO: 	Container install-cni ready: true, restart count 0
Mar 30 21:22:27.340: INFO: csi-cinder-nodeplugin-wnrkx from kube-system started at 2020-03-30 20:48:57 +0000 UTC (2 container statuses recorded)
Mar 30 21:22:27.340: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 30 21:22:27.340: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 30 21:22:27.340: INFO: eric-pm-server-node-exporter-v5bmh from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.341: INFO: 	Container eric-pm-server-node-exporter ready: true, restart count 0
Mar 30 21:22:27.341: INFO: kube-proxy-9w8kt from kube-system started at 2020-03-30 20:48:27 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.341: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 30 21:22:27.341: INFO: eric-lm-combined-server-license-server-client-945b6bc4c-5phm5 from kube-system started at 2020-03-30 20:54:37 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.341: INFO: 	Container eric-lm-license-server-client ready: true, restart count 0
Mar 30 21:22:27.341: INFO: eric-tm-external-connectivity-frontend-controller-78f9fd87bvc6l from kube-system started at 2020-03-30 20:50:36 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.341: INFO: 	Container controller ready: true, restart count 0
Mar 30 21:22:27.341: INFO: eric-tm-external-connectivity-frontend-speaker-m4f4p from kube-system started at 2020-03-30 20:50:36 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.341: INFO: 	Container speaker ready: true, restart count 0
Mar 30 21:22:27.341: INFO: metrics-server-8666db6c57-znld5 from kube-system started at 2020-03-30 20:53:13 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.341: INFO: 	Container metrics-server ready: true, restart count 0
Mar 30 21:22:27.341: INFO: 
Logging pods the kubelet thinks is on node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins before test
Mar 30 21:22:27.353: INFO: kube-proxy-4mzsc from kube-system started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.353: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 30 21:22:27.353: INFO: calico-node-hf278 from kube-system started at 2020-03-30 20:48:23 +0000 UTC (2 container statuses recorded)
Mar 30 21:22:27.353: INFO: 	Container calico-node ready: true, restart count 0
Mar 30 21:22:27.353: INFO: 	Container install-cni ready: true, restart count 0
Mar 30 21:22:27.353: INFO: kube-multus-ds-amd64-9mjsk from kube-system started at 2020-03-30 20:48:53 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.353: INFO: 	Container kube-multus ready: true, restart count 0
Mar 30 21:22:27.353: INFO: csi-cinder-nodeplugin-cfp5q from kube-system started at 2020-03-30 20:48:53 +0000 UTC (2 container statuses recorded)
Mar 30 21:22:27.353: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 30 21:22:27.353: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 30 21:22:27.353: INFO: eric-lcm-container-registry-registry-0 from kube-system started at 2020-03-30 20:49:39 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.353: INFO: 	Container registry ready: true, restart count 0
Mar 30 21:22:27.353: INFO: eric-pm-server-node-exporter-9cwgm from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.353: INFO: 	Container eric-pm-server-node-exporter ready: true, restart count 0
Mar 30 21:22:27.353: INFO: eric-pm-server-kube-state-metrics-66f7fbfd44-k7krp from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.353: INFO: 	Container eric-pm-server-kube-state-metrics ready: true, restart count 0
Mar 30 21:22:27.353: INFO: postgresql-postgresql-0 from kube-system started at 2020-03-30 20:53:52 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.353: INFO: 	Container postgresql ready: true, restart count 0
Mar 30 21:22:27.353: INFO: sonobuoy-systemd-logs-daemon-set-4495c60740044637-5rfld from sonobuoy started at 2020-03-30 21:04:37 +0000 UTC (2 container statuses recorded)
Mar 30 21:22:27.353: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 30 21:22:27.353: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 30 21:22:27.353: INFO: eric-tm-external-connectivity-frontend-speaker-8tmb7 from kube-system started at 2020-03-30 20:50:36 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.353: INFO: 	Container speaker ready: true, restart count 0
Mar 30 21:22:27.353: INFO: 
Logging pods the kubelet thinks is on node worker-pool1-zo88v95j-eccd-ci-os-12-jenkins before test
Mar 30 21:22:27.369: INFO: kube-multus-ds-amd64-6t2pw from kube-system started at 2020-03-30 20:48:21 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.369: INFO: 	Container kube-multus ready: true, restart count 0
Mar 30 21:22:27.369: INFO: eric-pm-server-node-exporter-ktbpn from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.369: INFO: 	Container eric-pm-server-node-exporter ready: true, restart count 0
Mar 30 21:22:27.369: INFO: sonobuoy-e2e-job-14477a96f0f5465c from sonobuoy started at 2020-03-30 21:04:37 +0000 UTC (2 container statuses recorded)
Mar 30 21:22:27.369: INFO: 	Container e2e ready: true, restart count 0
Mar 30 21:22:27.369: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 30 21:22:27.369: INFO: calico-node-xk89l from kube-system started at 2020-03-30 20:47:51 +0000 UTC (2 container statuses recorded)
Mar 30 21:22:27.369: INFO: 	Container calico-node ready: true, restart count 0
Mar 30 21:22:27.369: INFO: 	Container install-cni ready: true, restart count 0
Mar 30 21:22:27.369: INFO: csi-cinder-nodeplugin-qgrf4 from kube-system started at 2020-03-30 20:48:21 +0000 UTC (2 container statuses recorded)
Mar 30 21:22:27.369: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 30 21:22:27.369: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 30 21:22:27.369: INFO: nginx-ingress-controller-78f766b979-bfx4q from ingress-nginx started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.369: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 30 21:22:27.369: INFO: default-http-backend-6664c884c9-dxczc from ingress-nginx started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.369: INFO: 	Container default-http-backend ready: true, restart count 0
Mar 30 21:22:27.369: INFO: sonobuoy-systemd-logs-daemon-set-4495c60740044637-v7t76 from sonobuoy started at 2020-03-30 21:04:37 +0000 UTC (2 container statuses recorded)
Mar 30 21:22:27.369: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 30 21:22:27.369: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 30 21:22:27.369: INFO: kube-proxy-pcd5r from kube-system started at 2020-03-30 20:47:51 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.369: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 30 21:22:27.369: INFO: nginx-ingress-controller-78f766b979-wf7qb from ingress-nginx started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.369: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 30 21:22:27.369: INFO: eric-tm-external-connectivity-frontend-speaker-zwnfr from kube-system started at 2020-03-30 20:50:36 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.369: INFO: 	Container speaker ready: true, restart count 0
Mar 30 21:22:27.369: INFO: tiller-deploy-cd77547bd-2pv7h from kube-system started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 21:22:27.369: INFO: 	Container tiller ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-904c750b-997f-43d7-959a-bc524adba3ae 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-904c750b-997f-43d7-959a-bc524adba3ae off the node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins
STEP: verifying the node doesn't have the label kubernetes.io/e2e-904c750b-997f-43d7-959a-bc524adba3ae
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:22:45.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1423" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:18.452 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":278,"completed":51,"skipped":1014,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:22:45.514: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5403
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 30 21:22:45.715: INFO: Waiting up to 5m0s for pod "downwardapi-volume-77135506-73b7-4f6b-9c40-1f707ab3d0a4" in namespace "downward-api-5403" to be "success or failure"
Mar 30 21:22:45.719: INFO: Pod "downwardapi-volume-77135506-73b7-4f6b-9c40-1f707ab3d0a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.29143ms
Mar 30 21:22:47.724: INFO: Pod "downwardapi-volume-77135506-73b7-4f6b-9c40-1f707ab3d0a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008515283s
Mar 30 21:22:49.730: INFO: Pod "downwardapi-volume-77135506-73b7-4f6b-9c40-1f707ab3d0a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014317821s
Mar 30 21:22:51.735: INFO: Pod "downwardapi-volume-77135506-73b7-4f6b-9c40-1f707ab3d0a4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019140715s
Mar 30 21:22:53.742: INFO: Pod "downwardapi-volume-77135506-73b7-4f6b-9c40-1f707ab3d0a4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026483616s
Mar 30 21:22:55.749: INFO: Pod "downwardapi-volume-77135506-73b7-4f6b-9c40-1f707ab3d0a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.033323822s
STEP: Saw pod success
Mar 30 21:22:55.749: INFO: Pod "downwardapi-volume-77135506-73b7-4f6b-9c40-1f707ab3d0a4" satisfied condition "success or failure"
Mar 30 21:22:55.755: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod downwardapi-volume-77135506-73b7-4f6b-9c40-1f707ab3d0a4 container client-container: <nil>
STEP: delete the pod
Mar 30 21:22:55.830: INFO: Waiting for pod downwardapi-volume-77135506-73b7-4f6b-9c40-1f707ab3d0a4 to disappear
Mar 30 21:22:55.846: INFO: Pod downwardapi-volume-77135506-73b7-4f6b-9c40-1f707ab3d0a4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:22:55.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5403" for this suite.

• [SLOW TEST:10.347 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":278,"completed":52,"skipped":1025,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:22:55.863: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8850
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 21:22:56.068: INFO: Creating deployment "webserver-deployment"
Mar 30 21:22:56.131: INFO: Waiting for observed generation 1
Mar 30 21:22:58.200: INFO: Waiting for all required pods to come up
Mar 30 21:22:58.207: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar 30 21:23:12.221: INFO: Waiting for deployment "webserver-deployment" to complete
Mar 30 21:23:12.231: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar 30 21:23:12.244: INFO: Updating deployment webserver-deployment
Mar 30 21:23:12.244: INFO: Waiting for observed generation 2
Mar 30 21:23:14.261: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar 30 21:23:14.265: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar 30 21:23:14.269: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 30 21:23:14.280: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar 30 21:23:14.280: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar 30 21:23:14.284: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 30 21:23:14.290: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar 30 21:23:14.290: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar 30 21:23:14.321: INFO: Updating deployment webserver-deployment
Mar 30 21:23:14.321: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar 30 21:23:14.330: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar 30 21:23:14.337: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Mar 30 21:23:14.371: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8850 /apis/apps/v1/namespaces/deployment-8850/deployments/webserver-deployment 21969aeb-f931-4ffd-a6bf-bf3cc9087a71 16848 3 2020-03-30 21:22:56 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001138cd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-03-30 21:23:05 +0000 UTC,LastTransitionTime:2020-03-30 21:23:05 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-03-30 21:23:12 +0000 UTC,LastTransitionTime:2020-03-30 21:22:56 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar 30 21:23:14.380: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-8850 /apis/apps/v1/namespaces/deployment-8850/replicasets/webserver-deployment-c7997dcc8 e935667b-e9be-4f1f-a4e2-9bf50b00dae4 16851 3 2020-03-30 21:23:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 21969aeb-f931-4ffd-a6bf-bf3cc9087a71 0xc001139767 0xc001139768}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0011397d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 30 21:23:14.380: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar 30 21:23:14.380: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-8850 /apis/apps/v1/namespaces/deployment-8850/replicasets/webserver-deployment-595b5b9587 e0374662-e0d4-4883-95e8-e388614566d2 16849 3 2020-03-30 21:22:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 21969aeb-f931-4ffd-a6bf-bf3cc9087a71 0xc0011396a7 0xc0011396a8}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001139708 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar 30 21:23:14.395: INFO: Pod "webserver-deployment-595b5b9587-44jb8" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-44jb8 webserver-deployment-595b5b9587- deployment-8850 /api/v1/namespaces/deployment-8850/pods/webserver-deployment-595b5b9587-44jb8 c71871c9-048e-4c05-8f78-3b28693b1084 16766 0 2020-03-30 21:22:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.125.8"
    ],
    "dns": {}
}] kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 e0374662-e0d4-4883-95e8-e388614566d2 0xc000696cb7 0xc000696cb8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-msd9c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-msd9c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-msd9c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:22:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:22:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.4,PodIP:192.168.125.8,StartTime:2020-03-30 21:22:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-30 21:23:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://c03f1630ce4e728395c5fcfabef6ffe2db31a508ef8e2704a57c3c99cd298506,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.125.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 21:23:14.396: INFO: Pod "webserver-deployment-595b5b9587-8jfbv" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-8jfbv webserver-deployment-595b5b9587- deployment-8850 /api/v1/namespaces/deployment-8850/pods/webserver-deployment-595b5b9587-8jfbv 6ae4c589-cdb2-4a64-87ff-e6a0ff90abfc 16728 0 2020-03-30 21:22:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.185.214"
    ],
    "dns": {}
}] kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 e0374662-e0d4-4883-95e8-e388614566d2 0xc000697ad0 0xc000697ad1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-msd9c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-msd9c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-msd9c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-vvs2j292-eccd-ci-os-12-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:22:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:22:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.8,PodIP:192.168.185.214,StartTime:2020-03-30 21:22:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-30 21:23:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://f1179ad72b790c9051912706c01050ba9deb1791cd6b36f8a0f5d3a794211195,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.185.214,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 21:23:14.397: INFO: Pod "webserver-deployment-595b5b9587-9z7pv" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-9z7pv webserver-deployment-595b5b9587- deployment-8850 /api/v1/namespaces/deployment-8850/pods/webserver-deployment-595b5b9587-9z7pv 1be1b710-9029-402c-9505-ada4241824a8 16686 0 2020-03-30 21:22:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.14.110"
    ],
    "dns": {}
}] kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 e0374662-e0d4-4883-95e8-e388614566d2 0xc000697f50 0xc000697f51}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-msd9c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-msd9c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-msd9c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-tjim0te5-eccd-ci-os-12-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:22:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:22:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.5,PodIP:192.168.14.110,StartTime:2020-03-30 21:22:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-30 21:23:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://5980d449ee3caf2d840951980bd656297da87f616e39f8f1e30507375012a336,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.14.110,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 21:23:14.397: INFO: Pod "webserver-deployment-595b5b9587-b4hkm" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-b4hkm webserver-deployment-595b5b9587- deployment-8850 /api/v1/namespaces/deployment-8850/pods/webserver-deployment-595b5b9587-b4hkm a4a88672-2b46-4797-aa95-21136a54e51c 16715 0 2020-03-30 21:22:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.10.205"
    ],
    "dns": {}
}] kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 e0374662-e0d4-4883-95e8-e388614566d2 0xc000648100 0xc000648101}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-msd9c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-msd9c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-msd9c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-zo88v95j-eccd-ci-os-12-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:22:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:22:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.2,PodIP:192.168.10.205,StartTime:2020-03-30 21:22:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-30 21:23:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://57fdf4ca535a7e44dbca1a35337bd99385e5ca1798085ac3c9465c23a61a5075,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.10.205,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 21:23:14.398: INFO: Pod "webserver-deployment-595b5b9587-f75th" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-f75th webserver-deployment-595b5b9587- deployment-8850 /api/v1/namespaces/deployment-8850/pods/webserver-deployment-595b5b9587-f75th 68f60ce7-3618-4922-8e58-e9804524a5d2 16855 0 2020-03-30 21:23:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 e0374662-e0d4-4883-95e8-e388614566d2 0xc0006485b0 0xc0006485b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-msd9c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-msd9c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-msd9c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-vvs2j292-eccd-ci-os-12-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 21:23:14.398: INFO: Pod "webserver-deployment-595b5b9587-lb92k" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-lb92k webserver-deployment-595b5b9587- deployment-8850 /api/v1/namespaces/deployment-8850/pods/webserver-deployment-595b5b9587-lb92k 833af493-4303-488a-a4d9-a14bc219c68a 16774 0 2020-03-30 21:22:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.125.9"
    ],
    "dns": {}
}] kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 e0374662-e0d4-4883-95e8-e388614566d2 0xc0006488d0 0xc0006488d1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-msd9c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-msd9c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-msd9c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:22:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:22:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.4,PodIP:192.168.125.9,StartTime:2020-03-30 21:22:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-30 21:23:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://9d858f3f2040e51151bc67c39bc564a82ea4d30afaba3356f98a7e3ebe99aefe,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.125.9,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 21:23:14.399: INFO: Pod "webserver-deployment-595b5b9587-mfpjj" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mfpjj webserver-deployment-595b5b9587- deployment-8850 /api/v1/namespaces/deployment-8850/pods/webserver-deployment-595b5b9587-mfpjj 585338b1-a20d-40b6-93f1-5677776b7f2d 16857 0 2020-03-30 21:23:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 e0374662-e0d4-4883-95e8-e388614566d2 0xc000649d90 0xc000649d91}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-msd9c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-msd9c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-msd9c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 21:23:14.399: INFO: Pod "webserver-deployment-595b5b9587-qtp4l" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qtp4l webserver-deployment-595b5b9587- deployment-8850 /api/v1/namespaces/deployment-8850/pods/webserver-deployment-595b5b9587-qtp4l 61e40c09-35de-4c0c-9a1c-156b11efcecf 16856 0 2020-03-30 21:23:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 e0374662-e0d4-4883-95e8-e388614566d2 0xc0005fc160 0xc0005fc161}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-msd9c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-msd9c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-msd9c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-tjim0te5-eccd-ci-os-12-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 21:23:14.400: INFO: Pod "webserver-deployment-595b5b9587-rhgkg" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rhgkg webserver-deployment-595b5b9587- deployment-8850 /api/v1/namespaces/deployment-8850/pods/webserver-deployment-595b5b9587-rhgkg 40b5b610-25c8-4da7-b81c-e6e0f9d5dc3c 16722 0 2020-03-30 21:22:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.14.111"
    ],
    "dns": {}
}] kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 e0374662-e0d4-4883-95e8-e388614566d2 0xc0004f00c0 0xc0004f00c1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-msd9c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-msd9c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-msd9c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-tjim0te5-eccd-ci-os-12-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:22:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:22:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.5,PodIP:192.168.14.111,StartTime:2020-03-30 21:22:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-30 21:23:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://a79fc46ebd5d2bd2196ca80d61aa49bff75ad01b333d3a4f9a9ecbdc7d0536d0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.14.111,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 21:23:14.400: INFO: Pod "webserver-deployment-595b5b9587-xdrsh" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xdrsh webserver-deployment-595b5b9587- deployment-8850 /api/v1/namespaces/deployment-8850/pods/webserver-deployment-595b5b9587-xdrsh 6d073fa4-d8fd-4167-9447-279c355fec63 16731 0 2020-03-30 21:22:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.185.215"
    ],
    "dns": {}
}] kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 e0374662-e0d4-4883-95e8-e388614566d2 0xc00036ec90 0xc00036ec91}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-msd9c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-msd9c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-msd9c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-vvs2j292-eccd-ci-os-12-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:22:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:22:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.8,PodIP:192.168.185.215,StartTime:2020-03-30 21:22:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-30 21:23:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://baca9305bf0ac9182f0ba948e67577d484e69a8af2425fbf888453aa21b95c0e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.185.215,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 21:23:14.401: INFO: Pod "webserver-deployment-595b5b9587-xwjgf" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xwjgf webserver-deployment-595b5b9587- deployment-8850 /api/v1/namespaces/deployment-8850/pods/webserver-deployment-595b5b9587-xwjgf b2b96fa9-dac9-41ce-a311-2f972221c3b6 16670 0 2020-03-30 21:22:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.10.204"
    ],
    "dns": {}
}] kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 e0374662-e0d4-4883-95e8-e388614566d2 0xc00036f0a0 0xc00036f0a1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-msd9c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-msd9c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-msd9c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-zo88v95j-eccd-ci-os-12-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:22:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:22:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.2,PodIP:192.168.10.204,StartTime:2020-03-30 21:22:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-30 21:23:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://adfe2ef50630b46578e29a51568e7ecdba0de8c5116470cef96e93da15539410,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.10.204,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 21:23:14.401: INFO: Pod "webserver-deployment-c7997dcc8-56rgw" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-56rgw webserver-deployment-c7997dcc8- deployment-8850 /api/v1/namespaces/deployment-8850/pods/webserver-deployment-c7997dcc8-56rgw b2cfc359-f310-44f2-ba2f-70e5879dc9bb 16859 0 2020-03-30 21:23:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 e935667b-e9be-4f1f-a4e2-9bf50b00dae4 0xc000052d50 0xc000052d51}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-msd9c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-msd9c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-msd9c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 21:23:14.402: INFO: Pod "webserver-deployment-c7997dcc8-5jnnr" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-5jnnr webserver-deployment-c7997dcc8- deployment-8850 /api/v1/namespaces/deployment-8850/pods/webserver-deployment-c7997dcc8-5jnnr 5b41302e-d021-4246-a51f-901dbe6721d8 16828 0 2020-03-30 21:23:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 e935667b-e9be-4f1f-a4e2-9bf50b00dae4 0xc0000530a7 0xc0000530a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-msd9c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-msd9c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-msd9c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.4,PodIP:,StartTime:2020-03-30 21:23:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 21:23:14.402: INFO: Pod "webserver-deployment-c7997dcc8-9pj8d" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-9pj8d webserver-deployment-c7997dcc8- deployment-8850 /api/v1/namespaces/deployment-8850/pods/webserver-deployment-c7997dcc8-9pj8d 43bd8c11-e9d6-4d38-aedf-50b64c941dbe 16833 0 2020-03-30 21:23:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 e935667b-e9be-4f1f-a4e2-9bf50b00dae4 0xc0003b8510 0xc0003b8511}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-msd9c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-msd9c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-msd9c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-tjim0te5-eccd-ci-os-12-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.5,PodIP:,StartTime:2020-03-30 21:23:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 21:23:14.403: INFO: Pod "webserver-deployment-c7997dcc8-gzj57" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-gzj57 webserver-deployment-c7997dcc8- deployment-8850 /api/v1/namespaces/deployment-8850/pods/webserver-deployment-c7997dcc8-gzj57 898d8b35-ab8a-460c-a3f6-06083e04d123 16817 0 2020-03-30 21:23:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 e935667b-e9be-4f1f-a4e2-9bf50b00dae4 0xc0003b9520 0xc0003b9521}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-msd9c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-msd9c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-msd9c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-vvs2j292-eccd-ci-os-12-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.8,PodIP:,StartTime:2020-03-30 21:23:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 21:23:14.403: INFO: Pod "webserver-deployment-c7997dcc8-qcn4b" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-qcn4b webserver-deployment-c7997dcc8- deployment-8850 /api/v1/namespaces/deployment-8850/pods/webserver-deployment-c7997dcc8-qcn4b b40b0b33-6bdf-42b1-973b-00e13510874e 16807 0 2020-03-30 21:23:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 e935667b-e9be-4f1f-a4e2-9bf50b00dae4 0xc0003b9b20 0xc0003b9b21}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-msd9c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-msd9c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-msd9c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-tjim0te5-eccd-ci-os-12-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.5,PodIP:,StartTime:2020-03-30 21:23:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 21:23:14.403: INFO: Pod "webserver-deployment-c7997dcc8-zpvw5" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-zpvw5 webserver-deployment-c7997dcc8- deployment-8850 /api/v1/namespaces/deployment-8850/pods/webserver-deployment-c7997dcc8-zpvw5 2ba5fd35-e05d-412b-8052-d7e4d3cd2eae 16808 0 2020-03-30 21:23:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 e935667b-e9be-4f1f-a4e2-9bf50b00dae4 0xc0003b9e50 0xc0003b9e51}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-msd9c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-msd9c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-msd9c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-zo88v95j-eccd-ci-os-12-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:23:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.2,PodIP:,StartTime:2020-03-30 21:23:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:23:14.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8850" for this suite.

• [SLOW TEST:18.599 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":278,"completed":53,"skipped":1034,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:23:14.464: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-122
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service nodeport-service with the type=NodePort in namespace services-122
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-122
STEP: creating replication controller externalsvc in namespace services-122
I0330 21:23:14.869329      20 runners.go:189] Created replication controller with name: externalsvc, namespace: services-122, replica count: 2
I0330 21:23:17.921112      20 runners.go:189] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0330 21:23:20.924084      20 runners.go:189] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0330 21:23:23.925121      20 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Mar 30 21:23:23.987: INFO: Creating new exec pod
Mar 30 21:23:34.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=services-122 execpod97jll -- /bin/sh -x -c nslookup nodeport-service'
Mar 30 21:23:34.339: INFO: stderr: "+ nslookup nodeport-service\n"
Mar 30 21:23:34.339: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-122.svc.cluster.local\tcanonical name = externalsvc.services-122.svc.cluster.local.\nName:\texternalsvc.services-122.svc.cluster.local\nAddress: 10.103.197.151\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-122, will wait for the garbage collector to delete the pods
Mar 30 21:23:34.411: INFO: Deleting ReplicationController externalsvc took: 14.137238ms
Mar 30 21:23:35.011: INFO: Terminating ReplicationController externalsvc pods took: 600.363231ms
Mar 30 21:23:39.776: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:23:39.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-122" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:25.363 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":278,"completed":54,"skipped":1051,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:23:39.829: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5502
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-46f3d7bc-6a80-472b-b031-7252e18710f8
STEP: Creating a pod to test consume configMaps
Mar 30 21:23:40.038: INFO: Waiting up to 5m0s for pod "pod-configmaps-66470ec1-20bc-44f5-afc2-702d8660b994" in namespace "configmap-5502" to be "success or failure"
Mar 30 21:23:40.047: INFO: Pod "pod-configmaps-66470ec1-20bc-44f5-afc2-702d8660b994": Phase="Pending", Reason="", readiness=false. Elapsed: 8.221058ms
Mar 30 21:23:42.054: INFO: Pod "pod-configmaps-66470ec1-20bc-44f5-afc2-702d8660b994": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0153083s
Mar 30 21:23:44.059: INFO: Pod "pod-configmaps-66470ec1-20bc-44f5-afc2-702d8660b994": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020861182s
Mar 30 21:23:46.065: INFO: Pod "pod-configmaps-66470ec1-20bc-44f5-afc2-702d8660b994": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026027067s
Mar 30 21:23:48.072: INFO: Pod "pod-configmaps-66470ec1-20bc-44f5-afc2-702d8660b994": Phase="Running", Reason="", readiness=true. Elapsed: 8.033391672s
Mar 30 21:23:50.079: INFO: Pod "pod-configmaps-66470ec1-20bc-44f5-afc2-702d8660b994": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.040095657s
STEP: Saw pod success
Mar 30 21:23:50.079: INFO: Pod "pod-configmaps-66470ec1-20bc-44f5-afc2-702d8660b994" satisfied condition "success or failure"
Mar 30 21:23:50.082: INFO: Trying to get logs from node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins pod pod-configmaps-66470ec1-20bc-44f5-afc2-702d8660b994 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 30 21:23:50.115: INFO: Waiting for pod pod-configmaps-66470ec1-20bc-44f5-afc2-702d8660b994 to disappear
Mar 30 21:23:50.119: INFO: Pod pod-configmaps-66470ec1-20bc-44f5-afc2-702d8660b994 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:23:50.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5502" for this suite.

• [SLOW TEST:10.316 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":55,"skipped":1071,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:23:50.150: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9782
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9782.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9782.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9782.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9782.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9782.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9782.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9782.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9782.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9782.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9782.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 30 21:24:12.405: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:12.410: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:12.417: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:12.422: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:12.436: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:12.442: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:12.447: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:12.452: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:12.470: INFO: Lookups using dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9782.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9782.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local jessie_udp@dns-test-service-2.dns-9782.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9782.svc.cluster.local]

Mar 30 21:24:17.479: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:17.484: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:17.488: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:17.494: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:17.508: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:17.513: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:17.518: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:17.523: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:17.532: INFO: Lookups using dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9782.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9782.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local jessie_udp@dns-test-service-2.dns-9782.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9782.svc.cluster.local]

Mar 30 21:24:22.477: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:22.483: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:22.488: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:22.491: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:22.504: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:22.509: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:22.514: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:22.519: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:22.528: INFO: Lookups using dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9782.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9782.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local jessie_udp@dns-test-service-2.dns-9782.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9782.svc.cluster.local]

Mar 30 21:24:27.480: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:27.485: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:27.489: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:27.494: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:27.512: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:27.517: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:27.521: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:27.527: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:27.537: INFO: Lookups using dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9782.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9782.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local jessie_udp@dns-test-service-2.dns-9782.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9782.svc.cluster.local]

Mar 30 21:24:32.509: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local from pod dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4: the server could not find the requested resource (get pods dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4)
Mar 30 21:24:32.526: INFO: Lookups using dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4 failed for: [jessie_tcp@dns-querier-2.dns-test-service-2.dns-9782.svc.cluster.local]

Mar 30 21:24:37.537: INFO: DNS probes using dns-9782/dns-test-c4188aa9-0777-4e1c-ae0e-93c5b6306cf4 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:24:37.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9782" for this suite.

• [SLOW TEST:47.494 seconds]
[sig-network] DNS
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":278,"completed":56,"skipped":1132,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:24:37.646: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9117
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 30 21:24:37.851: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d52e2508-8943-4e81-aeca-629d87277158" in namespace "downward-api-9117" to be "success or failure"
Mar 30 21:24:37.855: INFO: Pod "downwardapi-volume-d52e2508-8943-4e81-aeca-629d87277158": Phase="Pending", Reason="", readiness=false. Elapsed: 4.187949ms
Mar 30 21:24:39.860: INFO: Pod "downwardapi-volume-d52e2508-8943-4e81-aeca-629d87277158": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009482212s
Mar 30 21:24:41.873: INFO: Pod "downwardapi-volume-d52e2508-8943-4e81-aeca-629d87277158": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021741278s
Mar 30 21:24:43.879: INFO: Pod "downwardapi-volume-d52e2508-8943-4e81-aeca-629d87277158": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028145526s
Mar 30 21:24:45.884: INFO: Pod "downwardapi-volume-d52e2508-8943-4e81-aeca-629d87277158": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.032973861s
STEP: Saw pod success
Mar 30 21:24:45.884: INFO: Pod "downwardapi-volume-d52e2508-8943-4e81-aeca-629d87277158" satisfied condition "success or failure"
Mar 30 21:24:45.892: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod downwardapi-volume-d52e2508-8943-4e81-aeca-629d87277158 container client-container: <nil>
STEP: delete the pod
Mar 30 21:24:45.922: INFO: Waiting for pod downwardapi-volume-d52e2508-8943-4e81-aeca-629d87277158 to disappear
Mar 30 21:24:45.926: INFO: Pod downwardapi-volume-d52e2508-8943-4e81-aeca-629d87277158 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:24:45.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9117" for this suite.

• [SLOW TEST:8.296 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":278,"completed":57,"skipped":1138,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:24:45.948: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2293
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-983706b2-541b-41a0-8ddf-99f81dda5144
STEP: Creating a pod to test consume configMaps
Mar 30 21:24:46.173: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f302def5-5ca8-4fd7-903d-99c08bf42b54" in namespace "projected-2293" to be "success or failure"
Mar 30 21:24:46.179: INFO: Pod "pod-projected-configmaps-f302def5-5ca8-4fd7-903d-99c08bf42b54": Phase="Pending", Reason="", readiness=false. Elapsed: 5.220628ms
Mar 30 21:24:48.185: INFO: Pod "pod-projected-configmaps-f302def5-5ca8-4fd7-903d-99c08bf42b54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011612427s
Mar 30 21:24:50.190: INFO: Pod "pod-projected-configmaps-f302def5-5ca8-4fd7-903d-99c08bf42b54": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016331973s
Mar 30 21:24:52.195: INFO: Pod "pod-projected-configmaps-f302def5-5ca8-4fd7-903d-99c08bf42b54": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021911253s
Mar 30 21:24:54.202: INFO: Pod "pod-projected-configmaps-f302def5-5ca8-4fd7-903d-99c08bf42b54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.028219141s
STEP: Saw pod success
Mar 30 21:24:54.202: INFO: Pod "pod-projected-configmaps-f302def5-5ca8-4fd7-903d-99c08bf42b54" satisfied condition "success or failure"
Mar 30 21:24:54.206: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-projected-configmaps-f302def5-5ca8-4fd7-903d-99c08bf42b54 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 30 21:24:54.244: INFO: Waiting for pod pod-projected-configmaps-f302def5-5ca8-4fd7-903d-99c08bf42b54 to disappear
Mar 30 21:24:54.250: INFO: Pod pod-projected-configmaps-f302def5-5ca8-4fd7-903d-99c08bf42b54 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:24:54.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2293" for this suite.

• [SLOW TEST:8.316 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":278,"completed":58,"skipped":1153,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:24:54.265: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6771
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl replace
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1897
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 30 21:24:54.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-6771'
Mar 30 21:24:54.600: INFO: stderr: ""
Mar 30 21:24:54.600: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Mar 30 21:25:04.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pod e2e-test-httpd-pod --namespace=kubectl-6771 -o json'
Mar 30 21:25:04.750: INFO: stderr: ""
Mar 30 21:25:04.751: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"192.168.14.119\\\"\\n    ],\\n    \\\"dns\\\": {}\\n}]\",\n            \"kubernetes.io/psp\": \"ccd-privileged\"\n        },\n        \"creationTimestamp\": \"2020-03-30T21:24:54Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6771\",\n        \"resourceVersion\": \"17912\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6771/pods/e2e-test-httpd-pod\",\n        \"uid\": \"f3fc2d25-3a8c-4919-89f8-90e07b4df76d\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-chcnp\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"worker-pool1-tjim0te5-eccd-ci-os-12-jenkins\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-chcnp\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-chcnp\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-30T21:24:54Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-30T21:25:01Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-30T21:25:01Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-30T21:24:54Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://cbbae112b90bd3ffac4e7b8e6e407a6f4ad90f62b4e30bf2d9eb0cf37c34ab26\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-03-30T21:25:01Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.10.5\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.14.119\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.14.119\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-03-30T21:24:54Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar 30 21:25:04.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 replace -f - --namespace=kubectl-6771'
Mar 30 21:25:05.148: INFO: stderr: ""
Mar 30 21:25:05.148: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1902
Mar 30 21:25:05.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 delete pods e2e-test-httpd-pod --namespace=kubectl-6771'
Mar 30 21:25:07.118: INFO: stderr: ""
Mar 30 21:25:07.118: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:25:07.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6771" for this suite.

• [SLOW TEST:12.869 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1893
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":278,"completed":59,"skipped":1155,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:25:07.135: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4086
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0330 21:25:47.404975      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 30 21:25:47.405: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:25:47.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4086" for this suite.

• [SLOW TEST:40.287 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":278,"completed":60,"skipped":1157,"failed":0}
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:25:47.422: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2494
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating secret secrets-2494/secret-test-7ee81128-8a69-4be8-aad4-e9f7c8758419
STEP: Creating a pod to test consume secrets
Mar 30 21:25:47.630: INFO: Waiting up to 5m0s for pod "pod-configmaps-deca782c-d9a6-4e1f-b2b7-5fcf4a7c0ccd" in namespace "secrets-2494" to be "success or failure"
Mar 30 21:25:47.634: INFO: Pod "pod-configmaps-deca782c-d9a6-4e1f-b2b7-5fcf4a7c0ccd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.955568ms
Mar 30 21:25:49.638: INFO: Pod "pod-configmaps-deca782c-d9a6-4e1f-b2b7-5fcf4a7c0ccd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008096869s
Mar 30 21:25:51.644: INFO: Pod "pod-configmaps-deca782c-d9a6-4e1f-b2b7-5fcf4a7c0ccd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01323527s
Mar 30 21:25:53.652: INFO: Pod "pod-configmaps-deca782c-d9a6-4e1f-b2b7-5fcf4a7c0ccd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021503666s
Mar 30 21:25:55.660: INFO: Pod "pod-configmaps-deca782c-d9a6-4e1f-b2b7-5fcf4a7c0ccd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.029605817s
Mar 30 21:25:57.665: INFO: Pod "pod-configmaps-deca782c-d9a6-4e1f-b2b7-5fcf4a7c0ccd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.035070887s
STEP: Saw pod success
Mar 30 21:25:57.666: INFO: Pod "pod-configmaps-deca782c-d9a6-4e1f-b2b7-5fcf4a7c0ccd" satisfied condition "success or failure"
Mar 30 21:25:57.672: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-configmaps-deca782c-d9a6-4e1f-b2b7-5fcf4a7c0ccd container env-test: <nil>
STEP: delete the pod
Mar 30 21:25:57.705: INFO: Waiting for pod pod-configmaps-deca782c-d9a6-4e1f-b2b7-5fcf4a7c0ccd to disappear
Mar 30 21:25:57.708: INFO: Pod pod-configmaps-deca782c-d9a6-4e1f-b2b7-5fcf4a7c0ccd no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:25:57.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2494" for this suite.

• [SLOW TEST:10.302 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":278,"completed":61,"skipped":1157,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:25:57.727: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3129
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 21:25:57.978: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar 30 21:25:58.008: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:25:58.009: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:25:58.009: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:25:58.014: INFO: Number of nodes with available pods: 0
Mar 30 21:25:58.014: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:25:59.023: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:25:59.023: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:25:59.023: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:25:59.028: INFO: Number of nodes with available pods: 0
Mar 30 21:25:59.028: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:26:00.022: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:00.022: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:00.022: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:00.027: INFO: Number of nodes with available pods: 0
Mar 30 21:26:00.027: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:26:01.023: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:01.024: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:01.024: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:01.029: INFO: Number of nodes with available pods: 0
Mar 30 21:26:01.029: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:26:02.022: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:02.023: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:02.023: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:02.027: INFO: Number of nodes with available pods: 0
Mar 30 21:26:02.027: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:26:03.022: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:03.022: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:03.022: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:03.026: INFO: Number of nodes with available pods: 0
Mar 30 21:26:03.026: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:26:04.024: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:04.024: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:04.024: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:04.028: INFO: Number of nodes with available pods: 0
Mar 30 21:26:04.028: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:26:05.026: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:05.026: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:05.026: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:05.041: INFO: Number of nodes with available pods: 0
Mar 30 21:26:05.041: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:26:06.022: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:06.022: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:06.022: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:06.027: INFO: Number of nodes with available pods: 2
Mar 30 21:26:06.027: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:26:07.020: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:07.020: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:07.020: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:07.026: INFO: Number of nodes with available pods: 4
Mar 30 21:26:07.026: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar 30 21:26:07.099: INFO: Wrong image for pod: daemon-set-7669x. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:07.099: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:07.099: INFO: Wrong image for pod: daemon-set-g868q. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:07.099: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:07.114: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:07.114: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:07.114: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:08.120: INFO: Wrong image for pod: daemon-set-7669x. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:08.120: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:08.120: INFO: Wrong image for pod: daemon-set-g868q. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:08.120: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:08.126: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:08.126: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:08.126: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:09.119: INFO: Wrong image for pod: daemon-set-7669x. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:09.119: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:09.119: INFO: Wrong image for pod: daemon-set-g868q. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:09.119: INFO: Pod daemon-set-g868q is not available
Mar 30 21:26:09.119: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:09.125: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:09.125: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:09.125: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:10.123: INFO: Wrong image for pod: daemon-set-7669x. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:10.123: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:10.123: INFO: Pod daemon-set-q4sfs is not available
Mar 30 21:26:10.123: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:10.130: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:10.130: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:10.130: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:11.133: INFO: Wrong image for pod: daemon-set-7669x. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:11.133: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:11.133: INFO: Pod daemon-set-q4sfs is not available
Mar 30 21:26:11.133: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:11.138: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:11.138: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:11.138: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:12.122: INFO: Wrong image for pod: daemon-set-7669x. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:12.122: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:12.122: INFO: Pod daemon-set-q4sfs is not available
Mar 30 21:26:12.122: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:12.129: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:12.129: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:12.129: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:13.123: INFO: Wrong image for pod: daemon-set-7669x. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:13.124: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:13.124: INFO: Pod daemon-set-q4sfs is not available
Mar 30 21:26:13.124: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:13.131: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:13.131: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:13.131: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:14.121: INFO: Wrong image for pod: daemon-set-7669x. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:14.121: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:14.121: INFO: Pod daemon-set-q4sfs is not available
Mar 30 21:26:14.121: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:14.127: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:14.127: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:14.127: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:15.121: INFO: Wrong image for pod: daemon-set-7669x. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:15.122: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:15.122: INFO: Pod daemon-set-q4sfs is not available
Mar 30 21:26:15.122: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:15.129: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:15.129: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:15.129: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:16.121: INFO: Wrong image for pod: daemon-set-7669x. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:16.121: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:16.121: INFO: Pod daemon-set-q4sfs is not available
Mar 30 21:26:16.121: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:16.130: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:16.130: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:16.130: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:17.121: INFO: Wrong image for pod: daemon-set-7669x. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:17.121: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:17.121: INFO: Pod daemon-set-q4sfs is not available
Mar 30 21:26:17.121: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:17.127: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:17.127: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:17.127: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:18.125: INFO: Wrong image for pod: daemon-set-7669x. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:18.125: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:18.125: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:18.132: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:18.133: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:18.133: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:19.121: INFO: Wrong image for pod: daemon-set-7669x. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:19.121: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:19.121: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:19.128: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:19.129: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:19.129: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:20.121: INFO: Wrong image for pod: daemon-set-7669x. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:20.121: INFO: Pod daemon-set-7669x is not available
Mar 30 21:26:20.121: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:20.121: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:20.127: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:20.127: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:20.127: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:21.129: INFO: Wrong image for pod: daemon-set-7669x. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:21.129: INFO: Pod daemon-set-7669x is not available
Mar 30 21:26:21.129: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:21.129: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:21.138: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:21.138: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:21.138: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:22.122: INFO: Wrong image for pod: daemon-set-7669x. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:22.122: INFO: Pod daemon-set-7669x is not available
Mar 30 21:26:22.122: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:22.122: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:22.128: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:22.129: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:22.129: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:23.120: INFO: Wrong image for pod: daemon-set-7669x. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:23.120: INFO: Pod daemon-set-7669x is not available
Mar 30 21:26:23.120: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:23.120: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:23.134: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:23.134: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:23.134: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:24.121: INFO: Wrong image for pod: daemon-set-7669x. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:24.121: INFO: Pod daemon-set-7669x is not available
Mar 30 21:26:24.121: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:24.121: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:24.130: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:24.130: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:24.130: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:25.122: INFO: Wrong image for pod: daemon-set-7669x. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:25.122: INFO: Pod daemon-set-7669x is not available
Mar 30 21:26:25.122: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:25.122: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:25.128: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:25.129: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:25.129: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:26.120: INFO: Wrong image for pod: daemon-set-7669x. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:26.120: INFO: Pod daemon-set-7669x is not available
Mar 30 21:26:26.120: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:26.120: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:26.129: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:26.129: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:26.129: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:27.122: INFO: Wrong image for pod: daemon-set-7669x. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:27.122: INFO: Pod daemon-set-7669x is not available
Mar 30 21:26:27.122: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:27.122: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:27.129: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:27.129: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:27.129: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:28.121: INFO: Wrong image for pod: daemon-set-7669x. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:28.121: INFO: Pod daemon-set-7669x is not available
Mar 30 21:26:28.121: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:28.122: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:28.128: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:28.128: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:28.128: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:29.122: INFO: Wrong image for pod: daemon-set-7669x. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:29.122: INFO: Pod daemon-set-7669x is not available
Mar 30 21:26:29.122: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:29.122: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:29.128: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:29.128: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:29.128: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:30.120: INFO: Wrong image for pod: daemon-set-7669x. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:30.120: INFO: Pod daemon-set-7669x is not available
Mar 30 21:26:30.120: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:30.120: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:30.125: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:30.125: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:30.125: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:31.129: INFO: Wrong image for pod: daemon-set-7669x. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:31.129: INFO: Pod daemon-set-7669x is not available
Mar 30 21:26:31.129: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:31.129: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:31.136: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:31.136: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:31.136: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:32.120: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:32.120: INFO: Pod daemon-set-ntkgv is not available
Mar 30 21:26:32.120: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:32.126: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:32.126: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:32.126: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:33.121: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:33.121: INFO: Pod daemon-set-ntkgv is not available
Mar 30 21:26:33.121: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:33.127: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:33.127: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:33.127: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:34.123: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:34.123: INFO: Pod daemon-set-ntkgv is not available
Mar 30 21:26:34.123: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:34.129: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:34.129: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:34.130: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:35.121: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:35.121: INFO: Pod daemon-set-ntkgv is not available
Mar 30 21:26:35.121: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:35.128: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:35.128: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:35.128: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:36.122: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:36.122: INFO: Pod daemon-set-ntkgv is not available
Mar 30 21:26:36.122: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:36.128: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:36.128: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:36.128: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:37.119: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:37.119: INFO: Pod daemon-set-ntkgv is not available
Mar 30 21:26:37.119: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:37.126: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:37.126: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:37.126: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:38.120: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:38.120: INFO: Pod daemon-set-ntkgv is not available
Mar 30 21:26:38.120: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:38.127: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:38.127: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:38.127: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:39.121: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:39.121: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:39.129: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:39.129: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:39.129: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:40.120: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:40.121: INFO: Pod daemon-set-fbqn4 is not available
Mar 30 21:26:40.121: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:40.127: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:40.127: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:40.127: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:41.125: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:41.125: INFO: Pod daemon-set-fbqn4 is not available
Mar 30 21:26:41.125: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:41.134: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:41.134: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:41.134: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:42.125: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:42.126: INFO: Pod daemon-set-fbqn4 is not available
Mar 30 21:26:42.126: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:42.134: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:42.134: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:42.135: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:43.120: INFO: Wrong image for pod: daemon-set-fbqn4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:43.120: INFO: Pod daemon-set-fbqn4 is not available
Mar 30 21:26:43.120: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:43.126: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:43.127: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:43.127: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:44.120: INFO: Pod daemon-set-4755q is not available
Mar 30 21:26:44.120: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:44.126: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:44.126: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:44.126: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:45.121: INFO: Pod daemon-set-4755q is not available
Mar 30 21:26:45.121: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:45.128: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:45.128: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:45.128: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:46.121: INFO: Pod daemon-set-4755q is not available
Mar 30 21:26:46.121: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:46.137: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:46.137: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:46.137: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:47.121: INFO: Pod daemon-set-4755q is not available
Mar 30 21:26:47.121: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:47.127: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:47.127: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:47.127: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:48.121: INFO: Pod daemon-set-4755q is not available
Mar 30 21:26:48.121: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:48.127: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:48.128: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:48.128: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:49.120: INFO: Pod daemon-set-4755q is not available
Mar 30 21:26:49.120: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:49.128: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:49.128: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:49.128: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:50.120: INFO: Pod daemon-set-4755q is not available
Mar 30 21:26:50.120: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:50.128: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:50.128: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:50.128: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:51.120: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:51.133: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:51.134: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:51.134: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:52.121: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:52.121: INFO: Pod daemon-set-t6xmz is not available
Mar 30 21:26:52.130: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:52.130: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:52.130: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:53.121: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:53.121: INFO: Pod daemon-set-t6xmz is not available
Mar 30 21:26:53.127: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:53.127: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:53.127: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:54.121: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:54.121: INFO: Pod daemon-set-t6xmz is not available
Mar 30 21:26:54.127: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:54.127: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:54.127: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:55.121: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:55.121: INFO: Pod daemon-set-t6xmz is not available
Mar 30 21:26:55.131: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:55.132: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:55.132: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:56.121: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:56.121: INFO: Pod daemon-set-t6xmz is not available
Mar 30 21:26:56.127: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:56.128: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:56.128: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:57.122: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:57.122: INFO: Pod daemon-set-t6xmz is not available
Mar 30 21:26:57.128: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:57.128: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:57.129: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:58.121: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:58.121: INFO: Pod daemon-set-t6xmz is not available
Mar 30 21:26:58.129: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:58.129: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:58.129: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:59.120: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:26:59.121: INFO: Pod daemon-set-t6xmz is not available
Mar 30 21:26:59.127: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:59.128: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:26:59.128: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:00.121: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:27:00.121: INFO: Pod daemon-set-t6xmz is not available
Mar 30 21:27:00.127: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:00.127: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:00.127: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:01.131: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:27:01.131: INFO: Pod daemon-set-t6xmz is not available
Mar 30 21:27:01.142: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:01.142: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:01.142: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:02.121: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:27:02.121: INFO: Pod daemon-set-t6xmz is not available
Mar 30 21:27:02.127: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:02.127: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:02.127: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:03.120: INFO: Wrong image for pod: daemon-set-t6xmz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar 30 21:27:03.120: INFO: Pod daemon-set-t6xmz is not available
Mar 30 21:27:03.125: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:03.125: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:03.125: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:04.123: INFO: Pod daemon-set-rcm8v is not available
Mar 30 21:27:04.131: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:04.131: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:04.131: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Mar 30 21:27:04.138: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:04.138: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:04.138: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:04.143: INFO: Number of nodes with available pods: 3
Mar 30 21:27:04.143: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:27:05.154: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:05.154: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:05.154: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:05.160: INFO: Number of nodes with available pods: 3
Mar 30 21:27:05.160: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:27:06.151: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:06.151: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:06.151: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:06.155: INFO: Number of nodes with available pods: 3
Mar 30 21:27:06.155: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:27:07.152: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:07.152: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:07.152: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:07.158: INFO: Number of nodes with available pods: 3
Mar 30 21:27:07.158: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:27:08.152: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:08.153: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:08.153: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:08.159: INFO: Number of nodes with available pods: 3
Mar 30 21:27:08.159: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:27:09.150: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:09.150: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:09.150: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:09.155: INFO: Number of nodes with available pods: 3
Mar 30 21:27:09.155: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:27:10.150: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:10.150: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:10.150: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:10.155: INFO: Number of nodes with available pods: 3
Mar 30 21:27:10.155: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:27:11.153: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:11.154: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:11.154: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:11.167: INFO: Number of nodes with available pods: 3
Mar 30 21:27:11.167: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:27:12.151: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:12.151: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:12.151: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:27:12.156: INFO: Number of nodes with available pods: 4
Mar 30 21:27:12.156: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3129, will wait for the garbage collector to delete the pods
Mar 30 21:27:12.244: INFO: Deleting DaemonSet.extensions daemon-set took: 13.671081ms
Mar 30 21:27:12.845: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.391133ms
Mar 30 21:27:23.550: INFO: Number of nodes with available pods: 0
Mar 30 21:27:23.550: INFO: Number of running nodes: 0, number of available pods: 0
Mar 30 21:27:23.554: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3129/daemonsets","resourceVersion":"19071"},"items":null}

Mar 30 21:27:23.558: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3129/pods","resourceVersion":"19071"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:27:23.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3129" for this suite.

• [SLOW TEST:85.878 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":278,"completed":62,"skipped":1172,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:27:23.613: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2239
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Mar 30 21:27:23.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 create -f - --namespace=kubectl-2239'
Mar 30 21:27:24.127: INFO: stderr: ""
Mar 30 21:27:24.127: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Mar 30 21:27:25.136: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 21:27:25.136: INFO: Found 0 / 1
Mar 30 21:27:26.132: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 21:27:26.132: INFO: Found 0 / 1
Mar 30 21:27:27.132: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 21:27:27.132: INFO: Found 0 / 1
Mar 30 21:27:28.133: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 21:27:28.134: INFO: Found 0 / 1
Mar 30 21:27:29.135: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 21:27:29.135: INFO: Found 0 / 1
Mar 30 21:27:30.133: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 21:27:30.133: INFO: Found 0 / 1
Mar 30 21:27:31.141: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 21:27:31.141: INFO: Found 0 / 1
Mar 30 21:27:32.140: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 21:27:32.140: INFO: Found 1 / 1
Mar 30 21:27:32.140: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar 30 21:27:32.144: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 21:27:32.144: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 30 21:27:32.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 patch pod agnhost-master-7wbqt --namespace=kubectl-2239 -p {"metadata":{"annotations":{"x":"y"}}}'
Mar 30 21:27:32.247: INFO: stderr: ""
Mar 30 21:27:32.247: INFO: stdout: "pod/agnhost-master-7wbqt patched\n"
STEP: checking annotations
Mar 30 21:27:32.253: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 21:27:32.253: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:27:32.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2239" for this suite.

• [SLOW TEST:8.671 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1539
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":278,"completed":63,"skipped":1198,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:27:32.288: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-183
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-183
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 30 21:27:32.462: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 30 21:28:08.627: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.14.69:8080/dial?request=hostname&protocol=udp&host=192.168.125.16&port=8081&tries=1'] Namespace:pod-network-test-183 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 30 21:28:08.627: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 21:28:08.855: INFO: Waiting for responses: map[]
Mar 30 21:28:08.860: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.14.69:8080/dial?request=hostname&protocol=udp&host=192.168.14.127&port=8081&tries=1'] Namespace:pod-network-test-183 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 30 21:28:08.860: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 21:28:09.048: INFO: Waiting for responses: map[]
Mar 30 21:28:09.055: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.14.69:8080/dial?request=hostname&protocol=udp&host=192.168.185.224&port=8081&tries=1'] Namespace:pod-network-test-183 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 30 21:28:09.055: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 21:28:09.233: INFO: Waiting for responses: map[]
Mar 30 21:28:09.240: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.14.69:8080/dial?request=hostname&protocol=udp&host=192.168.10.212&port=8081&tries=1'] Namespace:pod-network-test-183 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 30 21:28:09.240: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 21:28:09.433: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:28:09.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-183" for this suite.

• [SLOW TEST:37.163 seconds]
[sig-network] Networking
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":64,"skipped":1232,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:28:09.456: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6919
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-6919/configmap-test-fff20f38-9032-4516-a3ef-86222df80236
STEP: Creating a pod to test consume configMaps
Mar 30 21:28:09.670: INFO: Waiting up to 5m0s for pod "pod-configmaps-d59000d7-70a5-4a39-b762-d194c9fd878b" in namespace "configmap-6919" to be "success or failure"
Mar 30 21:28:09.674: INFO: Pod "pod-configmaps-d59000d7-70a5-4a39-b762-d194c9fd878b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.25644ms
Mar 30 21:28:11.682: INFO: Pod "pod-configmaps-d59000d7-70a5-4a39-b762-d194c9fd878b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011498801s
Mar 30 21:28:13.689: INFO: Pod "pod-configmaps-d59000d7-70a5-4a39-b762-d194c9fd878b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018555606s
Mar 30 21:28:15.701: INFO: Pod "pod-configmaps-d59000d7-70a5-4a39-b762-d194c9fd878b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030695055s
Mar 30 21:28:17.707: INFO: Pod "pod-configmaps-d59000d7-70a5-4a39-b762-d194c9fd878b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.036411851s
STEP: Saw pod success
Mar 30 21:28:17.707: INFO: Pod "pod-configmaps-d59000d7-70a5-4a39-b762-d194c9fd878b" satisfied condition "success or failure"
Mar 30 21:28:17.710: INFO: Trying to get logs from node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins pod pod-configmaps-d59000d7-70a5-4a39-b762-d194c9fd878b container env-test: <nil>
STEP: delete the pod
Mar 30 21:28:17.744: INFO: Waiting for pod pod-configmaps-d59000d7-70a5-4a39-b762-d194c9fd878b to disappear
Mar 30 21:28:17.748: INFO: Pod pod-configmaps-d59000d7-70a5-4a39-b762-d194c9fd878b no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:28:17.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6919" for this suite.

• [SLOW TEST:8.315 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":278,"completed":65,"skipped":1275,"failed":0}
SSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:28:17.774: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-5542
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:172
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating server pod server in namespace prestop-5542
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-5542
STEP: Deleting pre-stop pod
Mar 30 21:28:39.033: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:28:39.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-5542" for this suite.

• [SLOW TEST:21.291 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":278,"completed":66,"skipped":1280,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:28:39.073: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5428
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 21:28:39.303: INFO: Create a RollingUpdate DaemonSet
Mar 30 21:28:39.315: INFO: Check that daemon pods launch on every node of the cluster
Mar 30 21:28:39.322: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:39.322: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:39.322: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:39.332: INFO: Number of nodes with available pods: 0
Mar 30 21:28:39.332: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:28:40.340: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:40.340: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:40.340: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:40.349: INFO: Number of nodes with available pods: 0
Mar 30 21:28:40.349: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:28:41.340: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:41.340: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:41.340: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:41.345: INFO: Number of nodes with available pods: 0
Mar 30 21:28:41.345: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:28:42.350: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:42.350: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:42.351: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:42.361: INFO: Number of nodes with available pods: 0
Mar 30 21:28:42.361: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:28:43.339: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:43.339: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:43.339: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:43.344: INFO: Number of nodes with available pods: 0
Mar 30 21:28:43.344: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:28:44.346: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:44.346: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:44.346: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:44.353: INFO: Number of nodes with available pods: 0
Mar 30 21:28:44.353: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:28:45.340: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:45.340: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:45.340: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:45.346: INFO: Number of nodes with available pods: 0
Mar 30 21:28:45.346: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:28:46.339: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:46.339: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:46.339: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:46.343: INFO: Number of nodes with available pods: 0
Mar 30 21:28:46.344: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:28:47.339: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:47.339: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:47.339: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:47.344: INFO: Number of nodes with available pods: 2
Mar 30 21:28:47.344: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:28:48.339: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:48.339: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:48.339: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:48.344: INFO: Number of nodes with available pods: 4
Mar 30 21:28:48.344: INFO: Number of running nodes: 4, number of available pods: 4
Mar 30 21:28:48.345: INFO: Update the DaemonSet to trigger a rollout
Mar 30 21:28:48.367: INFO: Updating DaemonSet daemon-set
Mar 30 21:28:54.392: INFO: Roll back the DaemonSet before rollout is complete
Mar 30 21:28:54.404: INFO: Updating DaemonSet daemon-set
Mar 30 21:28:54.404: INFO: Make sure DaemonSet rollback is complete
Mar 30 21:28:54.412: INFO: Wrong image for pod: daemon-set-5vn7b. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar 30 21:28:54.412: INFO: Pod daemon-set-5vn7b is not available
Mar 30 21:28:54.419: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:54.419: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:54.419: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:55.426: INFO: Wrong image for pod: daemon-set-5vn7b. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar 30 21:28:55.426: INFO: Pod daemon-set-5vn7b is not available
Mar 30 21:28:55.432: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:55.432: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:55.432: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:56.427: INFO: Pod daemon-set-26nws is not available
Mar 30 21:28:56.432: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:56.433: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:28:56.433: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5428, will wait for the garbage collector to delete the pods
Mar 30 21:28:56.511: INFO: Deleting DaemonSet.extensions daemon-set took: 15.930876ms
Mar 30 21:28:57.111: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.314041ms
Mar 30 21:29:03.316: INFO: Number of nodes with available pods: 0
Mar 30 21:29:03.316: INFO: Number of running nodes: 0, number of available pods: 0
Mar 30 21:29:03.320: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5428/daemonsets","resourceVersion":"19967"},"items":null}

Mar 30 21:29:03.324: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5428/pods","resourceVersion":"19967"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:29:03.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5428" for this suite.

• [SLOW TEST:24.317 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":278,"completed":67,"skipped":1312,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:29:03.390: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7634
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 30 21:29:04.070: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 30 21:29:06.083: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200544, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200544, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200544, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200544, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:29:08.097: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200544, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200544, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200544, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200544, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:29:10.088: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200544, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200544, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200544, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200544, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 30 21:29:13.116: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Mar 30 21:29:23.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 attach --namespace=webhook-7634 to-be-attached-pod -i -c=container1'
Mar 30 21:29:23.293: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:29:23.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7634" for this suite.
STEP: Destroying namespace "webhook-7634-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:20.084 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":278,"completed":68,"skipped":1314,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:29:23.483: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1868
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
Mar 30 21:29:24.254: INFO: created pod pod-service-account-defaultsa
Mar 30 21:29:24.254: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar 30 21:29:24.268: INFO: created pod pod-service-account-mountsa
Mar 30 21:29:24.268: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar 30 21:29:24.280: INFO: created pod pod-service-account-nomountsa
Mar 30 21:29:24.280: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar 30 21:29:24.290: INFO: created pod pod-service-account-defaultsa-mountspec
Mar 30 21:29:24.290: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar 30 21:29:24.304: INFO: created pod pod-service-account-mountsa-mountspec
Mar 30 21:29:24.304: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar 30 21:29:24.369: INFO: created pod pod-service-account-nomountsa-mountspec
Mar 30 21:29:24.369: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar 30 21:29:24.378: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar 30 21:29:24.378: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar 30 21:29:24.398: INFO: created pod pod-service-account-mountsa-nomountspec
Mar 30 21:29:24.399: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar 30 21:29:24.412: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar 30 21:29:24.412: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:29:24.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1868" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":278,"completed":69,"skipped":1349,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:29:24.448: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-9243
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:29:24.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-9243" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":278,"completed":70,"skipped":1371,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:29:24.764: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9242
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 30 21:29:24.950: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8b014366-73ec-47d9-a154-5b663ed1d284" in namespace "projected-9242" to be "success or failure"
Mar 30 21:29:24.955: INFO: Pod "downwardapi-volume-8b014366-73ec-47d9-a154-5b663ed1d284": Phase="Pending", Reason="", readiness=false. Elapsed: 4.694135ms
Mar 30 21:29:27.026: INFO: Pod "downwardapi-volume-8b014366-73ec-47d9-a154-5b663ed1d284": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0755592s
Mar 30 21:29:29.031: INFO: Pod "downwardapi-volume-8b014366-73ec-47d9-a154-5b663ed1d284": Phase="Pending", Reason="", readiness=false. Elapsed: 4.080760645s
Mar 30 21:29:31.059: INFO: Pod "downwardapi-volume-8b014366-73ec-47d9-a154-5b663ed1d284": Phase="Pending", Reason="", readiness=false. Elapsed: 6.108469514s
Mar 30 21:29:33.066: INFO: Pod "downwardapi-volume-8b014366-73ec-47d9-a154-5b663ed1d284": Phase="Pending", Reason="", readiness=false. Elapsed: 8.115728495s
Mar 30 21:29:35.077: INFO: Pod "downwardapi-volume-8b014366-73ec-47d9-a154-5b663ed1d284": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.126232288s
STEP: Saw pod success
Mar 30 21:29:35.077: INFO: Pod "downwardapi-volume-8b014366-73ec-47d9-a154-5b663ed1d284" satisfied condition "success or failure"
Mar 30 21:29:35.081: INFO: Trying to get logs from node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins pod downwardapi-volume-8b014366-73ec-47d9-a154-5b663ed1d284 container client-container: <nil>
STEP: delete the pod
Mar 30 21:29:35.111: INFO: Waiting for pod downwardapi-volume-8b014366-73ec-47d9-a154-5b663ed1d284 to disappear
Mar 30 21:29:35.116: INFO: Pod downwardapi-volume-8b014366-73ec-47d9-a154-5b663ed1d284 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:29:35.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9242" for this suite.

• [SLOW TEST:10.365 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":278,"completed":71,"skipped":1388,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:29:35.131: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9724
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:29:43.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9724" for this suite.

• [SLOW TEST:8.243 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when scheduling a read only busybox container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":72,"skipped":1396,"failed":0}
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:29:43.376: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1472
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Mar 30 21:29:43.567: INFO: Waiting up to 5m0s for pod "downward-api-3f794709-d3bf-4a48-81ec-88e842aa3c89" in namespace "downward-api-1472" to be "success or failure"
Mar 30 21:29:43.573: INFO: Pod "downward-api-3f794709-d3bf-4a48-81ec-88e842aa3c89": Phase="Pending", Reason="", readiness=false. Elapsed: 6.220354ms
Mar 30 21:29:45.581: INFO: Pod "downward-api-3f794709-d3bf-4a48-81ec-88e842aa3c89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013496131s
Mar 30 21:29:47.589: INFO: Pod "downward-api-3f794709-d3bf-4a48-81ec-88e842aa3c89": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02229762s
Mar 30 21:29:49.594: INFO: Pod "downward-api-3f794709-d3bf-4a48-81ec-88e842aa3c89": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027404753s
Mar 30 21:29:51.600: INFO: Pod "downward-api-3f794709-d3bf-4a48-81ec-88e842aa3c89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.033283382s
STEP: Saw pod success
Mar 30 21:29:51.600: INFO: Pod "downward-api-3f794709-d3bf-4a48-81ec-88e842aa3c89" satisfied condition "success or failure"
Mar 30 21:29:51.605: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod downward-api-3f794709-d3bf-4a48-81ec-88e842aa3c89 container dapi-container: <nil>
STEP: delete the pod
Mar 30 21:29:51.641: INFO: Waiting for pod downward-api-3f794709-d3bf-4a48-81ec-88e842aa3c89 to disappear
Mar 30 21:29:51.645: INFO: Pod downward-api-3f794709-d3bf-4a48-81ec-88e842aa3c89 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:29:51.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1472" for this suite.

• [SLOW TEST:8.285 seconds]
[sig-node] Downward API
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:33
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":278,"completed":73,"skipped":1403,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:29:51.662: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7120
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 30 21:29:51.901: INFO: Waiting up to 5m0s for pod "pod-fbc6cfc7-58b0-4ec3-9688-23134f804138" in namespace "emptydir-7120" to be "success or failure"
Mar 30 21:29:51.909: INFO: Pod "pod-fbc6cfc7-58b0-4ec3-9688-23134f804138": Phase="Pending", Reason="", readiness=false. Elapsed: 7.465798ms
Mar 30 21:29:53.913: INFO: Pod "pod-fbc6cfc7-58b0-4ec3-9688-23134f804138": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01167285s
Mar 30 21:29:55.920: INFO: Pod "pod-fbc6cfc7-58b0-4ec3-9688-23134f804138": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018632938s
Mar 30 21:29:57.926: INFO: Pod "pod-fbc6cfc7-58b0-4ec3-9688-23134f804138": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025441644s
Mar 30 21:29:59.931: INFO: Pod "pod-fbc6cfc7-58b0-4ec3-9688-23134f804138": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.030249529s
STEP: Saw pod success
Mar 30 21:29:59.932: INFO: Pod "pod-fbc6cfc7-58b0-4ec3-9688-23134f804138" satisfied condition "success or failure"
Mar 30 21:29:59.936: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-fbc6cfc7-58b0-4ec3-9688-23134f804138 container test-container: <nil>
STEP: delete the pod
Mar 30 21:29:59.964: INFO: Waiting for pod pod-fbc6cfc7-58b0-4ec3-9688-23134f804138 to disappear
Mar 30 21:29:59.970: INFO: Pod pod-fbc6cfc7-58b0-4ec3-9688-23134f804138 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:29:59.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7120" for this suite.

• [SLOW TEST:8.324 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":74,"skipped":1418,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:29:59.987: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2090
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 30 21:30:20.238: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 30 21:30:20.243: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 30 21:30:22.243: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 30 21:30:22.251: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 30 21:30:24.243: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 30 21:30:24.249: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:30:24.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2090" for this suite.

• [SLOW TEST:24.279 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":278,"completed":75,"skipped":1427,"failed":0}
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:30:24.266: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2609
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Mar 30 21:30:34.635: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0330 21:30:34.635437      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:30:34.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2609" for this suite.

• [SLOW TEST:10.391 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":278,"completed":76,"skipped":1427,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:30:34.657: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8988
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 30 21:30:35.463: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 30 21:30:37.476: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200635, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200635, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200635, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200635, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:30:39.481: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200635, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200635, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200635, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200635, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:30:41.482: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200635, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200635, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200635, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200635, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:30:43.484: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200635, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200635, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200635, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200635, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 30 21:30:46.509: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:30:46.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8988" for this suite.
STEP: Destroying namespace "webhook-8988-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:12.136 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":278,"completed":77,"skipped":1435,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:30:46.802: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3621
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-d52d
STEP: Creating a pod to test atomic-volume-subpath
Mar 30 21:30:47.009: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-d52d" in namespace "subpath-3621" to be "success or failure"
Mar 30 21:30:47.013: INFO: Pod "pod-subpath-test-configmap-d52d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.930143ms
Mar 30 21:30:49.019: INFO: Pod "pod-subpath-test-configmap-d52d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009531937s
Mar 30 21:30:51.024: INFO: Pod "pod-subpath-test-configmap-d52d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014618512s
Mar 30 21:30:53.029: INFO: Pod "pod-subpath-test-configmap-d52d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019771785s
Mar 30 21:30:55.035: INFO: Pod "pod-subpath-test-configmap-d52d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025114598s
Mar 30 21:30:57.041: INFO: Pod "pod-subpath-test-configmap-d52d": Phase="Running", Reason="", readiness=true. Elapsed: 10.031707531s
Mar 30 21:30:59.047: INFO: Pod "pod-subpath-test-configmap-d52d": Phase="Running", Reason="", readiness=true. Elapsed: 12.037934749s
Mar 30 21:31:01.055: INFO: Pod "pod-subpath-test-configmap-d52d": Phase="Running", Reason="", readiness=true. Elapsed: 14.045145576s
Mar 30 21:31:03.066: INFO: Pod "pod-subpath-test-configmap-d52d": Phase="Running", Reason="", readiness=true. Elapsed: 16.057018763s
Mar 30 21:31:05.073: INFO: Pod "pod-subpath-test-configmap-d52d": Phase="Running", Reason="", readiness=true. Elapsed: 18.063533408s
Mar 30 21:31:07.078: INFO: Pod "pod-subpath-test-configmap-d52d": Phase="Running", Reason="", readiness=true. Elapsed: 20.06881546s
Mar 30 21:31:09.084: INFO: Pod "pod-subpath-test-configmap-d52d": Phase="Running", Reason="", readiness=true. Elapsed: 22.074153802s
Mar 30 21:31:11.094: INFO: Pod "pod-subpath-test-configmap-d52d": Phase="Running", Reason="", readiness=true. Elapsed: 24.084402967s
Mar 30 21:31:13.100: INFO: Pod "pod-subpath-test-configmap-d52d": Phase="Running", Reason="", readiness=true. Elapsed: 26.090661899s
Mar 30 21:31:15.108: INFO: Pod "pod-subpath-test-configmap-d52d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.098311118s
STEP: Saw pod success
Mar 30 21:31:15.108: INFO: Pod "pod-subpath-test-configmap-d52d" satisfied condition "success or failure"
Mar 30 21:31:15.115: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-subpath-test-configmap-d52d container test-container-subpath-configmap-d52d: <nil>
STEP: delete the pod
Mar 30 21:31:15.207: INFO: Waiting for pod pod-subpath-test-configmap-d52d to disappear
Mar 30 21:31:15.214: INFO: Pod pod-subpath-test-configmap-d52d no longer exists
STEP: Deleting pod pod-subpath-test-configmap-d52d
Mar 30 21:31:15.214: INFO: Deleting pod "pod-subpath-test-configmap-d52d" in namespace "subpath-3621"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:31:15.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3621" for this suite.

• [SLOW TEST:28.438 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":278,"completed":78,"skipped":1473,"failed":0}
SSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:31:15.242: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-8374
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 21:31:15.417: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8374
I0330 21:31:15.448108      20 runners.go:189] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8374, replica count: 1
I0330 21:31:16.499497      20 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0330 21:31:17.499988      20 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0330 21:31:18.500315      20 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0330 21:31:19.500714      20 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0330 21:31:20.500998      20 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0330 21:31:21.501252      20 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0330 21:31:22.502781      20 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0330 21:31:23.503075      20 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 30 21:31:23.633: INFO: Created: latency-svc-sw9tk
Mar 30 21:31:23.668: INFO: Got endpoints: latency-svc-sw9tk [65.299115ms]
Mar 30 21:31:23.709: INFO: Created: latency-svc-vxtmx
Mar 30 21:31:23.712: INFO: Got endpoints: latency-svc-vxtmx [42.829965ms]
Mar 30 21:31:23.780: INFO: Created: latency-svc-6kq8d
Mar 30 21:31:23.811: INFO: Created: latency-svc-r2w9k
Mar 30 21:31:23.814: INFO: Got endpoints: latency-svc-6kq8d [143.700962ms]
Mar 30 21:31:23.833: INFO: Got endpoints: latency-svc-r2w9k [162.015589ms]
Mar 30 21:31:23.837: INFO: Created: latency-svc-tfnnw
Mar 30 21:31:23.850: INFO: Got endpoints: latency-svc-tfnnw [180.258262ms]
Mar 30 21:31:23.864: INFO: Created: latency-svc-464t2
Mar 30 21:31:23.891: INFO: Got endpoints: latency-svc-464t2 [220.521705ms]
Mar 30 21:31:23.908: INFO: Created: latency-svc-m2ctw
Mar 30 21:31:23.930: INFO: Got endpoints: latency-svc-m2ctw [259.126612ms]
Mar 30 21:31:23.934: INFO: Created: latency-svc-7v2df
Mar 30 21:31:23.956: INFO: Got endpoints: latency-svc-7v2df [284.646014ms]
Mar 30 21:31:23.961: INFO: Created: latency-svc-pkxxh
Mar 30 21:31:23.981: INFO: Got endpoints: latency-svc-pkxxh [309.778163ms]
Mar 30 21:31:23.989: INFO: Created: latency-svc-4c9hh
Mar 30 21:31:24.037: INFO: Got endpoints: latency-svc-4c9hh [356.765769ms]
Mar 30 21:31:24.087: INFO: Created: latency-svc-q4bm9
Mar 30 21:31:24.138: INFO: Got endpoints: latency-svc-q4bm9 [457.381322ms]
Mar 30 21:31:24.194: INFO: Created: latency-svc-fz658
Mar 30 21:31:24.225: INFO: Got endpoints: latency-svc-fz658 [544.17842ms]
Mar 30 21:31:24.248: INFO: Created: latency-svc-qg2q5
Mar 30 21:31:24.262: INFO: Got endpoints: latency-svc-qg2q5 [581.815752ms]
Mar 30 21:31:24.277: INFO: Created: latency-svc-xzkj4
Mar 30 21:31:24.294: INFO: Got endpoints: latency-svc-xzkj4 [613.652737ms]
Mar 30 21:31:24.304: INFO: Created: latency-svc-42v6n
Mar 30 21:31:24.330: INFO: Got endpoints: latency-svc-42v6n [649.347685ms]
Mar 30 21:31:24.334: INFO: Created: latency-svc-mk2kd
Mar 30 21:31:24.344: INFO: Got endpoints: latency-svc-mk2kd [671.736445ms]
Mar 30 21:31:24.367: INFO: Created: latency-svc-qzqc7
Mar 30 21:31:24.371: INFO: Got endpoints: latency-svc-qzqc7 [659.195341ms]
Mar 30 21:31:24.410: INFO: Created: latency-svc-ntwtv
Mar 30 21:31:24.457: INFO: Got endpoints: latency-svc-ntwtv [642.927483ms]
Mar 30 21:31:24.477: INFO: Created: latency-svc-rvj9j
Mar 30 21:31:24.497: INFO: Got endpoints: latency-svc-rvj9j [663.369391ms]
Mar 30 21:31:24.589: INFO: Created: latency-svc-88jp8
Mar 30 21:31:24.589: INFO: Got endpoints: latency-svc-88jp8 [738.085135ms]
Mar 30 21:31:24.621: INFO: Created: latency-svc-nmq4p
Mar 30 21:31:24.670: INFO: Got endpoints: latency-svc-nmq4p [778.440588ms]
Mar 30 21:31:24.683: INFO: Created: latency-svc-gh62z
Mar 30 21:31:24.692: INFO: Got endpoints: latency-svc-gh62z [761.99993ms]
Mar 30 21:31:24.706: INFO: Created: latency-svc-q4rps
Mar 30 21:31:24.738: INFO: Got endpoints: latency-svc-q4rps [780.425899ms]
Mar 30 21:31:24.762: INFO: Created: latency-svc-gvr5k
Mar 30 21:31:24.797: INFO: Got endpoints: latency-svc-gvr5k [815.746126ms]
Mar 30 21:31:24.835: INFO: Created: latency-svc-j9x56
Mar 30 21:31:24.850: INFO: Created: latency-svc-gd9lk
Mar 30 21:31:24.850: INFO: Got endpoints: latency-svc-j9x56 [711.594423ms]
Mar 30 21:31:24.850: INFO: Got endpoints: latency-svc-gd9lk [813.29245ms]
Mar 30 21:31:24.890: INFO: Created: latency-svc-wdj2h
Mar 30 21:31:24.906: INFO: Got endpoints: latency-svc-wdj2h [680.330493ms]
Mar 30 21:31:24.913: INFO: Created: latency-svc-m4ldk
Mar 30 21:31:24.939: INFO: Got endpoints: latency-svc-m4ldk [676.782045ms]
Mar 30 21:31:24.946: INFO: Created: latency-svc-hbfj2
Mar 30 21:31:24.961: INFO: Got endpoints: latency-svc-hbfj2 [666.397463ms]
Mar 30 21:31:24.970: INFO: Created: latency-svc-w29tq
Mar 30 21:31:24.984: INFO: Got endpoints: latency-svc-w29tq [652.973586ms]
Mar 30 21:31:24.990: INFO: Created: latency-svc-fztfr
Mar 30 21:31:24.999: INFO: Got endpoints: latency-svc-fztfr [655.035786ms]
Mar 30 21:31:25.021: INFO: Created: latency-svc-pzjzk
Mar 30 21:31:25.053: INFO: Got endpoints: latency-svc-pzjzk [680.616283ms]
Mar 30 21:31:25.056: INFO: Created: latency-svc-mgmgb
Mar 30 21:31:25.080: INFO: Got endpoints: latency-svc-mgmgb [622.609816ms]
Mar 30 21:31:25.089: INFO: Created: latency-svc-czmq2
Mar 30 21:31:25.101: INFO: Got endpoints: latency-svc-czmq2 [602.531032ms]
Mar 30 21:31:25.115: INFO: Created: latency-svc-wcnmr
Mar 30 21:31:25.134: INFO: Got endpoints: latency-svc-wcnmr [544.716578ms]
Mar 30 21:31:25.157: INFO: Created: latency-svc-4dqfc
Mar 30 21:31:25.158: INFO: Got endpoints: latency-svc-4dqfc [487.706285ms]
Mar 30 21:31:25.175: INFO: Created: latency-svc-56htj
Mar 30 21:31:25.215: INFO: Got endpoints: latency-svc-56htj [522.330867ms]
Mar 30 21:31:25.223: INFO: Created: latency-svc-5fwfp
Mar 30 21:31:25.229: INFO: Got endpoints: latency-svc-5fwfp [491.219971ms]
Mar 30 21:31:25.281: INFO: Created: latency-svc-krncd
Mar 30 21:31:25.286: INFO: Got endpoints: latency-svc-krncd [489.070348ms]
Mar 30 21:31:25.304: INFO: Created: latency-svc-wcls7
Mar 30 21:31:25.313: INFO: Got endpoints: latency-svc-wcls7 [462.399558ms]
Mar 30 21:31:25.342: INFO: Created: latency-svc-bw6fz
Mar 30 21:31:25.347: INFO: Got endpoints: latency-svc-bw6fz [496.219321ms]
Mar 30 21:31:25.402: INFO: Created: latency-svc-ccnfd
Mar 30 21:31:25.415: INFO: Got endpoints: latency-svc-ccnfd [508.376732ms]
Mar 30 21:31:25.422: INFO: Created: latency-svc-8pttm
Mar 30 21:31:25.444: INFO: Got endpoints: latency-svc-8pttm [504.706707ms]
Mar 30 21:31:25.483: INFO: Created: latency-svc-wt6xp
Mar 30 21:31:25.489: INFO: Got endpoints: latency-svc-wt6xp [528.111699ms]
Mar 30 21:31:25.495: INFO: Created: latency-svc-th5bv
Mar 30 21:31:25.517: INFO: Got endpoints: latency-svc-th5bv [533.504657ms]
Mar 30 21:31:25.533: INFO: Created: latency-svc-rk9lb
Mar 30 21:31:25.566: INFO: Created: latency-svc-f9m72
Mar 30 21:31:25.587: INFO: Got endpoints: latency-svc-f9m72 [533.086024ms]
Mar 30 21:31:25.587: INFO: Got endpoints: latency-svc-rk9lb [587.926265ms]
Mar 30 21:31:25.628: INFO: Created: latency-svc-dkf24
Mar 30 21:31:25.628: INFO: Got endpoints: latency-svc-dkf24 [548.126167ms]
Mar 30 21:31:25.638: INFO: Created: latency-svc-zs5d4
Mar 30 21:31:25.717: INFO: Got endpoints: latency-svc-zs5d4 [615.830908ms]
Mar 30 21:31:25.717: INFO: Created: latency-svc-m6xpt
Mar 30 21:31:25.718: INFO: Got endpoints: latency-svc-m6xpt [583.364327ms]
Mar 30 21:31:25.743: INFO: Created: latency-svc-5bsll
Mar 30 21:31:25.759: INFO: Got endpoints: latency-svc-5bsll [601.077384ms]
Mar 30 21:31:25.786: INFO: Created: latency-svc-2bkn9
Mar 30 21:31:25.795: INFO: Created: latency-svc-nnght
Mar 30 21:31:25.829: INFO: Got endpoints: latency-svc-2bkn9 [599.680954ms]
Mar 30 21:31:25.830: INFO: Got endpoints: latency-svc-nnght [615.024308ms]
Mar 30 21:31:25.851: INFO: Created: latency-svc-b8zrw
Mar 30 21:31:25.860: INFO: Got endpoints: latency-svc-b8zrw [573.33489ms]
Mar 30 21:31:25.878: INFO: Created: latency-svc-h4s87
Mar 30 21:31:25.900: INFO: Created: latency-svc-fpz2z
Mar 30 21:31:25.905: INFO: Got endpoints: latency-svc-fpz2z [557.341836ms]
Mar 30 21:31:25.910: INFO: Got endpoints: latency-svc-h4s87 [596.898278ms]
Mar 30 21:31:25.948: INFO: Created: latency-svc-6lznr
Mar 30 21:31:25.973: INFO: Got endpoints: latency-svc-6lznr [558.035584ms]
Mar 30 21:31:25.986: INFO: Created: latency-svc-659gt
Mar 30 21:31:26.008: INFO: Got endpoints: latency-svc-659gt [564.357151ms]
Mar 30 21:31:26.018: INFO: Created: latency-svc-zvg6m
Mar 30 21:31:26.041: INFO: Got endpoints: latency-svc-zvg6m [551.730762ms]
Mar 30 21:31:26.046: INFO: Created: latency-svc-fq7sv
Mar 30 21:31:26.061: INFO: Got endpoints: latency-svc-fq7sv [543.38325ms]
Mar 30 21:31:26.072: INFO: Created: latency-svc-zdnrd
Mar 30 21:31:26.094: INFO: Created: latency-svc-59r8n
Mar 30 21:31:26.111: INFO: Got endpoints: latency-svc-59r8n [524.491686ms]
Mar 30 21:31:26.112: INFO: Got endpoints: latency-svc-zdnrd [525.465048ms]
Mar 30 21:31:26.162: INFO: Created: latency-svc-ghkz5
Mar 30 21:31:26.177: INFO: Got endpoints: latency-svc-ghkz5 [549.652439ms]
Mar 30 21:31:26.183: INFO: Created: latency-svc-slvtp
Mar 30 21:31:26.202: INFO: Got endpoints: latency-svc-slvtp [485.592716ms]
Mar 30 21:31:26.212: INFO: Created: latency-svc-qw2r2
Mar 30 21:31:26.235: INFO: Got endpoints: latency-svc-qw2r2 [516.846921ms]
Mar 30 21:31:26.259: INFO: Created: latency-svc-9wswv
Mar 30 21:31:26.277: INFO: Got endpoints: latency-svc-9wswv [517.998581ms]
Mar 30 21:31:26.290: INFO: Created: latency-svc-25kh9
Mar 30 21:31:26.299: INFO: Got endpoints: latency-svc-25kh9 [469.433405ms]
Mar 30 21:31:26.315: INFO: Created: latency-svc-k668r
Mar 30 21:31:26.329: INFO: Got endpoints: latency-svc-k668r [499.336808ms]
Mar 30 21:31:26.340: INFO: Created: latency-svc-g2wjv
Mar 30 21:31:26.348: INFO: Got endpoints: latency-svc-g2wjv [487.465595ms]
Mar 30 21:31:26.363: INFO: Created: latency-svc-45245
Mar 30 21:31:26.388: INFO: Got endpoints: latency-svc-45245 [483.024826ms]
Mar 30 21:31:26.433: INFO: Created: latency-svc-6zfhk
Mar 30 21:31:26.457: INFO: Got endpoints: latency-svc-6zfhk [546.698574ms]
Mar 30 21:31:26.469: INFO: Created: latency-svc-dj84z
Mar 30 21:31:26.501: INFO: Got endpoints: latency-svc-dj84z [527.357003ms]
Mar 30 21:31:26.502: INFO: Created: latency-svc-mlh95
Mar 30 21:31:26.506: INFO: Got endpoints: latency-svc-mlh95 [497.484722ms]
Mar 30 21:31:26.517: INFO: Created: latency-svc-k6w22
Mar 30 21:31:26.520: INFO: Got endpoints: latency-svc-k6w22 [474.058684ms]
Mar 30 21:31:26.554: INFO: Created: latency-svc-cmh7c
Mar 30 21:31:26.597: INFO: Got endpoints: latency-svc-cmh7c [535.877691ms]
Mar 30 21:31:26.605: INFO: Created: latency-svc-7jz8z
Mar 30 21:31:26.633: INFO: Got endpoints: latency-svc-7jz8z [521.147511ms]
Mar 30 21:31:26.634: INFO: Created: latency-svc-qjn8k
Mar 30 21:31:26.649: INFO: Got endpoints: latency-svc-qjn8k [536.042285ms]
Mar 30 21:31:26.686: INFO: Created: latency-svc-t9nvt
Mar 30 21:31:26.694: INFO: Got endpoints: latency-svc-t9nvt [515.959843ms]
Mar 30 21:31:26.754: INFO: Created: latency-svc-xdbbj
Mar 30 21:31:26.766: INFO: Got endpoints: latency-svc-xdbbj [563.349209ms]
Mar 30 21:31:26.798: INFO: Created: latency-svc-bnpnd
Mar 30 21:31:26.816: INFO: Got endpoints: latency-svc-bnpnd [581.226306ms]
Mar 30 21:31:26.845: INFO: Created: latency-svc-6dkp6
Mar 30 21:31:26.845: INFO: Got endpoints: latency-svc-6dkp6 [567.82024ms]
Mar 30 21:31:27.232: INFO: Created: latency-svc-hwtn5
Mar 30 21:31:27.242: INFO: Got endpoints: latency-svc-hwtn5 [943.369307ms]
Mar 30 21:31:27.258: INFO: Created: latency-svc-94c4x
Mar 30 21:31:27.264: INFO: Got endpoints: latency-svc-94c4x [935.037456ms]
Mar 30 21:31:27.278: INFO: Created: latency-svc-xkv6g
Mar 30 21:31:27.310: INFO: Got endpoints: latency-svc-xkv6g [962.424577ms]
Mar 30 21:31:27.320: INFO: Created: latency-svc-28x5k
Mar 30 21:31:27.327: INFO: Got endpoints: latency-svc-28x5k [938.920431ms]
Mar 30 21:31:27.384: INFO: Created: latency-svc-868rk
Mar 30 21:31:27.390: INFO: Got endpoints: latency-svc-868rk [932.957332ms]
Mar 30 21:31:27.431: INFO: Created: latency-svc-zpwhz
Mar 30 21:31:27.448: INFO: Got endpoints: latency-svc-zpwhz [947.153554ms]
Mar 30 21:31:27.468: INFO: Created: latency-svc-j4pwf
Mar 30 21:31:27.487: INFO: Got endpoints: latency-svc-j4pwf [981.365003ms]
Mar 30 21:31:27.502: INFO: Created: latency-svc-xtpjx
Mar 30 21:31:27.511: INFO: Got endpoints: latency-svc-xtpjx [990.64116ms]
Mar 30 21:31:27.598: INFO: Created: latency-svc-sbqbh
Mar 30 21:31:27.628: INFO: Created: latency-svc-28xc9
Mar 30 21:31:27.632: INFO: Got endpoints: latency-svc-sbqbh [1.034809778s]
Mar 30 21:31:27.646: INFO: Got endpoints: latency-svc-28xc9 [1.012429749s]
Mar 30 21:31:27.659: INFO: Created: latency-svc-25c54
Mar 30 21:31:27.667: INFO: Got endpoints: latency-svc-25c54 [1.017715157s]
Mar 30 21:31:27.696: INFO: Created: latency-svc-4lhff
Mar 30 21:31:27.705: INFO: Got endpoints: latency-svc-4lhff [1.010879429s]
Mar 30 21:31:27.713: INFO: Created: latency-svc-c9z7z
Mar 30 21:31:27.728: INFO: Got endpoints: latency-svc-c9z7z [961.988879ms]
Mar 30 21:31:27.755: INFO: Created: latency-svc-trjnl
Mar 30 21:31:27.766: INFO: Got endpoints: latency-svc-trjnl [949.600893ms]
Mar 30 21:31:27.780: INFO: Created: latency-svc-cwkk2
Mar 30 21:31:27.818: INFO: Got endpoints: latency-svc-cwkk2 [972.733252ms]
Mar 30 21:31:27.830: INFO: Created: latency-svc-45tx5
Mar 30 21:31:27.846: INFO: Created: latency-svc-pznnb
Mar 30 21:31:27.846: INFO: Got endpoints: latency-svc-45tx5 [604.008413ms]
Mar 30 21:31:27.854: INFO: Got endpoints: latency-svc-pznnb [588.810439ms]
Mar 30 21:31:27.908: INFO: Created: latency-svc-bhlxt
Mar 30 21:31:27.939: INFO: Got endpoints: latency-svc-bhlxt [628.440116ms]
Mar 30 21:31:27.948: INFO: Created: latency-svc-ts8ft
Mar 30 21:31:27.965: INFO: Got endpoints: latency-svc-ts8ft [638.273317ms]
Mar 30 21:31:27.971: INFO: Created: latency-svc-bzlnd
Mar 30 21:31:27.985: INFO: Got endpoints: latency-svc-bzlnd [595.421677ms]
Mar 30 21:31:28.033: INFO: Created: latency-svc-k5w9f
Mar 30 21:31:28.090: INFO: Got endpoints: latency-svc-k5w9f [641.908334ms]
Mar 30 21:31:28.098: INFO: Created: latency-svc-r56rk
Mar 30 21:31:28.116: INFO: Got endpoints: latency-svc-r56rk [628.851335ms]
Mar 30 21:31:28.133: INFO: Created: latency-svc-bmwc5
Mar 30 21:31:28.137: INFO: Got endpoints: latency-svc-bmwc5 [625.627824ms]
Mar 30 21:31:28.160: INFO: Created: latency-svc-9bmvq
Mar 30 21:31:28.174: INFO: Got endpoints: latency-svc-9bmvq [541.594711ms]
Mar 30 21:31:28.244: INFO: Created: latency-svc-67dfq
Mar 30 21:31:28.244: INFO: Got endpoints: latency-svc-67dfq [597.837084ms]
Mar 30 21:31:28.698: INFO: Created: latency-svc-ff52v
Mar 30 21:31:28.710: INFO: Got endpoints: latency-svc-ff52v [1.042809043s]
Mar 30 21:31:28.721: INFO: Created: latency-svc-9n69x
Mar 30 21:31:28.739: INFO: Got endpoints: latency-svc-9n69x [1.033997301s]
Mar 30 21:31:28.744: INFO: Created: latency-svc-547zz
Mar 30 21:31:28.759: INFO: Got endpoints: latency-svc-547zz [1.030700139s]
Mar 30 21:31:28.767: INFO: Created: latency-svc-hcl75
Mar 30 21:31:28.789: INFO: Got endpoints: latency-svc-hcl75 [1.022585709s]
Mar 30 21:31:28.802: INFO: Created: latency-svc-chqx5
Mar 30 21:31:28.811: INFO: Got endpoints: latency-svc-chqx5 [993.276723ms]
Mar 30 21:31:28.818: INFO: Created: latency-svc-r52tn
Mar 30 21:31:28.829: INFO: Got endpoints: latency-svc-r52tn [982.486292ms]
Mar 30 21:31:28.847: INFO: Created: latency-svc-69pcn
Mar 30 21:31:28.851: INFO: Got endpoints: latency-svc-69pcn [996.818909ms]
Mar 30 21:31:28.931: INFO: Created: latency-svc-mr862
Mar 30 21:31:28.938: INFO: Got endpoints: latency-svc-mr862 [999.500816ms]
Mar 30 21:31:28.941: INFO: Created: latency-svc-br8t2
Mar 30 21:31:28.954: INFO: Got endpoints: latency-svc-br8t2 [987.949311ms]
Mar 30 21:31:28.975: INFO: Created: latency-svc-wjd7g
Mar 30 21:31:28.986: INFO: Got endpoints: latency-svc-wjd7g [1.000337801s]
Mar 30 21:31:29.003: INFO: Created: latency-svc-npwjm
Mar 30 21:31:29.009: INFO: Got endpoints: latency-svc-npwjm [918.981137ms]
Mar 30 21:31:29.025: INFO: Created: latency-svc-dh8pj
Mar 30 21:31:29.035: INFO: Got endpoints: latency-svc-dh8pj [916.699913ms]
Mar 30 21:31:29.073: INFO: Created: latency-svc-s9vvs
Mar 30 21:31:29.075: INFO: Got endpoints: latency-svc-s9vvs [937.652457ms]
Mar 30 21:31:29.110: INFO: Created: latency-svc-8j7nk
Mar 30 21:31:29.118: INFO: Got endpoints: latency-svc-8j7nk [944.112782ms]
Mar 30 21:31:29.133: INFO: Created: latency-svc-2lpd4
Mar 30 21:31:29.152: INFO: Got endpoints: latency-svc-2lpd4 [907.385589ms]
Mar 30 21:31:29.163: INFO: Created: latency-svc-qgcl5
Mar 30 21:31:29.166: INFO: Got endpoints: latency-svc-qgcl5 [455.854339ms]
Mar 30 21:31:29.185: INFO: Created: latency-svc-cmpw2
Mar 30 21:31:29.211: INFO: Got endpoints: latency-svc-cmpw2 [472.206423ms]
Mar 30 21:31:29.219: INFO: Created: latency-svc-5sckv
Mar 30 21:31:29.289: INFO: Got endpoints: latency-svc-5sckv [529.66684ms]
Mar 30 21:31:29.330: INFO: Created: latency-svc-tr9rv
Mar 30 21:31:29.330: INFO: Got endpoints: latency-svc-tr9rv [541.679756ms]
Mar 30 21:31:29.365: INFO: Created: latency-svc-2cwwv
Mar 30 21:31:29.381: INFO: Got endpoints: latency-svc-2cwwv [569.919824ms]
Mar 30 21:31:29.395: INFO: Created: latency-svc-x69gr
Mar 30 21:31:29.440: INFO: Got endpoints: latency-svc-x69gr [610.672446ms]
Mar 30 21:31:29.448: INFO: Created: latency-svc-kcv2k
Mar 30 21:31:29.448: INFO: Got endpoints: latency-svc-kcv2k [597.036305ms]
Mar 30 21:31:29.473: INFO: Created: latency-svc-pf6wn
Mar 30 21:31:29.494: INFO: Got endpoints: latency-svc-pf6wn [555.804421ms]
Mar 30 21:31:29.500: INFO: Created: latency-svc-ddgzv
Mar 30 21:31:29.528: INFO: Got endpoints: latency-svc-ddgzv [574.525885ms]
Mar 30 21:31:29.542: INFO: Created: latency-svc-skzqv
Mar 30 21:31:29.554: INFO: Got endpoints: latency-svc-skzqv [567.842275ms]
Mar 30 21:31:29.595: INFO: Created: latency-svc-htct9
Mar 30 21:31:29.610: INFO: Got endpoints: latency-svc-htct9 [600.831978ms]
Mar 30 21:31:29.677: INFO: Created: latency-svc-mb9mz
Mar 30 21:31:29.695: INFO: Got endpoints: latency-svc-mb9mz [659.046359ms]
Mar 30 21:31:29.695: INFO: Created: latency-svc-jnv4n
Mar 30 21:31:29.752: INFO: Created: latency-svc-w2vf7
Mar 30 21:31:29.820: INFO: Got endpoints: latency-svc-jnv4n [743.979992ms]
Mar 30 21:31:29.821: INFO: Got endpoints: latency-svc-w2vf7 [702.860602ms]
Mar 30 21:31:29.855: INFO: Created: latency-svc-gxhx2
Mar 30 21:31:29.880: INFO: Got endpoints: latency-svc-gxhx2 [727.974828ms]
Mar 30 21:31:29.896: INFO: Created: latency-svc-cttcx
Mar 30 21:31:29.930: INFO: Got endpoints: latency-svc-cttcx [764.041667ms]
Mar 30 21:31:30.086: INFO: Created: latency-svc-67nq6
Mar 30 21:31:30.099: INFO: Got endpoints: latency-svc-67nq6 [887.821901ms]
Mar 30 21:31:30.108: INFO: Created: latency-svc-zh6gk
Mar 30 21:31:30.116: INFO: Got endpoints: latency-svc-zh6gk [825.37886ms]
Mar 30 21:31:30.124: INFO: Created: latency-svc-mh6x8
Mar 30 21:31:30.132: INFO: Got endpoints: latency-svc-mh6x8 [801.618564ms]
Mar 30 21:31:30.147: INFO: Created: latency-svc-g52jd
Mar 30 21:31:30.163: INFO: Got endpoints: latency-svc-g52jd [781.524958ms]
Mar 30 21:31:30.172: INFO: Created: latency-svc-jgjqg
Mar 30 21:31:30.220: INFO: Got endpoints: latency-svc-jgjqg [779.971926ms]
Mar 30 21:31:30.222: INFO: Created: latency-svc-c729x
Mar 30 21:31:30.227: INFO: Got endpoints: latency-svc-c729x [779.282191ms]
Mar 30 21:31:30.240: INFO: Created: latency-svc-v4hff
Mar 30 21:31:30.248: INFO: Got endpoints: latency-svc-v4hff [754.046014ms]
Mar 30 21:31:30.265: INFO: Created: latency-svc-4d7xh
Mar 30 21:31:30.277: INFO: Got endpoints: latency-svc-4d7xh [748.168674ms]
Mar 30 21:31:30.293: INFO: Created: latency-svc-hlgxn
Mar 30 21:31:30.318: INFO: Got endpoints: latency-svc-hlgxn [763.795344ms]
Mar 30 21:31:30.349: INFO: Created: latency-svc-7d6ww
Mar 30 21:31:30.357: INFO: Got endpoints: latency-svc-7d6ww [747.350779ms]
Mar 30 21:31:30.370: INFO: Created: latency-svc-nxxx9
Mar 30 21:31:30.384: INFO: Got endpoints: latency-svc-nxxx9 [689.542891ms]
Mar 30 21:31:30.398: INFO: Created: latency-svc-dhs6t
Mar 30 21:31:30.403: INFO: Got endpoints: latency-svc-dhs6t [582.337178ms]
Mar 30 21:31:30.416: INFO: Created: latency-svc-zhl87
Mar 30 21:31:30.427: INFO: Got endpoints: latency-svc-zhl87 [605.836832ms]
Mar 30 21:31:30.447: INFO: Created: latency-svc-zxsdq
Mar 30 21:31:30.478: INFO: Got endpoints: latency-svc-zxsdq [597.559006ms]
Mar 30 21:31:30.531: INFO: Created: latency-svc-csrb9
Mar 30 21:31:30.549: INFO: Got endpoints: latency-svc-csrb9 [616.955177ms]
Mar 30 21:31:30.569: INFO: Created: latency-svc-lck7n
Mar 30 21:31:30.583: INFO: Got endpoints: latency-svc-lck7n [484.401711ms]
Mar 30 21:31:30.591: INFO: Created: latency-svc-5hqbz
Mar 30 21:31:30.617: INFO: Got endpoints: latency-svc-5hqbz [500.963272ms]
Mar 30 21:31:30.639: INFO: Created: latency-svc-dt574
Mar 30 21:31:30.647: INFO: Got endpoints: latency-svc-dt574 [514.780013ms]
Mar 30 21:31:30.661: INFO: Created: latency-svc-5qq2j
Mar 30 21:31:30.678: INFO: Got endpoints: latency-svc-5qq2j [515.131817ms]
Mar 30 21:31:30.709: INFO: Created: latency-svc-6jbw6
Mar 30 21:31:30.740: INFO: Got endpoints: latency-svc-6jbw6 [520.2236ms]
Mar 30 21:31:30.748: INFO: Created: latency-svc-hprvm
Mar 30 21:31:30.765: INFO: Got endpoints: latency-svc-hprvm [537.507768ms]
Mar 30 21:31:30.770: INFO: Created: latency-svc-6594m
Mar 30 21:31:30.786: INFO: Got endpoints: latency-svc-6594m [537.648715ms]
Mar 30 21:31:30.792: INFO: Created: latency-svc-w76wf
Mar 30 21:31:30.800: INFO: Got endpoints: latency-svc-w76wf [523.462764ms]
Mar 30 21:31:30.825: INFO: Created: latency-svc-wnzpw
Mar 30 21:31:30.834: INFO: Created: latency-svc-ckpfb
Mar 30 21:31:30.841: INFO: Got endpoints: latency-svc-wnzpw [521.226925ms]
Mar 30 21:31:30.856: INFO: Got endpoints: latency-svc-ckpfb [498.510041ms]
Mar 30 21:31:30.861: INFO: Created: latency-svc-9bgpk
Mar 30 21:31:30.880: INFO: Got endpoints: latency-svc-9bgpk [495.438783ms]
Mar 30 21:31:30.907: INFO: Created: latency-svc-7n52v
Mar 30 21:31:30.929: INFO: Got endpoints: latency-svc-7n52v [526.251949ms]
Mar 30 21:31:30.936: INFO: Created: latency-svc-8x69g
Mar 30 21:31:30.953: INFO: Got endpoints: latency-svc-8x69g [72.639442ms]
Mar 30 21:31:30.978: INFO: Created: latency-svc-f878t
Mar 30 21:31:30.978: INFO: Got endpoints: latency-svc-f878t [550.593168ms]
Mar 30 21:31:31.002: INFO: Created: latency-svc-ddj7t
Mar 30 21:31:31.093: INFO: Got endpoints: latency-svc-ddj7t [615.497915ms]
Mar 30 21:31:31.104: INFO: Created: latency-svc-cd74n
Mar 30 21:31:31.109: INFO: Got endpoints: latency-svc-cd74n [559.19744ms]
Mar 30 21:31:31.163: INFO: Created: latency-svc-cshjr
Mar 30 21:31:31.163: INFO: Got endpoints: latency-svc-cshjr [579.151933ms]
Mar 30 21:31:31.197: INFO: Created: latency-svc-q5kwv
Mar 30 21:31:31.223: INFO: Got endpoints: latency-svc-q5kwv [605.711269ms]
Mar 30 21:31:31.264: INFO: Created: latency-svc-6kqwz
Mar 30 21:31:31.264: INFO: Got endpoints: latency-svc-6kqwz [617.210564ms]
Mar 30 21:31:31.269: INFO: Created: latency-svc-rpvrj
Mar 30 21:31:31.279: INFO: Got endpoints: latency-svc-rpvrj [601.201669ms]
Mar 30 21:31:31.300: INFO: Created: latency-svc-6sx5c
Mar 30 21:31:31.340: INFO: Got endpoints: latency-svc-6sx5c [599.848912ms]
Mar 30 21:31:31.352: INFO: Created: latency-svc-6m2zx
Mar 30 21:31:31.365: INFO: Got endpoints: latency-svc-6m2zx [600.385791ms]
Mar 30 21:31:31.377: INFO: Created: latency-svc-ktwjt
Mar 30 21:31:31.397: INFO: Got endpoints: latency-svc-ktwjt [610.780761ms]
Mar 30 21:31:31.411: INFO: Created: latency-svc-8xvrz
Mar 30 21:31:31.428: INFO: Got endpoints: latency-svc-8xvrz [627.232908ms]
Mar 30 21:31:31.433: INFO: Created: latency-svc-cjffz
Mar 30 21:31:31.443: INFO: Got endpoints: latency-svc-cjffz [601.673777ms]
Mar 30 21:31:31.479: INFO: Created: latency-svc-qj84m
Mar 30 21:31:31.496: INFO: Got endpoints: latency-svc-qj84m [639.642677ms]
Mar 30 21:31:31.501: INFO: Created: latency-svc-h7tqr
Mar 30 21:31:31.515: INFO: Got endpoints: latency-svc-h7tqr [585.434786ms]
Mar 30 21:31:31.528: INFO: Created: latency-svc-c7ffs
Mar 30 21:31:31.542: INFO: Got endpoints: latency-svc-c7ffs [588.414099ms]
Mar 30 21:31:31.587: INFO: Created: latency-svc-gld2n
Mar 30 21:31:31.611: INFO: Got endpoints: latency-svc-gld2n [632.844747ms]
Mar 30 21:31:31.626: INFO: Created: latency-svc-2pcj6
Mar 30 21:31:31.649: INFO: Created: latency-svc-5v6mv
Mar 30 21:31:31.650: INFO: Got endpoints: latency-svc-2pcj6 [556.510498ms]
Mar 30 21:31:31.661: INFO: Got endpoints: latency-svc-5v6mv [551.006167ms]
Mar 30 21:31:31.677: INFO: Created: latency-svc-pm2mz
Mar 30 21:31:31.684: INFO: Got endpoints: latency-svc-pm2mz [520.920752ms]
Mar 30 21:31:31.735: INFO: Created: latency-svc-74642
Mar 30 21:31:31.739: INFO: Got endpoints: latency-svc-74642 [515.28051ms]
Mar 30 21:31:31.770: INFO: Created: latency-svc-s6tq8
Mar 30 21:31:31.770: INFO: Got endpoints: latency-svc-s6tq8 [505.730938ms]
Mar 30 21:31:31.797: INFO: Created: latency-svc-9j9h8
Mar 30 21:31:31.798: INFO: Got endpoints: latency-svc-9j9h8 [518.912615ms]
Mar 30 21:31:31.818: INFO: Created: latency-svc-g4qqv
Mar 30 21:31:31.835: INFO: Got endpoints: latency-svc-g4qqv [494.472807ms]
Mar 30 21:31:31.843: INFO: Created: latency-svc-wzdk9
Mar 30 21:31:31.857: INFO: Got endpoints: latency-svc-wzdk9 [491.435831ms]
Mar 30 21:31:31.879: INFO: Created: latency-svc-wksjv
Mar 30 21:31:31.882: INFO: Got endpoints: latency-svc-wksjv [484.43128ms]
Mar 30 21:31:31.882: INFO: Created: latency-svc-bcfkq
Mar 30 21:31:31.897: INFO: Got endpoints: latency-svc-bcfkq [468.47671ms]
Mar 30 21:31:31.913: INFO: Created: latency-svc-bdjzh
Mar 30 21:31:31.930: INFO: Got endpoints: latency-svc-bdjzh [486.758936ms]
Mar 30 21:31:31.948: INFO: Created: latency-svc-zn7sg
Mar 30 21:31:31.959: INFO: Got endpoints: latency-svc-zn7sg [462.615092ms]
Mar 30 21:31:31.973: INFO: Created: latency-svc-cgjlg
Mar 30 21:31:31.993: INFO: Got endpoints: latency-svc-cgjlg [477.858467ms]
Mar 30 21:31:32.005: INFO: Created: latency-svc-6jwt4
Mar 30 21:31:32.011: INFO: Got endpoints: latency-svc-6jwt4 [469.094012ms]
Mar 30 21:31:32.029: INFO: Created: latency-svc-cxllc
Mar 30 21:31:32.039: INFO: Got endpoints: latency-svc-cxllc [428.117402ms]
Mar 30 21:31:32.040: INFO: Created: latency-svc-622km
Mar 30 21:31:32.058: INFO: Got endpoints: latency-svc-622km [407.842728ms]
Mar 30 21:31:32.082: INFO: Created: latency-svc-gftkz
Mar 30 21:31:32.094: INFO: Created: latency-svc-kxbh9
Mar 30 21:31:32.113: INFO: Created: latency-svc-2m7sx
Mar 30 21:31:32.116: INFO: Got endpoints: latency-svc-gftkz [455.313305ms]
Mar 30 21:31:32.149: INFO: Created: latency-svc-92zzt
Mar 30 21:31:32.155: INFO: Got endpoints: latency-svc-kxbh9 [471.310813ms]
Mar 30 21:31:32.254: INFO: Got endpoints: latency-svc-2m7sx [515.319958ms]
Mar 30 21:31:32.257: INFO: Got endpoints: latency-svc-92zzt [486.558854ms]
Mar 30 21:31:32.257: INFO: Latencies: [42.829965ms 72.639442ms 143.700962ms 162.015589ms 180.258262ms 220.521705ms 259.126612ms 284.646014ms 309.778163ms 356.765769ms 407.842728ms 428.117402ms 455.313305ms 455.854339ms 457.381322ms 462.399558ms 462.615092ms 468.47671ms 469.094012ms 469.433405ms 471.310813ms 472.206423ms 474.058684ms 477.858467ms 483.024826ms 484.401711ms 484.43128ms 485.592716ms 486.558854ms 486.758936ms 487.465595ms 487.706285ms 489.070348ms 491.219971ms 491.435831ms 494.472807ms 495.438783ms 496.219321ms 497.484722ms 498.510041ms 499.336808ms 500.963272ms 504.706707ms 505.730938ms 508.376732ms 514.780013ms 515.131817ms 515.28051ms 515.319958ms 515.959843ms 516.846921ms 517.998581ms 518.912615ms 520.2236ms 520.920752ms 521.147511ms 521.226925ms 522.330867ms 523.462764ms 524.491686ms 525.465048ms 526.251949ms 527.357003ms 528.111699ms 529.66684ms 533.086024ms 533.504657ms 535.877691ms 536.042285ms 537.507768ms 537.648715ms 541.594711ms 541.679756ms 543.38325ms 544.17842ms 544.716578ms 546.698574ms 548.126167ms 549.652439ms 550.593168ms 551.006167ms 551.730762ms 555.804421ms 556.510498ms 557.341836ms 558.035584ms 559.19744ms 563.349209ms 564.357151ms 567.82024ms 567.842275ms 569.919824ms 573.33489ms 574.525885ms 579.151933ms 581.226306ms 581.815752ms 582.337178ms 583.364327ms 585.434786ms 587.926265ms 588.414099ms 588.810439ms 595.421677ms 596.898278ms 597.036305ms 597.559006ms 597.837084ms 599.680954ms 599.848912ms 600.385791ms 600.831978ms 601.077384ms 601.201669ms 601.673777ms 602.531032ms 604.008413ms 605.711269ms 605.836832ms 610.672446ms 610.780761ms 613.652737ms 615.024308ms 615.497915ms 615.830908ms 616.955177ms 617.210564ms 622.609816ms 625.627824ms 627.232908ms 628.440116ms 628.851335ms 632.844747ms 638.273317ms 639.642677ms 641.908334ms 642.927483ms 649.347685ms 652.973586ms 655.035786ms 659.046359ms 659.195341ms 663.369391ms 666.397463ms 671.736445ms 676.782045ms 680.330493ms 680.616283ms 689.542891ms 702.860602ms 711.594423ms 727.974828ms 738.085135ms 743.979992ms 747.350779ms 748.168674ms 754.046014ms 761.99993ms 763.795344ms 764.041667ms 778.440588ms 779.282191ms 779.971926ms 780.425899ms 781.524958ms 801.618564ms 813.29245ms 815.746126ms 825.37886ms 887.821901ms 907.385589ms 916.699913ms 918.981137ms 932.957332ms 935.037456ms 937.652457ms 938.920431ms 943.369307ms 944.112782ms 947.153554ms 949.600893ms 961.988879ms 962.424577ms 972.733252ms 981.365003ms 982.486292ms 987.949311ms 990.64116ms 993.276723ms 996.818909ms 999.500816ms 1.000337801s 1.010879429s 1.012429749s 1.017715157s 1.022585709s 1.030700139s 1.033997301s 1.034809778s 1.042809043s]
Mar 30 21:31:32.257: INFO: 50 %ile: 587.926265ms
Mar 30 21:31:32.257: INFO: 90 %ile: 949.600893ms
Mar 30 21:31:32.257: INFO: 99 %ile: 1.034809778s
Mar 30 21:31:32.257: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:31:32.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8374" for this suite.

• [SLOW TEST:17.037 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":278,"completed":79,"skipped":1477,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:31:32.286: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1797
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 21:31:32.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 version'
Mar 30 21:31:32.573: INFO: stderr: ""
Mar 30 21:31:32.573: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.3\", GitCommit:\"06ad960bfd03b39c8310aaf92d1e7c12ce618213\", GitTreeState:\"clean\", BuildDate:\"2020-02-11T18:14:22Z\", GoVersion:\"go1.13.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.3\", GitCommit:\"06ad960bfd03b39c8310aaf92d1e7c12ce618213\", GitTreeState:\"clean\", BuildDate:\"2020-02-16T17:47:13Z\", GoVersion:\"go1.13.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:31:32.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1797" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":278,"completed":80,"skipped":1498,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:31:32.598: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4602
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 21:31:32.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 create -f - --namespace=kubectl-4602'
Mar 30 21:31:33.381: INFO: stderr: ""
Mar 30 21:31:33.381: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Mar 30 21:31:33.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 create -f - --namespace=kubectl-4602'
Mar 30 21:31:33.846: INFO: stderr: ""
Mar 30 21:31:33.846: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Mar 30 21:31:34.854: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 21:31:34.854: INFO: Found 0 / 1
Mar 30 21:31:35.853: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 21:31:35.853: INFO: Found 0 / 1
Mar 30 21:31:36.855: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 21:31:36.855: INFO: Found 0 / 1
Mar 30 21:31:37.856: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 21:31:37.856: INFO: Found 0 / 1
Mar 30 21:31:38.856: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 21:31:38.856: INFO: Found 0 / 1
Mar 30 21:31:39.854: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 21:31:39.854: INFO: Found 0 / 1
Mar 30 21:31:40.866: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 21:31:40.866: INFO: Found 0 / 1
Mar 30 21:31:41.855: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 21:31:41.855: INFO: Found 1 / 1
Mar 30 21:31:41.855: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 30 21:31:41.866: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 21:31:41.866: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 30 21:31:41.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 describe pod agnhost-master-kf8gr --namespace=kubectl-4602'
Mar 30 21:31:42.037: INFO: stderr: ""
Mar 30 21:31:42.038: INFO: stdout: "Name:         agnhost-master-kf8gr\nNamespace:    kubectl-4602\nPriority:     0\nNode:         worker-pool1-vvs2j292-eccd-ci-os-12-jenkins/10.0.10.8\nStart Time:   Mon, 30 Mar 2020 21:31:33 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"k8s-pod-network\",\n                    \"ips\": [\n                        \"192.168.185.233\"\n                    ],\n                    \"dns\": {}\n                }]\n              kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           192.168.185.233\nIPs:\n  IP:           192.168.185.233\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   docker://07efb00236f84005628681cf446d4f925aff5788515d684040c157fb3ec64afb\n    Image:          gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 30 Mar 2020 21:31:40 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-n5kb6 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-n5kb6:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-n5kb6\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                  Message\n  ----    ------     ----  ----                                                  -------\n  Normal  Scheduled  9s    default-scheduler                                     Successfully assigned kubectl-4602/agnhost-master-kf8gr to worker-pool1-vvs2j292-eccd-ci-os-12-jenkins\n  Normal  Pulled     2s    kubelet, worker-pool1-vvs2j292-eccd-ci-os-12-jenkins  Container image \"gcr.io/kubernetes-e2e-test-images/agnhost:2.8\" already present on machine\n  Normal  Created    2s    kubelet, worker-pool1-vvs2j292-eccd-ci-os-12-jenkins  Created container agnhost-master\n  Normal  Started    2s    kubelet, worker-pool1-vvs2j292-eccd-ci-os-12-jenkins  Started container agnhost-master\n"
Mar 30 21:31:42.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 describe rc agnhost-master --namespace=kubectl-4602'
Mar 30 21:31:42.233: INFO: stderr: ""
Mar 30 21:31:42.233: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-4602\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  9s    replication-controller  Created pod: agnhost-master-kf8gr\n"
Mar 30 21:31:42.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 describe service agnhost-master --namespace=kubectl-4602'
Mar 30 21:31:42.439: INFO: stderr: ""
Mar 30 21:31:42.439: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-4602\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                10.104.229.190\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         <none>\nSession Affinity:  None\nEvents:            <none>\n"
Mar 30 21:31:42.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 describe node master-0-eccd-ci-os-12-jenkins'
Mar 30 21:31:42.726: INFO: stderr: ""
Mar 30 21:31:42.726: INFO: stdout: "Name:               master-0-eccd-ci-os-12-jenkins\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=6a206a90-861c-4dde-ae91-16afc43dc070\n                    beta.kubernetes.io/os=linux\n                    ccd/version=2.8.0\n                    failure-domain.beta.kubernetes.io/region=regionOne\n                    failure-domain.beta.kubernetes.io/zone=nova\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=master-0-eccd-ci-os-12-jenkins\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/instance-type=6a206a90-861c-4dde-ae91-16afc43dc070\n                    node.uuid=ea691530-d18e-42ad-80af-1b78b94f0cad\n                    node.uuid_source=cloud-init\n                    topology.kubernetes.io/region=regionOne\n                    topology.kubernetes.io/zone=nova\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 30 Mar 2020 20:38:48 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  master-0-eccd-ci-os-12-jenkins\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 30 Mar 2020 21:31:33 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Mon, 30 Mar 2020 21:30:32 +0000   Mon, 30 Mar 2020 20:38:40 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Mon, 30 Mar 2020 21:30:32 +0000   Mon, 30 Mar 2020 20:38:40 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Mon, 30 Mar 2020 21:30:32 +0000   Mon, 30 Mar 2020 20:38:40 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Mon, 30 Mar 2020 21:30:32 +0000   Mon, 30 Mar 2020 20:41:09 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.0.10.15\n  Hostname:    master-0-eccd-ci-os-12-jenkins\nCapacity:\n  cpu:                2\n  ephemeral-storage:  20931216Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3064740Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  19290208634\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             2962340Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 ea691530d18e42ad80af1b78b94f0cad\n  System UUID:                ea691530-d18e-42ad-80af-1b78b94f0cad\n  Boot ID:                    1ea7cba6-0e2d-4475-9a05-f05b66999535\n  Kernel Version:             4.12.14-197.34-default\n  OS Image:                   SUSE Linux Enterprise Server 15 SP1\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://19.3.5\n  Kubelet Version:            v1.17.3\n  Kube-Proxy Version:         v1.17.3\nPodCIDR:                      192.168.0.0/24\nPodCIDRs:                     192.168.0.0/24\nProviderID:                   openstack:///ea691530-d18e-42ad-80af-1b78b94f0cad\nNon-terminated Pods:          (11 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-84bfdc9fb9-7sz92                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         50m\n  kube-system                 calico-node-hsdz4                                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         50m\n  kube-system                 coredns-b597b7c45-6vdgq                                    100m (5%)     0 (0%)      70Mi (2%)        170Mi (5%)     51m\n  kube-system                 kube-apiserver-master-0-eccd-ci-os-12-jenkins              250m (12%)    0 (0%)      0 (0%)           0 (0%)         50m\n  kube-system                 kube-controller-manager-master-0-eccd-ci-os-12-jenkins     200m (10%)    0 (0%)      0 (0%)           0 (0%)         50m\n  kube-system                 kube-multus-ds-amd64-9rz6v                                 100m (5%)     100m (5%)   50Mi (1%)        50Mi (1%)      48m\n  kube-system                 kube-proxy-wc4sf                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         49m\n  kube-system                 kube-scheduler-master-0-eccd-ci-os-12-jenkins              100m (5%)     0 (0%)      0 (0%)           0 (0%)         50m\n  kube-system                 openstack-cloud-controller-manager-q2xdr                   200m (10%)    0 (0%)      0 (0%)           0 (0%)         51m\n  monitoring                  eric-pm-server-node-exporter-57sfx                         100m (5%)     200m (10%)  100Mi (3%)       150Mi (5%)     39m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-4495c60740044637-27tzg    0 (0%)        0 (0%)      0 (0%)           0 (0%)         27m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1300m (65%)  300m (15%)\n  memory             220Mi (7%)   370Mi (12%)\n  ephemeral-storage  0 (0%)       0 (0%)\nEvents:\n  Type    Reason                   Age                From                                        Message\n  ----    ------                   ----               ----                                        -------\n  Normal  NodeHasSufficientMemory  53m (x5 over 53m)  kubelet, master-0-eccd-ci-os-12-jenkins     Node master-0-eccd-ci-os-12-jenkins status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    53m (x5 over 53m)  kubelet, master-0-eccd-ci-os-12-jenkins     Node master-0-eccd-ci-os-12-jenkins status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     53m (x4 over 53m)  kubelet, master-0-eccd-ci-os-12-jenkins     Node master-0-eccd-ci-os-12-jenkins status is now: NodeHasSufficientPID\n  Normal  Starting                 52m                kubelet, master-0-eccd-ci-os-12-jenkins     Starting kubelet.\n  Normal  NodeHasSufficientMemory  52m                kubelet, master-0-eccd-ci-os-12-jenkins     Node master-0-eccd-ci-os-12-jenkins status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    52m                kubelet, master-0-eccd-ci-os-12-jenkins     Node master-0-eccd-ci-os-12-jenkins status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     52m                kubelet, master-0-eccd-ci-os-12-jenkins     Node master-0-eccd-ci-os-12-jenkins status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  52m                kubelet, master-0-eccd-ci-os-12-jenkins     Updated Node Allocatable limit across pods\n  Normal  Starting                 51m                kubelet, master-0-eccd-ci-os-12-jenkins     Starting kubelet.\n  Normal  NodeHasSufficientMemory  51m                kubelet, master-0-eccd-ci-os-12-jenkins     Node master-0-eccd-ci-os-12-jenkins status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    51m                kubelet, master-0-eccd-ci-os-12-jenkins     Node master-0-eccd-ci-os-12-jenkins status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     51m                kubelet, master-0-eccd-ci-os-12-jenkins     Node master-0-eccd-ci-os-12-jenkins status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  51m                kubelet, master-0-eccd-ci-os-12-jenkins     Updated Node Allocatable limit across pods\n  Normal  NodeReady                50m                kubelet, master-0-eccd-ci-os-12-jenkins     Node master-0-eccd-ci-os-12-jenkins status is now: NodeReady\n  Normal  Starting                 49m                kube-proxy, master-0-eccd-ci-os-12-jenkins  Starting kube-proxy.\n"
Mar 30 21:31:42.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 describe namespace kubectl-4602'
Mar 30 21:31:42.883: INFO: stderr: ""
Mar 30 21:31:42.883: INFO: stdout: "Name:         kubectl-4602\nLabels:       e2e-framework=kubectl\n              e2e-run=cdbd2f2a-9a47-451c-bcee-d3eae1eea6f3\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:31:42.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4602" for this suite.

• [SLOW TEST:10.315 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1154
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":278,"completed":81,"skipped":1524,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:31:42.915: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5880
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Mar 30 21:31:43.160: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Mar 30 21:31:59.839: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 21:32:04.302: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:32:20.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5880" for this suite.

• [SLOW TEST:37.363 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":278,"completed":82,"skipped":1547,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:32:20.278: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4227
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0330 21:32:51.031543      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 30 21:32:51.032: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:32:51.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4227" for this suite.

• [SLOW TEST:30.770 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":278,"completed":83,"skipped":1565,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:32:51.056: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-21
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 21:32:51.260: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:32:52.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-21" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":278,"completed":84,"skipped":1600,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:32:52.345: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7675
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-375a38f4-54c3-474f-b29c-b2280e1f31e6
STEP: Creating a pod to test consume configMaps
Mar 30 21:32:52.563: INFO: Waiting up to 5m0s for pod "pod-configmaps-85d63966-dd1b-467d-83a5-9954dce5ae2c" in namespace "configmap-7675" to be "success or failure"
Mar 30 21:32:52.571: INFO: Pod "pod-configmaps-85d63966-dd1b-467d-83a5-9954dce5ae2c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.096414ms
Mar 30 21:32:54.577: INFO: Pod "pod-configmaps-85d63966-dd1b-467d-83a5-9954dce5ae2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013335824s
Mar 30 21:32:56.585: INFO: Pod "pod-configmaps-85d63966-dd1b-467d-83a5-9954dce5ae2c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021056653s
Mar 30 21:32:58.591: INFO: Pod "pod-configmaps-85d63966-dd1b-467d-83a5-9954dce5ae2c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027325977s
Mar 30 21:33:00.598: INFO: Pod "pod-configmaps-85d63966-dd1b-467d-83a5-9954dce5ae2c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.03479086s
Mar 30 21:33:02.814: INFO: Pod "pod-configmaps-85d63966-dd1b-467d-83a5-9954dce5ae2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.250294701s
STEP: Saw pod success
Mar 30 21:33:02.814: INFO: Pod "pod-configmaps-85d63966-dd1b-467d-83a5-9954dce5ae2c" satisfied condition "success or failure"
Mar 30 21:33:02.836: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-configmaps-85d63966-dd1b-467d-83a5-9954dce5ae2c container configmap-volume-test: <nil>
STEP: delete the pod
Mar 30 21:33:02.911: INFO: Waiting for pod pod-configmaps-85d63966-dd1b-467d-83a5-9954dce5ae2c to disappear
Mar 30 21:33:02.915: INFO: Pod pod-configmaps-85d63966-dd1b-467d-83a5-9954dce5ae2c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:33:02.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7675" for this suite.

• [SLOW TEST:10.615 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":85,"skipped":1609,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:33:02.963: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-180
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 30 21:33:04.603: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 30 21:33:06.622: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200785, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200785, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200785, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200784, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:33:08.629: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200785, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200785, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200785, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200784, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:33:10.627: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200785, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200785, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200785, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200784, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:33:12.627: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200785, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200785, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200785, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200784, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:33:14.628: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200785, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200785, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200785, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721200784, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 30 21:33:17.659: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:33:29.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-180" for this suite.
STEP: Destroying namespace "webhook-180-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:27.089 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":278,"completed":86,"skipped":1609,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:33:30.053: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2645
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 30 21:33:38.795: INFO: Successfully updated pod "pod-update-5b2c2132-a639-4a40-8203-78b198164663"
STEP: verifying the updated pod is in kubernetes
Mar 30 21:33:38.803: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:33:38.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2645" for this suite.

• [SLOW TEST:8.770 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":278,"completed":87,"skipped":1616,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:33:38.825: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2806
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0330 21:33:49.098718      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 30 21:33:49.099: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:33:49.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2806" for this suite.

• [SLOW TEST:10.292 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":278,"completed":88,"skipped":1630,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:33:49.118: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-6587
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
STEP: reading a file in the container
Mar 30 21:33:58.023: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6587 pod-service-account-edb8eedf-37cf-4948-b7df-db00805fb964 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Mar 30 21:33:58.598: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6587 pod-service-account-edb8eedf-37cf-4948-b7df-db00805fb964 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Mar 30 21:33:58.862: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6587 pod-service-account-edb8eedf-37cf-4948-b7df-db00805fb964 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:33:59.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6587" for this suite.

• [SLOW TEST:9.992 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":278,"completed":89,"skipped":1643,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:33:59.113: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4672
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl label
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1382
STEP: creating the pod
Mar 30 21:33:59.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 create -f - --namespace=kubectl-4672'
Mar 30 21:33:59.784: INFO: stderr: ""
Mar 30 21:33:59.784: INFO: stdout: "pod/pause created\n"
Mar 30 21:33:59.784: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar 30 21:33:59.785: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4672" to be "running and ready"
Mar 30 21:33:59.797: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 11.761109ms
Mar 30 21:34:01.803: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018464649s
Mar 30 21:34:03.810: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024801623s
Mar 30 21:34:05.815: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030561492s
Mar 30 21:34:07.828: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 8.042991393s
Mar 30 21:34:07.828: INFO: Pod "pause" satisfied condition "running and ready"
Mar 30 21:34:07.828: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: adding the label testing-label with value testing-label-value to a pod
Mar 30 21:34:07.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 label pods pause testing-label=testing-label-value --namespace=kubectl-4672'
Mar 30 21:34:07.936: INFO: stderr: ""
Mar 30 21:34:07.936: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar 30 21:34:07.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pod pause -L testing-label --namespace=kubectl-4672'
Mar 30 21:34:08.029: INFO: stderr: ""
Mar 30 21:34:08.029: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          9s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar 30 21:34:08.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 label pods pause testing-label- --namespace=kubectl-4672'
Mar 30 21:34:08.125: INFO: stderr: ""
Mar 30 21:34:08.125: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar 30 21:34:08.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pod pause -L testing-label --namespace=kubectl-4672'
Mar 30 21:34:08.204: INFO: stderr: ""
Mar 30 21:34:08.204: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          9s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
STEP: using delete to clean up resources
Mar 30 21:34:08.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 delete --grace-period=0 --force -f - --namespace=kubectl-4672'
Mar 30 21:34:08.317: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 30 21:34:08.317: INFO: stdout: "pod \"pause\" force deleted\n"
Mar 30 21:34:08.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get rc,svc -l name=pause --no-headers --namespace=kubectl-4672'
Mar 30 21:34:08.412: INFO: stderr: "No resources found in kubectl-4672 namespace.\n"
Mar 30 21:34:08.412: INFO: stdout: ""
Mar 30 21:34:08.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods -l name=pause --namespace=kubectl-4672 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 30 21:34:08.530: INFO: stderr: ""
Mar 30 21:34:08.530: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:34:08.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4672" for this suite.

• [SLOW TEST:9.456 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1379
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":278,"completed":90,"skipped":1663,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:34:08.569: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7219
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Mar 30 21:34:17.434: INFO: Successfully updated pod "annotationupdatec0e31d57-1cc6-4e09-bc3a-751b7caf05f6"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:34:19.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7219" for this suite.

• [SLOW TEST:10.907 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":278,"completed":91,"skipped":1677,"failed":0}
SSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:34:19.477: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-6634
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 21:34:19.683: INFO: (0) /api/v1/nodes/worker-pool1-zo88v95j-eccd-ci-os-12-jenkins:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 15.923131ms)
Mar 30 21:34:19.689: INFO: (1) /api/v1/nodes/worker-pool1-zo88v95j-eccd-ci-os-12-jenkins:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 5.871045ms)
Mar 30 21:34:19.696: INFO: (2) /api/v1/nodes/worker-pool1-zo88v95j-eccd-ci-os-12-jenkins:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 6.203955ms)
Mar 30 21:34:19.701: INFO: (3) /api/v1/nodes/worker-pool1-zo88v95j-eccd-ci-os-12-jenkins:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 5.222622ms)
Mar 30 21:34:19.706: INFO: (4) /api/v1/nodes/worker-pool1-zo88v95j-eccd-ci-os-12-jenkins:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 5.123695ms)
Mar 30 21:34:19.712: INFO: (5) /api/v1/nodes/worker-pool1-zo88v95j-eccd-ci-os-12-jenkins:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 5.878672ms)
Mar 30 21:34:19.718: INFO: (6) /api/v1/nodes/worker-pool1-zo88v95j-eccd-ci-os-12-jenkins:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 5.501564ms)
Mar 30 21:34:19.730: INFO: (7) /api/v1/nodes/worker-pool1-zo88v95j-eccd-ci-os-12-jenkins:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 12.344278ms)
Mar 30 21:34:19.737: INFO: (8) /api/v1/nodes/worker-pool1-zo88v95j-eccd-ci-os-12-jenkins:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 6.128996ms)
Mar 30 21:34:19.742: INFO: (9) /api/v1/nodes/worker-pool1-zo88v95j-eccd-ci-os-12-jenkins:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 4.976202ms)
Mar 30 21:34:19.747: INFO: (10) /api/v1/nodes/worker-pool1-zo88v95j-eccd-ci-os-12-jenkins:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 4.975449ms)
Mar 30 21:34:19.753: INFO: (11) /api/v1/nodes/worker-pool1-zo88v95j-eccd-ci-os-12-jenkins:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 6.038225ms)
Mar 30 21:34:19.759: INFO: (12) /api/v1/nodes/worker-pool1-zo88v95j-eccd-ci-os-12-jenkins:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 6.035792ms)
Mar 30 21:34:19.765: INFO: (13) /api/v1/nodes/worker-pool1-zo88v95j-eccd-ci-os-12-jenkins:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 5.488747ms)
Mar 30 21:34:19.771: INFO: (14) /api/v1/nodes/worker-pool1-zo88v95j-eccd-ci-os-12-jenkins:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 6.065138ms)
Mar 30 21:34:19.777: INFO: (15) /api/v1/nodes/worker-pool1-zo88v95j-eccd-ci-os-12-jenkins:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 5.696317ms)
Mar 30 21:34:19.783: INFO: (16) /api/v1/nodes/worker-pool1-zo88v95j-eccd-ci-os-12-jenkins:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 6.155041ms)
Mar 30 21:34:19.790: INFO: (17) /api/v1/nodes/worker-pool1-zo88v95j-eccd-ci-os-12-jenkins:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 6.336817ms)
Mar 30 21:34:19.795: INFO: (18) /api/v1/nodes/worker-pool1-zo88v95j-eccd-ci-os-12-jenkins:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 5.271343ms)
Mar 30 21:34:19.801: INFO: (19) /api/v1/nodes/worker-pool1-zo88v95j-eccd-ci-os-12-jenkins:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 5.819639ms)
[AfterEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:34:19.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6634" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":278,"completed":92,"skipped":1681,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:34:19.841: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-132
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 21:34:20.019: INFO: Creating ReplicaSet my-hostname-basic-ae183c53-6d27-485f-b67b-a08002ed19a6
Mar 30 21:34:20.034: INFO: Pod name my-hostname-basic-ae183c53-6d27-485f-b67b-a08002ed19a6: Found 0 pods out of 1
Mar 30 21:34:25.048: INFO: Pod name my-hostname-basic-ae183c53-6d27-485f-b67b-a08002ed19a6: Found 1 pods out of 1
Mar 30 21:34:25.048: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-ae183c53-6d27-485f-b67b-a08002ed19a6" is running
Mar 30 21:34:29.058: INFO: Pod "my-hostname-basic-ae183c53-6d27-485f-b67b-a08002ed19a6-wcnc9" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-30 21:34:20 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-30 21:34:20 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-ae183c53-6d27-485f-b67b-a08002ed19a6]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-30 21:34:20 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-ae183c53-6d27-485f-b67b-a08002ed19a6]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-30 21:34:20 +0000 UTC Reason: Message:}])
Mar 30 21:34:29.058: INFO: Trying to dial the pod
Mar 30 21:34:34.077: INFO: Controller my-hostname-basic-ae183c53-6d27-485f-b67b-a08002ed19a6: Got expected result from replica 1 [my-hostname-basic-ae183c53-6d27-485f-b67b-a08002ed19a6-wcnc9]: "my-hostname-basic-ae183c53-6d27-485f-b67b-a08002ed19a6-wcnc9", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:34:34.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-132" for this suite.

• [SLOW TEST:14.265 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":278,"completed":93,"skipped":1709,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:34:34.108: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1438
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-3782bf63-107a-450e-9995-e25237c4cddb
STEP: Creating a pod to test consume secrets
Mar 30 21:34:34.314: INFO: Waiting up to 5m0s for pod "pod-secrets-02eca3ba-6974-4d40-9ceb-02edb349b40c" in namespace "secrets-1438" to be "success or failure"
Mar 30 21:34:34.321: INFO: Pod "pod-secrets-02eca3ba-6974-4d40-9ceb-02edb349b40c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.854695ms
Mar 30 21:34:36.325: INFO: Pod "pod-secrets-02eca3ba-6974-4d40-9ceb-02edb349b40c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01117209s
Mar 30 21:34:38.330: INFO: Pod "pod-secrets-02eca3ba-6974-4d40-9ceb-02edb349b40c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01592455s
Mar 30 21:34:40.338: INFO: Pod "pod-secrets-02eca3ba-6974-4d40-9ceb-02edb349b40c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02436726s
Mar 30 21:34:42.345: INFO: Pod "pod-secrets-02eca3ba-6974-4d40-9ceb-02edb349b40c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.031575393s
Mar 30 21:34:44.353: INFO: Pod "pod-secrets-02eca3ba-6974-4d40-9ceb-02edb349b40c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.039641447s
STEP: Saw pod success
Mar 30 21:34:44.354: INFO: Pod "pod-secrets-02eca3ba-6974-4d40-9ceb-02edb349b40c" satisfied condition "success or failure"
Mar 30 21:34:44.358: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-secrets-02eca3ba-6974-4d40-9ceb-02edb349b40c container secret-volume-test: <nil>
STEP: delete the pod
Mar 30 21:34:44.391: INFO: Waiting for pod pod-secrets-02eca3ba-6974-4d40-9ceb-02edb349b40c to disappear
Mar 30 21:34:44.395: INFO: Pod pod-secrets-02eca3ba-6974-4d40-9ceb-02edb349b40c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:34:44.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1438" for this suite.

• [SLOW TEST:10.305 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":94,"skipped":1727,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:34:44.414: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2385
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 30 21:34:44.606: INFO: Waiting up to 5m0s for pod "pod-748b895b-c5a5-49bb-8332-f7c0ea1ef1f8" in namespace "emptydir-2385" to be "success or failure"
Mar 30 21:34:44.617: INFO: Pod "pod-748b895b-c5a5-49bb-8332-f7c0ea1ef1f8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.280427ms
Mar 30 21:34:46.623: INFO: Pod "pod-748b895b-c5a5-49bb-8332-f7c0ea1ef1f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016713001s
Mar 30 21:34:48.628: INFO: Pod "pod-748b895b-c5a5-49bb-8332-f7c0ea1ef1f8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022449872s
Mar 30 21:34:50.634: INFO: Pod "pod-748b895b-c5a5-49bb-8332-f7c0ea1ef1f8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027879167s
Mar 30 21:34:52.642: INFO: Pod "pod-748b895b-c5a5-49bb-8332-f7c0ea1ef1f8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.036065566s
Mar 30 21:34:54.649: INFO: Pod "pod-748b895b-c5a5-49bb-8332-f7c0ea1ef1f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.042652629s
STEP: Saw pod success
Mar 30 21:34:54.649: INFO: Pod "pod-748b895b-c5a5-49bb-8332-f7c0ea1ef1f8" satisfied condition "success or failure"
Mar 30 21:34:54.653: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-748b895b-c5a5-49bb-8332-f7c0ea1ef1f8 container test-container: <nil>
STEP: delete the pod
Mar 30 21:34:54.691: INFO: Waiting for pod pod-748b895b-c5a5-49bb-8332-f7c0ea1ef1f8 to disappear
Mar 30 21:34:54.695: INFO: Pod pod-748b895b-c5a5-49bb-8332-f7c0ea1ef1f8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:34:54.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2385" for this suite.

• [SLOW TEST:10.303 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":95,"skipped":1730,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:34:54.720: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-322
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar 30 21:35:03.966: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:35:05.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-322" for this suite.

• [SLOW TEST:10.305 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":278,"completed":96,"skipped":1746,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:35:05.030: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1965
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-9wp8s in namespace proxy-1965
I0330 21:35:05.253119      20 runners.go:189] Created replication controller with name: proxy-service-9wp8s, namespace: proxy-1965, replica count: 1
I0330 21:35:06.305238      20 runners.go:189] proxy-service-9wp8s Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0330 21:35:07.305773      20 runners.go:189] proxy-service-9wp8s Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0330 21:35:08.306153      20 runners.go:189] proxy-service-9wp8s Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0330 21:35:09.306593      20 runners.go:189] proxy-service-9wp8s Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0330 21:35:10.307161      20 runners.go:189] proxy-service-9wp8s Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0330 21:35:11.308466      20 runners.go:189] proxy-service-9wp8s Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0330 21:35:12.308957      20 runners.go:189] proxy-service-9wp8s Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0330 21:35:13.309696      20 runners.go:189] proxy-service-9wp8s Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0330 21:35:14.310444      20 runners.go:189] proxy-service-9wp8s Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0330 21:35:15.310956      20 runners.go:189] proxy-service-9wp8s Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0330 21:35:16.311206      20 runners.go:189] proxy-service-9wp8s Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0330 21:35:17.311543      20 runners.go:189] proxy-service-9wp8s Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0330 21:35:18.311876      20 runners.go:189] proxy-service-9wp8s Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0330 21:35:19.312526      20 runners.go:189] proxy-service-9wp8s Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0330 21:35:20.312821      20 runners.go:189] proxy-service-9wp8s Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0330 21:35:21.313671      20 runners.go:189] proxy-service-9wp8s Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0330 21:35:22.314163      20 runners.go:189] proxy-service-9wp8s Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 30 21:35:22.321: INFO: setup took 17.113345048s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar 30 21:35:22.339: INFO: (0) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">test<... (200; 16.615058ms)
Mar 30 21:35:22.339: INFO: (0) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">... (200; 17.039299ms)
Mar 30 21:35:22.339: INFO: (0) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 17.73713ms)
Mar 30 21:35:22.340: INFO: (0) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 17.417323ms)
Mar 30 21:35:22.340: INFO: (0) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname2/proxy/: tls qux (200; 18.844545ms)
Mar 30 21:35:22.341: INFO: (0) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:462/proxy/: tls qux (200; 19.32597ms)
Mar 30 21:35:22.342: INFO: (0) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname1/proxy/: foo (200; 20.2225ms)
Mar 30 21:35:22.344: INFO: (0) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname2/proxy/: bar (200; 22.324293ms)
Mar 30 21:35:22.344: INFO: (0) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/tlsrewritem... (200; 22.642295ms)
Mar 30 21:35:22.345: INFO: (0) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/rewriteme">test</a> (200; 22.372746ms)
Mar 30 21:35:22.345: INFO: (0) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 22.939107ms)
Mar 30 21:35:22.346: INFO: (0) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname2/proxy/: bar (200; 23.375826ms)
Mar 30 21:35:22.346: INFO: (0) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 23.77528ms)
Mar 30 21:35:22.346: INFO: (0) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname1/proxy/: foo (200; 24.639388ms)
Mar 30 21:35:22.346: INFO: (0) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:460/proxy/: tls baz (200; 24.075868ms)
Mar 30 21:35:22.346: INFO: (0) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname1/proxy/: tls baz (200; 24.495169ms)
Mar 30 21:35:22.352: INFO: (1) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/tlsrewritem... (200; 5.752349ms)
Mar 30 21:35:22.353: INFO: (1) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 5.721836ms)
Mar 30 21:35:22.354: INFO: (1) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">test<... (200; 7.431857ms)
Mar 30 21:35:22.354: INFO: (1) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">... (200; 7.603658ms)
Mar 30 21:35:22.356: INFO: (1) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 9.625217ms)
Mar 30 21:35:22.356: INFO: (1) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:460/proxy/: tls baz (200; 9.35005ms)
Mar 30 21:35:22.357: INFO: (1) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/rewriteme">test</a> (200; 10.10055ms)
Mar 30 21:35:22.357: INFO: (1) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 9.730568ms)
Mar 30 21:35:22.359: INFO: (1) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:462/proxy/: tls qux (200; 11.976907ms)
Mar 30 21:35:22.359: INFO: (1) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 11.31968ms)
Mar 30 21:35:22.359: INFO: (1) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname1/proxy/: tls baz (200; 11.965718ms)
Mar 30 21:35:22.359: INFO: (1) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname2/proxy/: bar (200; 12.322448ms)
Mar 30 21:35:22.359: INFO: (1) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname1/proxy/: foo (200; 12.935546ms)
Mar 30 21:35:22.359: INFO: (1) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname2/proxy/: bar (200; 12.260738ms)
Mar 30 21:35:22.360: INFO: (1) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname1/proxy/: foo (200; 12.684555ms)
Mar 30 21:35:22.361: INFO: (1) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname2/proxy/: tls qux (200; 13.813561ms)
Mar 30 21:35:22.366: INFO: (2) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:460/proxy/: tls baz (200; 4.871315ms)
Mar 30 21:35:22.369: INFO: (2) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 8.284682ms)
Mar 30 21:35:22.369: INFO: (2) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname1/proxy/: foo (200; 8.541497ms)
Mar 30 21:35:22.371: INFO: (2) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:462/proxy/: tls qux (200; 9.807109ms)
Mar 30 21:35:22.373: INFO: (2) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 11.121584ms)
Mar 30 21:35:22.373: INFO: (2) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/tlsrewritem... (200; 12.195605ms)
Mar 30 21:35:22.376: INFO: (2) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname2/proxy/: tls qux (200; 14.122453ms)
Mar 30 21:35:22.377: INFO: (2) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 15.585559ms)
Mar 30 21:35:22.378: INFO: (2) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 16.640913ms)
Mar 30 21:35:22.379: INFO: (2) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname1/proxy/: tls baz (200; 16.953408ms)
Mar 30 21:35:22.379: INFO: (2) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/rewriteme">test</a> (200; 17.194133ms)
Mar 30 21:35:22.379: INFO: (2) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">... (200; 17.933124ms)
Mar 30 21:35:22.379: INFO: (2) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">test<... (200; 17.998108ms)
Mar 30 21:35:22.380: INFO: (2) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname2/proxy/: bar (200; 18.439283ms)
Mar 30 21:35:22.380: INFO: (2) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname1/proxy/: foo (200; 18.913189ms)
Mar 30 21:35:22.380: INFO: (2) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname2/proxy/: bar (200; 18.917039ms)
Mar 30 21:35:22.389: INFO: (3) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname1/proxy/: foo (200; 8.625454ms)
Mar 30 21:35:22.390: INFO: (3) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">test<... (200; 8.429774ms)
Mar 30 21:35:22.390: INFO: (3) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 9.194878ms)
Mar 30 21:35:22.390: INFO: (3) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:460/proxy/: tls baz (200; 9.097722ms)
Mar 30 21:35:22.392: INFO: (3) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname2/proxy/: bar (200; 11.442064ms)
Mar 30 21:35:22.393: INFO: (3) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname1/proxy/: foo (200; 12.430474ms)
Mar 30 21:35:22.393: INFO: (3) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:462/proxy/: tls qux (200; 12.529618ms)
Mar 30 21:35:22.394: INFO: (3) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname2/proxy/: tls qux (200; 12.574592ms)
Mar 30 21:35:22.394: INFO: (3) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 12.971957ms)
Mar 30 21:35:22.395: INFO: (3) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 13.38389ms)
Mar 30 21:35:22.395: INFO: (3) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname2/proxy/: bar (200; 13.723901ms)
Mar 30 21:35:22.396: INFO: (3) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 14.454682ms)
Mar 30 21:35:22.396: INFO: (3) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">... (200; 14.314798ms)
Mar 30 21:35:22.396: INFO: (3) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/rewriteme">test</a> (200; 15.456338ms)
Mar 30 21:35:22.399: INFO: (3) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/tlsrewritem... (200; 17.304197ms)
Mar 30 21:35:22.400: INFO: (3) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname1/proxy/: tls baz (200; 18.497008ms)
Mar 30 21:35:22.405: INFO: (4) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:462/proxy/: tls qux (200; 5.704911ms)
Mar 30 21:35:22.409: INFO: (4) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 8.151676ms)
Mar 30 21:35:22.409: INFO: (4) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">test<... (200; 7.938045ms)
Mar 30 21:35:22.410: INFO: (4) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/rewriteme">test</a> (200; 8.21743ms)
Mar 30 21:35:22.411: INFO: (4) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/tlsrewritem... (200; 10.032805ms)
Mar 30 21:35:22.414: INFO: (4) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">... (200; 13.271626ms)
Mar 30 21:35:22.414: INFO: (4) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 13.884845ms)
Mar 30 21:35:22.414: INFO: (4) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname1/proxy/: foo (200; 14.459288ms)
Mar 30 21:35:22.415: INFO: (4) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname2/proxy/: bar (200; 14.957582ms)
Mar 30 21:35:22.416: INFO: (4) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 15.8296ms)
Mar 30 21:35:22.416: INFO: (4) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname1/proxy/: foo (200; 15.669578ms)
Mar 30 21:35:22.417: INFO: (4) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 15.310012ms)
Mar 30 21:35:22.417: INFO: (4) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname2/proxy/: tls qux (200; 16.394599ms)
Mar 30 21:35:22.417: INFO: (4) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:460/proxy/: tls baz (200; 16.275088ms)
Mar 30 21:35:22.417: INFO: (4) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname1/proxy/: tls baz (200; 16.531754ms)
Mar 30 21:35:22.417: INFO: (4) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname2/proxy/: bar (200; 15.247399ms)
Mar 30 21:35:22.426: INFO: (5) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/rewriteme">test</a> (200; 8.599932ms)
Mar 30 21:35:22.426: INFO: (5) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:460/proxy/: tls baz (200; 9.436031ms)
Mar 30 21:35:22.427: INFO: (5) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/tlsrewritem... (200; 9.930282ms)
Mar 30 21:35:22.427: INFO: (5) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 10.030036ms)
Mar 30 21:35:22.433: INFO: (5) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 15.633204ms)
Mar 30 21:35:22.433: INFO: (5) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:462/proxy/: tls qux (200; 15.958952ms)
Mar 30 21:35:22.434: INFO: (5) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname1/proxy/: foo (200; 16.965219ms)
Mar 30 21:35:22.434: INFO: (5) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname2/proxy/: tls qux (200; 16.994086ms)
Mar 30 21:35:22.434: INFO: (5) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">... (200; 16.813609ms)
Mar 30 21:35:22.435: INFO: (5) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 17.350238ms)
Mar 30 21:35:22.435: INFO: (5) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">test<... (200; 17.240028ms)
Mar 30 21:35:22.435: INFO: (5) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 17.196418ms)
Mar 30 21:35:22.437: INFO: (5) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname2/proxy/: bar (200; 19.56667ms)
Mar 30 21:35:22.438: INFO: (5) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname1/proxy/: tls baz (200; 20.853375ms)
Mar 30 21:35:22.440: INFO: (5) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname2/proxy/: bar (200; 22.621941ms)
Mar 30 21:35:22.441: INFO: (5) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname1/proxy/: foo (200; 23.430221ms)
Mar 30 21:35:22.452: INFO: (6) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 11.042537ms)
Mar 30 21:35:22.452: INFO: (6) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/rewriteme">test</a> (200; 10.820741ms)
Mar 30 21:35:22.452: INFO: (6) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 9.752933ms)
Mar 30 21:35:22.454: INFO: (6) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:460/proxy/: tls baz (200; 12.186182ms)
Mar 30 21:35:22.462: INFO: (6) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname2/proxy/: tls qux (200; 19.808178ms)
Mar 30 21:35:22.462: INFO: (6) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 19.376126ms)
Mar 30 21:35:22.462: INFO: (6) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname2/proxy/: bar (200; 20.548538ms)
Mar 30 21:35:22.462: INFO: (6) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">test<... (200; 19.222633ms)
Mar 30 21:35:22.463: INFO: (6) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname1/proxy/: foo (200; 20.804748ms)
Mar 30 21:35:22.464: INFO: (6) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 21.778291ms)
Mar 30 21:35:22.465: INFO: (6) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname2/proxy/: bar (200; 22.613385ms)
Mar 30 21:35:22.465: INFO: (6) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:462/proxy/: tls qux (200; 23.357266ms)
Mar 30 21:35:22.465: INFO: (6) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname1/proxy/: tls baz (200; 23.195533ms)
Mar 30 21:35:22.465: INFO: (6) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">... (200; 22.018478ms)
Mar 30 21:35:22.466: INFO: (6) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/tlsrewritem... (200; 22.766161ms)
Mar 30 21:35:22.469: INFO: (6) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname1/proxy/: foo (200; 26.464502ms)
Mar 30 21:35:22.484: INFO: (7) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/rewriteme">test</a> (200; 13.748072ms)
Mar 30 21:35:22.484: INFO: (7) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:460/proxy/: tls baz (200; 13.458931ms)
Mar 30 21:35:22.484: INFO: (7) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:462/proxy/: tls qux (200; 14.146535ms)
Mar 30 21:35:22.497: INFO: (7) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 25.755314ms)
Mar 30 21:35:22.497: INFO: (7) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/tlsrewritem... (200; 26.146622ms)
Mar 30 21:35:22.497: INFO: (7) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname2/proxy/: bar (200; 27.392341ms)
Mar 30 21:35:22.498: INFO: (7) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 26.200709ms)
Mar 30 21:35:22.498: INFO: (7) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 28.260166ms)
Mar 30 21:35:22.498: INFO: (7) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname1/proxy/: tls baz (200; 27.689216ms)
Mar 30 21:35:22.498: INFO: (7) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">... (200; 26.803398ms)
Mar 30 21:35:22.498: INFO: (7) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">test<... (200; 26.756782ms)
Mar 30 21:35:22.498: INFO: (7) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname1/proxy/: foo (200; 27.580102ms)
Mar 30 21:35:22.498: INFO: (7) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 27.478245ms)
Mar 30 21:35:22.499: INFO: (7) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname2/proxy/: tls qux (200; 28.158611ms)
Mar 30 21:35:22.505: INFO: (7) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname1/proxy/: foo (200; 35.545928ms)
Mar 30 21:35:22.505: INFO: (7) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname2/proxy/: bar (200; 34.756189ms)
Mar 30 21:35:22.515: INFO: (8) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">... (200; 7.164336ms)
Mar 30 21:35:22.520: INFO: (8) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 12.691134ms)
Mar 30 21:35:22.520: INFO: (8) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:460/proxy/: tls baz (200; 13.551906ms)
Mar 30 21:35:22.521: INFO: (8) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 13.500454ms)
Mar 30 21:35:22.521: INFO: (8) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname1/proxy/: tls baz (200; 15.068048ms)
Mar 30 21:35:22.521: INFO: (8) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 14.396765ms)
Mar 30 21:35:22.521: INFO: (8) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/tlsrewritem... (200; 14.098914ms)
Mar 30 21:35:22.522: INFO: (8) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">test<... (200; 14.019698ms)
Mar 30 21:35:22.522: INFO: (8) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname2/proxy/: tls qux (200; 15.41511ms)
Mar 30 21:35:22.522: INFO: (8) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:462/proxy/: tls qux (200; 16.008036ms)
Mar 30 21:35:22.522: INFO: (8) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/rewriteme">test</a> (200; 13.748657ms)
Mar 30 21:35:22.522: INFO: (8) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname2/proxy/: bar (200; 14.244622ms)
Mar 30 21:35:22.522: INFO: (8) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname1/proxy/: foo (200; 16.232411ms)
Mar 30 21:35:22.522: INFO: (8) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 15.481476ms)
Mar 30 21:35:22.523: INFO: (8) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname1/proxy/: foo (200; 15.975415ms)
Mar 30 21:35:22.526: INFO: (8) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname2/proxy/: bar (200; 18.997837ms)
Mar 30 21:35:22.531: INFO: (9) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 4.991992ms)
Mar 30 21:35:22.533: INFO: (9) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">... (200; 6.772362ms)
Mar 30 21:35:22.534: INFO: (9) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:460/proxy/: tls baz (200; 8.489221ms)
Mar 30 21:35:22.534: INFO: (9) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/tlsrewritem... (200; 8.229068ms)
Mar 30 21:35:22.535: INFO: (9) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname2/proxy/: bar (200; 9.059827ms)
Mar 30 21:35:22.537: INFO: (9) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 10.380922ms)
Mar 30 21:35:22.537: INFO: (9) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:462/proxy/: tls qux (200; 10.441681ms)
Mar 30 21:35:22.538: INFO: (9) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">test<... (200; 11.450625ms)
Mar 30 21:35:22.538: INFO: (9) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname1/proxy/: foo (200; 11.92133ms)
Mar 30 21:35:22.539: INFO: (9) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname1/proxy/: tls baz (200; 11.590122ms)
Mar 30 21:35:22.539: INFO: (9) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname2/proxy/: bar (200; 12.540552ms)
Mar 30 21:35:22.539: INFO: (9) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname1/proxy/: foo (200; 12.365606ms)
Mar 30 21:35:22.539: INFO: (9) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/rewriteme">test</a> (200; 12.53487ms)
Mar 30 21:35:22.540: INFO: (9) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 13.52169ms)
Mar 30 21:35:22.540: INFO: (9) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 13.763175ms)
Mar 30 21:35:22.541: INFO: (9) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname2/proxy/: tls qux (200; 14.184293ms)
Mar 30 21:35:22.552: INFO: (10) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/rewriteme">test</a> (200; 8.891311ms)
Mar 30 21:35:22.553: INFO: (10) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 10.722916ms)
Mar 30 21:35:22.553: INFO: (10) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:460/proxy/: tls baz (200; 10.956699ms)
Mar 30 21:35:22.553: INFO: (10) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">test<... (200; 10.720915ms)
Mar 30 21:35:22.553: INFO: (10) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 11.053089ms)
Mar 30 21:35:22.554: INFO: (10) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 11.359203ms)
Mar 30 21:35:22.554: INFO: (10) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">... (200; 11.236814ms)
Mar 30 21:35:22.554: INFO: (10) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/tlsrewritem... (200; 11.705476ms)
Mar 30 21:35:22.554: INFO: (10) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:462/proxy/: tls qux (200; 12.769251ms)
Mar 30 21:35:22.554: INFO: (10) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 11.618817ms)
Mar 30 21:35:22.561: INFO: (10) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname2/proxy/: bar (200; 19.3367ms)
Mar 30 21:35:22.562: INFO: (10) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname1/proxy/: foo (200; 19.106538ms)
Mar 30 21:35:22.562: INFO: (10) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname2/proxy/: tls qux (200; 19.823882ms)
Mar 30 21:35:22.563: INFO: (10) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname1/proxy/: foo (200; 21.46418ms)
Mar 30 21:35:22.563: INFO: (10) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname2/proxy/: bar (200; 20.452927ms)
Mar 30 21:35:22.565: INFO: (10) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname1/proxy/: tls baz (200; 23.504786ms)
Mar 30 21:35:22.571: INFO: (11) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 4.894922ms)
Mar 30 21:35:22.575: INFO: (11) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 8.21557ms)
Mar 30 21:35:22.575: INFO: (11) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">test<... (200; 8.442804ms)
Mar 30 21:35:22.575: INFO: (11) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">... (200; 8.565863ms)
Mar 30 21:35:22.576: INFO: (11) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 9.427251ms)
Mar 30 21:35:22.577: INFO: (11) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/tlsrewritem... (200; 10.656508ms)
Mar 30 21:35:22.578: INFO: (11) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/rewriteme">test</a> (200; 11.047195ms)
Mar 30 21:35:22.578: INFO: (11) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:460/proxy/: tls baz (200; 11.849464ms)
Mar 30 21:35:22.583: INFO: (11) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 16.897967ms)
Mar 30 21:35:22.585: INFO: (11) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname2/proxy/: bar (200; 18.976852ms)
Mar 30 21:35:22.585: INFO: (11) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname1/proxy/: foo (200; 18.670837ms)
Mar 30 21:35:22.586: INFO: (11) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:462/proxy/: tls qux (200; 19.551243ms)
Mar 30 21:35:22.587: INFO: (11) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname2/proxy/: bar (200; 20.658123ms)
Mar 30 21:35:22.588: INFO: (11) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname1/proxy/: foo (200; 20.620068ms)
Mar 30 21:35:22.589: INFO: (11) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname1/proxy/: tls baz (200; 21.55565ms)
Mar 30 21:35:22.590: INFO: (11) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname2/proxy/: tls qux (200; 22.308624ms)
Mar 30 21:35:22.598: INFO: (12) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:460/proxy/: tls baz (200; 8.039883ms)
Mar 30 21:35:22.599: INFO: (12) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:462/proxy/: tls qux (200; 8.623154ms)
Mar 30 21:35:22.599: INFO: (12) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 9.713432ms)
Mar 30 21:35:22.602: INFO: (12) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/tlsrewritem... (200; 11.406301ms)
Mar 30 21:35:22.603: INFO: (12) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 12.451136ms)
Mar 30 21:35:22.603: INFO: (12) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname2/proxy/: tls qux (200; 13.001183ms)
Mar 30 21:35:22.605: INFO: (12) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/rewriteme">test</a> (200; 15.17344ms)
Mar 30 21:35:22.606: INFO: (12) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname2/proxy/: bar (200; 16.348558ms)
Mar 30 21:35:22.608: INFO: (12) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname1/proxy/: tls baz (200; 18.325529ms)
Mar 30 21:35:22.608: INFO: (12) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname1/proxy/: foo (200; 18.392615ms)
Mar 30 21:35:22.608: INFO: (12) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname2/proxy/: bar (200; 18.169465ms)
Mar 30 21:35:22.609: INFO: (12) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 19.21317ms)
Mar 30 21:35:22.610: INFO: (12) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">... (200; 19.037587ms)
Mar 30 21:35:22.610: INFO: (12) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 19.530563ms)
Mar 30 21:35:22.612: INFO: (12) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">test<... (200; 21.095793ms)
Mar 30 21:35:22.612: INFO: (12) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname1/proxy/: foo (200; 21.214903ms)
Mar 30 21:35:22.620: INFO: (13) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 8.081874ms)
Mar 30 21:35:22.620: INFO: (13) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/rewriteme">test</a> (200; 8.595952ms)
Mar 30 21:35:22.622: INFO: (13) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname1/proxy/: foo (200; 10.682103ms)
Mar 30 21:35:22.622: INFO: (13) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 10.212397ms)
Mar 30 21:35:22.623: INFO: (13) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:460/proxy/: tls baz (200; 10.86355ms)
Mar 30 21:35:22.623: INFO: (13) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">test<... (200; 10.749443ms)
Mar 30 21:35:22.624: INFO: (13) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 11.449198ms)
Mar 30 21:35:22.624: INFO: (13) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname1/proxy/: foo (200; 12.237419ms)
Mar 30 21:35:22.625: INFO: (13) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname2/proxy/: tls qux (200; 12.933917ms)
Mar 30 21:35:22.626: INFO: (13) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/tlsrewritem... (200; 13.682815ms)
Mar 30 21:35:22.626: INFO: (13) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname2/proxy/: bar (200; 13.939807ms)
Mar 30 21:35:22.627: INFO: (13) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname1/proxy/: tls baz (200; 14.763599ms)
Mar 30 21:35:22.627: INFO: (13) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 14.35932ms)
Mar 30 21:35:22.630: INFO: (13) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:462/proxy/: tls qux (200; 18.033044ms)
Mar 30 21:35:22.633: INFO: (13) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">... (200; 20.454191ms)
Mar 30 21:35:22.633: INFO: (13) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname2/proxy/: bar (200; 21.243145ms)
Mar 30 21:35:22.644: INFO: (14) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/tlsrewritem... (200; 9.418239ms)
Mar 30 21:35:22.644: INFO: (14) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 9.908511ms)
Mar 30 21:35:22.645: INFO: (14) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 10.661254ms)
Mar 30 21:35:22.645: INFO: (14) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/rewriteme">test</a> (200; 10.222314ms)
Mar 30 21:35:22.645: INFO: (14) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">... (200; 10.752015ms)
Mar 30 21:35:22.648: INFO: (14) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname2/proxy/: tls qux (200; 14.72777ms)
Mar 30 21:35:22.649: INFO: (14) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:462/proxy/: tls qux (200; 15.675625ms)
Mar 30 21:35:22.650: INFO: (14) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:460/proxy/: tls baz (200; 16.087402ms)
Mar 30 21:35:22.650: INFO: (14) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">test<... (200; 15.931107ms)
Mar 30 21:35:22.650: INFO: (14) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname1/proxy/: foo (200; 16.002685ms)
Mar 30 21:35:22.651: INFO: (14) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname1/proxy/: foo (200; 17.31437ms)
Mar 30 21:35:22.661: INFO: (14) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname2/proxy/: bar (200; 26.23522ms)
Mar 30 21:35:22.661: INFO: (14) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 26.909523ms)
Mar 30 21:35:22.661: INFO: (14) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 26.490607ms)
Mar 30 21:35:22.661: INFO: (14) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname1/proxy/: tls baz (200; 27.658325ms)
Mar 30 21:35:22.661: INFO: (14) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname2/proxy/: bar (200; 27.484312ms)
Mar 30 21:35:22.671: INFO: (15) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 9.160649ms)
Mar 30 21:35:22.671: INFO: (15) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">... (200; 9.32975ms)
Mar 30 21:35:22.676: INFO: (15) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:460/proxy/: tls baz (200; 14.323388ms)
Mar 30 21:35:22.676: INFO: (15) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:462/proxy/: tls qux (200; 13.789595ms)
Mar 30 21:35:22.676: INFO: (15) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 13.988689ms)
Mar 30 21:35:22.677: INFO: (15) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/rewriteme">test</a> (200; 14.409343ms)
Mar 30 21:35:22.678: INFO: (15) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 16.104961ms)
Mar 30 21:35:22.679: INFO: (15) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 16.229607ms)
Mar 30 21:35:22.679: INFO: (15) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname1/proxy/: foo (200; 16.287694ms)
Mar 30 21:35:22.679: INFO: (15) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/tlsrewritem... (200; 16.336255ms)
Mar 30 21:35:22.679: INFO: (15) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname1/proxy/: tls baz (200; 16.549065ms)
Mar 30 21:35:22.680: INFO: (15) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname1/proxy/: foo (200; 17.017858ms)
Mar 30 21:35:22.680: INFO: (15) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname2/proxy/: tls qux (200; 17.47071ms)
Mar 30 21:35:22.680: INFO: (15) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname2/proxy/: bar (200; 18.209113ms)
Mar 30 21:35:22.681: INFO: (15) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname2/proxy/: bar (200; 18.855656ms)
Mar 30 21:35:22.681: INFO: (15) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">test<... (200; 19.779943ms)
Mar 30 21:35:22.690: INFO: (16) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/tlsrewritem... (200; 7.98559ms)
Mar 30 21:35:22.690: INFO: (16) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">... (200; 8.515166ms)
Mar 30 21:35:22.691: INFO: (16) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 8.525847ms)
Mar 30 21:35:22.691: INFO: (16) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">test<... (200; 9.456914ms)
Mar 30 21:35:22.699: INFO: (16) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname1/proxy/: foo (200; 16.466768ms)
Mar 30 21:35:22.699: INFO: (16) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname2/proxy/: bar (200; 16.025103ms)
Mar 30 21:35:22.699: INFO: (16) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 15.945546ms)
Mar 30 21:35:22.699: INFO: (16) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname2/proxy/: bar (200; 17.164556ms)
Mar 30 21:35:22.699: INFO: (16) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname2/proxy/: tls qux (200; 16.88332ms)
Mar 30 21:35:22.700: INFO: (16) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 16.764385ms)
Mar 30 21:35:22.700: INFO: (16) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:462/proxy/: tls qux (200; 17.405189ms)
Mar 30 21:35:22.700: INFO: (16) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/rewriteme">test</a> (200; 17.799214ms)
Mar 30 21:35:22.700: INFO: (16) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:460/proxy/: tls baz (200; 17.66756ms)
Mar 30 21:35:22.701: INFO: (16) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname1/proxy/: tls baz (200; 18.087565ms)
Mar 30 21:35:22.701: INFO: (16) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 17.624417ms)
Mar 30 21:35:22.701: INFO: (16) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname1/proxy/: foo (200; 18.868969ms)
Mar 30 21:35:22.718: INFO: (17) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/tlsrewritem... (200; 17.040944ms)
Mar 30 21:35:22.723: INFO: (17) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 20.811323ms)
Mar 30 21:35:22.723: INFO: (17) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:462/proxy/: tls qux (200; 21.728932ms)
Mar 30 21:35:22.723: INFO: (17) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">... (200; 22.133091ms)
Mar 30 21:35:22.727: INFO: (17) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 25.865004ms)
Mar 30 21:35:22.727: INFO: (17) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:460/proxy/: tls baz (200; 25.589183ms)
Mar 30 21:35:22.728: INFO: (17) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 25.911836ms)
Mar 30 21:35:22.730: INFO: (17) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname1/proxy/: foo (200; 28.682191ms)
Mar 30 21:35:22.731: INFO: (17) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname2/proxy/: bar (200; 29.638456ms)
Mar 30 21:35:22.731: INFO: (17) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname1/proxy/: tls baz (200; 29.579635ms)
Mar 30 21:35:22.731: INFO: (17) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname1/proxy/: foo (200; 30.560255ms)
Mar 30 21:35:22.732: INFO: (17) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/rewriteme">test</a> (200; 30.664994ms)
Mar 30 21:35:22.732: INFO: (17) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">test<... (200; 31.04246ms)
Mar 30 21:35:22.732: INFO: (17) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 30.427625ms)
Mar 30 21:35:22.733: INFO: (17) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname2/proxy/: bar (200; 30.806657ms)
Mar 30 21:35:22.733: INFO: (17) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname2/proxy/: tls qux (200; 31.404307ms)
Mar 30 21:35:22.740: INFO: (18) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 7.245176ms)
Mar 30 21:35:22.744: INFO: (18) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">test<... (200; 10.060881ms)
Mar 30 21:35:22.744: INFO: (18) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/rewriteme">test</a> (200; 11.051674ms)
Mar 30 21:35:22.745: INFO: (18) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:462/proxy/: tls qux (200; 11.680664ms)
Mar 30 21:35:22.745: INFO: (18) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/tlsrewritem... (200; 11.457724ms)
Mar 30 21:35:22.749: INFO: (18) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 14.803697ms)
Mar 30 21:35:22.749: INFO: (18) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:460/proxy/: tls baz (200; 14.993458ms)
Mar 30 21:35:22.751: INFO: (18) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname1/proxy/: tls baz (200; 17.163314ms)
Mar 30 21:35:22.752: INFO: (18) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname2/proxy/: bar (200; 18.374183ms)
Mar 30 21:35:22.752: INFO: (18) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname1/proxy/: foo (200; 17.517637ms)
Mar 30 21:35:22.752: INFO: (18) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 18.363085ms)
Mar 30 21:35:22.752: INFO: (18) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 18.455595ms)
Mar 30 21:35:22.753: INFO: (18) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname2/proxy/: tls qux (200; 19.009006ms)
Mar 30 21:35:22.753: INFO: (18) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">... (200; 18.499332ms)
Mar 30 21:35:22.753: INFO: (18) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname2/proxy/: bar (200; 19.464313ms)
Mar 30 21:35:22.754: INFO: (18) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname1/proxy/: foo (200; 20.470216ms)
Mar 30 21:35:22.762: INFO: (19) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:462/proxy/: tls qux (200; 7.847214ms)
Mar 30 21:35:22.763: INFO: (19) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 6.815935ms)
Mar 30 21:35:22.764: INFO: (19) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj/proxy/rewriteme">test</a> (200; 7.904374ms)
Mar 30 21:35:22.765: INFO: (19) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 9.626142ms)
Mar 30 21:35:22.765: INFO: (19) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">... (200; 9.055679ms)
Mar 30 21:35:22.767: INFO: (19) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname1/proxy/: foo (200; 12.763418ms)
Mar 30 21:35:22.767: INFO: (19) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname2/proxy/: bar (200; 11.170519ms)
Mar 30 21:35:22.768: INFO: (19) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:443/proxy/tlsrewritem... (200; 12.449798ms)
Mar 30 21:35:22.768: INFO: (19) /api/v1/namespaces/proxy-1965/pods/https:proxy-service-9wp8s-pf2lj:460/proxy/: tls baz (200; 12.884192ms)
Mar 30 21:35:22.768: INFO: (19) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname1/proxy/: tls baz (200; 13.326339ms)
Mar 30 21:35:22.770: INFO: (19) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:160/proxy/: foo (200; 14.749727ms)
Mar 30 21:35:22.771: INFO: (19) /api/v1/namespaces/proxy-1965/pods/http:proxy-service-9wp8s-pf2lj:162/proxy/: bar (200; 15.627471ms)
Mar 30 21:35:22.772: INFO: (19) /api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1965/pods/proxy-service-9wp8s-pf2lj:1080/proxy/rewriteme">test<... (200; 15.879661ms)
Mar 30 21:35:22.772: INFO: (19) /api/v1/namespaces/proxy-1965/services/https:proxy-service-9wp8s:tlsportname2/proxy/: tls qux (200; 17.237975ms)
Mar 30 21:35:22.772: INFO: (19) /api/v1/namespaces/proxy-1965/services/proxy-service-9wp8s:portname1/proxy/: foo (200; 16.658274ms)
Mar 30 21:35:22.773: INFO: (19) /api/v1/namespaces/proxy-1965/services/http:proxy-service-9wp8s:portname2/proxy/: bar (200; 17.552858ms)
STEP: deleting ReplicationController proxy-service-9wp8s in namespace proxy-1965, will wait for the garbage collector to delete the pods
Mar 30 21:35:22.843: INFO: Deleting ReplicationController proxy-service-9wp8s took: 14.366986ms
Mar 30 21:35:23.443: INFO: Terminating ReplicationController proxy-service-9wp8s pods took: 600.401606ms
[AfterEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:35:25.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1965" for this suite.

• [SLOW TEST:20.634 seconds]
[sig-network] Proxy
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":278,"completed":97,"skipped":1801,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:35:25.666: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6082
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Starting the proxy
Mar 30 21:35:25.870: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-408641372 proxy --unix-socket=/tmp/kubectl-proxy-unix258747955/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:35:25.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6082" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":278,"completed":98,"skipped":1816,"failed":0}
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:35:25.965: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5488
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override arguments
Mar 30 21:35:26.179: INFO: Waiting up to 5m0s for pod "client-containers-3c3b050b-5bd2-4e56-bbe2-1a00569635e3" in namespace "containers-5488" to be "success or failure"
Mar 30 21:35:26.186: INFO: Pod "client-containers-3c3b050b-5bd2-4e56-bbe2-1a00569635e3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.467584ms
Mar 30 21:35:28.197: INFO: Pod "client-containers-3c3b050b-5bd2-4e56-bbe2-1a00569635e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017836458s
Mar 30 21:35:30.204: INFO: Pod "client-containers-3c3b050b-5bd2-4e56-bbe2-1a00569635e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024614637s
Mar 30 21:35:32.211: INFO: Pod "client-containers-3c3b050b-5bd2-4e56-bbe2-1a00569635e3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.031141464s
Mar 30 21:35:34.216: INFO: Pod "client-containers-3c3b050b-5bd2-4e56-bbe2-1a00569635e3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.036826587s
Mar 30 21:35:36.221: INFO: Pod "client-containers-3c3b050b-5bd2-4e56-bbe2-1a00569635e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.041762032s
STEP: Saw pod success
Mar 30 21:35:36.221: INFO: Pod "client-containers-3c3b050b-5bd2-4e56-bbe2-1a00569635e3" satisfied condition "success or failure"
Mar 30 21:35:36.226: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod client-containers-3c3b050b-5bd2-4e56-bbe2-1a00569635e3 container test-container: <nil>
STEP: delete the pod
Mar 30 21:35:36.257: INFO: Waiting for pod client-containers-3c3b050b-5bd2-4e56-bbe2-1a00569635e3 to disappear
Mar 30 21:35:36.261: INFO: Pod client-containers-3c3b050b-5bd2-4e56-bbe2-1a00569635e3 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:35:36.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5488" for this suite.

• [SLOW TEST:10.316 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":278,"completed":99,"skipped":1822,"failed":0}
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:35:36.282: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6068
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override all
Mar 30 21:35:36.471: INFO: Waiting up to 5m0s for pod "client-containers-c4cb5389-8397-4a9d-99fe-36fbd3e713ac" in namespace "containers-6068" to be "success or failure"
Mar 30 21:35:36.476: INFO: Pod "client-containers-c4cb5389-8397-4a9d-99fe-36fbd3e713ac": Phase="Pending", Reason="", readiness=false. Elapsed: 5.265264ms
Mar 30 21:35:38.482: INFO: Pod "client-containers-c4cb5389-8397-4a9d-99fe-36fbd3e713ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011252869s
Mar 30 21:35:40.488: INFO: Pod "client-containers-c4cb5389-8397-4a9d-99fe-36fbd3e713ac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017121279s
Mar 30 21:35:42.495: INFO: Pod "client-containers-c4cb5389-8397-4a9d-99fe-36fbd3e713ac": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024224701s
Mar 30 21:35:44.500: INFO: Pod "client-containers-c4cb5389-8397-4a9d-99fe-36fbd3e713ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.029271782s
STEP: Saw pod success
Mar 30 21:35:44.501: INFO: Pod "client-containers-c4cb5389-8397-4a9d-99fe-36fbd3e713ac" satisfied condition "success or failure"
Mar 30 21:35:44.506: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod client-containers-c4cb5389-8397-4a9d-99fe-36fbd3e713ac container test-container: <nil>
STEP: delete the pod
Mar 30 21:35:44.535: INFO: Waiting for pod client-containers-c4cb5389-8397-4a9d-99fe-36fbd3e713ac to disappear
Mar 30 21:35:44.540: INFO: Pod client-containers-c4cb5389-8397-4a9d-99fe-36fbd3e713ac no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:35:44.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6068" for this suite.

• [SLOW TEST:8.278 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":278,"completed":100,"skipped":1828,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:35:44.562: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1293
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar 30 21:35:44.761: INFO: Pod name pod-release: Found 0 pods out of 1
Mar 30 21:35:49.771: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:35:50.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1293" for this suite.

• [SLOW TEST:6.257 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":278,"completed":101,"skipped":1835,"failed":0}
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:35:50.819: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5404
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test env composition
Mar 30 21:35:51.009: INFO: Waiting up to 5m0s for pod "var-expansion-35a22e87-cdf3-4505-ae35-9c38bb2d395e" in namespace "var-expansion-5404" to be "success or failure"
Mar 30 21:35:51.013: INFO: Pod "var-expansion-35a22e87-cdf3-4505-ae35-9c38bb2d395e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.931215ms
Mar 30 21:35:53.020: INFO: Pod "var-expansion-35a22e87-cdf3-4505-ae35-9c38bb2d395e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011351937s
Mar 30 21:35:55.025: INFO: Pod "var-expansion-35a22e87-cdf3-4505-ae35-9c38bb2d395e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016647718s
Mar 30 21:35:57.035: INFO: Pod "var-expansion-35a22e87-cdf3-4505-ae35-9c38bb2d395e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026007342s
Mar 30 21:35:59.044: INFO: Pod "var-expansion-35a22e87-cdf3-4505-ae35-9c38bb2d395e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.034960496s
Mar 30 21:36:01.049: INFO: Pod "var-expansion-35a22e87-cdf3-4505-ae35-9c38bb2d395e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.039839742s
STEP: Saw pod success
Mar 30 21:36:01.049: INFO: Pod "var-expansion-35a22e87-cdf3-4505-ae35-9c38bb2d395e" satisfied condition "success or failure"
Mar 30 21:36:01.060: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod var-expansion-35a22e87-cdf3-4505-ae35-9c38bb2d395e container dapi-container: <nil>
STEP: delete the pod
Mar 30 21:36:01.095: INFO: Waiting for pod var-expansion-35a22e87-cdf3-4505-ae35-9c38bb2d395e to disappear
Mar 30 21:36:01.100: INFO: Pod var-expansion-35a22e87-cdf3-4505-ae35-9c38bb2d395e no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:36:01.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5404" for this suite.

• [SLOW TEST:10.300 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":278,"completed":102,"skipped":1840,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:36:01.121: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8757
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-8757
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-8757
STEP: Creating statefulset with conflicting port in namespace statefulset-8757
STEP: Waiting until pod test-pod will start running in namespace statefulset-8757
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8757
Mar 30 21:36:09.402: INFO: Observed stateful pod in namespace: statefulset-8757, name: ss-0, uid: fd88ee58-11a3-4ced-ae37-a6d2a6e9d1c5, status phase: Pending. Waiting for statefulset controller to delete.
Mar 30 21:36:09.990: INFO: Observed stateful pod in namespace: statefulset-8757, name: ss-0, uid: fd88ee58-11a3-4ced-ae37-a6d2a6e9d1c5, status phase: Failed. Waiting for statefulset controller to delete.
Mar 30 21:36:10.000: INFO: Observed stateful pod in namespace: statefulset-8757, name: ss-0, uid: fd88ee58-11a3-4ced-ae37-a6d2a6e9d1c5, status phase: Failed. Waiting for statefulset controller to delete.
Mar 30 21:36:10.009: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8757
STEP: Removing pod with conflicting port in namespace statefulset-8757
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8757 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Mar 30 21:36:20.066: INFO: Deleting all statefulset in ns statefulset-8757
Mar 30 21:36:20.071: INFO: Scaling statefulset ss to 0
Mar 30 21:36:40.098: INFO: Waiting for statefulset status.replicas updated to 0
Mar 30 21:36:40.104: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:36:40.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8757" for this suite.

• [SLOW TEST:39.029 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":278,"completed":103,"skipped":1868,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:36:40.154: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3096
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-3096
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Mar 30 21:36:40.688: INFO: Found 0 stateful pods, waiting for 3
Mar 30 21:36:50.698: INFO: Found 2 stateful pods, waiting for 3
Mar 30 21:37:00.698: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 21:37:00.698: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 21:37:00.698: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Mar 30 21:37:10.698: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 21:37:10.699: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 21:37:10.699: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 21:37:10.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-3096 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 30 21:37:10.997: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 30 21:37:10.997: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 30 21:37:10.997: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar 30 21:37:21.050: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar 30 21:37:31.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-3096 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:37:31.520: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 30 21:37:31.520: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 30 21:37:31.520: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 30 21:37:41.563: INFO: Waiting for StatefulSet statefulset-3096/ss2 to complete update
Mar 30 21:37:41.563: INFO: Waiting for Pod statefulset-3096/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 30 21:37:41.563: INFO: Waiting for Pod statefulset-3096/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 30 21:38:01.577: INFO: Waiting for StatefulSet statefulset-3096/ss2 to complete update
Mar 30 21:38:01.577: INFO: Waiting for Pod statefulset-3096/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 30 21:38:11.576: INFO: Waiting for StatefulSet statefulset-3096/ss2 to complete update
STEP: Rolling back to a previous revision
Mar 30 21:38:21.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-3096 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 30 21:38:21.950: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 30 21:38:21.950: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 30 21:38:21.950: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 30 21:38:31.996: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar 30 21:38:42.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-3096 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:38:42.327: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 30 21:38:42.327: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 30 21:38:42.327: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 30 21:38:52.358: INFO: Waiting for StatefulSet statefulset-3096/ss2 to complete update
Mar 30 21:38:52.358: INFO: Waiting for Pod statefulset-3096/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 30 21:38:52.358: INFO: Waiting for Pod statefulset-3096/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 30 21:39:02.372: INFO: Waiting for StatefulSet statefulset-3096/ss2 to complete update
Mar 30 21:39:02.372: INFO: Waiting for Pod statefulset-3096/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 30 21:39:02.372: INFO: Waiting for Pod statefulset-3096/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 30 21:39:12.372: INFO: Waiting for StatefulSet statefulset-3096/ss2 to complete update
Mar 30 21:39:12.372: INFO: Waiting for Pod statefulset-3096/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 30 21:39:22.381: INFO: Waiting for StatefulSet statefulset-3096/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Mar 30 21:39:32.371: INFO: Deleting all statefulset in ns statefulset-3096
Mar 30 21:39:32.376: INFO: Scaling statefulset ss2 to 0
Mar 30 21:39:52.406: INFO: Waiting for statefulset status.replicas updated to 0
Mar 30 21:39:52.412: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:39:52.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3096" for this suite.

• [SLOW TEST:192.300 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":278,"completed":104,"skipped":1905,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:39:52.456: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-307
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Mar 30 21:39:52.683: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:40:15.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-307" for this suite.

• [SLOW TEST:22.636 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":278,"completed":105,"skipped":1906,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:40:15.095: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2417
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 30 21:40:15.298: INFO: Waiting up to 5m0s for pod "pod-24c58a0e-0897-4acf-be20-f41b3a9ebf9c" in namespace "emptydir-2417" to be "success or failure"
Mar 30 21:40:15.308: INFO: Pod "pod-24c58a0e-0897-4acf-be20-f41b3a9ebf9c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.229523ms
Mar 30 21:40:17.312: INFO: Pod "pod-24c58a0e-0897-4acf-be20-f41b3a9ebf9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013796886s
Mar 30 21:40:19.319: INFO: Pod "pod-24c58a0e-0897-4acf-be20-f41b3a9ebf9c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020974007s
Mar 30 21:40:21.325: INFO: Pod "pod-24c58a0e-0897-4acf-be20-f41b3a9ebf9c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026313296s
Mar 30 21:40:23.330: INFO: Pod "pod-24c58a0e-0897-4acf-be20-f41b3a9ebf9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.031413738s
STEP: Saw pod success
Mar 30 21:40:23.330: INFO: Pod "pod-24c58a0e-0897-4acf-be20-f41b3a9ebf9c" satisfied condition "success or failure"
Mar 30 21:40:23.334: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-24c58a0e-0897-4acf-be20-f41b3a9ebf9c container test-container: <nil>
STEP: delete the pod
Mar 30 21:40:23.368: INFO: Waiting for pod pod-24c58a0e-0897-4acf-be20-f41b3a9ebf9c to disappear
Mar 30 21:40:23.375: INFO: Pod pod-24c58a0e-0897-4acf-be20-f41b3a9ebf9c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:40:23.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2417" for this suite.

• [SLOW TEST:8.302 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":106,"skipped":1913,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:40:23.397: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-642
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:40:23.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-642" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":278,"completed":107,"skipped":1937,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:40:23.598: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7732
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 30 21:40:23.790: INFO: Waiting up to 5m0s for pod "pod-26c8a549-868d-4f58-b73f-93d00a00ad9d" in namespace "emptydir-7732" to be "success or failure"
Mar 30 21:40:23.795: INFO: Pod "pod-26c8a549-868d-4f58-b73f-93d00a00ad9d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.079668ms
Mar 30 21:40:25.799: INFO: Pod "pod-26c8a549-868d-4f58-b73f-93d00a00ad9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009323998s
Mar 30 21:40:27.805: INFO: Pod "pod-26c8a549-868d-4f58-b73f-93d00a00ad9d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01504017s
Mar 30 21:40:29.812: INFO: Pod "pod-26c8a549-868d-4f58-b73f-93d00a00ad9d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02235889s
Mar 30 21:40:31.819: INFO: Pod "pod-26c8a549-868d-4f58-b73f-93d00a00ad9d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.028768756s
Mar 30 21:40:33.823: INFO: Pod "pod-26c8a549-868d-4f58-b73f-93d00a00ad9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.033042012s
STEP: Saw pod success
Mar 30 21:40:33.823: INFO: Pod "pod-26c8a549-868d-4f58-b73f-93d00a00ad9d" satisfied condition "success or failure"
Mar 30 21:40:33.826: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-26c8a549-868d-4f58-b73f-93d00a00ad9d container test-container: <nil>
STEP: delete the pod
Mar 30 21:40:33.862: INFO: Waiting for pod pod-26c8a549-868d-4f58-b73f-93d00a00ad9d to disappear
Mar 30 21:40:33.872: INFO: Pod pod-26c8a549-868d-4f58-b73f-93d00a00ad9d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:40:33.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7732" for this suite.

• [SLOW TEST:10.295 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":108,"skipped":1953,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:40:33.894: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1005
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Mar 30 21:40:34.112: INFO: Waiting up to 5m0s for pod "downward-api-aefcc3c8-7062-45ec-b07b-22e8f2233b9b" in namespace "downward-api-1005" to be "success or failure"
Mar 30 21:40:34.118: INFO: Pod "downward-api-aefcc3c8-7062-45ec-b07b-22e8f2233b9b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.420921ms
Mar 30 21:40:36.127: INFO: Pod "downward-api-aefcc3c8-7062-45ec-b07b-22e8f2233b9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015238023s
Mar 30 21:40:38.135: INFO: Pod "downward-api-aefcc3c8-7062-45ec-b07b-22e8f2233b9b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022971401s
Mar 30 21:40:40.143: INFO: Pod "downward-api-aefcc3c8-7062-45ec-b07b-22e8f2233b9b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030515448s
Mar 30 21:40:42.149: INFO: Pod "downward-api-aefcc3c8-7062-45ec-b07b-22e8f2233b9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.036375991s
STEP: Saw pod success
Mar 30 21:40:42.149: INFO: Pod "downward-api-aefcc3c8-7062-45ec-b07b-22e8f2233b9b" satisfied condition "success or failure"
Mar 30 21:40:42.153: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod downward-api-aefcc3c8-7062-45ec-b07b-22e8f2233b9b container dapi-container: <nil>
STEP: delete the pod
Mar 30 21:40:42.196: INFO: Waiting for pod downward-api-aefcc3c8-7062-45ec-b07b-22e8f2233b9b to disappear
Mar 30 21:40:42.200: INFO: Pod downward-api-aefcc3c8-7062-45ec-b07b-22e8f2233b9b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:40:42.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1005" for this suite.

• [SLOW TEST:8.322 seconds]
[sig-node] Downward API
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:33
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":278,"completed":109,"skipped":1963,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:40:42.222: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2731
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2731 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2731;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2731 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2731;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2731.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2731.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2731.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2731.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2731.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2731.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2731.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2731.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2731.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2731.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2731.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2731.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2731.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 145.87.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.87.145_udp@PTR;check="$$(dig +tcp +noall +answer +search 145.87.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.87.145_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2731 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2731;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2731 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2731;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2731.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2731.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2731.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2731.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2731.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2731.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2731.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2731.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2731.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2731.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2731.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2731.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2731.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 145.87.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.87.145_udp@PTR;check="$$(dig +tcp +noall +answer +search 145.87.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.87.145_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 30 21:40:52.490: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:40:52.496: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:40:52.501: INFO: Unable to read wheezy_udp@dns-test-service.dns-2731 from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:40:52.506: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2731 from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:40:52.511: INFO: Unable to read wheezy_udp@dns-test-service.dns-2731.svc from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:40:52.517: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2731.svc from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:40:52.527: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2731.svc from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:40:52.560: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:40:52.567: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:40:52.573: INFO: Unable to read jessie_udp@dns-test-service.dns-2731 from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:40:52.582: INFO: Unable to read jessie_tcp@dns-test-service.dns-2731 from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:40:52.589: INFO: Unable to read jessie_udp@dns-test-service.dns-2731.svc from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:40:52.595: INFO: Unable to read jessie_tcp@dns-test-service.dns-2731.svc from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:40:52.633: INFO: Lookups using dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2731 wheezy_tcp@dns-test-service.dns-2731 wheezy_udp@dns-test-service.dns-2731.svc wheezy_tcp@dns-test-service.dns-2731.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2731.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2731 jessie_tcp@dns-test-service.dns-2731 jessie_udp@dns-test-service.dns-2731.svc jessie_tcp@dns-test-service.dns-2731.svc]

Mar 30 21:40:57.641: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:40:57.647: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:40:57.652: INFO: Unable to read wheezy_udp@dns-test-service.dns-2731 from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:40:57.657: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2731 from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:40:57.662: INFO: Unable to read wheezy_udp@dns-test-service.dns-2731.svc from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:40:57.666: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2731.svc from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:40:57.675: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2731.svc from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:40:57.708: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:40:57.712: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:40:57.717: INFO: Unable to read jessie_udp@dns-test-service.dns-2731 from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:40:57.722: INFO: Unable to read jessie_tcp@dns-test-service.dns-2731 from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:40:57.727: INFO: Unable to read jessie_udp@dns-test-service.dns-2731.svc from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:40:57.741: INFO: Unable to read jessie_tcp@dns-test-service.dns-2731.svc from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:40:57.785: INFO: Lookups using dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2731 wheezy_tcp@dns-test-service.dns-2731 wheezy_udp@dns-test-service.dns-2731.svc wheezy_tcp@dns-test-service.dns-2731.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2731.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2731 jessie_tcp@dns-test-service.dns-2731 jessie_udp@dns-test-service.dns-2731.svc jessie_tcp@dns-test-service.dns-2731.svc]

Mar 30 21:41:02.641: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:02.646: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:02.651: INFO: Unable to read wheezy_udp@dns-test-service.dns-2731 from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:02.657: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2731 from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:02.662: INFO: Unable to read wheezy_udp@dns-test-service.dns-2731.svc from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:02.666: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2731.svc from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:02.677: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2731.svc from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:02.726: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:02.731: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:02.736: INFO: Unable to read jessie_udp@dns-test-service.dns-2731 from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:02.741: INFO: Unable to read jessie_tcp@dns-test-service.dns-2731 from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:02.746: INFO: Unable to read jessie_udp@dns-test-service.dns-2731.svc from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:02.752: INFO: Unable to read jessie_tcp@dns-test-service.dns-2731.svc from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:02.791: INFO: Lookups using dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2731 wheezy_tcp@dns-test-service.dns-2731 wheezy_udp@dns-test-service.dns-2731.svc wheezy_tcp@dns-test-service.dns-2731.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2731.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2731 jessie_tcp@dns-test-service.dns-2731 jessie_udp@dns-test-service.dns-2731.svc jessie_tcp@dns-test-service.dns-2731.svc]

Mar 30 21:41:07.641: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:07.646: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:07.652: INFO: Unable to read wheezy_udp@dns-test-service.dns-2731 from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:07.657: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2731 from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:07.661: INFO: Unable to read wheezy_udp@dns-test-service.dns-2731.svc from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:07.666: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2731.svc from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:07.711: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:07.715: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:07.726: INFO: Unable to read jessie_udp@dns-test-service.dns-2731 from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:07.731: INFO: Unable to read jessie_tcp@dns-test-service.dns-2731 from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:07.735: INFO: Unable to read jessie_udp@dns-test-service.dns-2731.svc from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:07.757: INFO: Unable to read jessie_tcp@dns-test-service.dns-2731.svc from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:07.795: INFO: Lookups using dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2731 wheezy_tcp@dns-test-service.dns-2731 wheezy_udp@dns-test-service.dns-2731.svc wheezy_tcp@dns-test-service.dns-2731.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2731 jessie_tcp@dns-test-service.dns-2731 jessie_udp@dns-test-service.dns-2731.svc jessie_tcp@dns-test-service.dns-2731.svc]

Mar 30 21:41:12.642: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:12.649: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:12.654: INFO: Unable to read wheezy_udp@dns-test-service.dns-2731 from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:12.660: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2731 from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:12.667: INFO: Unable to read wheezy_udp@dns-test-service.dns-2731.svc from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:12.674: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2731.svc from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:12.749: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:12.755: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:12.760: INFO: Unable to read jessie_udp@dns-test-service.dns-2731 from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:12.765: INFO: Unable to read jessie_tcp@dns-test-service.dns-2731 from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:12.775: INFO: Unable to read jessie_udp@dns-test-service.dns-2731.svc from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:12.782: INFO: Unable to read jessie_tcp@dns-test-service.dns-2731.svc from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:12.824: INFO: Lookups using dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2731 wheezy_tcp@dns-test-service.dns-2731 wheezy_udp@dns-test-service.dns-2731.svc wheezy_tcp@dns-test-service.dns-2731.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2731 jessie_tcp@dns-test-service.dns-2731 jessie_udp@dns-test-service.dns-2731.svc jessie_tcp@dns-test-service.dns-2731.svc]

Mar 30 21:41:17.641: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:17.646: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:17.651: INFO: Unable to read wheezy_udp@dns-test-service.dns-2731 from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:17.657: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2731 from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:17.662: INFO: Unable to read wheezy_udp@dns-test-service.dns-2731.svc from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:17.667: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2731.svc from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:17.715: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:17.723: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:17.730: INFO: Unable to read jessie_udp@dns-test-service.dns-2731 from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:17.735: INFO: Unable to read jessie_tcp@dns-test-service.dns-2731 from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:17.741: INFO: Unable to read jessie_udp@dns-test-service.dns-2731.svc from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:17.746: INFO: Unable to read jessie_tcp@dns-test-service.dns-2731.svc from pod dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304: the server could not find the requested resource (get pods dns-test-26e077f2-bea7-43b4-9973-056212273304)
Mar 30 21:41:17.794: INFO: Lookups using dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2731 wheezy_tcp@dns-test-service.dns-2731 wheezy_udp@dns-test-service.dns-2731.svc wheezy_tcp@dns-test-service.dns-2731.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2731 jessie_tcp@dns-test-service.dns-2731 jessie_udp@dns-test-service.dns-2731.svc jessie_tcp@dns-test-service.dns-2731.svc]

Mar 30 21:41:22.802: INFO: DNS probes using dns-2731/dns-test-26e077f2-bea7-43b4-9973-056212273304 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:41:22.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2731" for this suite.

• [SLOW TEST:40.758 seconds]
[sig-network] DNS
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":278,"completed":110,"skipped":1991,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:41:22.982: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9129
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 21:41:23.195: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar 30 21:41:28.201: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 30 21:41:32.213: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar 30 21:41:34.230: INFO: Creating deployment "test-rollover-deployment"
Mar 30 21:41:34.246: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar 30 21:41:36.256: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar 30 21:41:36.264: INFO: Ensure that both replica sets have 1 created replica
Mar 30 21:41:36.272: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar 30 21:41:36.286: INFO: Updating deployment test-rollover-deployment
Mar 30 21:41:36.286: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar 30 21:41:38.295: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar 30 21:41:38.305: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar 30 21:41:38.313: INFO: all replica sets need to contain the pod-template-hash label
Mar 30 21:41:38.313: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201294, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201294, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201296, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201294, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:41:40.323: INFO: all replica sets need to contain the pod-template-hash label
Mar 30 21:41:40.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201294, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201294, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201296, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201294, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:41:42.328: INFO: all replica sets need to contain the pod-template-hash label
Mar 30 21:41:42.329: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201294, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201294, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201296, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201294, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:41:44.324: INFO: all replica sets need to contain the pod-template-hash label
Mar 30 21:41:44.324: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201294, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201294, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201296, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201294, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:41:46.323: INFO: all replica sets need to contain the pod-template-hash label
Mar 30 21:41:46.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201294, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201294, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201304, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201294, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:41:48.327: INFO: all replica sets need to contain the pod-template-hash label
Mar 30 21:41:48.328: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201294, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201294, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201304, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201294, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:41:50.323: INFO: all replica sets need to contain the pod-template-hash label
Mar 30 21:41:50.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201294, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201294, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201304, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201294, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:41:52.331: INFO: all replica sets need to contain the pod-template-hash label
Mar 30 21:41:52.331: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201294, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201294, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201304, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201294, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:41:54.323: INFO: all replica sets need to contain the pod-template-hash label
Mar 30 21:41:54.324: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201294, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201294, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201304, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721201294, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:41:56.324: INFO: 
Mar 30 21:41:56.324: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Mar 30 21:41:56.343: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-9129 /apis/apps/v1/namespaces/deployment-9129/deployments/test-rollover-deployment d8f2fa20-0b68-4eb9-8fe8-372a69cb94e1 27285 2 2020-03-30 21:41:34 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0039ef078 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-03-30 21:41:34 +0000 UTC,LastTransitionTime:2020-03-30 21:41:34 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-574d6dfbff" has successfully progressed.,LastUpdateTime:2020-03-30 21:41:54 +0000 UTC,LastTransitionTime:2020-03-30 21:41:34 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 30 21:41:56.347: INFO: New ReplicaSet "test-rollover-deployment-574d6dfbff" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-574d6dfbff  deployment-9129 /apis/apps/v1/namespaces/deployment-9129/replicasets/test-rollover-deployment-574d6dfbff 67c545eb-d8cc-4dc4-bd96-bf13f898f256 27275 2 2020-03-30 21:41:36 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment d8f2fa20-0b68-4eb9-8fe8-372a69cb94e1 0xc003a10327 0xc003a10328}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 574d6dfbff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003a103b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 30 21:41:56.347: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar 30 21:41:56.347: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9129 /apis/apps/v1/namespaces/deployment-9129/replicasets/test-rollover-controller 39a5c70c-0b80-47dd-bb22-6d8ed2a784ae 27284 2 2020-03-30 21:41:23 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment d8f2fa20-0b68-4eb9-8fe8-372a69cb94e1 0xc003a10217 0xc003a10218}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003a102b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 30 21:41:56.347: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-9129 /apis/apps/v1/namespaces/deployment-9129/replicasets/test-rollover-deployment-f6c94f66c d0ed53cf-43b3-4559-bc0c-0c1d4b7014b2 27185 2 2020-03-30 21:41:34 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment d8f2fa20-0b68-4eb9-8fe8-372a69cb94e1 0xc003a10430 0xc003a10431}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003a104c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 30 21:41:56.353: INFO: Pod "test-rollover-deployment-574d6dfbff-26p57" is available:
&Pod{ObjectMeta:{test-rollover-deployment-574d6dfbff-26p57 test-rollover-deployment-574d6dfbff- deployment-9129 /api/v1/namespaces/deployment-9129/pods/test-rollover-deployment-574d6dfbff-26p57 6207b6af-893d-4b7a-9fc2-f109ddf6b061 27231 0 2020-03-30 21:41:36 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.14.102"
    ],
    "dns": {}
}] kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-574d6dfbff 67c545eb-d8cc-4dc4-bd96-bf13f898f256 0xc0039ef527 0xc0039ef528}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-n7mt9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-n7mt9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-n7mt9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-tjim0te5-eccd-ci-os-12-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:41:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:41:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:41:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 21:41:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.5,PodIP:192.168.14.102,StartTime:2020-03-30 21:41:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-30 21:41:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://3cb5a438150c1b891831c860e0c3504e515a06b82f48fdcb94067dbab8649e75,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.14.102,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:41:56.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9129" for this suite.

• [SLOW TEST:33.390 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":278,"completed":111,"skipped":1995,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:41:56.375: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5660
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 21:42:04.651: INFO: Waiting up to 5m0s for pod "client-envvars-daa39e79-517e-4326-a50b-6145a4a03dbd" in namespace "pods-5660" to be "success or failure"
Mar 30 21:42:04.667: INFO: Pod "client-envvars-daa39e79-517e-4326-a50b-6145a4a03dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 15.770277ms
Mar 30 21:42:06.674: INFO: Pod "client-envvars-daa39e79-517e-4326-a50b-6145a4a03dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022336646s
Mar 30 21:42:08.679: INFO: Pod "client-envvars-daa39e79-517e-4326-a50b-6145a4a03dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027035817s
Mar 30 21:42:10.691: INFO: Pod "client-envvars-daa39e79-517e-4326-a50b-6145a4a03dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.039476056s
Mar 30 21:42:12.698: INFO: Pod "client-envvars-daa39e79-517e-4326-a50b-6145a4a03dbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.046828982s
STEP: Saw pod success
Mar 30 21:42:12.699: INFO: Pod "client-envvars-daa39e79-517e-4326-a50b-6145a4a03dbd" satisfied condition "success or failure"
Mar 30 21:42:12.703: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod client-envvars-daa39e79-517e-4326-a50b-6145a4a03dbd container env3cont: <nil>
STEP: delete the pod
Mar 30 21:42:12.755: INFO: Waiting for pod client-envvars-daa39e79-517e-4326-a50b-6145a4a03dbd to disappear
Mar 30 21:42:12.760: INFO: Pod client-envvars-daa39e79-517e-4326-a50b-6145a4a03dbd no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:42:12.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5660" for this suite.

• [SLOW TEST:16.408 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":278,"completed":112,"skipped":2010,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:42:12.785: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7595
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-7595
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating stateful set ss in namespace statefulset-7595
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7595
Mar 30 21:42:13.003: INFO: Found 0 stateful pods, waiting for 1
Mar 30 21:42:23.010: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar 30 21:42:23.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 30 21:42:23.291: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 30 21:42:23.291: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 30 21:42:23.291: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 30 21:42:23.297: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 30 21:42:33.305: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 30 21:42:33.306: INFO: Waiting for statefulset status.replicas updated to 0
Mar 30 21:42:33.333: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Mar 30 21:42:33.333: INFO: ss-0  worker-pool1-tjim0te5-eccd-ci-os-12-jenkins  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:13 +0000 UTC  }]
Mar 30 21:42:33.333: INFO: 
Mar 30 21:42:33.333: INFO: StatefulSet ss has not reached scale 3, at 1
Mar 30 21:42:34.340: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989156652s
Mar 30 21:42:35.345: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.982481332s
Mar 30 21:42:36.351: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.977219016s
Mar 30 21:42:37.357: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.971707248s
Mar 30 21:42:38.364: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.96564937s
Mar 30 21:42:39.368: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.958898324s
Mar 30 21:42:40.374: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.954313556s
Mar 30 21:42:41.382: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.948470438s
Mar 30 21:42:42.389: INFO: Verifying statefulset ss doesn't scale past 3 for another 940.712014ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7595
Mar 30 21:42:43.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:42:43.673: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 30 21:42:43.673: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 30 21:42:43.673: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 30 21:42:43.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:42:43.937: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 30 21:42:43.937: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 30 21:42:43.937: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 30 21:42:43.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:42:44.224: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 30 21:42:44.224: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 30 21:42:44.224: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 30 21:42:44.230: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Mar 30 21:42:54.245: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 21:42:54.245: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 21:42:54.245: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar 30 21:42:54.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 30 21:42:54.550: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 30 21:42:54.550: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 30 21:42:54.550: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 30 21:42:54.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 30 21:42:55.010: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 30 21:42:55.010: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 30 21:42:55.010: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 30 21:42:55.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 30 21:42:55.314: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 30 21:42:55.314: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 30 21:42:55.314: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 30 21:42:55.314: INFO: Waiting for statefulset status.replicas updated to 0
Mar 30 21:42:55.320: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar 30 21:43:05.333: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 30 21:43:05.333: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 30 21:43:05.333: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 30 21:43:05.352: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Mar 30 21:43:05.352: INFO: ss-0  worker-pool1-tjim0te5-eccd-ci-os-12-jenkins  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:13 +0000 UTC  }]
Mar 30 21:43:05.352: INFO: ss-1  worker-pool1-vvs2j292-eccd-ci-os-12-jenkins  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:33 +0000 UTC  }]
Mar 30 21:43:05.352: INFO: ss-2  worker-pool1-zo88v95j-eccd-ci-os-12-jenkins  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:33 +0000 UTC  }]
Mar 30 21:43:05.352: INFO: 
Mar 30 21:43:05.353: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 30 21:43:06.358: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Mar 30 21:43:06.358: INFO: ss-0  worker-pool1-tjim0te5-eccd-ci-os-12-jenkins  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:13 +0000 UTC  }]
Mar 30 21:43:06.358: INFO: ss-1  worker-pool1-vvs2j292-eccd-ci-os-12-jenkins  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:33 +0000 UTC  }]
Mar 30 21:43:06.358: INFO: ss-2  worker-pool1-zo88v95j-eccd-ci-os-12-jenkins  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:33 +0000 UTC  }]
Mar 30 21:43:06.358: INFO: 
Mar 30 21:43:06.358: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 30 21:43:07.368: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Mar 30 21:43:07.368: INFO: ss-0  worker-pool1-tjim0te5-eccd-ci-os-12-jenkins  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:13 +0000 UTC  }]
Mar 30 21:43:07.368: INFO: ss-1  worker-pool1-vvs2j292-eccd-ci-os-12-jenkins  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:33 +0000 UTC  }]
Mar 30 21:43:07.368: INFO: ss-2  worker-pool1-zo88v95j-eccd-ci-os-12-jenkins  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:33 +0000 UTC  }]
Mar 30 21:43:07.368: INFO: 
Mar 30 21:43:07.368: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 30 21:43:08.374: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Mar 30 21:43:08.374: INFO: ss-0  worker-pool1-tjim0te5-eccd-ci-os-12-jenkins  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:13 +0000 UTC  }]
Mar 30 21:43:08.374: INFO: 
Mar 30 21:43:08.374: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 30 21:43:09.380: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Mar 30 21:43:09.380: INFO: ss-0  worker-pool1-tjim0te5-eccd-ci-os-12-jenkins  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:13 +0000 UTC  }]
Mar 30 21:43:09.380: INFO: 
Mar 30 21:43:09.380: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 30 21:43:10.388: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Mar 30 21:43:10.388: INFO: ss-0  worker-pool1-tjim0te5-eccd-ci-os-12-jenkins  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:13 +0000 UTC  }]
Mar 30 21:43:10.388: INFO: 
Mar 30 21:43:10.388: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 30 21:43:11.397: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Mar 30 21:43:11.397: INFO: ss-0  worker-pool1-tjim0te5-eccd-ci-os-12-jenkins  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:13 +0000 UTC  }]
Mar 30 21:43:11.397: INFO: 
Mar 30 21:43:11.397: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 30 21:43:12.403: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Mar 30 21:43:12.403: INFO: ss-0  worker-pool1-tjim0te5-eccd-ci-os-12-jenkins  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:13 +0000 UTC  }]
Mar 30 21:43:12.403: INFO: 
Mar 30 21:43:12.403: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 30 21:43:13.411: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Mar 30 21:43:13.411: INFO: ss-0  worker-pool1-tjim0te5-eccd-ci-os-12-jenkins  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:13 +0000 UTC  }]
Mar 30 21:43:13.411: INFO: 
Mar 30 21:43:13.411: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 30 21:43:14.425: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Mar 30 21:43:14.425: INFO: ss-0  worker-pool1-tjim0te5-eccd-ci-os-12-jenkins  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-30 21:42:13 +0000 UTC  }]
Mar 30 21:43:14.425: INFO: 
Mar 30 21:43:14.425: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7595
Mar 30 21:43:15.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:43:15.571: INFO: rc: 1
Mar 30 21:43:15.571: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Mar 30 21:43:25.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:43:25.671: INFO: rc: 1
Mar 30 21:43:25.671: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 30 21:43:35.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:43:35.772: INFO: rc: 1
Mar 30 21:43:35.772: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 30 21:43:45.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:43:45.876: INFO: rc: 1
Mar 30 21:43:45.876: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 30 21:43:55.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:43:55.967: INFO: rc: 1
Mar 30 21:43:55.967: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 30 21:44:05.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:44:06.369: INFO: rc: 1
Mar 30 21:44:06.369: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 30 21:44:16.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:44:16.524: INFO: rc: 1
Mar 30 21:44:16.524: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 30 21:44:26.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:44:26.649: INFO: rc: 1
Mar 30 21:44:26.649: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 30 21:44:36.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:44:36.759: INFO: rc: 1
Mar 30 21:44:36.759: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 30 21:44:46.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:44:46.896: INFO: rc: 1
Mar 30 21:44:46.897: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 30 21:44:56.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:44:57.023: INFO: rc: 1
Mar 30 21:44:57.023: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 30 21:45:07.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:45:07.126: INFO: rc: 1
Mar 30 21:45:07.126: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 30 21:45:17.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:45:17.235: INFO: rc: 1
Mar 30 21:45:17.235: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 30 21:45:27.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:45:27.331: INFO: rc: 1
Mar 30 21:45:27.331: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 30 21:45:37.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:45:37.438: INFO: rc: 1
Mar 30 21:45:37.438: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 30 21:45:47.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:45:47.548: INFO: rc: 1
Mar 30 21:45:47.548: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 30 21:45:57.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:45:57.656: INFO: rc: 1
Mar 30 21:45:57.656: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 30 21:46:07.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:46:07.754: INFO: rc: 1
Mar 30 21:46:07.754: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 30 21:46:17.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:46:17.907: INFO: rc: 1
Mar 30 21:46:17.908: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 30 21:46:27.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:46:28.021: INFO: rc: 1
Mar 30 21:46:28.021: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 30 21:46:38.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:46:38.125: INFO: rc: 1
Mar 30 21:46:38.126: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 30 21:46:48.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:46:48.228: INFO: rc: 1
Mar 30 21:46:48.229: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 30 21:46:58.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:46:58.323: INFO: rc: 1
Mar 30 21:46:58.323: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 30 21:47:08.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:47:08.427: INFO: rc: 1
Mar 30 21:47:08.427: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 30 21:47:18.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:47:18.549: INFO: rc: 1
Mar 30 21:47:18.550: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 30 21:47:28.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:47:28.640: INFO: rc: 1
Mar 30 21:47:28.640: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 30 21:47:38.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:47:38.787: INFO: rc: 1
Mar 30 21:47:38.787: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 30 21:47:48.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:47:48.887: INFO: rc: 1
Mar 30 21:47:48.887: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 30 21:47:58.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:47:58.980: INFO: rc: 1
Mar 30 21:47:58.980: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 30 21:48:08.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:48:09.100: INFO: rc: 1
Mar 30 21:48:09.100: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 30 21:48:19.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 21:48:19.202: INFO: rc: 1
Mar 30 21:48:19.202: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
Mar 30 21:48:19.202: INFO: Scaling statefulset ss to 0
Mar 30 21:48:19.228: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Mar 30 21:48:19.232: INFO: Deleting all statefulset in ns statefulset-7595
Mar 30 21:48:19.235: INFO: Scaling statefulset ss to 0
Mar 30 21:48:19.252: INFO: Waiting for statefulset status.replicas updated to 0
Mar 30 21:48:19.256: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:48:19.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7595" for this suite.

• [SLOW TEST:366.512 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":278,"completed":113,"skipped":2028,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:48:19.300: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-6371
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 21:48:19.504: INFO: Waiting up to 5m0s for pod "busybox-user-65534-ef9e3eb0-6331-44e8-bbb7-34b30af1239e" in namespace "security-context-test-6371" to be "success or failure"
Mar 30 21:48:19.508: INFO: Pod "busybox-user-65534-ef9e3eb0-6331-44e8-bbb7-34b30af1239e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.913348ms
Mar 30 21:48:21.513: INFO: Pod "busybox-user-65534-ef9e3eb0-6331-44e8-bbb7-34b30af1239e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008842095s
Mar 30 21:48:23.523: INFO: Pod "busybox-user-65534-ef9e3eb0-6331-44e8-bbb7-34b30af1239e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019729679s
Mar 30 21:48:25.529: INFO: Pod "busybox-user-65534-ef9e3eb0-6331-44e8-bbb7-34b30af1239e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024928081s
Mar 30 21:48:27.535: INFO: Pod "busybox-user-65534-ef9e3eb0-6331-44e8-bbb7-34b30af1239e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.031022359s
Mar 30 21:48:27.535: INFO: Pod "busybox-user-65534-ef9e3eb0-6331-44e8-bbb7-34b30af1239e" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:48:27.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6371" for this suite.

• [SLOW TEST:8.252 seconds]
[k8s.io] Security Context
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  When creating a container with runAsUser
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:43
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":114,"skipped":2048,"failed":0}
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:48:27.553: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9523
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:48:35.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9523" for this suite.

• [SLOW TEST:8.266 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":278,"completed":115,"skipped":2048,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:48:35.821: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2705
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 30 21:48:54.134: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 30 21:48:54.139: INFO: Pod pod-with-poststart-http-hook still exists
Mar 30 21:48:56.139: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 30 21:48:56.146: INFO: Pod pod-with-poststart-http-hook still exists
Mar 30 21:48:58.139: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 30 21:48:58.144: INFO: Pod pod-with-poststart-http-hook still exists
Mar 30 21:49:00.139: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 30 21:49:00.144: INFO: Pod pod-with-poststart-http-hook still exists
Mar 30 21:49:02.139: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 30 21:49:02.145: INFO: Pod pod-with-poststart-http-hook still exists
Mar 30 21:49:04.139: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 30 21:49:04.148: INFO: Pod pod-with-poststart-http-hook still exists
Mar 30 21:49:06.139: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 30 21:49:06.144: INFO: Pod pod-with-poststart-http-hook still exists
Mar 30 21:49:08.139: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 30 21:49:08.147: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:49:08.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2705" for this suite.

• [SLOW TEST:32.344 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":278,"completed":116,"skipped":2060,"failed":0}
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:49:08.166: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5044
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:50:08.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5044" for this suite.

• [SLOW TEST:60.238 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":278,"completed":117,"skipped":2060,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:50:08.407: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5390
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0330 21:50:10.202418      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 30 21:50:10.203: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:50:10.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5390" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":278,"completed":118,"skipped":2097,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:50:10.220: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1776
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 21:50:10.397: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Mar 30 21:50:12.472: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:50:12.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1776" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":278,"completed":119,"skipped":2126,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:50:12.500: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1434
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Mar 30 21:50:12.726: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 30 21:50:12.747: INFO: Waiting for terminating namespaces to be deleted...
Mar 30 21:50:12.751: INFO: 
Logging pods the kubelet thinks is on node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins before test
Mar 30 21:50:12.769: INFO: eric-tm-external-connectivity-frontend-speaker-vmndg from kube-system started at 2020-03-30 20:50:36 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.769: INFO: 	Container speaker ready: true, restart count 0
Mar 30 21:50:12.769: INFO: eric-pm-server-pushgateway-7798d479ff-fcnk9 from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.769: INFO: 	Container eric-pm-server-pushgateway ready: true, restart count 0
Mar 30 21:50:12.770: INFO: calico-node-kswgg from kube-system started at 2020-03-30 20:48:23 +0000 UTC (2 container statuses recorded)
Mar 30 21:50:12.770: INFO: 	Container calico-node ready: true, restart count 0
Mar 30 21:50:12.770: INFO: 	Container install-cni ready: true, restart count 0
Mar 30 21:50:12.770: INFO: kube-proxy-746bb from kube-system started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.770: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 30 21:50:12.770: INFO: csi-cinder-nodeplugin-ftdf2 from kube-system started at 2020-03-30 20:48:53 +0000 UTC (2 container statuses recorded)
Mar 30 21:50:12.771: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 30 21:50:12.771: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 30 21:50:12.771: INFO: eric-lm-combined-server-license-consumer-handler-58597bc9679zxp from kube-system started at 2020-03-30 20:54:37 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.771: INFO: 	Container eric-lm-license-consumer-handler ready: true, restart count 0
Mar 30 21:50:12.771: INFO: ccd-license-consumer-5c7ff9fc96-b2hn2 from kube-system started at 2020-03-30 20:55:33 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.771: INFO: 	Container ccd-license-consumer ready: true, restart count 0
Mar 30 21:50:12.771: INFO: kube-multus-ds-amd64-vm7r6 from kube-system started at 2020-03-30 20:48:53 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.771: INFO: 	Container kube-multus ready: true, restart count 0
Mar 30 21:50:12.772: INFO: eric-pm-server-eric-pm-server-7b4dd54bc5-m7fjg from monitoring started at 2020-03-30 20:51:51 +0000 UTC (2 container statuses recorded)
Mar 30 21:50:12.772: INFO: 	Container eric-pm-server-eric-pm-server ready: true, restart count 0
Mar 30 21:50:12.772: INFO: 	Container eric-pm-server-eric-pm-server-eric-pm-configmap-reload ready: true, restart count 0
Mar 30 21:50:12.772: INFO: eric-pm-server-node-exporter-m4kjc from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.772: INFO: 	Container eric-pm-server-node-exporter ready: true, restart count 0
Mar 30 21:50:12.772: INFO: sonobuoy from sonobuoy started at 2020-03-30 21:04:25 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.773: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 30 21:50:12.773: INFO: sonobuoy-systemd-logs-daemon-set-4495c60740044637-62zgj from sonobuoy started at 2020-03-30 21:04:37 +0000 UTC (2 container statuses recorded)
Mar 30 21:50:12.773: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 30 21:50:12.773: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 30 21:50:12.773: INFO: 
Logging pods the kubelet thinks is on node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins before test
Mar 30 21:50:12.786: INFO: eric-tm-external-connectivity-frontend-controller-78f9fd87bvc6l from kube-system started at 2020-03-30 20:50:36 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.786: INFO: 	Container controller ready: true, restart count 0
Mar 30 21:50:12.786: INFO: eric-tm-external-connectivity-frontend-speaker-m4f4p from kube-system started at 2020-03-30 20:50:36 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.786: INFO: 	Container speaker ready: true, restart count 0
Mar 30 21:50:12.786: INFO: metrics-server-8666db6c57-znld5 from kube-system started at 2020-03-30 20:53:13 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.786: INFO: 	Container metrics-server ready: true, restart count 0
Mar 30 21:50:12.786: INFO: eric-lm-combined-server-license-server-client-945b6bc4c-5phm5 from kube-system started at 2020-03-30 20:54:37 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.786: INFO: 	Container eric-lm-license-server-client ready: true, restart count 0
Mar 30 21:50:12.786: INFO: kube-multus-ds-amd64-q9869 from kube-system started at 2020-03-30 20:48:57 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.786: INFO: 	Container kube-multus ready: true, restart count 0
Mar 30 21:50:12.786: INFO: eric-pm-server-alertmanager-648979cdd-z57mc from monitoring started at 2020-03-30 20:51:51 +0000 UTC (2 container statuses recorded)
Mar 30 21:50:12.786: INFO: 	Container eric-pm-server-alertmanager ready: true, restart count 0
Mar 30 21:50:12.786: INFO: 	Container eric-pm-server-alertmanager-configmap-reload-for-alertmanager ready: true, restart count 0
Mar 30 21:50:12.786: INFO: sonobuoy-systemd-logs-daemon-set-4495c60740044637-gqxkr from sonobuoy started at 2020-03-30 21:04:37 +0000 UTC (2 container statuses recorded)
Mar 30 21:50:12.786: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 30 21:50:12.786: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 30 21:50:12.786: INFO: calico-node-lrw4b from kube-system started at 2020-03-30 20:48:27 +0000 UTC (2 container statuses recorded)
Mar 30 21:50:12.786: INFO: 	Container calico-node ready: true, restart count 0
Mar 30 21:50:12.786: INFO: 	Container install-cni ready: true, restart count 0
Mar 30 21:50:12.786: INFO: csi-cinder-nodeplugin-wnrkx from kube-system started at 2020-03-30 20:48:57 +0000 UTC (2 container statuses recorded)
Mar 30 21:50:12.786: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 30 21:50:12.786: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 30 21:50:12.786: INFO: eric-pm-server-node-exporter-v5bmh from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.786: INFO: 	Container eric-pm-server-node-exporter ready: true, restart count 0
Mar 30 21:50:12.786: INFO: test-webserver-2e41b538-1fe7-45e0-8ebd-3ffb4d257013 from container-probe-5044 started at 2020-03-30 21:49:08 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.786: INFO: 	Container test-webserver ready: false, restart count 0
Mar 30 21:50:12.786: INFO: condition-test-zwjrw from replication-controller-1776 started at 2020-03-30 21:50:11 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.786: INFO: 	Container httpd ready: false, restart count 0
Mar 30 21:50:12.786: INFO: kube-proxy-9w8kt from kube-system started at 2020-03-30 20:48:27 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.786: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 30 21:50:12.786: INFO: 
Logging pods the kubelet thinks is on node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins before test
Mar 30 21:50:12.808: INFO: calico-node-hf278 from kube-system started at 2020-03-30 20:48:23 +0000 UTC (2 container statuses recorded)
Mar 30 21:50:12.808: INFO: 	Container calico-node ready: true, restart count 0
Mar 30 21:50:12.808: INFO: 	Container install-cni ready: true, restart count 0
Mar 30 21:50:12.808: INFO: eric-pm-server-node-exporter-9cwgm from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.808: INFO: 	Container eric-pm-server-node-exporter ready: true, restart count 0
Mar 30 21:50:12.808: INFO: eric-pm-server-kube-state-metrics-66f7fbfd44-k7krp from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.808: INFO: 	Container eric-pm-server-kube-state-metrics ready: true, restart count 0
Mar 30 21:50:12.808: INFO: eric-tm-external-connectivity-frontend-speaker-8tmb7 from kube-system started at 2020-03-30 20:50:36 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.808: INFO: 	Container speaker ready: true, restart count 0
Mar 30 21:50:12.808: INFO: postgresql-postgresql-0 from kube-system started at 2020-03-30 20:53:52 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.808: INFO: 	Container postgresql ready: true, restart count 0
Mar 30 21:50:12.808: INFO: sonobuoy-systemd-logs-daemon-set-4495c60740044637-5rfld from sonobuoy started at 2020-03-30 21:04:37 +0000 UTC (2 container statuses recorded)
Mar 30 21:50:12.808: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 30 21:50:12.808: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 30 21:50:12.808: INFO: condition-test-gl7nd from replication-controller-1776 started at 2020-03-30 21:50:11 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.808: INFO: 	Container httpd ready: false, restart count 0
Mar 30 21:50:12.808: INFO: kube-proxy-4mzsc from kube-system started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.808: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 30 21:50:12.808: INFO: kube-multus-ds-amd64-9mjsk from kube-system started at 2020-03-30 20:48:53 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.808: INFO: 	Container kube-multus ready: true, restart count 0
Mar 30 21:50:12.808: INFO: csi-cinder-nodeplugin-cfp5q from kube-system started at 2020-03-30 20:48:53 +0000 UTC (2 container statuses recorded)
Mar 30 21:50:12.808: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 30 21:50:12.808: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 30 21:50:12.808: INFO: eric-lcm-container-registry-registry-0 from kube-system started at 2020-03-30 20:49:39 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.808: INFO: 	Container registry ready: true, restart count 0
Mar 30 21:50:12.808: INFO: 
Logging pods the kubelet thinks is on node worker-pool1-zo88v95j-eccd-ci-os-12-jenkins before test
Mar 30 21:50:12.826: INFO: tiller-deploy-cd77547bd-2pv7h from kube-system started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.827: INFO: 	Container tiller ready: true, restart count 0
Mar 30 21:50:12.827: INFO: kube-multus-ds-amd64-6t2pw from kube-system started at 2020-03-30 20:48:21 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.827: INFO: 	Container kube-multus ready: true, restart count 0
Mar 30 21:50:12.827: INFO: eric-pm-server-node-exporter-ktbpn from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.827: INFO: 	Container eric-pm-server-node-exporter ready: true, restart count 0
Mar 30 21:50:12.827: INFO: sonobuoy-e2e-job-14477a96f0f5465c from sonobuoy started at 2020-03-30 21:04:37 +0000 UTC (2 container statuses recorded)
Mar 30 21:50:12.828: INFO: 	Container e2e ready: true, restart count 0
Mar 30 21:50:12.828: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 30 21:50:12.828: INFO: sonobuoy-systemd-logs-daemon-set-4495c60740044637-v7t76 from sonobuoy started at 2020-03-30 21:04:37 +0000 UTC (2 container statuses recorded)
Mar 30 21:50:12.828: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 30 21:50:12.828: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 30 21:50:12.828: INFO: calico-node-xk89l from kube-system started at 2020-03-30 20:47:51 +0000 UTC (2 container statuses recorded)
Mar 30 21:50:12.829: INFO: 	Container calico-node ready: true, restart count 0
Mar 30 21:50:12.829: INFO: 	Container install-cni ready: true, restart count 0
Mar 30 21:50:12.829: INFO: csi-cinder-nodeplugin-qgrf4 from kube-system started at 2020-03-30 20:48:21 +0000 UTC (2 container statuses recorded)
Mar 30 21:50:12.829: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 30 21:50:12.829: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 30 21:50:12.829: INFO: nginx-ingress-controller-78f766b979-bfx4q from ingress-nginx started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.829: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 30 21:50:12.829: INFO: default-http-backend-6664c884c9-dxczc from ingress-nginx started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.830: INFO: 	Container default-http-backend ready: true, restart count 0
Mar 30 21:50:12.830: INFO: kube-proxy-pcd5r from kube-system started at 2020-03-30 20:47:51 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.830: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 30 21:50:12.830: INFO: nginx-ingress-controller-78f766b979-wf7qb from ingress-nginx started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.830: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 30 21:50:12.830: INFO: eric-tm-external-connectivity-frontend-speaker-zwnfr from kube-system started at 2020-03-30 20:50:36 +0000 UTC (1 container statuses recorded)
Mar 30 21:50:12.831: INFO: 	Container speaker ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-eee42aa8-28ad-4235-b8df-81fc25ab18bc 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-eee42aa8-28ad-4235-b8df-81fc25ab18bc off the node worker-pool1-zo88v95j-eccd-ci-os-12-jenkins
STEP: verifying the node doesn't have the label kubernetes.io/e2e-eee42aa8-28ad-4235-b8df-81fc25ab18bc
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:55:31.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1434" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:318.529 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":278,"completed":120,"skipped":2129,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:55:31.029: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8684
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 30 21:55:31.999: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 30 21:55:34.021: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202132, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202132, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202132, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202132, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:55:36.026: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202132, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202132, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202132, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202132, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:55:38.029: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202132, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202132, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202132, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202132, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:55:40.027: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202132, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202132, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202132, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202132, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 30 21:55:43.075: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 21:55:43.085: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:55:44.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8684" for this suite.
STEP: Destroying namespace "webhook-8684-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.474 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":278,"completed":121,"skipped":2132,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:55:44.510: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8549
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1861
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 30 21:55:44.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-8549'
Mar 30 21:55:45.264: INFO: stderr: ""
Mar 30 21:55:45.264: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1866
Mar 30 21:55:45.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 delete pods e2e-test-httpd-pod --namespace=kubectl-8549'
Mar 30 21:55:56.740: INFO: stderr: ""
Mar 30 21:55:56.740: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:55:56.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8549" for this suite.

• [SLOW TEST:12.255 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1857
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":278,"completed":122,"skipped":2155,"failed":0}
S
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:55:56.766: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4775
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name secret-emptykey-test-2a15fa55-d536-447b-b3d2-4ff56e9ad1c7
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:55:56.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4775" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":278,"completed":123,"skipped":2156,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:55:56.962: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-6987
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 21:55:57.180: INFO: (0) /api/v1/nodes/worker-pool1-vvs2j292-eccd-ci-os-12-jenkins/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 25.090038ms)
Mar 30 21:55:57.186: INFO: (1) /api/v1/nodes/worker-pool1-vvs2j292-eccd-ci-os-12-jenkins/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 5.317898ms)
Mar 30 21:55:57.191: INFO: (2) /api/v1/nodes/worker-pool1-vvs2j292-eccd-ci-os-12-jenkins/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 4.889631ms)
Mar 30 21:55:57.197: INFO: (3) /api/v1/nodes/worker-pool1-vvs2j292-eccd-ci-os-12-jenkins/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 5.961373ms)
Mar 30 21:55:57.203: INFO: (4) /api/v1/nodes/worker-pool1-vvs2j292-eccd-ci-os-12-jenkins/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 5.858424ms)
Mar 30 21:55:57.208: INFO: (5) /api/v1/nodes/worker-pool1-vvs2j292-eccd-ci-os-12-jenkins/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 5.569085ms)
Mar 30 21:55:57.215: INFO: (6) /api/v1/nodes/worker-pool1-vvs2j292-eccd-ci-os-12-jenkins/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 6.210707ms)
Mar 30 21:55:57.220: INFO: (7) /api/v1/nodes/worker-pool1-vvs2j292-eccd-ci-os-12-jenkins/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 5.286814ms)
Mar 30 21:55:57.226: INFO: (8) /api/v1/nodes/worker-pool1-vvs2j292-eccd-ci-os-12-jenkins/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 5.330262ms)
Mar 30 21:55:57.235: INFO: (9) /api/v1/nodes/worker-pool1-vvs2j292-eccd-ci-os-12-jenkins/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 8.959147ms)
Mar 30 21:55:57.241: INFO: (10) /api/v1/nodes/worker-pool1-vvs2j292-eccd-ci-os-12-jenkins/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 6.015394ms)
Mar 30 21:55:57.246: INFO: (11) /api/v1/nodes/worker-pool1-vvs2j292-eccd-ci-os-12-jenkins/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 4.843903ms)
Mar 30 21:55:57.251: INFO: (12) /api/v1/nodes/worker-pool1-vvs2j292-eccd-ci-os-12-jenkins/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 5.176606ms)
Mar 30 21:55:57.256: INFO: (13) /api/v1/nodes/worker-pool1-vvs2j292-eccd-ci-os-12-jenkins/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 5.14964ms)
Mar 30 21:55:57.262: INFO: (14) /api/v1/nodes/worker-pool1-vvs2j292-eccd-ci-os-12-jenkins/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 5.113778ms)
Mar 30 21:55:57.267: INFO: (15) /api/v1/nodes/worker-pool1-vvs2j292-eccd-ci-os-12-jenkins/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 4.812971ms)
Mar 30 21:55:57.284: INFO: (16) /api/v1/nodes/worker-pool1-vvs2j292-eccd-ci-os-12-jenkins/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 17.046526ms)
Mar 30 21:55:57.292: INFO: (17) /api/v1/nodes/worker-pool1-vvs2j292-eccd-ci-os-12-jenkins/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 7.846002ms)
Mar 30 21:55:57.300: INFO: (18) /api/v1/nodes/worker-pool1-vvs2j292-eccd-ci-os-12-jenkins/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 8.593369ms)
Mar 30 21:55:57.306: INFO: (19) /api/v1/nodes/worker-pool1-vvs2j292-eccd-ci-os-12-jenkins/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="YaST2/">YaST2/</a>
<a href="acpid">acp... (200; 5.672803ms)
[AfterEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:55:57.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6987" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":278,"completed":124,"skipped":2166,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:55:57.334: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7331
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 30 21:55:57.523: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e770017d-7224-4377-a32c-5e2a20d0c4b9" in namespace "projected-7331" to be "success or failure"
Mar 30 21:55:57.528: INFO: Pod "downwardapi-volume-e770017d-7224-4377-a32c-5e2a20d0c4b9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.200218ms
Mar 30 21:55:59.533: INFO: Pod "downwardapi-volume-e770017d-7224-4377-a32c-5e2a20d0c4b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010602188s
Mar 30 21:56:01.542: INFO: Pod "downwardapi-volume-e770017d-7224-4377-a32c-5e2a20d0c4b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018818083s
Mar 30 21:56:03.547: INFO: Pod "downwardapi-volume-e770017d-7224-4377-a32c-5e2a20d0c4b9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02454932s
Mar 30 21:56:05.554: INFO: Pod "downwardapi-volume-e770017d-7224-4377-a32c-5e2a20d0c4b9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.031249202s
Mar 30 21:56:07.560: INFO: Pod "downwardapi-volume-e770017d-7224-4377-a32c-5e2a20d0c4b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.036681861s
STEP: Saw pod success
Mar 30 21:56:07.560: INFO: Pod "downwardapi-volume-e770017d-7224-4377-a32c-5e2a20d0c4b9" satisfied condition "success or failure"
Mar 30 21:56:07.563: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod downwardapi-volume-e770017d-7224-4377-a32c-5e2a20d0c4b9 container client-container: <nil>
STEP: delete the pod
Mar 30 21:56:07.599: INFO: Waiting for pod downwardapi-volume-e770017d-7224-4377-a32c-5e2a20d0c4b9 to disappear
Mar 30 21:56:07.603: INFO: Pod downwardapi-volume-e770017d-7224-4377-a32c-5e2a20d0c4b9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:56:07.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7331" for this suite.

• [SLOW TEST:10.286 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":278,"completed":125,"skipped":2212,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:56:07.623: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6004
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 30 21:56:07.814: INFO: Waiting up to 5m0s for pod "pod-74c4e3a2-0659-410a-970c-e91cb235343b" in namespace "emptydir-6004" to be "success or failure"
Mar 30 21:56:07.821: INFO: Pod "pod-74c4e3a2-0659-410a-970c-e91cb235343b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.613278ms
Mar 30 21:56:09.826: INFO: Pod "pod-74c4e3a2-0659-410a-970c-e91cb235343b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011862675s
Mar 30 21:56:11.832: INFO: Pod "pod-74c4e3a2-0659-410a-970c-e91cb235343b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017670595s
Mar 30 21:56:13.839: INFO: Pod "pod-74c4e3a2-0659-410a-970c-e91cb235343b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024722324s
Mar 30 21:56:15.846: INFO: Pod "pod-74c4e3a2-0659-410a-970c-e91cb235343b": Phase="Running", Reason="", readiness=true. Elapsed: 8.031716609s
Mar 30 21:56:17.858: INFO: Pod "pod-74c4e3a2-0659-410a-970c-e91cb235343b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.043689703s
STEP: Saw pod success
Mar 30 21:56:17.858: INFO: Pod "pod-74c4e3a2-0659-410a-970c-e91cb235343b" satisfied condition "success or failure"
Mar 30 21:56:17.863: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-74c4e3a2-0659-410a-970c-e91cb235343b container test-container: <nil>
STEP: delete the pod
Mar 30 21:56:17.896: INFO: Waiting for pod pod-74c4e3a2-0659-410a-970c-e91cb235343b to disappear
Mar 30 21:56:17.900: INFO: Pod pod-74c4e3a2-0659-410a-970c-e91cb235343b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:56:17.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6004" for this suite.

• [SLOW TEST:10.293 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":126,"skipped":2242,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:56:17.917: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5498
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 30 21:56:18.117: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b0da51e9-a48c-451f-b17f-2acfeebb231c" in namespace "projected-5498" to be "success or failure"
Mar 30 21:56:18.120: INFO: Pod "downwardapi-volume-b0da51e9-a48c-451f-b17f-2acfeebb231c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.387877ms
Mar 30 21:56:20.128: INFO: Pod "downwardapi-volume-b0da51e9-a48c-451f-b17f-2acfeebb231c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011459086s
Mar 30 21:56:22.134: INFO: Pod "downwardapi-volume-b0da51e9-a48c-451f-b17f-2acfeebb231c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017551895s
Mar 30 21:56:24.140: INFO: Pod "downwardapi-volume-b0da51e9-a48c-451f-b17f-2acfeebb231c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023358754s
Mar 30 21:56:26.146: INFO: Pod "downwardapi-volume-b0da51e9-a48c-451f-b17f-2acfeebb231c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.028972787s
Mar 30 21:56:28.153: INFO: Pod "downwardapi-volume-b0da51e9-a48c-451f-b17f-2acfeebb231c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.035612514s
STEP: Saw pod success
Mar 30 21:56:28.153: INFO: Pod "downwardapi-volume-b0da51e9-a48c-451f-b17f-2acfeebb231c" satisfied condition "success or failure"
Mar 30 21:56:28.157: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod downwardapi-volume-b0da51e9-a48c-451f-b17f-2acfeebb231c container client-container: <nil>
STEP: delete the pod
Mar 30 21:56:28.185: INFO: Waiting for pod downwardapi-volume-b0da51e9-a48c-451f-b17f-2acfeebb231c to disappear
Mar 30 21:56:28.190: INFO: Pod downwardapi-volume-b0da51e9-a48c-451f-b17f-2acfeebb231c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:56:28.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5498" for this suite.

• [SLOW TEST:10.290 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":278,"completed":127,"skipped":2247,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:56:28.207: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9642
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 30 21:56:28.398: INFO: Waiting up to 5m0s for pod "pod-c6837ba1-51d7-4a4a-a775-fd4dcc990c2a" in namespace "emptydir-9642" to be "success or failure"
Mar 30 21:56:28.402: INFO: Pod "pod-c6837ba1-51d7-4a4a-a775-fd4dcc990c2a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.20535ms
Mar 30 21:56:30.407: INFO: Pod "pod-c6837ba1-51d7-4a4a-a775-fd4dcc990c2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008696036s
Mar 30 21:56:32.414: INFO: Pod "pod-c6837ba1-51d7-4a4a-a775-fd4dcc990c2a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015624761s
Mar 30 21:56:34.419: INFO: Pod "pod-c6837ba1-51d7-4a4a-a775-fd4dcc990c2a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020859482s
Mar 30 21:56:36.425: INFO: Pod "pod-c6837ba1-51d7-4a4a-a775-fd4dcc990c2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.026231102s
STEP: Saw pod success
Mar 30 21:56:36.425: INFO: Pod "pod-c6837ba1-51d7-4a4a-a775-fd4dcc990c2a" satisfied condition "success or failure"
Mar 30 21:56:36.429: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-c6837ba1-51d7-4a4a-a775-fd4dcc990c2a container test-container: <nil>
STEP: delete the pod
Mar 30 21:56:36.468: INFO: Waiting for pod pod-c6837ba1-51d7-4a4a-a775-fd4dcc990c2a to disappear
Mar 30 21:56:36.472: INFO: Pod pod-c6837ba1-51d7-4a4a-a775-fd4dcc990c2a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:56:36.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9642" for this suite.

• [SLOW TEST:8.289 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":128,"skipped":2249,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:56:36.497: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2838
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Mar 30 21:56:36.700: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:56:56.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2838" for this suite.

• [SLOW TEST:20.306 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":278,"completed":129,"skipped":2257,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:56:56.806: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-9344
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 21:56:57.038: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-d35cbf62-9065-4a5b-a362-027d6c7afa85" in namespace "security-context-test-9344" to be "success or failure"
Mar 30 21:56:57.045: INFO: Pod "alpine-nnp-false-d35cbf62-9065-4a5b-a362-027d6c7afa85": Phase="Pending", Reason="", readiness=false. Elapsed: 6.322126ms
Mar 30 21:56:59.050: INFO: Pod "alpine-nnp-false-d35cbf62-9065-4a5b-a362-027d6c7afa85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012174079s
Mar 30 21:57:01.057: INFO: Pod "alpine-nnp-false-d35cbf62-9065-4a5b-a362-027d6c7afa85": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019014853s
Mar 30 21:57:03.063: INFO: Pod "alpine-nnp-false-d35cbf62-9065-4a5b-a362-027d6c7afa85": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024996821s
Mar 30 21:57:05.075: INFO: Pod "alpine-nnp-false-d35cbf62-9065-4a5b-a362-027d6c7afa85": Phase="Pending", Reason="", readiness=false. Elapsed: 8.036501791s
Mar 30 21:57:07.081: INFO: Pod "alpine-nnp-false-d35cbf62-9065-4a5b-a362-027d6c7afa85": Phase="Pending", Reason="", readiness=false. Elapsed: 10.042545661s
Mar 30 21:57:09.086: INFO: Pod "alpine-nnp-false-d35cbf62-9065-4a5b-a362-027d6c7afa85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.047796293s
Mar 30 21:57:09.086: INFO: Pod "alpine-nnp-false-d35cbf62-9065-4a5b-a362-027d6c7afa85" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:57:09.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9344" for this suite.

• [SLOW TEST:12.324 seconds]
[k8s.io] Security Context
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:289
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":130,"skipped":2301,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:57:09.140: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7946
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 30 21:57:09.450: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:09.451: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:09.451: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:09.457: INFO: Number of nodes with available pods: 0
Mar 30 21:57:09.457: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:57:10.465: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:10.465: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:10.465: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:10.471: INFO: Number of nodes with available pods: 0
Mar 30 21:57:10.471: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:57:11.470: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:11.470: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:11.470: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:11.475: INFO: Number of nodes with available pods: 0
Mar 30 21:57:11.475: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:57:12.466: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:12.466: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:12.466: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:12.472: INFO: Number of nodes with available pods: 0
Mar 30 21:57:12.472: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:57:13.467: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:13.467: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:13.467: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:13.472: INFO: Number of nodes with available pods: 0
Mar 30 21:57:13.472: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:57:14.464: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:14.464: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:14.464: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:14.473: INFO: Number of nodes with available pods: 0
Mar 30 21:57:14.473: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:57:15.466: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:15.466: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:15.466: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:15.471: INFO: Number of nodes with available pods: 0
Mar 30 21:57:15.471: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:57:16.488: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:16.488: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:16.488: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:16.498: INFO: Number of nodes with available pods: 1
Mar 30 21:57:16.499: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:57:17.466: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:17.467: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:17.467: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:17.472: INFO: Number of nodes with available pods: 3
Mar 30 21:57:17.472: INFO: Node worker-pool1-zo88v95j-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:57:18.466: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:18.466: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:18.467: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:18.472: INFO: Number of nodes with available pods: 4
Mar 30 21:57:18.472: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar 30 21:57:18.512: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:18.512: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:18.512: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:18.518: INFO: Number of nodes with available pods: 3
Mar 30 21:57:18.518: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:57:19.526: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:19.526: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:19.526: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:19.533: INFO: Number of nodes with available pods: 3
Mar 30 21:57:19.533: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:57:20.532: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:20.532: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:20.533: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:20.544: INFO: Number of nodes with available pods: 3
Mar 30 21:57:20.544: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:57:21.526: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:21.526: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:21.527: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:21.532: INFO: Number of nodes with available pods: 3
Mar 30 21:57:21.532: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:57:22.526: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:22.526: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:22.526: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:22.531: INFO: Number of nodes with available pods: 3
Mar 30 21:57:22.531: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:57:23.526: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:23.526: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:23.526: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:23.533: INFO: Number of nodes with available pods: 3
Mar 30 21:57:23.533: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:57:24.527: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:24.527: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:24.527: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:24.533: INFO: Number of nodes with available pods: 3
Mar 30 21:57:24.533: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:57:25.526: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:25.526: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:25.526: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:25.532: INFO: Number of nodes with available pods: 3
Mar 30 21:57:25.532: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:57:26.526: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:26.526: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:26.526: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:26.530: INFO: Number of nodes with available pods: 3
Mar 30 21:57:26.530: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:57:27.527: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:27.527: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:27.527: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:27.533: INFO: Number of nodes with available pods: 3
Mar 30 21:57:27.533: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:57:28.525: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:28.525: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:28.525: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:28.530: INFO: Number of nodes with available pods: 3
Mar 30 21:57:28.530: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:57:29.534: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:29.534: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:29.534: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:29.541: INFO: Number of nodes with available pods: 3
Mar 30 21:57:29.541: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:57:30.545: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:30.545: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:30.545: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:30.550: INFO: Number of nodes with available pods: 3
Mar 30 21:57:30.550: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 21:57:31.526: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:31.526: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:31.527: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 21:57:31.541: INFO: Number of nodes with available pods: 4
Mar 30 21:57:31.541: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7946, will wait for the garbage collector to delete the pods
Mar 30 21:57:31.618: INFO: Deleting DaemonSet.extensions daemon-set took: 16.703445ms
Mar 30 21:57:32.222: INFO: Terminating DaemonSet.extensions daemon-set pods took: 604.444017ms
Mar 30 21:57:41.431: INFO: Number of nodes with available pods: 0
Mar 30 21:57:41.431: INFO: Number of running nodes: 0, number of available pods: 0
Mar 30 21:57:41.436: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7946/daemonsets","resourceVersion":"32436"},"items":null}

Mar 30 21:57:41.438: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7946/pods","resourceVersion":"32436"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:57:41.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7946" for this suite.

• [SLOW TEST:32.340 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":278,"completed":131,"skipped":2340,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:57:41.483: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1516
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar 30 21:57:57.763: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 30 21:57:57.767: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 30 21:57:59.767: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 30 21:57:59.778: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 30 21:58:01.768: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 30 21:58:01.773: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 30 21:58:03.767: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 30 21:58:03.773: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:58:03.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1516" for this suite.

• [SLOW TEST:22.332 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":278,"completed":132,"skipped":2348,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:58:03.820: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4783
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-2de5b870-acef-445c-be3f-fc9e7aecdaf2
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:58:12.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4783" for this suite.

• [SLOW TEST:8.299 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":133,"skipped":2350,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:58:12.129: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2891
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Mar 30 21:58:22.895: INFO: Successfully updated pod "labelsupdatea2781883-1357-4612-a944-43913b00b22d"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:58:24.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2891" for this suite.

• [SLOW TEST:12.830 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":278,"completed":134,"skipped":2397,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:58:24.960: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3925
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 30 21:58:25.881: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 30 21:58:27.899: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202305, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202305, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202305, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202305, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:58:29.904: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202305, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202305, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202305, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202305, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 21:58:31.904: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202305, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202305, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202305, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202305, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 30 21:58:34.935: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:58:35.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3925" for this suite.
STEP: Destroying namespace "webhook-3925-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.218 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":278,"completed":135,"skipped":2400,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:58:35.178: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1923
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-51bba60c-0e57-41f4-953b-50696a6665fa
STEP: Creating configMap with name cm-test-opt-upd-43c686b6-2a28-48d0-be63-a90d46569412
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-51bba60c-0e57-41f4-953b-50696a6665fa
STEP: Updating configmap cm-test-opt-upd-43c686b6-2a28-48d0-be63-a90d46569412
STEP: Creating configMap with name cm-test-opt-create-7e70ab19-5250-4107-9223-81d1ed7bd9dd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:58:45.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1923" for this suite.

• [SLOW TEST:10.373 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":136,"skipped":2402,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:58:45.552: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7437
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1692
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 30 21:58:45.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-7437'
Mar 30 21:58:45.894: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 30 21:58:45.894: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Mar 30 21:58:45.911: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Mar 30 21:58:45.918: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Mar 30 21:58:45.989: INFO: scanned /root for discovery docs: <nil>
Mar 30 21:58:45.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-7437'
Mar 30 21:59:05.966: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar 30 21:59:05.966: INFO: stdout: "Created e2e-test-httpd-rc-7f45515b33f21725cd520187bdca4253\nScaling up e2e-test-httpd-rc-7f45515b33f21725cd520187bdca4253 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-7f45515b33f21725cd520187bdca4253 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-7f45515b33f21725cd520187bdca4253 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Mar 30 21:59:05.966: INFO: stdout: "Created e2e-test-httpd-rc-7f45515b33f21725cd520187bdca4253\nScaling up e2e-test-httpd-rc-7f45515b33f21725cd520187bdca4253 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-7f45515b33f21725cd520187bdca4253 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-7f45515b33f21725cd520187bdca4253 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Mar 30 21:59:05.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-7437'
Mar 30 21:59:06.075: INFO: stderr: ""
Mar 30 21:59:06.075: INFO: stdout: "e2e-test-httpd-rc-2qdxw e2e-test-httpd-rc-7f45515b33f21725cd520187bdca4253-z8jbh "
STEP: Replicas for run=e2e-test-httpd-rc: expected=1 actual=2
Mar 30 21:59:11.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-7437'
Mar 30 21:59:11.225: INFO: stderr: ""
Mar 30 21:59:11.225: INFO: stdout: "e2e-test-httpd-rc-7f45515b33f21725cd520187bdca4253-z8jbh "
Mar 30 21:59:11.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods e2e-test-httpd-rc-7f45515b33f21725cd520187bdca4253-z8jbh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7437'
Mar 30 21:59:11.410: INFO: stderr: ""
Mar 30 21:59:11.410: INFO: stdout: "true"
Mar 30 21:59:11.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods e2e-test-httpd-rc-7f45515b33f21725cd520187bdca4253-z8jbh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7437'
Mar 30 21:59:11.548: INFO: stderr: ""
Mar 30 21:59:11.548: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Mar 30 21:59:11.548: INFO: e2e-test-httpd-rc-7f45515b33f21725cd520187bdca4253-z8jbh is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1698
Mar 30 21:59:11.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 delete rc e2e-test-httpd-rc --namespace=kubectl-7437'
Mar 30 21:59:11.684: INFO: stderr: ""
Mar 30 21:59:11.684: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 21:59:11.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7437" for this suite.

• [SLOW TEST:26.154 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl rolling-update should support rolling-update to same image  [Conformance]","total":278,"completed":137,"skipped":2407,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 21:59:11.710: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2783
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-1580017e-6329-4c20-8f38-d6423606ac02
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-1580017e-6329-4c20-8f38-d6423606ac02
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:00:24.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2783" for this suite.

• [SLOW TEST:72.738 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":138,"skipped":2440,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:00:24.450: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5378
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-55ac56e7-5be0-4b69-83f4-bdd1c0cb5c26
STEP: Creating a pod to test consume configMaps
Mar 30 22:00:24.672: INFO: Waiting up to 5m0s for pod "pod-configmaps-fa3566c2-2868-4942-9f4e-a41e8a51a829" in namespace "configmap-5378" to be "success or failure"
Mar 30 22:00:24.680: INFO: Pod "pod-configmaps-fa3566c2-2868-4942-9f4e-a41e8a51a829": Phase="Pending", Reason="", readiness=false. Elapsed: 8.45033ms
Mar 30 22:00:26.685: INFO: Pod "pod-configmaps-fa3566c2-2868-4942-9f4e-a41e8a51a829": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013259974s
Mar 30 22:00:28.698: INFO: Pod "pod-configmaps-fa3566c2-2868-4942-9f4e-a41e8a51a829": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025804061s
Mar 30 22:00:30.707: INFO: Pod "pod-configmaps-fa3566c2-2868-4942-9f4e-a41e8a51a829": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035122438s
Mar 30 22:00:32.713: INFO: Pod "pod-configmaps-fa3566c2-2868-4942-9f4e-a41e8a51a829": Phase="Pending", Reason="", readiness=false. Elapsed: 8.041662505s
Mar 30 22:00:34.721: INFO: Pod "pod-configmaps-fa3566c2-2868-4942-9f4e-a41e8a51a829": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.048887656s
STEP: Saw pod success
Mar 30 22:00:34.721: INFO: Pod "pod-configmaps-fa3566c2-2868-4942-9f4e-a41e8a51a829" satisfied condition "success or failure"
Mar 30 22:00:34.724: INFO: Trying to get logs from node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins pod pod-configmaps-fa3566c2-2868-4942-9f4e-a41e8a51a829 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 30 22:00:34.757: INFO: Waiting for pod pod-configmaps-fa3566c2-2868-4942-9f4e-a41e8a51a829 to disappear
Mar 30 22:00:34.762: INFO: Pod pod-configmaps-fa3566c2-2868-4942-9f4e-a41e8a51a829 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:00:34.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5378" for this suite.

• [SLOW TEST:10.330 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":139,"skipped":2448,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:00:34.784: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2459
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 22:00:34.969: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 30 22:00:39.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 --namespace=crd-publish-openapi-2459 create -f -'
Mar 30 22:00:40.543: INFO: stderr: ""
Mar 30 22:00:40.543: INFO: stdout: "e2e-test-crd-publish-openapi-7897-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 30 22:00:40.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 --namespace=crd-publish-openapi-2459 delete e2e-test-crd-publish-openapi-7897-crds test-cr'
Mar 30 22:00:40.661: INFO: stderr: ""
Mar 30 22:00:40.661: INFO: stdout: "e2e-test-crd-publish-openapi-7897-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar 30 22:00:40.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 --namespace=crd-publish-openapi-2459 apply -f -'
Mar 30 22:00:41.301: INFO: stderr: ""
Mar 30 22:00:41.301: INFO: stdout: "e2e-test-crd-publish-openapi-7897-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 30 22:00:41.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 --namespace=crd-publish-openapi-2459 delete e2e-test-crd-publish-openapi-7897-crds test-cr'
Mar 30 22:00:41.500: INFO: stderr: ""
Mar 30 22:00:41.500: INFO: stdout: "e2e-test-crd-publish-openapi-7897-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Mar 30 22:00:41.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 explain e2e-test-crd-publish-openapi-7897-crds'
Mar 30 22:00:42.301: INFO: stderr: ""
Mar 30 22:00:42.301: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7897-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:00:46.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2459" for this suite.

• [SLOW TEST:11.995 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":278,"completed":140,"skipped":2479,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:00:46.780: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9110
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:00:53.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9110" for this suite.

• [SLOW TEST:7.224 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":278,"completed":141,"skipped":2485,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:00:54.006: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3371
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-7c494fcf-52c3-4e24-833f-5b0d9889e82e
STEP: Creating a pod to test consume secrets
Mar 30 22:00:54.213: INFO: Waiting up to 5m0s for pod "pod-secrets-7ce84dc5-305e-454b-a7e1-cbc8c65b1c52" in namespace "secrets-3371" to be "success or failure"
Mar 30 22:00:54.216: INFO: Pod "pod-secrets-7ce84dc5-305e-454b-a7e1-cbc8c65b1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.702504ms
Mar 30 22:00:56.222: INFO: Pod "pod-secrets-7ce84dc5-305e-454b-a7e1-cbc8c65b1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008921657s
Mar 30 22:00:58.229: INFO: Pod "pod-secrets-7ce84dc5-305e-454b-a7e1-cbc8c65b1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015355334s
Mar 30 22:01:00.234: INFO: Pod "pod-secrets-7ce84dc5-305e-454b-a7e1-cbc8c65b1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020820046s
Mar 30 22:01:02.240: INFO: Pod "pod-secrets-7ce84dc5-305e-454b-a7e1-cbc8c65b1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026465322s
Mar 30 22:01:04.245: INFO: Pod "pod-secrets-7ce84dc5-305e-454b-a7e1-cbc8c65b1c52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.031733465s
STEP: Saw pod success
Mar 30 22:01:04.245: INFO: Pod "pod-secrets-7ce84dc5-305e-454b-a7e1-cbc8c65b1c52" satisfied condition "success or failure"
Mar 30 22:01:04.250: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-secrets-7ce84dc5-305e-454b-a7e1-cbc8c65b1c52 container secret-volume-test: <nil>
STEP: delete the pod
Mar 30 22:01:04.284: INFO: Waiting for pod pod-secrets-7ce84dc5-305e-454b-a7e1-cbc8c65b1c52 to disappear
Mar 30 22:01:04.293: INFO: Pod pod-secrets-7ce84dc5-305e-454b-a7e1-cbc8c65b1c52 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:01:04.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3371" for this suite.

• [SLOW TEST:10.306 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":142,"skipped":2495,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:01:04.314: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3096
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:330
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the initial replication controller
Mar 30 22:01:04.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 create -f - --namespace=kubectl-3096'
Mar 30 22:01:04.917: INFO: stderr: ""
Mar 30 22:01:04.917: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 30 22:01:04.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3096'
Mar 30 22:01:05.008: INFO: stderr: ""
Mar 30 22:01:05.008: INFO: stdout: "update-demo-nautilus-cknnq update-demo-nautilus-pt664 "
Mar 30 22:01:05.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-nautilus-cknnq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3096'
Mar 30 22:01:05.097: INFO: stderr: ""
Mar 30 22:01:05.097: INFO: stdout: ""
Mar 30 22:01:05.097: INFO: update-demo-nautilus-cknnq is created but not running
Mar 30 22:01:10.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3096'
Mar 30 22:01:10.204: INFO: stderr: ""
Mar 30 22:01:10.204: INFO: stdout: "update-demo-nautilus-cknnq update-demo-nautilus-pt664 "
Mar 30 22:01:10.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-nautilus-cknnq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3096'
Mar 30 22:01:10.294: INFO: stderr: ""
Mar 30 22:01:10.294: INFO: stdout: ""
Mar 30 22:01:10.294: INFO: update-demo-nautilus-cknnq is created but not running
Mar 30 22:01:15.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3096'
Mar 30 22:01:15.421: INFO: stderr: ""
Mar 30 22:01:15.421: INFO: stdout: "update-demo-nautilus-cknnq update-demo-nautilus-pt664 "
Mar 30 22:01:15.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-nautilus-cknnq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3096'
Mar 30 22:01:15.529: INFO: stderr: ""
Mar 30 22:01:15.529: INFO: stdout: "true"
Mar 30 22:01:15.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-nautilus-cknnq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3096'
Mar 30 22:01:15.619: INFO: stderr: ""
Mar 30 22:01:15.619: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 30 22:01:15.619: INFO: validating pod update-demo-nautilus-cknnq
Mar 30 22:01:15.630: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 30 22:01:15.630: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 30 22:01:15.630: INFO: update-demo-nautilus-cknnq is verified up and running
Mar 30 22:01:15.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-nautilus-pt664 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3096'
Mar 30 22:01:15.716: INFO: stderr: ""
Mar 30 22:01:15.716: INFO: stdout: "true"
Mar 30 22:01:15.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-nautilus-pt664 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3096'
Mar 30 22:01:15.802: INFO: stderr: ""
Mar 30 22:01:15.802: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 30 22:01:15.802: INFO: validating pod update-demo-nautilus-pt664
Mar 30 22:01:15.810: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 30 22:01:15.810: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 30 22:01:15.810: INFO: update-demo-nautilus-pt664 is verified up and running
STEP: rolling-update to new replication controller
Mar 30 22:01:15.814: INFO: scanned /root for discovery docs: <nil>
Mar 30 22:01:15.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-3096'
Mar 30 22:01:48.612: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar 30 22:01:48.612: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 30 22:01:48.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3096'
Mar 30 22:01:48.722: INFO: stderr: ""
Mar 30 22:01:48.722: INFO: stdout: "update-demo-kitten-8pbcm update-demo-kitten-rmz7k "
Mar 30 22:01:48.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-kitten-8pbcm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3096'
Mar 30 22:01:48.807: INFO: stderr: ""
Mar 30 22:01:48.807: INFO: stdout: "true"
Mar 30 22:01:48.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-kitten-8pbcm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3096'
Mar 30 22:01:48.895: INFO: stderr: ""
Mar 30 22:01:48.895: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar 30 22:01:48.895: INFO: validating pod update-demo-kitten-8pbcm
Mar 30 22:01:48.905: INFO: got data: {
  "image": "kitten.jpg"
}

Mar 30 22:01:48.906: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar 30 22:01:48.906: INFO: update-demo-kitten-8pbcm is verified up and running
Mar 30 22:01:48.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-kitten-rmz7k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3096'
Mar 30 22:01:48.996: INFO: stderr: ""
Mar 30 22:01:48.996: INFO: stdout: "true"
Mar 30 22:01:48.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-kitten-rmz7k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3096'
Mar 30 22:01:49.078: INFO: stderr: ""
Mar 30 22:01:49.078: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar 30 22:01:49.078: INFO: validating pod update-demo-kitten-rmz7k
Mar 30 22:01:49.090: INFO: got data: {
  "image": "kitten.jpg"
}

Mar 30 22:01:49.090: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar 30 22:01:49.090: INFO: update-demo-kitten-rmz7k is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:01:49.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3096" for this suite.

• [SLOW TEST:44.793 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:328
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should do a rolling update of a replication controller  [Conformance]","total":278,"completed":143,"skipped":2505,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:01:49.108: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-7752
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:01:49.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7752" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":278,"completed":144,"skipped":2515,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:01:49.312: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2054
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 30 22:01:49.500: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1914af7e-0b58-4dc8-bbcf-6ee7509445ac" in namespace "downward-api-2054" to be "success or failure"
Mar 30 22:01:49.511: INFO: Pod "downwardapi-volume-1914af7e-0b58-4dc8-bbcf-6ee7509445ac": Phase="Pending", Reason="", readiness=false. Elapsed: 10.148684ms
Mar 30 22:01:51.522: INFO: Pod "downwardapi-volume-1914af7e-0b58-4dc8-bbcf-6ee7509445ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021660221s
Mar 30 22:01:53.528: INFO: Pod "downwardapi-volume-1914af7e-0b58-4dc8-bbcf-6ee7509445ac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026774673s
Mar 30 22:01:55.534: INFO: Pod "downwardapi-volume-1914af7e-0b58-4dc8-bbcf-6ee7509445ac": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033174393s
Mar 30 22:01:57.541: INFO: Pod "downwardapi-volume-1914af7e-0b58-4dc8-bbcf-6ee7509445ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.039804599s
STEP: Saw pod success
Mar 30 22:01:57.541: INFO: Pod "downwardapi-volume-1914af7e-0b58-4dc8-bbcf-6ee7509445ac" satisfied condition "success or failure"
Mar 30 22:01:57.557: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod downwardapi-volume-1914af7e-0b58-4dc8-bbcf-6ee7509445ac container client-container: <nil>
STEP: delete the pod
Mar 30 22:01:57.590: INFO: Waiting for pod downwardapi-volume-1914af7e-0b58-4dc8-bbcf-6ee7509445ac to disappear
Mar 30 22:01:57.594: INFO: Pod downwardapi-volume-1914af7e-0b58-4dc8-bbcf-6ee7509445ac no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:01:57.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2054" for this suite.

• [SLOW TEST:8.300 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":278,"completed":145,"skipped":2520,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:01:57.615: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-210
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:02:13.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-210" for this suite.

• [SLOW TEST:16.362 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":278,"completed":146,"skipped":2532,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:02:13.977: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5324
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 22:02:14.182: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:02:22.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5324" for this suite.

• [SLOW TEST:8.449 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":278,"completed":147,"skipped":2534,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:02:22.428: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1313
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Mar 30 22:02:22.660: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 22:02:26.591: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:02:42.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1313" for this suite.

• [SLOW TEST:20.500 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":278,"completed":148,"skipped":2564,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:02:42.935: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4452
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4452.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4452.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4452.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4452.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4452.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4452.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 30 22:02:53.229: INFO: DNS probes using dns-4452/dns-test-c453d405-999f-47f7-9bd3-6d901026f9bb succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:02:53.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4452" for this suite.

• [SLOW TEST:10.441 seconds]
[sig-network] DNS
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":278,"completed":149,"skipped":2585,"failed":0}
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:02:53.377: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6933
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-secret-gjjr
STEP: Creating a pod to test atomic-volume-subpath
Mar 30 22:02:53.589: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-gjjr" in namespace "subpath-6933" to be "success or failure"
Mar 30 22:02:53.593: INFO: Pod "pod-subpath-test-secret-gjjr": Phase="Pending", Reason="", readiness=false. Elapsed: 3.588082ms
Mar 30 22:02:55.600: INFO: Pod "pod-subpath-test-secret-gjjr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009699244s
Mar 30 22:02:57.607: INFO: Pod "pod-subpath-test-secret-gjjr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017465129s
Mar 30 22:02:59.613: INFO: Pod "pod-subpath-test-secret-gjjr": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02278849s
Mar 30 22:03:01.622: INFO: Pod "pod-subpath-test-secret-gjjr": Phase="Pending", Reason="", readiness=false. Elapsed: 8.031672652s
Mar 30 22:03:03.628: INFO: Pod "pod-subpath-test-secret-gjjr": Phase="Running", Reason="", readiness=true. Elapsed: 10.038259775s
Mar 30 22:03:05.637: INFO: Pod "pod-subpath-test-secret-gjjr": Phase="Running", Reason="", readiness=true. Elapsed: 12.047577695s
Mar 30 22:03:07.642: INFO: Pod "pod-subpath-test-secret-gjjr": Phase="Running", Reason="", readiness=true. Elapsed: 14.051708818s
Mar 30 22:03:09.645: INFO: Pod "pod-subpath-test-secret-gjjr": Phase="Running", Reason="", readiness=true. Elapsed: 16.055638228s
Mar 30 22:03:11.652: INFO: Pod "pod-subpath-test-secret-gjjr": Phase="Running", Reason="", readiness=true. Elapsed: 18.062583336s
Mar 30 22:03:13.663: INFO: Pod "pod-subpath-test-secret-gjjr": Phase="Running", Reason="", readiness=true. Elapsed: 20.073264015s
Mar 30 22:03:15.670: INFO: Pod "pod-subpath-test-secret-gjjr": Phase="Running", Reason="", readiness=true. Elapsed: 22.080063535s
Mar 30 22:03:17.677: INFO: Pod "pod-subpath-test-secret-gjjr": Phase="Running", Reason="", readiness=true. Elapsed: 24.087044813s
Mar 30 22:03:19.683: INFO: Pod "pod-subpath-test-secret-gjjr": Phase="Running", Reason="", readiness=true. Elapsed: 26.092857056s
Mar 30 22:03:21.688: INFO: Pod "pod-subpath-test-secret-gjjr": Phase="Running", Reason="", readiness=true. Elapsed: 28.097713816s
Mar 30 22:03:23.692: INFO: Pod "pod-subpath-test-secret-gjjr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.101968368s
STEP: Saw pod success
Mar 30 22:03:23.692: INFO: Pod "pod-subpath-test-secret-gjjr" satisfied condition "success or failure"
Mar 30 22:03:23.699: INFO: Trying to get logs from node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins pod pod-subpath-test-secret-gjjr container test-container-subpath-secret-gjjr: <nil>
STEP: delete the pod
Mar 30 22:03:23.732: INFO: Waiting for pod pod-subpath-test-secret-gjjr to disappear
Mar 30 22:03:23.737: INFO: Pod pod-subpath-test-secret-gjjr no longer exists
STEP: Deleting pod pod-subpath-test-secret-gjjr
Mar 30 22:03:23.738: INFO: Deleting pod "pod-subpath-test-secret-gjjr" in namespace "subpath-6933"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:03:23.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6933" for this suite.

• [SLOW TEST:30.382 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":278,"completed":150,"skipped":2587,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:03:23.762: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-788
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 22:03:23.954: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 30 22:03:28.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 --namespace=crd-publish-openapi-788 create -f -'
Mar 30 22:03:29.509: INFO: stderr: ""
Mar 30 22:03:29.509: INFO: stdout: "e2e-test-crd-publish-openapi-7538-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 30 22:03:29.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 --namespace=crd-publish-openapi-788 delete e2e-test-crd-publish-openapi-7538-crds test-cr'
Mar 30 22:03:29.610: INFO: stderr: ""
Mar 30 22:03:29.610: INFO: stdout: "e2e-test-crd-publish-openapi-7538-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar 30 22:03:29.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 --namespace=crd-publish-openapi-788 apply -f -'
Mar 30 22:03:30.324: INFO: stderr: ""
Mar 30 22:03:30.324: INFO: stdout: "e2e-test-crd-publish-openapi-7538-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 30 22:03:30.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 --namespace=crd-publish-openapi-788 delete e2e-test-crd-publish-openapi-7538-crds test-cr'
Mar 30 22:03:30.428: INFO: stderr: ""
Mar 30 22:03:30.428: INFO: stdout: "e2e-test-crd-publish-openapi-7538-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar 30 22:03:30.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 explain e2e-test-crd-publish-openapi-7538-crds'
Mar 30 22:03:30.782: INFO: stderr: ""
Mar 30 22:03:30.783: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7538-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:03:34.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-788" for this suite.

• [SLOW TEST:10.971 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":278,"completed":151,"skipped":2597,"failed":0}
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:03:34.738: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8505
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-8505
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-8505
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8505
Mar 30 22:03:34.938: INFO: Found 0 stateful pods, waiting for 1
Mar 30 22:03:44.946: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar 30 22:03:44.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-8505 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 30 22:03:45.222: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 30 22:03:45.222: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 30 22:03:45.222: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 30 22:03:45.227: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 30 22:03:55.248: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 30 22:03:55.248: INFO: Waiting for statefulset status.replicas updated to 0
Mar 30 22:03:55.270: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999702s
Mar 30 22:03:56.276: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996459483s
Mar 30 22:03:57.282: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990124276s
Mar 30 22:03:58.287: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.983868709s
Mar 30 22:03:59.293: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.978885802s
Mar 30 22:04:00.299: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.973480957s
Mar 30 22:04:01.305: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.967029487s
Mar 30 22:04:02.311: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.9613502s
Mar 30 22:04:03.318: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.955000634s
Mar 30 22:04:04.324: INFO: Verifying statefulset ss doesn't scale past 1 for another 947.709424ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8505
Mar 30 22:04:05.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-8505 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 22:04:05.648: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 30 22:04:05.648: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 30 22:04:05.648: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 30 22:04:05.654: INFO: Found 1 stateful pods, waiting for 3
Mar 30 22:04:15.663: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 22:04:15.663: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 22:04:15.663: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Mar 30 22:04:25.665: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 22:04:25.665: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 22:04:25.665: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar 30 22:04:25.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-8505 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 30 22:04:25.926: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 30 22:04:25.926: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 30 22:04:25.926: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 30 22:04:25.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-8505 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 30 22:04:26.197: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 30 22:04:26.197: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 30 22:04:26.197: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 30 22:04:26.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-8505 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 30 22:04:26.591: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 30 22:04:26.591: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 30 22:04:26.591: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 30 22:04:26.591: INFO: Waiting for statefulset status.replicas updated to 0
Mar 30 22:04:26.596: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar 30 22:04:36.608: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 30 22:04:36.608: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 30 22:04:36.608: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 30 22:04:36.637: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999946s
Mar 30 22:04:37.645: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990398585s
Mar 30 22:04:38.651: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.982264302s
Mar 30 22:04:39.656: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.976071448s
Mar 30 22:04:40.663: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.971102963s
Mar 30 22:04:41.670: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.964298646s
Mar 30 22:04:42.677: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.957304598s
Mar 30 22:04:43.689: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.94707283s
Mar 30 22:04:44.697: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.938155206s
Mar 30 22:04:45.703: INFO: Verifying statefulset ss doesn't scale past 3 for another 930.530223ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8505
Mar 30 22:04:46.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-8505 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 22:04:47.041: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 30 22:04:47.041: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 30 22:04:47.041: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 30 22:04:47.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-8505 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 22:04:47.339: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 30 22:04:47.339: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 30 22:04:47.339: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 30 22:04:47.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=statefulset-8505 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 22:04:47.613: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 30 22:04:47.613: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 30 22:04:47.613: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 30 22:04:47.613: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Mar 30 22:05:07.657: INFO: Deleting all statefulset in ns statefulset-8505
Mar 30 22:05:07.662: INFO: Scaling statefulset ss to 0
Mar 30 22:05:07.674: INFO: Waiting for statefulset status.replicas updated to 0
Mar 30 22:05:07.678: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:05:07.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8505" for this suite.

• [SLOW TEST:92.991 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":278,"completed":152,"skipped":2602,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:05:07.736: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5345
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating cluster-info
Mar 30 22:05:07.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 cluster-info'
Mar 30 22:05:08.181: INFO: stderr: ""
Mar 30 22:05:08.181: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:05:08.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5345" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":278,"completed":153,"skipped":2683,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:05:08.200: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5572
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 30 22:05:09.266: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 30 22:05:11.421: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202709, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202709, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202709, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202709, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:05:13.428: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202709, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202709, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202709, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202709, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:05:15.431: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202709, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202709, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202709, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202709, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:05:17.428: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202709, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202709, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202709, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721202709, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 30 22:05:20.460: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 22:05:20.467: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2123-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:05:21.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5572" for this suite.
STEP: Destroying namespace "webhook-5572-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.589 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":278,"completed":154,"skipped":2686,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:05:21.791: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-790
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 30 22:05:22.018: INFO: Waiting up to 5m0s for pod "pod-33491ea4-7476-404c-882f-4d7efae6f089" in namespace "emptydir-790" to be "success or failure"
Mar 30 22:05:22.025: INFO: Pod "pod-33491ea4-7476-404c-882f-4d7efae6f089": Phase="Pending", Reason="", readiness=false. Elapsed: 6.159799ms
Mar 30 22:05:24.030: INFO: Pod "pod-33491ea4-7476-404c-882f-4d7efae6f089": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0113512s
Mar 30 22:05:26.035: INFO: Pod "pod-33491ea4-7476-404c-882f-4d7efae6f089": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01673465s
Mar 30 22:05:28.043: INFO: Pod "pod-33491ea4-7476-404c-882f-4d7efae6f089": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024532161s
Mar 30 22:05:30.049: INFO: Pod "pod-33491ea4-7476-404c-882f-4d7efae6f089": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.030922864s
STEP: Saw pod success
Mar 30 22:05:30.049: INFO: Pod "pod-33491ea4-7476-404c-882f-4d7efae6f089" satisfied condition "success or failure"
Mar 30 22:05:30.053: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-33491ea4-7476-404c-882f-4d7efae6f089 container test-container: <nil>
STEP: delete the pod
Mar 30 22:05:30.086: INFO: Waiting for pod pod-33491ea4-7476-404c-882f-4d7efae6f089 to disappear
Mar 30 22:05:30.090: INFO: Pod pod-33491ea4-7476-404c-882f-4d7efae6f089 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:05:30.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-790" for this suite.

• [SLOW TEST:8.322 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":155,"skipped":2693,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:05:30.114: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2970
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 22:05:30.341: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"1d2f8820-6e91-4aea-b4b4-e0bc2d62355a", Controller:(*bool)(0xc005e1c26e), BlockOwnerDeletion:(*bool)(0xc005e1c26f)}}
Mar 30 22:05:30.370: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"d0db5f37-5f22-4fcf-a001-a11057c9b57e", Controller:(*bool)(0xc005d55e3e), BlockOwnerDeletion:(*bool)(0xc005d55e3f)}}
Mar 30 22:05:30.385: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"24110b5d-589b-4535-b7f2-550de92e1104", Controller:(*bool)(0xc005e1c58e), BlockOwnerDeletion:(*bool)(0xc005e1c58f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:05:35.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2970" for this suite.

• [SLOW TEST:5.342 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":278,"completed":156,"skipped":2706,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:05:35.457: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-473
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar 30 22:05:51.723: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 30 22:05:51.729: INFO: Pod pod-with-prestop-http-hook still exists
Mar 30 22:05:53.730: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 30 22:05:53.735: INFO: Pod pod-with-prestop-http-hook still exists
Mar 30 22:05:55.730: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 30 22:05:55.735: INFO: Pod pod-with-prestop-http-hook still exists
Mar 30 22:05:57.730: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 30 22:05:57.737: INFO: Pod pod-with-prestop-http-hook still exists
Mar 30 22:05:59.730: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 30 22:05:59.739: INFO: Pod pod-with-prestop-http-hook still exists
Mar 30 22:06:01.731: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 30 22:06:01.737: INFO: Pod pod-with-prestop-http-hook still exists
Mar 30 22:06:03.730: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 30 22:06:03.735: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:06:03.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-473" for this suite.

• [SLOW TEST:28.319 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":278,"completed":157,"skipped":2732,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:06:03.778: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5036
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 30 22:06:03.977: INFO: Waiting up to 5m0s for pod "downwardapi-volume-292c3bec-bf4f-4236-a413-bf2df20e11f5" in namespace "projected-5036" to be "success or failure"
Mar 30 22:06:03.981: INFO: Pod "downwardapi-volume-292c3bec-bf4f-4236-a413-bf2df20e11f5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.890361ms
Mar 30 22:06:05.988: INFO: Pod "downwardapi-volume-292c3bec-bf4f-4236-a413-bf2df20e11f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010261871s
Mar 30 22:06:07.994: INFO: Pod "downwardapi-volume-292c3bec-bf4f-4236-a413-bf2df20e11f5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016122088s
Mar 30 22:06:10.000: INFO: Pod "downwardapi-volume-292c3bec-bf4f-4236-a413-bf2df20e11f5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021937709s
Mar 30 22:06:12.010: INFO: Pod "downwardapi-volume-292c3bec-bf4f-4236-a413-bf2df20e11f5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.032211282s
Mar 30 22:06:14.019: INFO: Pod "downwardapi-volume-292c3bec-bf4f-4236-a413-bf2df20e11f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.041433424s
STEP: Saw pod success
Mar 30 22:06:14.019: INFO: Pod "downwardapi-volume-292c3bec-bf4f-4236-a413-bf2df20e11f5" satisfied condition "success or failure"
Mar 30 22:06:14.023: INFO: Trying to get logs from node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins pod downwardapi-volume-292c3bec-bf4f-4236-a413-bf2df20e11f5 container client-container: <nil>
STEP: delete the pod
Mar 30 22:06:14.066: INFO: Waiting for pod downwardapi-volume-292c3bec-bf4f-4236-a413-bf2df20e11f5 to disappear
Mar 30 22:06:14.071: INFO: Pod downwardapi-volume-292c3bec-bf4f-4236-a413-bf2df20e11f5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:06:14.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5036" for this suite.

• [SLOW TEST:10.311 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":158,"skipped":2740,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:06:14.094: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8021
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-75534e7b-c7f6-4411-b42b-92264f376397
STEP: Creating a pod to test consume secrets
Mar 30 22:06:14.345: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-068cf123-f015-40d9-affa-9270767911ef" in namespace "projected-8021" to be "success or failure"
Mar 30 22:06:14.362: INFO: Pod "pod-projected-secrets-068cf123-f015-40d9-affa-9270767911ef": Phase="Pending", Reason="", readiness=false. Elapsed: 17.391102ms
Mar 30 22:06:16.369: INFO: Pod "pod-projected-secrets-068cf123-f015-40d9-affa-9270767911ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023980646s
Mar 30 22:06:18.374: INFO: Pod "pod-projected-secrets-068cf123-f015-40d9-affa-9270767911ef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029575192s
Mar 30 22:06:20.379: INFO: Pod "pod-projected-secrets-068cf123-f015-40d9-affa-9270767911ef": Phase="Pending", Reason="", readiness=false. Elapsed: 6.034534029s
Mar 30 22:06:22.388: INFO: Pod "pod-projected-secrets-068cf123-f015-40d9-affa-9270767911ef": Phase="Pending", Reason="", readiness=false. Elapsed: 8.043111723s
Mar 30 22:06:24.393: INFO: Pod "pod-projected-secrets-068cf123-f015-40d9-affa-9270767911ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.048158793s
STEP: Saw pod success
Mar 30 22:06:24.393: INFO: Pod "pod-projected-secrets-068cf123-f015-40d9-affa-9270767911ef" satisfied condition "success or failure"
Mar 30 22:06:24.397: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-projected-secrets-068cf123-f015-40d9-affa-9270767911ef container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 30 22:06:24.428: INFO: Waiting for pod pod-projected-secrets-068cf123-f015-40d9-affa-9270767911ef to disappear
Mar 30 22:06:24.432: INFO: Pod pod-projected-secrets-068cf123-f015-40d9-affa-9270767911ef no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:06:24.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8021" for this suite.

• [SLOW TEST:10.355 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":278,"completed":159,"skipped":2743,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:06:24.450: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8975
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8975.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8975.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8975.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8975.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8975.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8975.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8975.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8975.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8975.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8975.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8975.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 18.173.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.173.18_udp@PTR;check="$$(dig +tcp +noall +answer +search 18.173.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.173.18_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8975.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8975.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8975.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8975.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8975.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8975.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8975.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8975.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8975.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8975.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8975.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 18.173.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.173.18_udp@PTR;check="$$(dig +tcp +noall +answer +search 18.173.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.173.18_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 30 22:06:34.708: INFO: Unable to read wheezy_udp@dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:34.715: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:34.722: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:34.727: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:34.765: INFO: Unable to read jessie_udp@dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:34.771: INFO: Unable to read jessie_tcp@dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:34.776: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:34.782: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:34.811: INFO: Lookups using dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f failed for: [wheezy_udp@dns-test-service.dns-8975.svc.cluster.local wheezy_tcp@dns-test-service.dns-8975.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local jessie_udp@dns-test-service.dns-8975.svc.cluster.local jessie_tcp@dns-test-service.dns-8975.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local]

Mar 30 22:06:39.820: INFO: Unable to read wheezy_udp@dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:39.825: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:39.829: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:39.834: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:39.873: INFO: Unable to read jessie_udp@dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:39.879: INFO: Unable to read jessie_tcp@dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:39.905: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:39.909: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:39.942: INFO: Lookups using dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f failed for: [wheezy_udp@dns-test-service.dns-8975.svc.cluster.local wheezy_tcp@dns-test-service.dns-8975.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local jessie_udp@dns-test-service.dns-8975.svc.cluster.local jessie_tcp@dns-test-service.dns-8975.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local]

Mar 30 22:06:44.820: INFO: Unable to read wheezy_udp@dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:44.827: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:44.834: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:44.839: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:44.875: INFO: Unable to read jessie_udp@dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:44.885: INFO: Unable to read jessie_tcp@dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:44.890: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:44.895: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:44.929: INFO: Lookups using dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f failed for: [wheezy_udp@dns-test-service.dns-8975.svc.cluster.local wheezy_tcp@dns-test-service.dns-8975.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local jessie_udp@dns-test-service.dns-8975.svc.cluster.local jessie_tcp@dns-test-service.dns-8975.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local]

Mar 30 22:06:49.819: INFO: Unable to read wheezy_udp@dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:49.825: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:49.831: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:49.836: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:49.869: INFO: Unable to read jessie_udp@dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:49.874: INFO: Unable to read jessie_tcp@dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:49.878: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:49.882: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:49.912: INFO: Lookups using dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f failed for: [wheezy_udp@dns-test-service.dns-8975.svc.cluster.local wheezy_tcp@dns-test-service.dns-8975.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local jessie_udp@dns-test-service.dns-8975.svc.cluster.local jessie_tcp@dns-test-service.dns-8975.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local]

Mar 30 22:06:54.821: INFO: Unable to read wheezy_udp@dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:54.827: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:54.832: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:54.838: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:54.874: INFO: Unable to read jessie_udp@dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:54.880: INFO: Unable to read jessie_tcp@dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:54.885: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:54.889: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:54.918: INFO: Lookups using dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f failed for: [wheezy_udp@dns-test-service.dns-8975.svc.cluster.local wheezy_tcp@dns-test-service.dns-8975.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local jessie_udp@dns-test-service.dns-8975.svc.cluster.local jessie_tcp@dns-test-service.dns-8975.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local]

Mar 30 22:06:59.819: INFO: Unable to read wheezy_udp@dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:59.853: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:59.861: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:59.872: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:59.932: INFO: Unable to read jessie_udp@dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:59.937: INFO: Unable to read jessie_tcp@dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:59.942: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:59.947: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local from pod dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f: the server could not find the requested resource (get pods dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f)
Mar 30 22:06:59.975: INFO: Lookups using dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f failed for: [wheezy_udp@dns-test-service.dns-8975.svc.cluster.local wheezy_tcp@dns-test-service.dns-8975.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local jessie_udp@dns-test-service.dns-8975.svc.cluster.local jessie_tcp@dns-test-service.dns-8975.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8975.svc.cluster.local]

Mar 30 22:07:04.915: INFO: DNS probes using dns-8975/dns-test-085c1d79-4f62-45ec-a0b9-987c2a89006f succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:07:05.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8975" for this suite.

• [SLOW TEST:40.828 seconds]
[sig-network] DNS
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":278,"completed":160,"skipped":2745,"failed":0}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:07:05.284: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6021
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 22:07:05.579: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar 30 22:07:05.593: INFO: Number of nodes with available pods: 0
Mar 30 22:07:05.593: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Mar 30 22:07:05.634: INFO: Number of nodes with available pods: 0
Mar 30 22:07:05.634: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 22:07:06.641: INFO: Number of nodes with available pods: 0
Mar 30 22:07:06.641: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 22:07:07.641: INFO: Number of nodes with available pods: 0
Mar 30 22:07:07.641: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 22:07:08.639: INFO: Number of nodes with available pods: 0
Mar 30 22:07:08.639: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 22:07:09.640: INFO: Number of nodes with available pods: 0
Mar 30 22:07:09.640: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 22:07:10.640: INFO: Number of nodes with available pods: 0
Mar 30 22:07:10.640: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 22:07:11.640: INFO: Number of nodes with available pods: 0
Mar 30 22:07:11.640: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 22:07:12.639: INFO: Number of nodes with available pods: 0
Mar 30 22:07:12.639: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 22:07:13.641: INFO: Number of nodes with available pods: 0
Mar 30 22:07:13.641: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 22:07:14.641: INFO: Number of nodes with available pods: 1
Mar 30 22:07:14.641: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar 30 22:07:14.668: INFO: Number of nodes with available pods: 1
Mar 30 22:07:14.669: INFO: Number of running nodes: 0, number of available pods: 1
Mar 30 22:07:15.676: INFO: Number of nodes with available pods: 0
Mar 30 22:07:15.676: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar 30 22:07:15.701: INFO: Number of nodes with available pods: 0
Mar 30 22:07:15.701: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 22:07:16.709: INFO: Number of nodes with available pods: 0
Mar 30 22:07:16.709: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 22:07:17.707: INFO: Number of nodes with available pods: 0
Mar 30 22:07:17.707: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 22:07:18.707: INFO: Number of nodes with available pods: 0
Mar 30 22:07:18.707: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 22:07:19.706: INFO: Number of nodes with available pods: 0
Mar 30 22:07:19.706: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 22:07:20.707: INFO: Number of nodes with available pods: 0
Mar 30 22:07:20.707: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 22:07:21.707: INFO: Number of nodes with available pods: 0
Mar 30 22:07:21.707: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 22:07:22.709: INFO: Number of nodes with available pods: 0
Mar 30 22:07:22.709: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 22:07:23.707: INFO: Number of nodes with available pods: 0
Mar 30 22:07:23.707: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 22:07:24.709: INFO: Number of nodes with available pods: 0
Mar 30 22:07:24.709: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 22:07:25.709: INFO: Number of nodes with available pods: 0
Mar 30 22:07:25.709: INFO: Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 22:07:26.709: INFO: Number of nodes with available pods: 1
Mar 30 22:07:26.710: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6021, will wait for the garbage collector to delete the pods
Mar 30 22:07:26.796: INFO: Deleting DaemonSet.extensions daemon-set took: 12.467545ms
Mar 30 22:07:27.396: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.368142ms
Mar 30 22:07:30.101: INFO: Number of nodes with available pods: 0
Mar 30 22:07:30.101: INFO: Number of running nodes: 0, number of available pods: 0
Mar 30 22:07:30.105: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6021/daemonsets","resourceVersion":"36753"},"items":null}

Mar 30 22:07:30.108: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6021/pods","resourceVersion":"36753"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:07:30.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6021" for this suite.

• [SLOW TEST:24.890 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":278,"completed":161,"skipped":2749,"failed":0}
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:07:30.174: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3188
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 30 22:07:30.361: INFO: Waiting up to 5m0s for pod "downwardapi-volume-32cd0353-b1ab-4330-bd80-297738e167cb" in namespace "downward-api-3188" to be "success or failure"
Mar 30 22:07:30.368: INFO: Pod "downwardapi-volume-32cd0353-b1ab-4330-bd80-297738e167cb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.155338ms
Mar 30 22:07:32.373: INFO: Pod "downwardapi-volume-32cd0353-b1ab-4330-bd80-297738e167cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011539016s
Mar 30 22:07:34.381: INFO: Pod "downwardapi-volume-32cd0353-b1ab-4330-bd80-297738e167cb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019494002s
Mar 30 22:07:36.386: INFO: Pod "downwardapi-volume-32cd0353-b1ab-4330-bd80-297738e167cb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024878243s
Mar 30 22:07:38.393: INFO: Pod "downwardapi-volume-32cd0353-b1ab-4330-bd80-297738e167cb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.031293092s
Mar 30 22:07:40.397: INFO: Pod "downwardapi-volume-32cd0353-b1ab-4330-bd80-297738e167cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.035450599s
STEP: Saw pod success
Mar 30 22:07:40.397: INFO: Pod "downwardapi-volume-32cd0353-b1ab-4330-bd80-297738e167cb" satisfied condition "success or failure"
Mar 30 22:07:40.400: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod downwardapi-volume-32cd0353-b1ab-4330-bd80-297738e167cb container client-container: <nil>
STEP: delete the pod
Mar 30 22:07:40.436: INFO: Waiting for pod downwardapi-volume-32cd0353-b1ab-4330-bd80-297738e167cb to disappear
Mar 30 22:07:40.439: INFO: Pod downwardapi-volume-32cd0353-b1ab-4330-bd80-297738e167cb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:07:40.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3188" for this suite.

• [SLOW TEST:10.292 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":278,"completed":162,"skipped":2749,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:07:40.467: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9553
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-d1b3e3c7-50c2-424d-a766-956bc4aa16c2
STEP: Creating a pod to test consume secrets
Mar 30 22:07:40.670: INFO: Waiting up to 5m0s for pod "pod-secrets-b1511b7a-b64b-47f2-affb-598712c3930f" in namespace "secrets-9553" to be "success or failure"
Mar 30 22:07:40.673: INFO: Pod "pod-secrets-b1511b7a-b64b-47f2-affb-598712c3930f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.829422ms
Mar 30 22:07:42.680: INFO: Pod "pod-secrets-b1511b7a-b64b-47f2-affb-598712c3930f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009938834s
Mar 30 22:07:44.685: INFO: Pod "pod-secrets-b1511b7a-b64b-47f2-affb-598712c3930f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014754586s
Mar 30 22:07:46.690: INFO: Pod "pod-secrets-b1511b7a-b64b-47f2-affb-598712c3930f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020372399s
Mar 30 22:07:48.698: INFO: Pod "pod-secrets-b1511b7a-b64b-47f2-affb-598712c3930f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.027869158s
STEP: Saw pod success
Mar 30 22:07:48.698: INFO: Pod "pod-secrets-b1511b7a-b64b-47f2-affb-598712c3930f" satisfied condition "success or failure"
Mar 30 22:07:48.703: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-secrets-b1511b7a-b64b-47f2-affb-598712c3930f container secret-volume-test: <nil>
STEP: delete the pod
Mar 30 22:07:48.740: INFO: Waiting for pod pod-secrets-b1511b7a-b64b-47f2-affb-598712c3930f to disappear
Mar 30 22:07:48.745: INFO: Pod pod-secrets-b1511b7a-b64b-47f2-affb-598712c3930f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:07:48.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9553" for this suite.

• [SLOW TEST:8.296 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":278,"completed":163,"skipped":2760,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:07:48.764: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4016
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name projected-secret-test-956ab6ca-e8fb-4670-b692-03e6cd9df7cb
STEP: Creating a pod to test consume secrets
Mar 30 22:07:48.966: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2786cd08-5cde-4d57-befd-1a00c578ceda" in namespace "projected-4016" to be "success or failure"
Mar 30 22:07:48.970: INFO: Pod "pod-projected-secrets-2786cd08-5cde-4d57-befd-1a00c578ceda": Phase="Pending", Reason="", readiness=false. Elapsed: 3.982556ms
Mar 30 22:07:50.976: INFO: Pod "pod-projected-secrets-2786cd08-5cde-4d57-befd-1a00c578ceda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010383259s
Mar 30 22:07:52.982: INFO: Pod "pod-projected-secrets-2786cd08-5cde-4d57-befd-1a00c578ceda": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016286195s
Mar 30 22:07:54.991: INFO: Pod "pod-projected-secrets-2786cd08-5cde-4d57-befd-1a00c578ceda": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025233647s
Mar 30 22:07:56.999: INFO: Pod "pod-projected-secrets-2786cd08-5cde-4d57-befd-1a00c578ceda": Phase="Pending", Reason="", readiness=false. Elapsed: 8.033413904s
Mar 30 22:07:59.005: INFO: Pod "pod-projected-secrets-2786cd08-5cde-4d57-befd-1a00c578ceda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.039615886s
STEP: Saw pod success
Mar 30 22:07:59.006: INFO: Pod "pod-projected-secrets-2786cd08-5cde-4d57-befd-1a00c578ceda" satisfied condition "success or failure"
Mar 30 22:07:59.010: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-projected-secrets-2786cd08-5cde-4d57-befd-1a00c578ceda container secret-volume-test: <nil>
STEP: delete the pod
Mar 30 22:07:59.049: INFO: Waiting for pod pod-projected-secrets-2786cd08-5cde-4d57-befd-1a00c578ceda to disappear
Mar 30 22:07:59.052: INFO: Pod pod-projected-secrets-2786cd08-5cde-4d57-befd-1a00c578ceda no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:07:59.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4016" for this suite.

• [SLOW TEST:10.309 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":278,"completed":164,"skipped":2770,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:07:59.075: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4482
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4482.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4482.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 30 22:08:09.360: INFO: DNS probes using dns-4482/dns-test-31b899e2-a0a8-4c04-9987-790486909a03 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:08:09.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4482" for this suite.

• [SLOW TEST:10.326 seconds]
[sig-network] DNS
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":278,"completed":165,"skipped":2776,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:08:09.409: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8315
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:08:20.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8315" for this suite.

• [SLOW TEST:11.341 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":278,"completed":166,"skipped":2778,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:08:20.751: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-188
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-485bd447-c7d5-41ab-88cc-1a7a068a77cb
STEP: Creating secret with name s-test-opt-upd-9ca5eab6-6f68-4bf5-ae7e-db8f91a1aaef
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-485bd447-c7d5-41ab-88cc-1a7a068a77cb
STEP: Updating secret s-test-opt-upd-9ca5eab6-6f68-4bf5-ae7e-db8f91a1aaef
STEP: Creating secret with name s-test-opt-create-e930c3d8-8aa4-4948-9e5b-cb8e24f6fd1f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:09:59.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-188" for this suite.

• [SLOW TEST:99.064 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":167,"skipped":2779,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:09:59.818: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2146
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar 30 22:10:00.021: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2146 /api/v1/namespaces/watch-2146/configmaps/e2e-watch-test-watch-closed b25613d8-d7a9-4706-a959-e1ab1a657d1d 37626 0 2020-03-30 22:10:00 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 30 22:10:00.022: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2146 /api/v1/namespaces/watch-2146/configmaps/e2e-watch-test-watch-closed b25613d8-d7a9-4706-a959-e1ab1a657d1d 37627 0 2020-03-30 22:10:00 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar 30 22:10:00.045: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2146 /api/v1/namespaces/watch-2146/configmaps/e2e-watch-test-watch-closed b25613d8-d7a9-4706-a959-e1ab1a657d1d 37628 0 2020-03-30 22:10:00 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 30 22:10:00.045: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2146 /api/v1/namespaces/watch-2146/configmaps/e2e-watch-test-watch-closed b25613d8-d7a9-4706-a959-e1ab1a657d1d 37629 0 2020-03-30 22:10:00 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:10:00.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2146" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":278,"completed":168,"skipped":2795,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:10:00.068: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5107
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 30 22:10:00.301: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:00.301: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:00.301: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:00.316: INFO: Number of nodes with available pods: 0
Mar 30 22:10:00.316: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 22:10:01.325: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:01.326: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:01.326: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:01.337: INFO: Number of nodes with available pods: 0
Mar 30 22:10:01.337: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 22:10:02.326: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:02.326: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:02.326: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:02.331: INFO: Number of nodes with available pods: 0
Mar 30 22:10:02.331: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 22:10:03.325: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:03.326: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:03.326: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:03.332: INFO: Number of nodes with available pods: 0
Mar 30 22:10:03.332: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 22:10:04.326: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:04.326: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:04.327: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:04.332: INFO: Number of nodes with available pods: 0
Mar 30 22:10:04.332: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 22:10:05.338: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:05.338: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:05.338: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:05.345: INFO: Number of nodes with available pods: 0
Mar 30 22:10:05.345: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 22:10:06.327: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:06.327: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:06.327: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:06.331: INFO: Number of nodes with available pods: 0
Mar 30 22:10:06.331: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 22:10:07.325: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:07.325: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:07.325: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:07.330: INFO: Number of nodes with available pods: 0
Mar 30 22:10:07.330: INFO: Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 22:10:08.325: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:08.326: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:08.326: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:08.331: INFO: Number of nodes with available pods: 2
Mar 30 22:10:08.331: INFO: Node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins is running more than one daemon pod
Mar 30 22:10:09.324: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:09.325: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:09.325: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:09.330: INFO: Number of nodes with available pods: 4
Mar 30 22:10:09.330: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar 30 22:10:09.355: INFO: DaemonSet pods can't tolerate node master-0-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:09.356: INFO: DaemonSet pods can't tolerate node master-1-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:09.356: INFO: DaemonSet pods can't tolerate node master-2-eccd-ci-os-12-jenkins with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 22:10:09.363: INFO: Number of nodes with available pods: 4
Mar 30 22:10:09.363: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5107, will wait for the garbage collector to delete the pods
Mar 30 22:10:10.469: INFO: Deleting DaemonSet.extensions daemon-set took: 33.685981ms
Mar 30 22:10:11.072: INFO: Terminating DaemonSet.extensions daemon-set pods took: 603.302912ms
Mar 30 22:10:13.778: INFO: Number of nodes with available pods: 0
Mar 30 22:10:13.778: INFO: Number of running nodes: 0, number of available pods: 0
Mar 30 22:10:13.782: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5107/daemonsets","resourceVersion":"37840"},"items":null}

Mar 30 22:10:13.785: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5107/pods","resourceVersion":"37840"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:10:13.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5107" for this suite.

• [SLOW TEST:13.762 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":278,"completed":169,"skipped":2859,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:10:13.839: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-570
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 30 22:10:14.047: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a5c9e95c-e2a5-4c42-9d78-638d445597ea" in namespace "downward-api-570" to be "success or failure"
Mar 30 22:10:14.055: INFO: Pod "downwardapi-volume-a5c9e95c-e2a5-4c42-9d78-638d445597ea": Phase="Pending", Reason="", readiness=false. Elapsed: 7.324209ms
Mar 30 22:10:16.060: INFO: Pod "downwardapi-volume-a5c9e95c-e2a5-4c42-9d78-638d445597ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013098469s
Mar 30 22:10:18.068: INFO: Pod "downwardapi-volume-a5c9e95c-e2a5-4c42-9d78-638d445597ea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020920742s
Mar 30 22:10:20.075: INFO: Pod "downwardapi-volume-a5c9e95c-e2a5-4c42-9d78-638d445597ea": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027995213s
Mar 30 22:10:22.081: INFO: Pod "downwardapi-volume-a5c9e95c-e2a5-4c42-9d78-638d445597ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.033766908s
STEP: Saw pod success
Mar 30 22:10:22.081: INFO: Pod "downwardapi-volume-a5c9e95c-e2a5-4c42-9d78-638d445597ea" satisfied condition "success or failure"
Mar 30 22:10:22.087: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod downwardapi-volume-a5c9e95c-e2a5-4c42-9d78-638d445597ea container client-container: <nil>
STEP: delete the pod
Mar 30 22:10:22.130: INFO: Waiting for pod downwardapi-volume-a5c9e95c-e2a5-4c42-9d78-638d445597ea to disappear
Mar 30 22:10:22.139: INFO: Pod downwardapi-volume-a5c9e95c-e2a5-4c42-9d78-638d445597ea no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:10:22.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-570" for this suite.

• [SLOW TEST:8.318 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":170,"skipped":2870,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:10:22.160: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5488
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating pod
Mar 30 22:10:30.376: INFO: Pod pod-hostip-412446da-2edc-4966-ac26-83e0fc4b17b9 has hostIP: 10.0.10.5
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:10:30.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5488" for this suite.

• [SLOW TEST:8.236 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":278,"completed":171,"skipped":2901,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:10:30.399: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3944
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-7fd50c44-65f9-4a77-8d19-3de1fde19aa1
STEP: Creating a pod to test consume secrets
Mar 30 22:10:30.603: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d3487968-f9d8-4913-9d88-4ad6229a1c81" in namespace "projected-3944" to be "success or failure"
Mar 30 22:10:30.616: INFO: Pod "pod-projected-secrets-d3487968-f9d8-4913-9d88-4ad6229a1c81": Phase="Pending", Reason="", readiness=false. Elapsed: 12.601895ms
Mar 30 22:10:32.622: INFO: Pod "pod-projected-secrets-d3487968-f9d8-4913-9d88-4ad6229a1c81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018934624s
Mar 30 22:10:34.631: INFO: Pod "pod-projected-secrets-d3487968-f9d8-4913-9d88-4ad6229a1c81": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027983933s
Mar 30 22:10:36.639: INFO: Pod "pod-projected-secrets-d3487968-f9d8-4913-9d88-4ad6229a1c81": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035898433s
Mar 30 22:10:38.645: INFO: Pod "pod-projected-secrets-d3487968-f9d8-4913-9d88-4ad6229a1c81": Phase="Pending", Reason="", readiness=false. Elapsed: 8.041796809s
Mar 30 22:10:40.667: INFO: Pod "pod-projected-secrets-d3487968-f9d8-4913-9d88-4ad6229a1c81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.063831608s
STEP: Saw pod success
Mar 30 22:10:40.667: INFO: Pod "pod-projected-secrets-d3487968-f9d8-4913-9d88-4ad6229a1c81" satisfied condition "success or failure"
Mar 30 22:10:40.678: INFO: Trying to get logs from node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins pod pod-projected-secrets-d3487968-f9d8-4913-9d88-4ad6229a1c81 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 30 22:10:40.728: INFO: Waiting for pod pod-projected-secrets-d3487968-f9d8-4913-9d88-4ad6229a1c81 to disappear
Mar 30 22:10:40.733: INFO: Pod pod-projected-secrets-d3487968-f9d8-4913-9d88-4ad6229a1c81 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:10:40.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3944" for this suite.

• [SLOW TEST:10.357 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":278,"completed":172,"skipped":2906,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:10:40.759: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7799
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:10:58.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7799" for this suite.

• [SLOW TEST:17.274 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":278,"completed":173,"skipped":2915,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:10:58.035: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4771
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 30 22:11:07.304: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:11:07.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4771" for this suite.

• [SLOW TEST:9.311 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:131
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":278,"completed":174,"skipped":2928,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:11:07.349: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9489
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1788
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 30 22:11:07.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-9489'
Mar 30 22:11:07.678: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 30 22:11:07.678: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1793
Mar 30 22:11:07.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 delete jobs e2e-test-httpd-job --namespace=kubectl-9489'
Mar 30 22:11:07.803: INFO: stderr: ""
Mar 30 22:11:07.803: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:11:07.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9489" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run job should create a job from an image when restart is OnFailure  [Conformance]","total":278,"completed":175,"skipped":2955,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:11:07.825: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3893
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:11:21.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3893" for this suite.

• [SLOW TEST:13.324 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":278,"completed":176,"skipped":2963,"failed":0}
SSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:11:21.159: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6521
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-9b6bbe4a-25c2-4af4-9d3d-7edf9beb05be in namespace container-probe-6521
Mar 30 22:11:29.377: INFO: Started pod busybox-9b6bbe4a-25c2-4af4-9d3d-7edf9beb05be in namespace container-probe-6521
STEP: checking the pod's current state and verifying that restartCount is present
Mar 30 22:11:29.383: INFO: Initial restart count of pod busybox-9b6bbe4a-25c2-4af4-9d3d-7edf9beb05be is 0
Mar 30 22:12:21.586: INFO: Restart count of pod container-probe-6521/busybox-9b6bbe4a-25c2-4af4-9d3d-7edf9beb05be is now 1 (52.20379528s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:12:21.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6521" for this suite.

• [SLOW TEST:60.471 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":278,"completed":177,"skipped":2966,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:12:21.631: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6096
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 22:12:21.843: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar 30 22:12:21.859: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 30 22:12:26.873: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 30 22:12:30.888: INFO: Creating deployment "test-rolling-update-deployment"
Mar 30 22:12:30.901: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar 30 22:12:30.908: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar 30 22:12:32.926: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar 30 22:12:32.931: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203150, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203150, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203150, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203150, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67cf4f6444\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:12:34.937: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203150, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203150, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203150, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203150, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67cf4f6444\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:12:36.945: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203150, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203150, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203150, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203150, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67cf4f6444\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:12:38.939: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203150, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203150, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203150, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203150, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67cf4f6444\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:12:40.937: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Mar 30 22:12:40.953: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6096 /apis/apps/v1/namespaces/deployment-6096/deployments/test-rolling-update-deployment facf1ebd-c6f2-4b14-b2a8-c56243fc61fd 38826 1 2020-03-30 22:12:30 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003496948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-03-30 22:12:30 +0000 UTC,LastTransitionTime:2020-03-30 22:12:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67cf4f6444" has successfully progressed.,LastUpdateTime:2020-03-30 22:12:39 +0000 UTC,LastTransitionTime:2020-03-30 22:12:30 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 30 22:12:40.958: INFO: New ReplicaSet "test-rolling-update-deployment-67cf4f6444" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67cf4f6444  deployment-6096 /apis/apps/v1/namespaces/deployment-6096/replicasets/test-rolling-update-deployment-67cf4f6444 e10c642d-cf5f-404e-96db-98e7fc85f303 38815 1 2020-03-30 22:12:30 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment facf1ebd-c6f2-4b14-b2a8-c56243fc61fd 0xc003496e17 0xc003496e18}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67cf4f6444,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003496e88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 30 22:12:40.958: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar 30 22:12:40.959: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6096 /apis/apps/v1/namespaces/deployment-6096/replicasets/test-rolling-update-controller f952d8e4-94db-43be-8662-494b8fe086cc 38825 2 2020-03-30 22:12:21 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment facf1ebd-c6f2-4b14-b2a8-c56243fc61fd 0xc003496d47 0xc003496d48}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003496da8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 30 22:12:40.964: INFO: Pod "test-rolling-update-deployment-67cf4f6444-jzlqk" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67cf4f6444-jzlqk test-rolling-update-deployment-67cf4f6444- deployment-6096 /api/v1/namespaces/deployment-6096/pods/test-rolling-update-deployment-67cf4f6444-jzlqk 18cd2c55-9245-4f32-b768-095598b18d6a 38814 0 2020-03-30 22:12:30 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.185.200"
    ],
    "dns": {}
}] kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-67cf4f6444 e10c642d-cf5f-404e-96db-98e7fc85f303 0xc003497307 0xc003497308}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fflgq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fflgq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fflgq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-vvs2j292-eccd-ci-os-12-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 22:12:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 22:12:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 22:12:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 22:12:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.8,PodIP:192.168.185.200,StartTime:2020-03-30 22:12:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-30 22:12:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://c69a59b2109eb180aaadf3b55b11cc969c093e3c43c5d341b42985cc0e83d225,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.185.200,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:12:40.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6096" for this suite.

• [SLOW TEST:19.352 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":278,"completed":178,"skipped":2984,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:12:40.984: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-1362
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar 30 22:12:41.557: INFO: Pod name wrapped-volume-race-e569e37c-ccae-486b-8dc2-e1bcfd059c64: Found 0 pods out of 5
Mar 30 22:12:46.587: INFO: Pod name wrapped-volume-race-e569e37c-ccae-486b-8dc2-e1bcfd059c64: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e569e37c-ccae-486b-8dc2-e1bcfd059c64 in namespace emptydir-wrapper-1362, will wait for the garbage collector to delete the pods
Mar 30 22:13:04.695: INFO: Deleting ReplicationController wrapped-volume-race-e569e37c-ccae-486b-8dc2-e1bcfd059c64 took: 18.372795ms
Mar 30 22:13:05.296: INFO: Terminating ReplicationController wrapped-volume-race-e569e37c-ccae-486b-8dc2-e1bcfd059c64 pods took: 600.363628ms
STEP: Creating RC which spawns configmap-volume pods
Mar 30 22:13:21.628: INFO: Pod name wrapped-volume-race-2ded2faf-9516-4d25-a10c-ad2559038105: Found 0 pods out of 5
Mar 30 22:13:26.642: INFO: Pod name wrapped-volume-race-2ded2faf-9516-4d25-a10c-ad2559038105: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-2ded2faf-9516-4d25-a10c-ad2559038105 in namespace emptydir-wrapper-1362, will wait for the garbage collector to delete the pods
Mar 30 22:13:46.744: INFO: Deleting ReplicationController wrapped-volume-race-2ded2faf-9516-4d25-a10c-ad2559038105 took: 15.62097ms
Mar 30 22:13:47.344: INFO: Terminating ReplicationController wrapped-volume-race-2ded2faf-9516-4d25-a10c-ad2559038105 pods took: 600.580287ms
STEP: Creating RC which spawns configmap-volume pods
Mar 30 22:13:54.391: INFO: Pod name wrapped-volume-race-17559376-679a-442d-8372-b46cac57781f: Found 0 pods out of 5
Mar 30 22:13:59.402: INFO: Pod name wrapped-volume-race-17559376-679a-442d-8372-b46cac57781f: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-17559376-679a-442d-8372-b46cac57781f in namespace emptydir-wrapper-1362, will wait for the garbage collector to delete the pods
Mar 30 22:14:17.508: INFO: Deleting ReplicationController wrapped-volume-race-17559376-679a-442d-8372-b46cac57781f took: 15.10792ms
Mar 30 22:14:18.109: INFO: Terminating ReplicationController wrapped-volume-race-17559376-679a-442d-8372-b46cac57781f pods took: 600.446775ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:14:27.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1362" for this suite.

• [SLOW TEST:106.562 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":278,"completed":179,"skipped":2993,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:14:27.548: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9281
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 30 22:14:28.316: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 30 22:14:30.331: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203268, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203268, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203268, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203268, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:14:32.338: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203268, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203268, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203268, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203268, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:14:34.337: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203268, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203268, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203268, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203268, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:14:36.337: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203268, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203268, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203268, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203268, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 30 22:14:39.385: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:14:39.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9281" for this suite.
STEP: Destroying namespace "webhook-9281-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.982 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":278,"completed":180,"skipped":3008,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:14:39.531: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-79
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 30 22:14:40.149: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 30 22:14:42.166: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203280, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203280, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203280, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203280, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:14:44.181: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203280, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203280, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203280, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203280, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:14:46.175: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203280, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203280, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203280, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203280, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:14:48.171: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203280, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203280, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203280, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721203280, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 30 22:14:51.200: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:15:01.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-79" for this suite.
STEP: Destroying namespace "webhook-79-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:22.263 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":278,"completed":181,"skipped":3015,"failed":0}
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:15:01.795: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2936
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-30f707cc-653c-4e8d-8170-69564f795669
STEP: Creating a pod to test consume secrets
Mar 30 22:15:02.014: INFO: Waiting up to 5m0s for pod "pod-secrets-35d0bbf8-cf19-4f7b-ae73-d25b2ce9f435" in namespace "secrets-2936" to be "success or failure"
Mar 30 22:15:02.018: INFO: Pod "pod-secrets-35d0bbf8-cf19-4f7b-ae73-d25b2ce9f435": Phase="Pending", Reason="", readiness=false. Elapsed: 3.648387ms
Mar 30 22:15:04.026: INFO: Pod "pod-secrets-35d0bbf8-cf19-4f7b-ae73-d25b2ce9f435": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012025944s
Mar 30 22:15:06.032: INFO: Pod "pod-secrets-35d0bbf8-cf19-4f7b-ae73-d25b2ce9f435": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017339478s
Mar 30 22:15:08.037: INFO: Pod "pod-secrets-35d0bbf8-cf19-4f7b-ae73-d25b2ce9f435": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022479344s
Mar 30 22:15:10.044: INFO: Pod "pod-secrets-35d0bbf8-cf19-4f7b-ae73-d25b2ce9f435": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.030086674s
STEP: Saw pod success
Mar 30 22:15:10.045: INFO: Pod "pod-secrets-35d0bbf8-cf19-4f7b-ae73-d25b2ce9f435" satisfied condition "success or failure"
Mar 30 22:15:10.051: INFO: Trying to get logs from node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins pod pod-secrets-35d0bbf8-cf19-4f7b-ae73-d25b2ce9f435 container secret-env-test: <nil>
STEP: delete the pod
Mar 30 22:15:10.084: INFO: Waiting for pod pod-secrets-35d0bbf8-cf19-4f7b-ae73-d25b2ce9f435 to disappear
Mar 30 22:15:10.089: INFO: Pod pod-secrets-35d0bbf8-cf19-4f7b-ae73-d25b2ce9f435 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:15:10.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2936" for this suite.

• [SLOW TEST:8.310 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":278,"completed":182,"skipped":3015,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:15:10.108: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8482
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-92db886c-7678-4b7b-9e11-38bca85114aa
STEP: Creating a pod to test consume configMaps
Mar 30 22:15:10.303: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-62188af2-fada-427b-b804-963bc904c274" in namespace "projected-8482" to be "success or failure"
Mar 30 22:15:10.316: INFO: Pod "pod-projected-configmaps-62188af2-fada-427b-b804-963bc904c274": Phase="Pending", Reason="", readiness=false. Elapsed: 12.698662ms
Mar 30 22:15:12.325: INFO: Pod "pod-projected-configmaps-62188af2-fada-427b-b804-963bc904c274": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021332377s
Mar 30 22:15:14.330: INFO: Pod "pod-projected-configmaps-62188af2-fada-427b-b804-963bc904c274": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026434732s
Mar 30 22:15:16.337: INFO: Pod "pod-projected-configmaps-62188af2-fada-427b-b804-963bc904c274": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033753493s
Mar 30 22:15:18.343: INFO: Pod "pod-projected-configmaps-62188af2-fada-427b-b804-963bc904c274": Phase="Pending", Reason="", readiness=false. Elapsed: 8.039907626s
Mar 30 22:15:20.349: INFO: Pod "pod-projected-configmaps-62188af2-fada-427b-b804-963bc904c274": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.045374815s
STEP: Saw pod success
Mar 30 22:15:20.349: INFO: Pod "pod-projected-configmaps-62188af2-fada-427b-b804-963bc904c274" satisfied condition "success or failure"
Mar 30 22:15:20.353: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-projected-configmaps-62188af2-fada-427b-b804-963bc904c274 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 30 22:15:20.393: INFO: Waiting for pod pod-projected-configmaps-62188af2-fada-427b-b804-963bc904c274 to disappear
Mar 30 22:15:20.397: INFO: Pod pod-projected-configmaps-62188af2-fada-427b-b804-963bc904c274 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:15:20.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8482" for this suite.

• [SLOW TEST:10.320 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":278,"completed":183,"skipped":3034,"failed":0}
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:15:20.430: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1366
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 30 22:15:29.709: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:15:29.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1366" for this suite.

• [SLOW TEST:9.324 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:131
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":278,"completed":184,"skipped":3035,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:15:29.756: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-308
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-8055e410-2141-4ffd-8403-bd55422063ed in namespace container-probe-308
Mar 30 22:15:40.018: INFO: Started pod liveness-8055e410-2141-4ffd-8403-bd55422063ed in namespace container-probe-308
STEP: checking the pod's current state and verifying that restartCount is present
Mar 30 22:15:40.023: INFO: Initial restart count of pod liveness-8055e410-2141-4ffd-8403-bd55422063ed is 0
Mar 30 22:15:56.089: INFO: Restart count of pod container-probe-308/liveness-8055e410-2141-4ffd-8403-bd55422063ed is now 1 (16.065853201s elapsed)
Mar 30 22:16:16.153: INFO: Restart count of pod container-probe-308/liveness-8055e410-2141-4ffd-8403-bd55422063ed is now 2 (36.130093775s elapsed)
Mar 30 22:16:36.241: INFO: Restart count of pod container-probe-308/liveness-8055e410-2141-4ffd-8403-bd55422063ed is now 3 (56.218022507s elapsed)
Mar 30 22:16:56.304: INFO: Restart count of pod container-probe-308/liveness-8055e410-2141-4ffd-8403-bd55422063ed is now 4 (1m16.281681147s elapsed)
Mar 30 22:18:08.546: INFO: Restart count of pod container-probe-308/liveness-8055e410-2141-4ffd-8403-bd55422063ed is now 5 (2m28.522717791s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:18:08.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-308" for this suite.

• [SLOW TEST:158.833 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":278,"completed":185,"skipped":3043,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:18:08.591: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4621
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:330
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Mar 30 22:18:08.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 create -f - --namespace=kubectl-4621'
Mar 30 22:18:09.478: INFO: stderr: ""
Mar 30 22:18:09.478: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 30 22:18:09.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4621'
Mar 30 22:18:09.579: INFO: stderr: ""
Mar 30 22:18:09.579: INFO: stdout: "update-demo-nautilus-nnn68 update-demo-nautilus-srxsl "
Mar 30 22:18:09.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-nautilus-nnn68 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4621'
Mar 30 22:18:09.662: INFO: stderr: ""
Mar 30 22:18:09.662: INFO: stdout: ""
Mar 30 22:18:09.662: INFO: update-demo-nautilus-nnn68 is created but not running
Mar 30 22:18:14.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4621'
Mar 30 22:18:14.779: INFO: stderr: ""
Mar 30 22:18:14.779: INFO: stdout: "update-demo-nautilus-nnn68 update-demo-nautilus-srxsl "
Mar 30 22:18:14.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-nautilus-nnn68 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4621'
Mar 30 22:18:14.868: INFO: stderr: ""
Mar 30 22:18:14.868: INFO: stdout: ""
Mar 30 22:18:14.868: INFO: update-demo-nautilus-nnn68 is created but not running
Mar 30 22:18:19.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4621'
Mar 30 22:18:19.966: INFO: stderr: ""
Mar 30 22:18:19.966: INFO: stdout: "update-demo-nautilus-nnn68 update-demo-nautilus-srxsl "
Mar 30 22:18:19.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-nautilus-nnn68 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4621'
Mar 30 22:18:20.077: INFO: stderr: ""
Mar 30 22:18:20.077: INFO: stdout: "true"
Mar 30 22:18:20.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-nautilus-nnn68 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4621'
Mar 30 22:18:20.164: INFO: stderr: ""
Mar 30 22:18:20.164: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 30 22:18:20.164: INFO: validating pod update-demo-nautilus-nnn68
Mar 30 22:18:20.173: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 30 22:18:20.174: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 30 22:18:20.174: INFO: update-demo-nautilus-nnn68 is verified up and running
Mar 30 22:18:20.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-nautilus-srxsl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4621'
Mar 30 22:18:20.260: INFO: stderr: ""
Mar 30 22:18:20.260: INFO: stdout: "true"
Mar 30 22:18:20.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-nautilus-srxsl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4621'
Mar 30 22:18:20.350: INFO: stderr: ""
Mar 30 22:18:20.350: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 30 22:18:20.350: INFO: validating pod update-demo-nautilus-srxsl
Mar 30 22:18:20.358: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 30 22:18:20.358: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 30 22:18:20.358: INFO: update-demo-nautilus-srxsl is verified up and running
STEP: scaling down the replication controller
Mar 30 22:18:20.362: INFO: scanned /root for discovery docs: <nil>
Mar 30 22:18:20.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-4621'
Mar 30 22:18:21.491: INFO: stderr: ""
Mar 30 22:18:21.491: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 30 22:18:21.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4621'
Mar 30 22:18:21.614: INFO: stderr: ""
Mar 30 22:18:21.614: INFO: stdout: "update-demo-nautilus-nnn68 update-demo-nautilus-srxsl "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar 30 22:18:26.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4621'
Mar 30 22:18:26.751: INFO: stderr: ""
Mar 30 22:18:26.751: INFO: stdout: "update-demo-nautilus-nnn68 "
Mar 30 22:18:26.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-nautilus-nnn68 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4621'
Mar 30 22:18:26.872: INFO: stderr: ""
Mar 30 22:18:26.872: INFO: stdout: "true"
Mar 30 22:18:26.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-nautilus-nnn68 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4621'
Mar 30 22:18:26.963: INFO: stderr: ""
Mar 30 22:18:26.963: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 30 22:18:26.963: INFO: validating pod update-demo-nautilus-nnn68
Mar 30 22:18:26.971: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 30 22:18:26.972: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 30 22:18:26.972: INFO: update-demo-nautilus-nnn68 is verified up and running
STEP: scaling up the replication controller
Mar 30 22:18:26.975: INFO: scanned /root for discovery docs: <nil>
Mar 30 22:18:26.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-4621'
Mar 30 22:18:28.089: INFO: stderr: ""
Mar 30 22:18:28.089: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 30 22:18:28.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4621'
Mar 30 22:18:28.180: INFO: stderr: ""
Mar 30 22:18:28.180: INFO: stdout: "update-demo-nautilus-cg2dg update-demo-nautilus-nnn68 "
Mar 30 22:18:28.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-nautilus-cg2dg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4621'
Mar 30 22:18:28.280: INFO: stderr: ""
Mar 30 22:18:28.280: INFO: stdout: ""
Mar 30 22:18:28.280: INFO: update-demo-nautilus-cg2dg is created but not running
Mar 30 22:18:33.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4621'
Mar 30 22:18:33.376: INFO: stderr: ""
Mar 30 22:18:33.376: INFO: stdout: "update-demo-nautilus-cg2dg update-demo-nautilus-nnn68 "
Mar 30 22:18:33.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-nautilus-cg2dg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4621'
Mar 30 22:18:33.468: INFO: stderr: ""
Mar 30 22:18:33.468: INFO: stdout: ""
Mar 30 22:18:33.468: INFO: update-demo-nautilus-cg2dg is created but not running
Mar 30 22:18:38.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4621'
Mar 30 22:18:38.586: INFO: stderr: ""
Mar 30 22:18:38.586: INFO: stdout: "update-demo-nautilus-cg2dg update-demo-nautilus-nnn68 "
Mar 30 22:18:38.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-nautilus-cg2dg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4621'
Mar 30 22:18:38.665: INFO: stderr: ""
Mar 30 22:18:38.665: INFO: stdout: "true"
Mar 30 22:18:38.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-nautilus-cg2dg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4621'
Mar 30 22:18:38.760: INFO: stderr: ""
Mar 30 22:18:38.760: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 30 22:18:38.760: INFO: validating pod update-demo-nautilus-cg2dg
Mar 30 22:18:38.769: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 30 22:18:38.769: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 30 22:18:38.769: INFO: update-demo-nautilus-cg2dg is verified up and running
Mar 30 22:18:38.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-nautilus-nnn68 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4621'
Mar 30 22:18:38.862: INFO: stderr: ""
Mar 30 22:18:38.862: INFO: stdout: "true"
Mar 30 22:18:38.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-nautilus-nnn68 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4621'
Mar 30 22:18:38.958: INFO: stderr: ""
Mar 30 22:18:38.958: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 30 22:18:38.958: INFO: validating pod update-demo-nautilus-nnn68
Mar 30 22:18:38.966: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 30 22:18:38.966: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 30 22:18:38.966: INFO: update-demo-nautilus-nnn68 is verified up and running
STEP: using delete to clean up resources
Mar 30 22:18:38.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 delete --grace-period=0 --force -f - --namespace=kubectl-4621'
Mar 30 22:18:39.059: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 30 22:18:39.059: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 30 22:18:39.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4621'
Mar 30 22:18:39.149: INFO: stderr: "No resources found in kubectl-4621 namespace.\n"
Mar 30 22:18:39.149: INFO: stdout: ""
Mar 30 22:18:39.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods -l name=update-demo --namespace=kubectl-4621 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 30 22:18:39.243: INFO: stderr: ""
Mar 30 22:18:39.243: INFO: stdout: "update-demo-nautilus-cg2dg\nupdate-demo-nautilus-nnn68\n"
Mar 30 22:18:39.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4621'
Mar 30 22:18:39.847: INFO: stderr: "No resources found in kubectl-4621 namespace.\n"
Mar 30 22:18:39.847: INFO: stdout: ""
Mar 30 22:18:39.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods -l name=update-demo --namespace=kubectl-4621 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 30 22:18:39.960: INFO: stderr: ""
Mar 30 22:18:39.960: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:18:39.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4621" for this suite.

• [SLOW TEST:31.391 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:328
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":278,"completed":186,"skipped":3085,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:18:39.983: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4018
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-29416081-e864-41d9-b483-a8f5a0bf4d3c in namespace container-probe-4018
Mar 30 22:18:48.374: INFO: Started pod liveness-29416081-e864-41d9-b483-a8f5a0bf4d3c in namespace container-probe-4018
STEP: checking the pod's current state and verifying that restartCount is present
Mar 30 22:18:48.378: INFO: Initial restart count of pod liveness-29416081-e864-41d9-b483-a8f5a0bf4d3c is 0
Mar 30 22:19:08.468: INFO: Restart count of pod container-probe-4018/liveness-29416081-e864-41d9-b483-a8f5a0bf4d3c is now 1 (20.089560282s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:19:08.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4018" for this suite.

• [SLOW TEST:28.529 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":278,"completed":187,"skipped":3087,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:19:08.518: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9047
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl logs
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1464
STEP: creating an pod
Mar 30 22:19:08.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.8 --namespace=kubectl-9047 -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar 30 22:19:08.831: INFO: stderr: ""
Mar 30 22:19:08.831: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Waiting for log generator to start.
Mar 30 22:19:08.831: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar 30 22:19:08.831: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9047" to be "running and ready, or succeeded"
Mar 30 22:19:08.839: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.520262ms
Mar 30 22:19:10.846: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014749916s
Mar 30 22:19:12.853: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0224912s
Mar 30 22:19:14.861: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030142107s
Mar 30 22:19:16.867: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 8.035628557s
Mar 30 22:19:16.867: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar 30 22:19:16.867: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Mar 30 22:19:16.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 logs logs-generator logs-generator --namespace=kubectl-9047'
Mar 30 22:19:17.001: INFO: stderr: ""
Mar 30 22:19:17.001: INFO: stdout: "I0330 22:19:15.774364       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/kk4 332\nI0330 22:19:15.974338       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/zlz 523\nI0330 22:19:16.174355       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/zhf 250\nI0330 22:19:16.374326       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/nd72 389\nI0330 22:19:16.574325       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/6jxb 582\nI0330 22:19:16.774344       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/shgx 449\nI0330 22:19:16.974322       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/whh 388\n"
STEP: limiting log lines
Mar 30 22:19:17.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 logs logs-generator logs-generator --namespace=kubectl-9047 --tail=1'
Mar 30 22:19:17.138: INFO: stderr: ""
Mar 30 22:19:17.138: INFO: stdout: "I0330 22:19:16.974322       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/whh 388\n"
Mar 30 22:19:17.138: INFO: got output "I0330 22:19:16.974322       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/whh 388\n"
STEP: limiting log bytes
Mar 30 22:19:17.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 logs logs-generator logs-generator --namespace=kubectl-9047 --limit-bytes=1'
Mar 30 22:19:17.253: INFO: stderr: ""
Mar 30 22:19:17.253: INFO: stdout: "I"
Mar 30 22:19:17.253: INFO: got output "I"
STEP: exposing timestamps
Mar 30 22:19:17.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 logs logs-generator logs-generator --namespace=kubectl-9047 --tail=1 --timestamps'
Mar 30 22:19:17.354: INFO: stderr: ""
Mar 30 22:19:17.354: INFO: stdout: "2020-03-30T22:19:17.174579102Z I0330 22:19:17.174373       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/z5r 588\n"
Mar 30 22:19:17.354: INFO: got output "2020-03-30T22:19:17.174579102Z I0330 22:19:17.174373       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/z5r 588\n"
STEP: restricting to a time range
Mar 30 22:19:19.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 logs logs-generator logs-generator --namespace=kubectl-9047 --since=1s'
Mar 30 22:19:19.959: INFO: stderr: ""
Mar 30 22:19:19.959: INFO: stdout: "I0330 22:19:18.974326       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/4gmm 515\nI0330 22:19:19.174405       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/xlfp 496\nI0330 22:19:19.374342       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/r8n 526\nI0330 22:19:19.574326       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/5vw 295\nI0330 22:19:19.774383       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/5qw 386\n"
Mar 30 22:19:19.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 logs logs-generator logs-generator --namespace=kubectl-9047 --since=24h'
Mar 30 22:19:20.066: INFO: stderr: ""
Mar 30 22:19:20.066: INFO: stdout: "I0330 22:19:15.774364       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/kk4 332\nI0330 22:19:15.974338       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/zlz 523\nI0330 22:19:16.174355       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/zhf 250\nI0330 22:19:16.374326       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/nd72 389\nI0330 22:19:16.574325       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/6jxb 582\nI0330 22:19:16.774344       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/shgx 449\nI0330 22:19:16.974322       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/whh 388\nI0330 22:19:17.174373       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/z5r 588\nI0330 22:19:17.374355       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/gkxc 545\nI0330 22:19:17.574327       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/sgs 453\nI0330 22:19:17.774328       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/mmk2 363\nI0330 22:19:17.974338       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/vsp 475\nI0330 22:19:18.174349       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/v2dl 293\nI0330 22:19:18.374395       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/tdk 396\nI0330 22:19:18.574331       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/5gdl 395\nI0330 22:19:18.774338       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/8ph 560\nI0330 22:19:18.974326       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/4gmm 515\nI0330 22:19:19.174405       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/xlfp 496\nI0330 22:19:19.374342       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/r8n 526\nI0330 22:19:19.574326       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/5vw 295\nI0330 22:19:19.774383       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/5qw 386\nI0330 22:19:19.974324       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/t4t 513\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1470
Mar 30 22:19:20.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 delete pod logs-generator --namespace=kubectl-9047'
Mar 30 22:19:26.744: INFO: stderr: ""
Mar 30 22:19:26.744: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:19:26.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9047" for this suite.

• [SLOW TEST:18.247 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":278,"completed":188,"skipped":3128,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:19:26.766: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8495
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:19:27.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8495" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":278,"completed":189,"skipped":3151,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:19:27.041: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2092
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1632
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 30 22:19:27.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-2092'
Mar 30 22:19:27.349: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 30 22:19:27.349: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Mar 30 22:19:27.377: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-2lllk]
Mar 30 22:19:27.377: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-2lllk" in namespace "kubectl-2092" to be "running and ready"
Mar 30 22:19:27.381: INFO: Pod "e2e-test-httpd-rc-2lllk": Phase="Pending", Reason="", readiness=false. Elapsed: 3.26654ms
Mar 30 22:19:29.387: INFO: Pod "e2e-test-httpd-rc-2lllk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009445688s
Mar 30 22:19:31.393: INFO: Pod "e2e-test-httpd-rc-2lllk": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015115364s
Mar 30 22:19:33.398: INFO: Pod "e2e-test-httpd-rc-2lllk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021052131s
Mar 30 22:19:35.405: INFO: Pod "e2e-test-httpd-rc-2lllk": Phase="Running", Reason="", readiness=true. Elapsed: 8.027160427s
Mar 30 22:19:35.405: INFO: Pod "e2e-test-httpd-rc-2lllk" satisfied condition "running and ready"
Mar 30 22:19:35.405: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-2lllk]
Mar 30 22:19:35.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 logs rc/e2e-test-httpd-rc --namespace=kubectl-2092'
Mar 30 22:19:35.538: INFO: stderr: ""
Mar 30 22:19:35.538: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 192.168.14.100. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 192.168.14.100. Set the 'ServerName' directive globally to suppress this message\n[Mon Mar 30 22:19:34.371967 2020] [mpm_event:notice] [pid 1:tid 139995787975528] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Mon Mar 30 22:19:34.372039 2020] [core:notice] [pid 1:tid 139995787975528] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1637
Mar 30 22:19:35.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 delete rc e2e-test-httpd-rc --namespace=kubectl-2092'
Mar 30 22:19:35.642: INFO: stderr: ""
Mar 30 22:19:35.642: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:19:35.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2092" for this suite.

• [SLOW TEST:8.623 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1628
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run rc should create an rc from an image  [Conformance]","total":278,"completed":190,"skipped":3151,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:19:35.665: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3313
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Mar 30 22:19:35.857: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 30 22:19:35.877: INFO: Waiting for terminating namespaces to be deleted...
Mar 30 22:19:35.890: INFO: 
Logging pods the kubelet thinks is on node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins before test
Mar 30 22:19:35.908: INFO: calico-node-kswgg from kube-system started at 2020-03-30 20:48:23 +0000 UTC (2 container statuses recorded)
Mar 30 22:19:35.908: INFO: 	Container calico-node ready: true, restart count 0
Mar 30 22:19:35.908: INFO: 	Container install-cni ready: true, restart count 0
Mar 30 22:19:35.908: INFO: kube-proxy-746bb from kube-system started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.908: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 30 22:19:35.908: INFO: csi-cinder-nodeplugin-ftdf2 from kube-system started at 2020-03-30 20:48:53 +0000 UTC (2 container statuses recorded)
Mar 30 22:19:35.908: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 30 22:19:35.908: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 30 22:19:35.908: INFO: eric-lm-combined-server-license-consumer-handler-58597bc9679zxp from kube-system started at 2020-03-30 20:54:37 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.908: INFO: 	Container eric-lm-license-consumer-handler ready: true, restart count 0
Mar 30 22:19:35.908: INFO: ccd-license-consumer-5c7ff9fc96-b2hn2 from kube-system started at 2020-03-30 20:55:33 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.908: INFO: 	Container ccd-license-consumer ready: true, restart count 0
Mar 30 22:19:35.908: INFO: sonobuoy-systemd-logs-daemon-set-4495c60740044637-62zgj from sonobuoy started at 2020-03-30 21:04:37 +0000 UTC (2 container statuses recorded)
Mar 30 22:19:35.908: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 30 22:19:35.908: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 30 22:19:35.908: INFO: kube-multus-ds-amd64-vm7r6 from kube-system started at 2020-03-30 20:48:53 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.908: INFO: 	Container kube-multus ready: true, restart count 0
Mar 30 22:19:35.908: INFO: eric-pm-server-eric-pm-server-7b4dd54bc5-m7fjg from monitoring started at 2020-03-30 20:51:51 +0000 UTC (2 container statuses recorded)
Mar 30 22:19:35.908: INFO: 	Container eric-pm-server-eric-pm-server ready: true, restart count 0
Mar 30 22:19:35.908: INFO: 	Container eric-pm-server-eric-pm-server-eric-pm-configmap-reload ready: true, restart count 0
Mar 30 22:19:35.908: INFO: eric-pm-server-node-exporter-m4kjc from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.908: INFO: 	Container eric-pm-server-node-exporter ready: true, restart count 0
Mar 30 22:19:35.908: INFO: sonobuoy from sonobuoy started at 2020-03-30 21:04:25 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.908: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 30 22:19:35.908: INFO: eric-tm-external-connectivity-frontend-speaker-vmndg from kube-system started at 2020-03-30 20:50:36 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.909: INFO: 	Container speaker ready: true, restart count 0
Mar 30 22:19:35.909: INFO: eric-pm-server-pushgateway-7798d479ff-fcnk9 from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.909: INFO: 	Container eric-pm-server-pushgateway ready: true, restart count 0
Mar 30 22:19:35.909: INFO: 
Logging pods the kubelet thinks is on node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins before test
Mar 30 22:19:35.920: INFO: kube-multus-ds-amd64-q9869 from kube-system started at 2020-03-30 20:48:57 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.920: INFO: 	Container kube-multus ready: true, restart count 0
Mar 30 22:19:35.921: INFO: eric-pm-server-alertmanager-648979cdd-z57mc from monitoring started at 2020-03-30 20:51:51 +0000 UTC (2 container statuses recorded)
Mar 30 22:19:35.921: INFO: 	Container eric-pm-server-alertmanager ready: true, restart count 0
Mar 30 22:19:35.921: INFO: 	Container eric-pm-server-alertmanager-configmap-reload-for-alertmanager ready: true, restart count 0
Mar 30 22:19:35.921: INFO: sonobuoy-systemd-logs-daemon-set-4495c60740044637-gqxkr from sonobuoy started at 2020-03-30 21:04:37 +0000 UTC (2 container statuses recorded)
Mar 30 22:19:35.921: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 30 22:19:35.921: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 30 22:19:35.921: INFO: calico-node-lrw4b from kube-system started at 2020-03-30 20:48:27 +0000 UTC (2 container statuses recorded)
Mar 30 22:19:35.921: INFO: 	Container calico-node ready: true, restart count 0
Mar 30 22:19:35.921: INFO: 	Container install-cni ready: true, restart count 0
Mar 30 22:19:35.921: INFO: csi-cinder-nodeplugin-wnrkx from kube-system started at 2020-03-30 20:48:57 +0000 UTC (2 container statuses recorded)
Mar 30 22:19:35.921: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 30 22:19:35.921: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 30 22:19:35.922: INFO: eric-pm-server-node-exporter-v5bmh from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.922: INFO: 	Container eric-pm-server-node-exporter ready: true, restart count 0
Mar 30 22:19:35.922: INFO: kube-proxy-9w8kt from kube-system started at 2020-03-30 20:48:27 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.922: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 30 22:19:35.922: INFO: eric-tm-external-connectivity-frontend-controller-78f9fd87bvc6l from kube-system started at 2020-03-30 20:50:36 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.922: INFO: 	Container controller ready: true, restart count 0
Mar 30 22:19:35.922: INFO: eric-tm-external-connectivity-frontend-speaker-m4f4p from kube-system started at 2020-03-30 20:50:36 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.922: INFO: 	Container speaker ready: true, restart count 0
Mar 30 22:19:35.922: INFO: metrics-server-8666db6c57-znld5 from kube-system started at 2020-03-30 20:53:13 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.922: INFO: 	Container metrics-server ready: true, restart count 0
Mar 30 22:19:35.922: INFO: eric-lm-combined-server-license-server-client-945b6bc4c-5phm5 from kube-system started at 2020-03-30 20:54:37 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.922: INFO: 	Container eric-lm-license-server-client ready: true, restart count 0
Mar 30 22:19:35.923: INFO: e2e-test-httpd-rc-2lllk from kubectl-2092 started at 2020-03-30 22:19:27 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.923: INFO: 	Container e2e-test-httpd-rc ready: true, restart count 0
Mar 30 22:19:35.923: INFO: 
Logging pods the kubelet thinks is on node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins before test
Mar 30 22:19:35.938: INFO: calico-node-hf278 from kube-system started at 2020-03-30 20:48:23 +0000 UTC (2 container statuses recorded)
Mar 30 22:19:35.938: INFO: 	Container calico-node ready: true, restart count 0
Mar 30 22:19:35.938: INFO: 	Container install-cni ready: true, restart count 0
Mar 30 22:19:35.938: INFO: eric-pm-server-node-exporter-9cwgm from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.938: INFO: 	Container eric-pm-server-node-exporter ready: true, restart count 0
Mar 30 22:19:35.938: INFO: eric-pm-server-kube-state-metrics-66f7fbfd44-k7krp from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.938: INFO: 	Container eric-pm-server-kube-state-metrics ready: true, restart count 0
Mar 30 22:19:35.938: INFO: eric-tm-external-connectivity-frontend-speaker-8tmb7 from kube-system started at 2020-03-30 20:50:36 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.938: INFO: 	Container speaker ready: true, restart count 0
Mar 30 22:19:35.938: INFO: eric-lcm-container-registry-registry-0 from kube-system started at 2020-03-30 20:49:39 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.938: INFO: 	Container registry ready: true, restart count 0
Mar 30 22:19:35.938: INFO: postgresql-postgresql-0 from kube-system started at 2020-03-30 20:53:52 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.938: INFO: 	Container postgresql ready: true, restart count 0
Mar 30 22:19:35.938: INFO: sonobuoy-systemd-logs-daemon-set-4495c60740044637-5rfld from sonobuoy started at 2020-03-30 21:04:37 +0000 UTC (2 container statuses recorded)
Mar 30 22:19:35.938: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 30 22:19:35.938: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 30 22:19:35.938: INFO: kube-proxy-4mzsc from kube-system started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.938: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 30 22:19:35.938: INFO: kube-multus-ds-amd64-9mjsk from kube-system started at 2020-03-30 20:48:53 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.938: INFO: 	Container kube-multus ready: true, restart count 0
Mar 30 22:19:35.938: INFO: csi-cinder-nodeplugin-cfp5q from kube-system started at 2020-03-30 20:48:53 +0000 UTC (2 container statuses recorded)
Mar 30 22:19:35.938: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 30 22:19:35.938: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 30 22:19:35.938: INFO: 
Logging pods the kubelet thinks is on node worker-pool1-zo88v95j-eccd-ci-os-12-jenkins before test
Mar 30 22:19:35.954: INFO: default-http-backend-6664c884c9-dxczc from ingress-nginx started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.954: INFO: 	Container default-http-backend ready: true, restart count 0
Mar 30 22:19:35.954: INFO: sonobuoy-systemd-logs-daemon-set-4495c60740044637-v7t76 from sonobuoy started at 2020-03-30 21:04:37 +0000 UTC (2 container statuses recorded)
Mar 30 22:19:35.954: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 30 22:19:35.954: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 30 22:19:35.954: INFO: calico-node-xk89l from kube-system started at 2020-03-30 20:47:51 +0000 UTC (2 container statuses recorded)
Mar 30 22:19:35.954: INFO: 	Container calico-node ready: true, restart count 0
Mar 30 22:19:35.954: INFO: 	Container install-cni ready: true, restart count 0
Mar 30 22:19:35.954: INFO: csi-cinder-nodeplugin-qgrf4 from kube-system started at 2020-03-30 20:48:21 +0000 UTC (2 container statuses recorded)
Mar 30 22:19:35.954: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 30 22:19:35.954: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 30 22:19:35.954: INFO: nginx-ingress-controller-78f766b979-bfx4q from ingress-nginx started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.954: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 30 22:19:35.954: INFO: kube-proxy-pcd5r from kube-system started at 2020-03-30 20:47:51 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.954: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 30 22:19:35.954: INFO: nginx-ingress-controller-78f766b979-wf7qb from ingress-nginx started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.954: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 30 22:19:35.954: INFO: eric-tm-external-connectivity-frontend-speaker-zwnfr from kube-system started at 2020-03-30 20:50:36 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.954: INFO: 	Container speaker ready: true, restart count 0
Mar 30 22:19:35.954: INFO: tiller-deploy-cd77547bd-2pv7h from kube-system started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.954: INFO: 	Container tiller ready: true, restart count 0
Mar 30 22:19:35.954: INFO: kube-multus-ds-amd64-6t2pw from kube-system started at 2020-03-30 20:48:21 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.954: INFO: 	Container kube-multus ready: true, restart count 0
Mar 30 22:19:35.954: INFO: eric-pm-server-node-exporter-ktbpn from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 22:19:35.954: INFO: 	Container eric-pm-server-node-exporter ready: true, restart count 0
Mar 30 22:19:35.954: INFO: sonobuoy-e2e-job-14477a96f0f5465c from sonobuoy started at 2020-03-30 21:04:37 +0000 UTC (2 container statuses recorded)
Mar 30 22:19:35.954: INFO: 	Container e2e ready: true, restart count 0
Mar 30 22:19:35.954: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.160134f63e2214ca], Reason = [FailedScheduling], Message = [0/7 nodes are available: 7 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:19:37.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3313" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":278,"completed":191,"skipped":3158,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:19:37.041: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7409
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Mar 30 22:19:37.227: INFO: Created pod &Pod{ObjectMeta:{dns-7409  dns-7409 /api/v1/namespaces/dns-7409/pods/dns-7409 9edae6ff-4726-41c1-8f06-80865cedfd78 42205 0 2020-03-30 22:19:37 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:ccd-privileged] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xr8hs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xr8hs,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xr8hs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
STEP: Verifying customized DNS suffix list is configured on pod...
Mar 30 22:19:47.242: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7409 PodName:dns-7409 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 30 22:19:47.243: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Verifying customized DNS server is configured on pod...
Mar 30 22:19:47.420: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7409 PodName:dns-7409 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 30 22:19:47.420: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 22:19:47.640: INFO: Deleting pod dns-7409...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:19:47.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7409" for this suite.

• [SLOW TEST:10.641 seconds]
[sig-network] DNS
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":278,"completed":192,"skipped":3163,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:19:47.687: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1971
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1971
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1971
STEP: creating replication controller externalsvc in namespace services-1971
I0330 22:19:47.994934      20 runners.go:189] Created replication controller with name: externalsvc, namespace: services-1971, replica count: 2
I0330 22:19:51.046521      20 runners.go:189] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0330 22:19:54.047110      20 runners.go:189] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0330 22:19:57.047451      20 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Mar 30 22:19:57.086: INFO: Creating new exec pod
Mar 30 22:20:05.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=services-1971 execpodphxgr -- /bin/sh -x -c nslookup clusterip-service'
Mar 30 22:20:05.439: INFO: stderr: "+ nslookup clusterip-service\n"
Mar 30 22:20:05.440: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-1971.svc.cluster.local\tcanonical name = externalsvc.services-1971.svc.cluster.local.\nName:\texternalsvc.services-1971.svc.cluster.local\nAddress: 10.98.202.95\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1971, will wait for the garbage collector to delete the pods
Mar 30 22:20:05.508: INFO: Deleting ReplicationController externalsvc took: 12.393896ms
Mar 30 22:20:06.108: INFO: Terminating ReplicationController externalsvc pods took: 600.354374ms
Mar 30 22:20:16.873: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:20:16.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1971" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:29.246 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":278,"completed":193,"skipped":3202,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:20:16.934: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7620
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar 30 22:20:17.128: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7620 /api/v1/namespaces/watch-7620/configmaps/e2e-watch-test-configmap-a 5993a017-804e-48e8-ac29-b6da8427ad6b 42524 0 2020-03-30 22:20:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 30 22:20:17.129: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7620 /api/v1/namespaces/watch-7620/configmaps/e2e-watch-test-configmap-a 5993a017-804e-48e8-ac29-b6da8427ad6b 42524 0 2020-03-30 22:20:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar 30 22:20:27.143: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7620 /api/v1/namespaces/watch-7620/configmaps/e2e-watch-test-configmap-a 5993a017-804e-48e8-ac29-b6da8427ad6b 42596 0 2020-03-30 22:20:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar 30 22:20:27.143: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7620 /api/v1/namespaces/watch-7620/configmaps/e2e-watch-test-configmap-a 5993a017-804e-48e8-ac29-b6da8427ad6b 42596 0 2020-03-30 22:20:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar 30 22:20:37.169: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7620 /api/v1/namespaces/watch-7620/configmaps/e2e-watch-test-configmap-a 5993a017-804e-48e8-ac29-b6da8427ad6b 42638 0 2020-03-30 22:20:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 30 22:20:37.172: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7620 /api/v1/namespaces/watch-7620/configmaps/e2e-watch-test-configmap-a 5993a017-804e-48e8-ac29-b6da8427ad6b 42638 0 2020-03-30 22:20:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar 30 22:20:47.194: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7620 /api/v1/namespaces/watch-7620/configmaps/e2e-watch-test-configmap-a 5993a017-804e-48e8-ac29-b6da8427ad6b 42682 0 2020-03-30 22:20:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 30 22:20:47.194: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7620 /api/v1/namespaces/watch-7620/configmaps/e2e-watch-test-configmap-a 5993a017-804e-48e8-ac29-b6da8427ad6b 42682 0 2020-03-30 22:20:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar 30 22:20:57.213: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7620 /api/v1/namespaces/watch-7620/configmaps/e2e-watch-test-configmap-b 3dc88bed-1fd6-41ff-a34b-6129716d280a 42723 0 2020-03-30 22:20:57 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 30 22:20:57.213: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7620 /api/v1/namespaces/watch-7620/configmaps/e2e-watch-test-configmap-b 3dc88bed-1fd6-41ff-a34b-6129716d280a 42723 0 2020-03-30 22:20:57 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar 30 22:21:07.228: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7620 /api/v1/namespaces/watch-7620/configmaps/e2e-watch-test-configmap-b 3dc88bed-1fd6-41ff-a34b-6129716d280a 42765 0 2020-03-30 22:20:57 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 30 22:21:07.228: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7620 /api/v1/namespaces/watch-7620/configmaps/e2e-watch-test-configmap-b 3dc88bed-1fd6-41ff-a34b-6129716d280a 42765 0 2020-03-30 22:20:57 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:21:17.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7620" for this suite.

• [SLOW TEST:60.321 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":278,"completed":194,"skipped":3219,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:21:17.258: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-886
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:330
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Mar 30 22:21:17.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 create -f - --namespace=kubectl-886'
Mar 30 22:21:17.837: INFO: stderr: ""
Mar 30 22:21:17.837: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 30 22:21:17.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-886'
Mar 30 22:21:17.945: INFO: stderr: ""
Mar 30 22:21:17.945: INFO: stdout: "update-demo-nautilus-fv9lp update-demo-nautilus-xhrt7 "
Mar 30 22:21:17.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-nautilus-fv9lp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-886'
Mar 30 22:21:18.031: INFO: stderr: ""
Mar 30 22:21:18.031: INFO: stdout: ""
Mar 30 22:21:18.031: INFO: update-demo-nautilus-fv9lp is created but not running
Mar 30 22:21:23.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-886'
Mar 30 22:21:23.147: INFO: stderr: ""
Mar 30 22:21:23.147: INFO: stdout: "update-demo-nautilus-fv9lp update-demo-nautilus-xhrt7 "
Mar 30 22:21:23.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-nautilus-fv9lp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-886'
Mar 30 22:21:23.247: INFO: stderr: ""
Mar 30 22:21:23.247: INFO: stdout: ""
Mar 30 22:21:23.247: INFO: update-demo-nautilus-fv9lp is created but not running
Mar 30 22:21:28.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-886'
Mar 30 22:21:28.346: INFO: stderr: ""
Mar 30 22:21:28.346: INFO: stdout: "update-demo-nautilus-fv9lp update-demo-nautilus-xhrt7 "
Mar 30 22:21:28.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-nautilus-fv9lp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-886'
Mar 30 22:21:28.427: INFO: stderr: ""
Mar 30 22:21:28.427: INFO: stdout: "true"
Mar 30 22:21:28.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-nautilus-fv9lp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-886'
Mar 30 22:21:28.512: INFO: stderr: ""
Mar 30 22:21:28.512: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 30 22:21:28.512: INFO: validating pod update-demo-nautilus-fv9lp
Mar 30 22:21:28.522: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 30 22:21:28.522: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 30 22:21:28.522: INFO: update-demo-nautilus-fv9lp is verified up and running
Mar 30 22:21:28.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-nautilus-xhrt7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-886'
Mar 30 22:21:28.604: INFO: stderr: ""
Mar 30 22:21:28.604: INFO: stdout: "true"
Mar 30 22:21:28.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods update-demo-nautilus-xhrt7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-886'
Mar 30 22:21:28.681: INFO: stderr: ""
Mar 30 22:21:28.681: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 30 22:21:28.681: INFO: validating pod update-demo-nautilus-xhrt7
Mar 30 22:21:28.690: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 30 22:21:28.690: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 30 22:21:28.690: INFO: update-demo-nautilus-xhrt7 is verified up and running
STEP: using delete to clean up resources
Mar 30 22:21:28.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 delete --grace-period=0 --force -f - --namespace=kubectl-886'
Mar 30 22:21:28.789: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 30 22:21:28.790: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 30 22:21:28.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-886'
Mar 30 22:21:28.885: INFO: stderr: "No resources found in kubectl-886 namespace.\n"
Mar 30 22:21:28.885: INFO: stdout: ""
Mar 30 22:21:28.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods -l name=update-demo --namespace=kubectl-886 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 30 22:21:28.973: INFO: stderr: ""
Mar 30 22:21:28.973: INFO: stdout: "update-demo-nautilus-fv9lp\nupdate-demo-nautilus-xhrt7\n"
Mar 30 22:21:29.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-886'
Mar 30 22:21:29.574: INFO: stderr: "No resources found in kubectl-886 namespace.\n"
Mar 30 22:21:29.574: INFO: stdout: ""
Mar 30 22:21:29.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 get pods -l name=update-demo --namespace=kubectl-886 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 30 22:21:29.660: INFO: stderr: ""
Mar 30 22:21:29.660: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:21:29.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-886" for this suite.

• [SLOW TEST:12.436 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:328
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":278,"completed":195,"skipped":3286,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:21:29.695: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3070
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating replication controller my-hostname-basic-7cc6ec2b-16a2-4a48-a859-842d3cfef887
Mar 30 22:21:29.916: INFO: Pod name my-hostname-basic-7cc6ec2b-16a2-4a48-a859-842d3cfef887: Found 0 pods out of 1
Mar 30 22:21:34.921: INFO: Pod name my-hostname-basic-7cc6ec2b-16a2-4a48-a859-842d3cfef887: Found 1 pods out of 1
Mar 30 22:21:34.922: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-7cc6ec2b-16a2-4a48-a859-842d3cfef887" are running
Mar 30 22:21:38.932: INFO: Pod "my-hostname-basic-7cc6ec2b-16a2-4a48-a859-842d3cfef887-7pmf7" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-30 22:21:29 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-30 22:21:29 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-7cc6ec2b-16a2-4a48-a859-842d3cfef887]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-30 22:21:29 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-7cc6ec2b-16a2-4a48-a859-842d3cfef887]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-30 22:21:29 +0000 UTC Reason: Message:}])
Mar 30 22:21:38.933: INFO: Trying to dial the pod
Mar 30 22:21:43.953: INFO: Controller my-hostname-basic-7cc6ec2b-16a2-4a48-a859-842d3cfef887: Got expected result from replica 1 [my-hostname-basic-7cc6ec2b-16a2-4a48-a859-842d3cfef887-7pmf7]: "my-hostname-basic-7cc6ec2b-16a2-4a48-a859-842d3cfef887-7pmf7", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:21:43.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3070" for this suite.

• [SLOW TEST:14.273 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":278,"completed":196,"skipped":3288,"failed":0}
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:21:43.970: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-3741
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 22:21:44.228: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-cffe0ad9-999e-4ff4-9a4e-7453a9a2fb8b" in namespace "security-context-test-3741" to be "success or failure"
Mar 30 22:21:44.236: INFO: Pod "busybox-privileged-false-cffe0ad9-999e-4ff4-9a4e-7453a9a2fb8b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.114156ms
Mar 30 22:21:46.242: INFO: Pod "busybox-privileged-false-cffe0ad9-999e-4ff4-9a4e-7453a9a2fb8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014081166s
Mar 30 22:21:48.247: INFO: Pod "busybox-privileged-false-cffe0ad9-999e-4ff4-9a4e-7453a9a2fb8b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019460215s
Mar 30 22:21:50.252: INFO: Pod "busybox-privileged-false-cffe0ad9-999e-4ff4-9a4e-7453a9a2fb8b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024529156s
Mar 30 22:21:52.263: INFO: Pod "busybox-privileged-false-cffe0ad9-999e-4ff4-9a4e-7453a9a2fb8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.035005762s
Mar 30 22:21:52.263: INFO: Pod "busybox-privileged-false-cffe0ad9-999e-4ff4-9a4e-7453a9a2fb8b" satisfied condition "success or failure"
Mar 30 22:21:52.281: INFO: Got logs for pod "busybox-privileged-false-cffe0ad9-999e-4ff4-9a4e-7453a9a2fb8b": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:21:52.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3741" for this suite.

• [SLOW TEST:8.329 seconds]
[k8s.io] Security Context
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  When creating a pod with privileged
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:225
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":197,"skipped":3288,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:21:52.309: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7840
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:22:03.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7840" for this suite.

• [SLOW TEST:11.282 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":278,"completed":198,"skipped":3294,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:22:03.594: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2800
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Mar 30 22:22:03.770: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:22:13.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2800" for this suite.

• [SLOW TEST:9.799 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":278,"completed":199,"skipped":3315,"failed":0}
SSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:22:13.393: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-744
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar 30 22:22:25.727: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-744 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 30 22:22:25.728: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 22:22:25.896: INFO: Exec stderr: ""
Mar 30 22:22:25.897: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-744 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 30 22:22:25.897: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 22:22:26.102: INFO: Exec stderr: ""
Mar 30 22:22:26.102: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-744 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 30 22:22:26.102: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 22:22:26.261: INFO: Exec stderr: ""
Mar 30 22:22:26.262: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-744 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 30 22:22:26.262: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 22:22:26.449: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar 30 22:22:26.449: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-744 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 30 22:22:26.449: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 22:22:26.677: INFO: Exec stderr: ""
Mar 30 22:22:26.677: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-744 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 30 22:22:26.677: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 22:22:26.961: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar 30 22:22:26.961: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-744 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 30 22:22:26.961: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 22:22:27.136: INFO: Exec stderr: ""
Mar 30 22:22:27.136: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-744 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 30 22:22:27.136: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 22:22:27.340: INFO: Exec stderr: ""
Mar 30 22:22:27.340: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-744 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 30 22:22:27.340: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 22:22:27.508: INFO: Exec stderr: ""
Mar 30 22:22:27.508: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-744 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 30 22:22:27.508: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 22:22:27.784: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:22:27.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-744" for this suite.

• [SLOW TEST:14.411 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":200,"skipped":3321,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:22:27.806: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-43
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Mar 30 22:22:27.983: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 30 22:22:28.003: INFO: Waiting for terminating namespaces to be deleted...
Mar 30 22:22:28.008: INFO: 
Logging pods the kubelet thinks is on node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins before test
Mar 30 22:22:28.020: INFO: eric-pm-server-node-exporter-m4kjc from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.020: INFO: 	Container eric-pm-server-node-exporter ready: true, restart count 0
Mar 30 22:22:28.020: INFO: sonobuoy from sonobuoy started at 2020-03-30 21:04:25 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.020: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 30 22:22:28.020: INFO: sonobuoy-systemd-logs-daemon-set-4495c60740044637-62zgj from sonobuoy started at 2020-03-30 21:04:37 +0000 UTC (2 container statuses recorded)
Mar 30 22:22:28.020: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 30 22:22:28.020: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 30 22:22:28.020: INFO: kube-multus-ds-amd64-vm7r6 from kube-system started at 2020-03-30 20:48:53 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.020: INFO: 	Container kube-multus ready: true, restart count 0
Mar 30 22:22:28.021: INFO: eric-pm-server-eric-pm-server-7b4dd54bc5-m7fjg from monitoring started at 2020-03-30 20:51:51 +0000 UTC (2 container statuses recorded)
Mar 30 22:22:28.021: INFO: 	Container eric-pm-server-eric-pm-server ready: true, restart count 0
Mar 30 22:22:28.021: INFO: 	Container eric-pm-server-eric-pm-server-eric-pm-configmap-reload ready: true, restart count 0
Mar 30 22:22:28.021: INFO: eric-tm-external-connectivity-frontend-speaker-vmndg from kube-system started at 2020-03-30 20:50:36 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.021: INFO: 	Container speaker ready: true, restart count 0
Mar 30 22:22:28.021: INFO: eric-pm-server-pushgateway-7798d479ff-fcnk9 from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.021: INFO: 	Container eric-pm-server-pushgateway ready: true, restart count 0
Mar 30 22:22:28.021: INFO: csi-cinder-nodeplugin-ftdf2 from kube-system started at 2020-03-30 20:48:53 +0000 UTC (2 container statuses recorded)
Mar 30 22:22:28.021: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 30 22:22:28.021: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 30 22:22:28.021: INFO: calico-node-kswgg from kube-system started at 2020-03-30 20:48:23 +0000 UTC (2 container statuses recorded)
Mar 30 22:22:28.021: INFO: 	Container calico-node ready: true, restart count 0
Mar 30 22:22:28.021: INFO: 	Container install-cni ready: true, restart count 0
Mar 30 22:22:28.021: INFO: kube-proxy-746bb from kube-system started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.021: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 30 22:22:28.021: INFO: eric-lm-combined-server-license-consumer-handler-58597bc9679zxp from kube-system started at 2020-03-30 20:54:37 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.021: INFO: 	Container eric-lm-license-consumer-handler ready: true, restart count 0
Mar 30 22:22:28.021: INFO: ccd-license-consumer-5c7ff9fc96-b2hn2 from kube-system started at 2020-03-30 20:55:33 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.021: INFO: 	Container ccd-license-consumer ready: true, restart count 0
Mar 30 22:22:28.021: INFO: 
Logging pods the kubelet thinks is on node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins before test
Mar 30 22:22:28.037: INFO: kube-proxy-9w8kt from kube-system started at 2020-03-30 20:48:27 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.037: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 30 22:22:28.037: INFO: metrics-server-8666db6c57-znld5 from kube-system started at 2020-03-30 20:53:13 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.038: INFO: 	Container metrics-server ready: true, restart count 0
Mar 30 22:22:28.038: INFO: eric-lm-combined-server-license-server-client-945b6bc4c-5phm5 from kube-system started at 2020-03-30 20:54:37 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.038: INFO: 	Container eric-lm-license-server-client ready: true, restart count 0
Mar 30 22:22:28.038: INFO: eric-tm-external-connectivity-frontend-controller-78f9fd87bvc6l from kube-system started at 2020-03-30 20:50:36 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.038: INFO: 	Container controller ready: true, restart count 0
Mar 30 22:22:28.038: INFO: eric-tm-external-connectivity-frontend-speaker-m4f4p from kube-system started at 2020-03-30 20:50:36 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.038: INFO: 	Container speaker ready: true, restart count 0
Mar 30 22:22:28.038: INFO: sonobuoy-systemd-logs-daemon-set-4495c60740044637-gqxkr from sonobuoy started at 2020-03-30 21:04:37 +0000 UTC (2 container statuses recorded)
Mar 30 22:22:28.038: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 30 22:22:28.038: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 30 22:22:28.038: INFO: kube-multus-ds-amd64-q9869 from kube-system started at 2020-03-30 20:48:57 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.038: INFO: 	Container kube-multus ready: true, restart count 0
Mar 30 22:22:28.038: INFO: eric-pm-server-alertmanager-648979cdd-z57mc from monitoring started at 2020-03-30 20:51:51 +0000 UTC (2 container statuses recorded)
Mar 30 22:22:28.038: INFO: 	Container eric-pm-server-alertmanager ready: true, restart count 0
Mar 30 22:22:28.038: INFO: 	Container eric-pm-server-alertmanager-configmap-reload-for-alertmanager ready: true, restart count 0
Mar 30 22:22:28.038: INFO: eric-pm-server-node-exporter-v5bmh from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.038: INFO: 	Container eric-pm-server-node-exporter ready: true, restart count 0
Mar 30 22:22:28.038: INFO: test-pod from e2e-kubelet-etc-hosts-744 started at 2020-03-30 22:22:13 +0000 UTC (3 container statuses recorded)
Mar 30 22:22:28.038: INFO: 	Container busybox-1 ready: true, restart count 0
Mar 30 22:22:28.038: INFO: 	Container busybox-2 ready: true, restart count 0
Mar 30 22:22:28.038: INFO: 	Container busybox-3 ready: true, restart count 0
Mar 30 22:22:28.038: INFO: calico-node-lrw4b from kube-system started at 2020-03-30 20:48:27 +0000 UTC (2 container statuses recorded)
Mar 30 22:22:28.038: INFO: 	Container calico-node ready: true, restart count 0
Mar 30 22:22:28.038: INFO: 	Container install-cni ready: true, restart count 0
Mar 30 22:22:28.038: INFO: csi-cinder-nodeplugin-wnrkx from kube-system started at 2020-03-30 20:48:57 +0000 UTC (2 container statuses recorded)
Mar 30 22:22:28.038: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 30 22:22:28.038: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 30 22:22:28.038: INFO: 
Logging pods the kubelet thinks is on node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins before test
Mar 30 22:22:28.052: INFO: eric-pm-server-node-exporter-9cwgm from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.052: INFO: 	Container eric-pm-server-node-exporter ready: true, restart count 0
Mar 30 22:22:28.052: INFO: eric-pm-server-kube-state-metrics-66f7fbfd44-k7krp from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.052: INFO: 	Container eric-pm-server-kube-state-metrics ready: true, restart count 0
Mar 30 22:22:28.052: INFO: calico-node-hf278 from kube-system started at 2020-03-30 20:48:23 +0000 UTC (2 container statuses recorded)
Mar 30 22:22:28.052: INFO: 	Container calico-node ready: true, restart count 0
Mar 30 22:22:28.052: INFO: 	Container install-cni ready: true, restart count 0
Mar 30 22:22:28.052: INFO: test-host-network-pod from e2e-kubelet-etc-hosts-744 started at 2020-03-30 22:22:23 +0000 UTC (2 container statuses recorded)
Mar 30 22:22:28.052: INFO: 	Container busybox-1 ready: true, restart count 0
Mar 30 22:22:28.052: INFO: 	Container busybox-2 ready: true, restart count 0
Mar 30 22:22:28.052: INFO: eric-tm-external-connectivity-frontend-speaker-8tmb7 from kube-system started at 2020-03-30 20:50:36 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.052: INFO: 	Container speaker ready: true, restart count 0
Mar 30 22:22:28.052: INFO: kube-multus-ds-amd64-9mjsk from kube-system started at 2020-03-30 20:48:53 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.052: INFO: 	Container kube-multus ready: true, restart count 0
Mar 30 22:22:28.053: INFO: csi-cinder-nodeplugin-cfp5q from kube-system started at 2020-03-30 20:48:53 +0000 UTC (2 container statuses recorded)
Mar 30 22:22:28.053: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 30 22:22:28.053: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 30 22:22:28.053: INFO: eric-lcm-container-registry-registry-0 from kube-system started at 2020-03-30 20:49:39 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.053: INFO: 	Container registry ready: true, restart count 0
Mar 30 22:22:28.053: INFO: postgresql-postgresql-0 from kube-system started at 2020-03-30 20:53:52 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.053: INFO: 	Container postgresql ready: true, restart count 0
Mar 30 22:22:28.053: INFO: sonobuoy-systemd-logs-daemon-set-4495c60740044637-5rfld from sonobuoy started at 2020-03-30 21:04:37 +0000 UTC (2 container statuses recorded)
Mar 30 22:22:28.053: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 30 22:22:28.053: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 30 22:22:28.053: INFO: kube-proxy-4mzsc from kube-system started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.053: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 30 22:22:28.053: INFO: 
Logging pods the kubelet thinks is on node worker-pool1-zo88v95j-eccd-ci-os-12-jenkins before test
Mar 30 22:22:28.066: INFO: tiller-deploy-cd77547bd-2pv7h from kube-system started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.066: INFO: 	Container tiller ready: true, restart count 0
Mar 30 22:22:28.066: INFO: kube-multus-ds-amd64-6t2pw from kube-system started at 2020-03-30 20:48:21 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.066: INFO: 	Container kube-multus ready: true, restart count 0
Mar 30 22:22:28.066: INFO: eric-pm-server-node-exporter-ktbpn from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.066: INFO: 	Container eric-pm-server-node-exporter ready: true, restart count 0
Mar 30 22:22:28.066: INFO: sonobuoy-e2e-job-14477a96f0f5465c from sonobuoy started at 2020-03-30 21:04:37 +0000 UTC (2 container statuses recorded)
Mar 30 22:22:28.066: INFO: 	Container e2e ready: true, restart count 0
Mar 30 22:22:28.066: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 30 22:22:28.066: INFO: calico-node-xk89l from kube-system started at 2020-03-30 20:47:51 +0000 UTC (2 container statuses recorded)
Mar 30 22:22:28.066: INFO: 	Container calico-node ready: true, restart count 0
Mar 30 22:22:28.066: INFO: 	Container install-cni ready: true, restart count 0
Mar 30 22:22:28.066: INFO: csi-cinder-nodeplugin-qgrf4 from kube-system started at 2020-03-30 20:48:21 +0000 UTC (2 container statuses recorded)
Mar 30 22:22:28.066: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 30 22:22:28.066: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 30 22:22:28.066: INFO: nginx-ingress-controller-78f766b979-bfx4q from ingress-nginx started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.066: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 30 22:22:28.066: INFO: default-http-backend-6664c884c9-dxczc from ingress-nginx started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.066: INFO: 	Container default-http-backend ready: true, restart count 0
Mar 30 22:22:28.066: INFO: sonobuoy-systemd-logs-daemon-set-4495c60740044637-v7t76 from sonobuoy started at 2020-03-30 21:04:37 +0000 UTC (2 container statuses recorded)
Mar 30 22:22:28.066: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 30 22:22:28.067: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 30 22:22:28.067: INFO: kube-proxy-pcd5r from kube-system started at 2020-03-30 20:47:51 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.067: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 30 22:22:28.067: INFO: nginx-ingress-controller-78f766b979-wf7qb from ingress-nginx started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.067: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 30 22:22:28.067: INFO: eric-tm-external-connectivity-frontend-speaker-zwnfr from kube-system started at 2020-03-30 20:50:36 +0000 UTC (1 container statuses recorded)
Mar 30 22:22:28.067: INFO: 	Container speaker ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: verifying the node has the label node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins
STEP: verifying the node has the label node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins
STEP: verifying the node has the label node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins
STEP: verifying the node has the label node worker-pool1-zo88v95j-eccd-ci-os-12-jenkins
Mar 30 22:22:28.224: INFO: Pod test-host-network-pod requesting resource cpu=0m on Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins
Mar 30 22:22:28.224: INFO: Pod test-pod requesting resource cpu=0m on Node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins
Mar 30 22:22:28.224: INFO: Pod default-http-backend-6664c884c9-dxczc requesting resource cpu=10m on Node worker-pool1-zo88v95j-eccd-ci-os-12-jenkins
Mar 30 22:22:28.224: INFO: Pod nginx-ingress-controller-78f766b979-bfx4q requesting resource cpu=0m on Node worker-pool1-zo88v95j-eccd-ci-os-12-jenkins
Mar 30 22:22:28.224: INFO: Pod nginx-ingress-controller-78f766b979-wf7qb requesting resource cpu=0m on Node worker-pool1-zo88v95j-eccd-ci-os-12-jenkins
Mar 30 22:22:28.224: INFO: Pod calico-node-hf278 requesting resource cpu=250m on Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins
Mar 30 22:22:28.224: INFO: Pod calico-node-kswgg requesting resource cpu=250m on Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins
Mar 30 22:22:28.224: INFO: Pod calico-node-lrw4b requesting resource cpu=250m on Node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins
Mar 30 22:22:28.224: INFO: Pod calico-node-xk89l requesting resource cpu=250m on Node worker-pool1-zo88v95j-eccd-ci-os-12-jenkins
Mar 30 22:22:28.224: INFO: Pod ccd-license-consumer-5c7ff9fc96-b2hn2 requesting resource cpu=0m on Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins
Mar 30 22:22:28.224: INFO: Pod csi-cinder-nodeplugin-cfp5q requesting resource cpu=0m on Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod csi-cinder-nodeplugin-ftdf2 requesting resource cpu=0m on Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod csi-cinder-nodeplugin-qgrf4 requesting resource cpu=0m on Node worker-pool1-zo88v95j-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod csi-cinder-nodeplugin-wnrkx requesting resource cpu=0m on Node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod eric-lcm-container-registry-registry-0 requesting resource cpu=100m on Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod eric-lm-combined-server-license-consumer-handler-58597bc9679zxp requesting resource cpu=100m on Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod eric-lm-combined-server-license-server-client-945b6bc4c-5phm5 requesting resource cpu=100m on Node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod eric-tm-external-connectivity-frontend-controller-78f9fd87bvc6l requesting resource cpu=50m on Node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod eric-tm-external-connectivity-frontend-speaker-8tmb7 requesting resource cpu=50m on Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod eric-tm-external-connectivity-frontend-speaker-m4f4p requesting resource cpu=50m on Node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod eric-tm-external-connectivity-frontend-speaker-vmndg requesting resource cpu=50m on Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod eric-tm-external-connectivity-frontend-speaker-zwnfr requesting resource cpu=50m on Node worker-pool1-zo88v95j-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod kube-multus-ds-amd64-6t2pw requesting resource cpu=100m on Node worker-pool1-zo88v95j-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod kube-multus-ds-amd64-9mjsk requesting resource cpu=100m on Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod kube-multus-ds-amd64-q9869 requesting resource cpu=100m on Node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod kube-multus-ds-amd64-vm7r6 requesting resource cpu=100m on Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod kube-proxy-4mzsc requesting resource cpu=0m on Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod kube-proxy-746bb requesting resource cpu=0m on Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod kube-proxy-9w8kt requesting resource cpu=0m on Node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod kube-proxy-pcd5r requesting resource cpu=0m on Node worker-pool1-zo88v95j-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod metrics-server-8666db6c57-znld5 requesting resource cpu=0m on Node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod postgresql-postgresql-0 requesting resource cpu=250m on Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod tiller-deploy-cd77547bd-2pv7h requesting resource cpu=0m on Node worker-pool1-zo88v95j-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod eric-pm-server-alertmanager-648979cdd-z57mc requesting resource cpu=110m on Node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod eric-pm-server-eric-pm-server-7b4dd54bc5-m7fjg requesting resource cpu=350m on Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod eric-pm-server-kube-state-metrics-66f7fbfd44-k7krp requesting resource cpu=100m on Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod eric-pm-server-node-exporter-9cwgm requesting resource cpu=100m on Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod eric-pm-server-node-exporter-ktbpn requesting resource cpu=100m on Node worker-pool1-zo88v95j-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod eric-pm-server-node-exporter-m4kjc requesting resource cpu=100m on Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod eric-pm-server-node-exporter-v5bmh requesting resource cpu=100m on Node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod eric-pm-server-pushgateway-7798d479ff-fcnk9 requesting resource cpu=0m on Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod sonobuoy requesting resource cpu=0m on Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod sonobuoy-e2e-job-14477a96f0f5465c requesting resource cpu=0m on Node worker-pool1-zo88v95j-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod sonobuoy-systemd-logs-daemon-set-4495c60740044637-5rfld requesting resource cpu=0m on Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod sonobuoy-systemd-logs-daemon-set-4495c60740044637-62zgj requesting resource cpu=0m on Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod sonobuoy-systemd-logs-daemon-set-4495c60740044637-gqxkr requesting resource cpu=0m on Node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins
Mar 30 22:22:28.225: INFO: Pod sonobuoy-systemd-logs-daemon-set-4495c60740044637-v7t76 requesting resource cpu=0m on Node worker-pool1-zo88v95j-eccd-ci-os-12-jenkins
STEP: Starting Pods to consume most of the cluster CPU.
Mar 30 22:22:28.225: INFO: Creating a pod which consumes cpu=1043m on Node worker-pool1-zo88v95j-eccd-ci-os-12-jenkins
Mar 30 22:22:28.239: INFO: Creating a pod which consumes cpu=735m on Node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins
Mar 30 22:22:28.250: INFO: Creating a pod which consumes cpu=868m on Node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins
Mar 30 22:22:28.261: INFO: Creating a pod which consumes cpu=735m on Node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5604a212-93da-4ec3-a57f-6fe2531ea46f.1601351e598ff74c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-43/filler-pod-5604a212-93da-4ec3-a57f-6fe2531ea46f to worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5604a212-93da-4ec3-a57f-6fe2531ea46f.1601351feee593ac], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5604a212-93da-4ec3-a57f-6fe2531ea46f.1601351ff4577fa2], Reason = [Created], Message = [Created container filler-pod-5604a212-93da-4ec3-a57f-6fe2531ea46f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5604a212-93da-4ec3-a57f-6fe2531ea46f.1601351ffc7a5040], Reason = [Started], Message = [Started container filler-pod-5604a212-93da-4ec3-a57f-6fe2531ea46f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-79d98524-6aeb-4a6f-b051-042a755132fc.1601351e59895e1b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-43/filler-pod-79d98524-6aeb-4a6f-b051-042a755132fc to worker-pool1-tjim0te5-eccd-ci-os-12-jenkins]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-79d98524-6aeb-4a6f-b051-042a755132fc.1601351ff63213ba], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-79d98524-6aeb-4a6f-b051-042a755132fc.1601351ffb3e76e0], Reason = [Created], Message = [Created container filler-pod-79d98524-6aeb-4a6f-b051-042a755132fc]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-79d98524-6aeb-4a6f-b051-042a755132fc.1601352006afa9ca], Reason = [Started], Message = [Started container filler-pod-79d98524-6aeb-4a6f-b051-042a755132fc]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bdffe96c-5474-422d-977b-75402a2f8d71.1601351e5a74f805], Reason = [Scheduled], Message = [Successfully assigned sched-pred-43/filler-pod-bdffe96c-5474-422d-977b-75402a2f8d71 to worker-pool1-vvs2j292-eccd-ci-os-12-jenkins]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bdffe96c-5474-422d-977b-75402a2f8d71.1601351ff4a68bcd], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bdffe96c-5474-422d-977b-75402a2f8d71.1601351ff8bd7e37], Reason = [Created], Message = [Created container filler-pod-bdffe96c-5474-422d-977b-75402a2f8d71]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bdffe96c-5474-422d-977b-75402a2f8d71.1601352003f739ce], Reason = [Started], Message = [Started container filler-pod-bdffe96c-5474-422d-977b-75402a2f8d71]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ce742f1b-8d1b-4d46-9cb9-e1830cd12e9b.1601351e5837f995], Reason = [Scheduled], Message = [Successfully assigned sched-pred-43/filler-pod-ce742f1b-8d1b-4d46-9cb9-e1830cd12e9b to worker-pool1-zo88v95j-eccd-ci-os-12-jenkins]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ce742f1b-8d1b-4d46-9cb9-e1830cd12e9b.1601351ff5803d5d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ce742f1b-8d1b-4d46-9cb9-e1830cd12e9b.1601351ff8c740c4], Reason = [Created], Message = [Created container filler-pod-ce742f1b-8d1b-4d46-9cb9-e1830cd12e9b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ce742f1b-8d1b-4d46-9cb9-e1830cd12e9b.1601352002b68289], Reason = [Started], Message = [Started container filler-pod-ce742f1b-8d1b-4d46-9cb9-e1830cd12e9b]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16013520b09356cf], Reason = [FailedScheduling], Message = [0/7 nodes are available: 7 Insufficient cpu.]
STEP: removing the label node off the node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node worker-pool1-zo88v95j-eccd-ci-os-12-jenkins
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:22:39.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-43" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:11.700 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":278,"completed":201,"skipped":3324,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:22:39.507: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4445
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 22:22:39.684: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:22:39.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4445" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":278,"completed":202,"skipped":3330,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:22:39.831: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2992
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 30 22:22:40.067: INFO: Waiting up to 5m0s for pod "downwardapi-volume-261540c6-0047-4839-ab2a-c1823cffede8" in namespace "projected-2992" to be "success or failure"
Mar 30 22:22:40.084: INFO: Pod "downwardapi-volume-261540c6-0047-4839-ab2a-c1823cffede8": Phase="Pending", Reason="", readiness=false. Elapsed: 17.000423ms
Mar 30 22:22:42.091: INFO: Pod "downwardapi-volume-261540c6-0047-4839-ab2a-c1823cffede8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023279519s
Mar 30 22:22:44.097: INFO: Pod "downwardapi-volume-261540c6-0047-4839-ab2a-c1823cffede8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029888242s
Mar 30 22:22:46.106: INFO: Pod "downwardapi-volume-261540c6-0047-4839-ab2a-c1823cffede8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.038535954s
Mar 30 22:22:48.112: INFO: Pod "downwardapi-volume-261540c6-0047-4839-ab2a-c1823cffede8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.044675727s
STEP: Saw pod success
Mar 30 22:22:48.112: INFO: Pod "downwardapi-volume-261540c6-0047-4839-ab2a-c1823cffede8" satisfied condition "success or failure"
Mar 30 22:22:48.116: INFO: Trying to get logs from node worker-pool1-zo88v95j-eccd-ci-os-12-jenkins pod downwardapi-volume-261540c6-0047-4839-ab2a-c1823cffede8 container client-container: <nil>
STEP: delete the pod
Mar 30 22:22:48.154: INFO: Waiting for pod downwardapi-volume-261540c6-0047-4839-ab2a-c1823cffede8 to disappear
Mar 30 22:22:48.158: INFO: Pod downwardapi-volume-261540c6-0047-4839-ab2a-c1823cffede8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:22:48.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2992" for this suite.

• [SLOW TEST:8.343 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":203,"skipped":3339,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:22:48.178: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5780
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5780
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-5780
I0330 22:22:48.419967      20 runners.go:189] Created replication controller with name: externalname-service, namespace: services-5780, replica count: 2
I0330 22:22:51.481213      20 runners.go:189] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0330 22:22:54.481542      20 runners.go:189] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 30 22:22:57.482: INFO: Creating new exec pod
I0330 22:22:57.482137      20 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 30 22:23:06.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=services-5780 execpodghcbm -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar 30 22:23:06.930: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 30 22:23:06.930: INFO: stdout: ""
Mar 30 22:23:06.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=services-5780 execpodghcbm -- /bin/sh -x -c nc -zv -t -w 2 10.96.226.44 80'
Mar 30 22:23:07.205: INFO: stderr: "+ nc -zv -t -w 2 10.96.226.44 80\nConnection to 10.96.226.44 80 port [tcp/http] succeeded!\n"
Mar 30 22:23:07.205: INFO: stdout: ""
Mar 30 22:23:07.205: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:23:07.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5780" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:19.139 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":278,"completed":204,"skipped":3367,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:23:07.319: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7256
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Mar 30 22:23:17.573: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7256 PodName:pod-sharedvolume-c6c513b3-871c-4195-b5dd-c4f4b3599f49 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 30 22:23:17.573: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 22:23:17.760: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:23:17.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7256" for this suite.

• [SLOW TEST:10.470 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":278,"completed":205,"skipped":3413,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:23:17.793: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5681
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 22:23:18.014: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar 30 22:23:23.019: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 30 22:23:27.031: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Mar 30 22:23:27.062: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5681 /apis/apps/v1/namespaces/deployment-5681/deployments/test-cleanup-deployment 1d0fa926-0669-4566-8633-c8aee2b09fb4 44015 1 2020-03-30 22:23:27 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006bf10d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Mar 30 22:23:27.070: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Mar 30 22:23:27.070: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Mar 30 22:23:27.070: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-5681 /apis/apps/v1/namespaces/deployment-5681/replicasets/test-cleanup-controller 78bbe9ab-26c2-4260-9a5f-48bb3005ac35 44016 1 2020-03-30 22:23:17 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 1d0fa926-0669-4566-8633-c8aee2b09fb4 0xc006bf1437 0xc006bf1438}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006bf1498 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 30 22:23:27.078: INFO: Pod "test-cleanup-controller-59dqk" is available:
&Pod{ObjectMeta:{test-cleanup-controller-59dqk test-cleanup-controller- deployment-5681 /api/v1/namespaces/deployment-5681/pods/test-cleanup-controller-59dqk e4321d2b-460d-4608-90d5-fce7483d044e 44007 0 2020-03-30 22:23:18 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.185.209"
    ],
    "dns": {}
}] kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-controller 78bbe9ab-26c2-4260-9a5f-48bb3005ac35 0xc006bf17b7 0xc006bf17b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-9g59x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-9g59x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-9g59x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-vvs2j292-eccd-ci-os-12-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 22:23:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 22:23:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 22:23:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 22:23:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.8,PodIP:192.168.185.209,StartTime:2020-03-30 22:23:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-30 22:23:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://707c37a6653bafa06bea0b32ff5f0545d7dca6db7cf0cfead2f64722e7530983,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.185.209,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:23:27.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5681" for this suite.

• [SLOW TEST:9.326 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":278,"completed":206,"skipped":3434,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:23:27.119: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3773
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Mar 30 22:23:27.394: INFO: Waiting up to 5m0s for pod "downward-api-44b7201f-4a23-4d48-832c-ba693bbd017f" in namespace "downward-api-3773" to be "success or failure"
Mar 30 22:23:27.404: INFO: Pod "downward-api-44b7201f-4a23-4d48-832c-ba693bbd017f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.550453ms
Mar 30 22:23:29.412: INFO: Pod "downward-api-44b7201f-4a23-4d48-832c-ba693bbd017f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018267557s
Mar 30 22:23:31.419: INFO: Pod "downward-api-44b7201f-4a23-4d48-832c-ba693bbd017f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025610983s
Mar 30 22:23:33.427: INFO: Pod "downward-api-44b7201f-4a23-4d48-832c-ba693bbd017f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033123385s
Mar 30 22:23:35.431: INFO: Pod "downward-api-44b7201f-4a23-4d48-832c-ba693bbd017f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.037380129s
STEP: Saw pod success
Mar 30 22:23:35.431: INFO: Pod "downward-api-44b7201f-4a23-4d48-832c-ba693bbd017f" satisfied condition "success or failure"
Mar 30 22:23:35.436: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod downward-api-44b7201f-4a23-4d48-832c-ba693bbd017f container dapi-container: <nil>
STEP: delete the pod
Mar 30 22:23:35.505: INFO: Waiting for pod downward-api-44b7201f-4a23-4d48-832c-ba693bbd017f to disappear
Mar 30 22:23:35.513: INFO: Pod downward-api-44b7201f-4a23-4d48-832c-ba693bbd017f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:23:35.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3773" for this suite.

• [SLOW TEST:8.408 seconds]
[sig-node] Downward API
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:33
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":278,"completed":207,"skipped":3442,"failed":0}
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:23:35.529: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3305
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override command
Mar 30 22:23:35.778: INFO: Waiting up to 5m0s for pod "client-containers-99a42f17-a881-4131-8982-9db0fed341e9" in namespace "containers-3305" to be "success or failure"
Mar 30 22:23:35.789: INFO: Pod "client-containers-99a42f17-a881-4131-8982-9db0fed341e9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.017365ms
Mar 30 22:23:37.795: INFO: Pod "client-containers-99a42f17-a881-4131-8982-9db0fed341e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016688699s
Mar 30 22:23:39.801: INFO: Pod "client-containers-99a42f17-a881-4131-8982-9db0fed341e9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022295177s
Mar 30 22:23:41.807: INFO: Pod "client-containers-99a42f17-a881-4131-8982-9db0fed341e9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029100742s
Mar 30 22:23:43.814: INFO: Pod "client-containers-99a42f17-a881-4131-8982-9db0fed341e9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.035614452s
Mar 30 22:23:45.818: INFO: Pod "client-containers-99a42f17-a881-4131-8982-9db0fed341e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.040156079s
STEP: Saw pod success
Mar 30 22:23:45.819: INFO: Pod "client-containers-99a42f17-a881-4131-8982-9db0fed341e9" satisfied condition "success or failure"
Mar 30 22:23:45.822: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod client-containers-99a42f17-a881-4131-8982-9db0fed341e9 container test-container: <nil>
STEP: delete the pod
Mar 30 22:23:45.861: INFO: Waiting for pod client-containers-99a42f17-a881-4131-8982-9db0fed341e9 to disappear
Mar 30 22:23:45.865: INFO: Pod client-containers-99a42f17-a881-4131-8982-9db0fed341e9 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:23:45.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3305" for this suite.

• [SLOW TEST:10.355 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":278,"completed":208,"skipped":3444,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:23:45.885: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8852
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 22:23:46.052: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Mar 30 22:23:50.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 --namespace=crd-publish-openapi-8852 create -f -'
Mar 30 22:23:50.788: INFO: stderr: ""
Mar 30 22:23:50.788: INFO: stdout: "e2e-test-crd-publish-openapi-4316-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 30 22:23:50.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 --namespace=crd-publish-openapi-8852 delete e2e-test-crd-publish-openapi-4316-crds test-foo'
Mar 30 22:23:50.881: INFO: stderr: ""
Mar 30 22:23:50.882: INFO: stdout: "e2e-test-crd-publish-openapi-4316-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar 30 22:23:50.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 --namespace=crd-publish-openapi-8852 apply -f -'
Mar 30 22:23:51.430: INFO: stderr: ""
Mar 30 22:23:51.430: INFO: stdout: "e2e-test-crd-publish-openapi-4316-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 30 22:23:51.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 --namespace=crd-publish-openapi-8852 delete e2e-test-crd-publish-openapi-4316-crds test-foo'
Mar 30 22:23:51.580: INFO: stderr: ""
Mar 30 22:23:51.580: INFO: stdout: "e2e-test-crd-publish-openapi-4316-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Mar 30 22:23:51.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 --namespace=crd-publish-openapi-8852 create -f -'
Mar 30 22:23:51.842: INFO: rc: 1
Mar 30 22:23:51.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 --namespace=crd-publish-openapi-8852 apply -f -'
Mar 30 22:23:52.178: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Mar 30 22:23:52.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 --namespace=crd-publish-openapi-8852 create -f -'
Mar 30 22:23:52.386: INFO: rc: 1
Mar 30 22:23:52.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 --namespace=crd-publish-openapi-8852 apply -f -'
Mar 30 22:23:52.672: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Mar 30 22:23:52.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 explain e2e-test-crd-publish-openapi-4316-crds'
Mar 30 22:23:52.987: INFO: stderr: ""
Mar 30 22:23:52.987: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4316-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Mar 30 22:23:52.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 explain e2e-test-crd-publish-openapi-4316-crds.metadata'
Mar 30 22:23:53.307: INFO: stderr: ""
Mar 30 22:23:53.307: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4316-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar 30 22:23:53.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 explain e2e-test-crd-publish-openapi-4316-crds.spec'
Mar 30 22:23:53.654: INFO: stderr: ""
Mar 30 22:23:53.654: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4316-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar 30 22:23:53.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 explain e2e-test-crd-publish-openapi-4316-crds.spec.bars'
Mar 30 22:23:54.012: INFO: stderr: ""
Mar 30 22:23:54.012: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4316-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Mar 30 22:23:54.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 explain e2e-test-crd-publish-openapi-4316-crds.spec.bars2'
Mar 30 22:23:54.352: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:23:58.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8852" for this suite.

• [SLOW TEST:12.901 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":278,"completed":209,"skipped":3451,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:23:58.788: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-39
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-0c1e3f59-2075-43f3-8608-7377b94930a6
STEP: Creating secret with name s-test-opt-upd-abb84584-a842-4a44-bbfe-942943c21e61
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-0c1e3f59-2075-43f3-8608-7377b94930a6
STEP: Updating secret s-test-opt-upd-abb84584-a842-4a44-bbfe-942943c21e61
STEP: Creating secret with name s-test-opt-create-77806776-8909-4a0e-bdde-ca9f66f0d09a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:25:25.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-39" for this suite.

• [SLOW TEST:87.060 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":210,"skipped":3470,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:25:25.850: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4909
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with configMap that has name projected-configmap-test-upd-219a6e5c-49c9-4690-9dd0-e896427b6f92
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-219a6e5c-49c9-4690-9dd0-e896427b6f92
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:25:36.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4909" for this suite.

• [SLOW TEST:10.310 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":211,"skipped":3499,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:25:36.163: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3787
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:25:47.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3787" for this suite.

• [SLOW TEST:11.294 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":278,"completed":212,"skipped":3500,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:25:47.458: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7207
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:26:29.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7207" for this suite.

• [SLOW TEST:41.765 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":278,"completed":213,"skipped":3507,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:26:29.225: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4016
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 30 22:26:29.438: INFO: Waiting up to 5m0s for pod "pod-3b39b862-e9a4-4118-abec-a263d99bbbf8" in namespace "emptydir-4016" to be "success or failure"
Mar 30 22:26:29.443: INFO: Pod "pod-3b39b862-e9a4-4118-abec-a263d99bbbf8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.740599ms
Mar 30 22:26:31.453: INFO: Pod "pod-3b39b862-e9a4-4118-abec-a263d99bbbf8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014405057s
Mar 30 22:26:33.459: INFO: Pod "pod-3b39b862-e9a4-4118-abec-a263d99bbbf8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020490219s
Mar 30 22:26:35.465: INFO: Pod "pod-3b39b862-e9a4-4118-abec-a263d99bbbf8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027098458s
Mar 30 22:26:37.474: INFO: Pod "pod-3b39b862-e9a4-4118-abec-a263d99bbbf8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.035794917s
STEP: Saw pod success
Mar 30 22:26:37.474: INFO: Pod "pod-3b39b862-e9a4-4118-abec-a263d99bbbf8" satisfied condition "success or failure"
Mar 30 22:26:37.477: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-3b39b862-e9a4-4118-abec-a263d99bbbf8 container test-container: <nil>
STEP: delete the pod
Mar 30 22:26:37.531: INFO: Waiting for pod pod-3b39b862-e9a4-4118-abec-a263d99bbbf8 to disappear
Mar 30 22:26:37.534: INFO: Pod pod-3b39b862-e9a4-4118-abec-a263d99bbbf8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:26:37.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4016" for this suite.

• [SLOW TEST:8.325 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":214,"skipped":3515,"failed":0}
SSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:26:37.554: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3515
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod test-webserver-0146f0d6-49b4-4618-b847-77c562aea898 in namespace container-probe-3515
Mar 30 22:26:45.831: INFO: Started pod test-webserver-0146f0d6-49b4-4618-b847-77c562aea898 in namespace container-probe-3515
STEP: checking the pod's current state and verifying that restartCount is present
Mar 30 22:26:45.836: INFO: Initial restart count of pod test-webserver-0146f0d6-49b4-4618-b847-77c562aea898 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:30:46.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3515" for this suite.

• [SLOW TEST:249.192 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":278,"completed":215,"skipped":3519,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:30:46.750: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-7954
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1346
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6081
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:31:23.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7954" for this suite.
STEP: Destroying namespace "nsdeletetest-1346" for this suite.
Mar 30 22:31:23.407: INFO: Namespace nsdeletetest-1346 was already deleted
STEP: Destroying namespace "nsdeletetest-6081" for this suite.

• [SLOW TEST:36.667 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":278,"completed":216,"skipped":3529,"failed":0}
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:31:23.418: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8054
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-8054
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 30 22:31:23.612: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 30 22:31:57.842: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.125.31:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8054 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 30 22:31:57.842: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 22:31:58.013: INFO: Found all expected endpoints: [netserver-0]
Mar 30 22:31:58.018: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.14.118:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8054 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 30 22:31:58.018: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 22:31:58.214: INFO: Found all expected endpoints: [netserver-1]
Mar 30 22:31:58.219: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.185.211:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8054 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 30 22:31:58.220: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 22:31:58.397: INFO: Found all expected endpoints: [netserver-2]
Mar 30 22:31:58.404: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.10.238:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8054 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 30 22:31:58.404: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 22:31:58.602: INFO: Found all expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:31:58.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8054" for this suite.

• [SLOW TEST:35.219 seconds]
[sig-network] Networking
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":217,"skipped":3534,"failed":0}
SSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:31:58.637: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1805
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:46
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Mar 30 22:32:06.881: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-408641372 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Mar 30 22:32:17.211: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:32:17.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1805" for this suite.

• [SLOW TEST:18.596 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should be submitted and removed [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period should be submitted and removed [Conformance]","total":278,"completed":218,"skipped":3537,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:32:17.235: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8365
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar 30 22:32:17.481: INFO: Waiting up to 5m0s for pod "pod-83b038dc-ed4f-4524-ae16-e0e1b5779e36" in namespace "emptydir-8365" to be "success or failure"
Mar 30 22:32:17.488: INFO: Pod "pod-83b038dc-ed4f-4524-ae16-e0e1b5779e36": Phase="Pending", Reason="", readiness=false. Elapsed: 6.271417ms
Mar 30 22:32:19.495: INFO: Pod "pod-83b038dc-ed4f-4524-ae16-e0e1b5779e36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013078436s
Mar 30 22:32:21.705: INFO: Pod "pod-83b038dc-ed4f-4524-ae16-e0e1b5779e36": Phase="Pending", Reason="", readiness=false. Elapsed: 4.222994448s
Mar 30 22:32:23.711: INFO: Pod "pod-83b038dc-ed4f-4524-ae16-e0e1b5779e36": Phase="Pending", Reason="", readiness=false. Elapsed: 6.229605042s
Mar 30 22:32:25.717: INFO: Pod "pod-83b038dc-ed4f-4524-ae16-e0e1b5779e36": Phase="Pending", Reason="", readiness=false. Elapsed: 8.235350841s
Mar 30 22:32:27.724: INFO: Pod "pod-83b038dc-ed4f-4524-ae16-e0e1b5779e36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.242257749s
STEP: Saw pod success
Mar 30 22:32:27.724: INFO: Pod "pod-83b038dc-ed4f-4524-ae16-e0e1b5779e36" satisfied condition "success or failure"
Mar 30 22:32:27.729: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-83b038dc-ed4f-4524-ae16-e0e1b5779e36 container test-container: <nil>
STEP: delete the pod
Mar 30 22:32:27.760: INFO: Waiting for pod pod-83b038dc-ed4f-4524-ae16-e0e1b5779e36 to disappear
Mar 30 22:32:27.766: INFO: Pod pod-83b038dc-ed4f-4524-ae16-e0e1b5779e36 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:32:27.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8365" for this suite.

• [SLOW TEST:10.550 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":219,"skipped":3547,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:32:27.787: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6648
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 30 22:32:27.993: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9e540e09-5034-413c-961e-fcee1d128494" in namespace "downward-api-6648" to be "success or failure"
Mar 30 22:32:28.011: INFO: Pod "downwardapi-volume-9e540e09-5034-413c-961e-fcee1d128494": Phase="Pending", Reason="", readiness=false. Elapsed: 17.290982ms
Mar 30 22:32:30.016: INFO: Pod "downwardapi-volume-9e540e09-5034-413c-961e-fcee1d128494": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022327733s
Mar 30 22:32:32.025: INFO: Pod "downwardapi-volume-9e540e09-5034-413c-961e-fcee1d128494": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031830744s
Mar 30 22:32:34.031: INFO: Pod "downwardapi-volume-9e540e09-5034-413c-961e-fcee1d128494": Phase="Pending", Reason="", readiness=false. Elapsed: 6.038186281s
Mar 30 22:32:36.039: INFO: Pod "downwardapi-volume-9e540e09-5034-413c-961e-fcee1d128494": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.045413903s
STEP: Saw pod success
Mar 30 22:32:36.039: INFO: Pod "downwardapi-volume-9e540e09-5034-413c-961e-fcee1d128494" satisfied condition "success or failure"
Mar 30 22:32:36.043: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod downwardapi-volume-9e540e09-5034-413c-961e-fcee1d128494 container client-container: <nil>
STEP: delete the pod
Mar 30 22:32:36.085: INFO: Waiting for pod downwardapi-volume-9e540e09-5034-413c-961e-fcee1d128494 to disappear
Mar 30 22:32:36.091: INFO: Pod downwardapi-volume-9e540e09-5034-413c-961e-fcee1d128494 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:32:36.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6648" for this suite.

• [SLOW TEST:8.323 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":278,"completed":220,"skipped":3557,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:32:36.113: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9773
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9773
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-9773
I0330 22:32:36.396828      20 runners.go:189] Created replication controller with name: externalname-service, namespace: services-9773, replica count: 2
I0330 22:32:39.449769      20 runners.go:189] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0330 22:32:42.450156      20 runners.go:189] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0330 22:32:45.450486      20 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 30 22:32:45.450: INFO: Creating new exec pod
Mar 30 22:32:54.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=services-9773 execpodbl6jb -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar 30 22:32:54.793: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 30 22:32:54.793: INFO: stdout: ""
Mar 30 22:32:54.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=services-9773 execpodbl6jb -- /bin/sh -x -c nc -zv -t -w 2 10.97.214.145 80'
Mar 30 22:32:55.042: INFO: stderr: "+ nc -zv -t -w 2 10.97.214.145 80\nConnection to 10.97.214.145 80 port [tcp/http] succeeded!\n"
Mar 30 22:32:55.043: INFO: stdout: ""
Mar 30 22:32:55.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=services-9773 execpodbl6jb -- /bin/sh -x -c nc -zv -t -w 2 10.0.10.2 32064'
Mar 30 22:32:55.292: INFO: stderr: "+ nc -zv -t -w 2 10.0.10.2 32064\nConnection to 10.0.10.2 32064 port [tcp/32064] succeeded!\n"
Mar 30 22:32:55.292: INFO: stdout: ""
Mar 30 22:32:55.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=services-9773 execpodbl6jb -- /bin/sh -x -c nc -zv -t -w 2 10.0.10.8 32064'
Mar 30 22:32:55.585: INFO: stderr: "+ nc -zv -t -w 2 10.0.10.8 32064\nConnection to 10.0.10.8 32064 port [tcp/32064] succeeded!\n"
Mar 30 22:32:55.585: INFO: stdout: ""
Mar 30 22:32:55.585: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:32:55.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9773" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:19.565 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":278,"completed":221,"skipped":3569,"failed":0}
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:32:55.678: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6826
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 30 22:32:55.907: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ca77c1e7-126a-401e-b9f2-234171a1fa14" in namespace "downward-api-6826" to be "success or failure"
Mar 30 22:32:55.911: INFO: Pod "downwardapi-volume-ca77c1e7-126a-401e-b9f2-234171a1fa14": Phase="Pending", Reason="", readiness=false. Elapsed: 4.342791ms
Mar 30 22:32:57.920: INFO: Pod "downwardapi-volume-ca77c1e7-126a-401e-b9f2-234171a1fa14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012419168s
Mar 30 22:32:59.926: INFO: Pod "downwardapi-volume-ca77c1e7-126a-401e-b9f2-234171a1fa14": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019008677s
Mar 30 22:33:01.933: INFO: Pod "downwardapi-volume-ca77c1e7-126a-401e-b9f2-234171a1fa14": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025379022s
Mar 30 22:33:03.939: INFO: Pod "downwardapi-volume-ca77c1e7-126a-401e-b9f2-234171a1fa14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.031548091s
STEP: Saw pod success
Mar 30 22:33:03.939: INFO: Pod "downwardapi-volume-ca77c1e7-126a-401e-b9f2-234171a1fa14" satisfied condition "success or failure"
Mar 30 22:33:03.944: INFO: Trying to get logs from node worker-pool1-zo88v95j-eccd-ci-os-12-jenkins pod downwardapi-volume-ca77c1e7-126a-401e-b9f2-234171a1fa14 container client-container: <nil>
STEP: delete the pod
Mar 30 22:33:03.979: INFO: Waiting for pod downwardapi-volume-ca77c1e7-126a-401e-b9f2-234171a1fa14 to disappear
Mar 30 22:33:03.984: INFO: Pod downwardapi-volume-ca77c1e7-126a-401e-b9f2-234171a1fa14 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:33:03.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6826" for this suite.

• [SLOW TEST:8.338 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":222,"skipped":3569,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:33:04.018: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-6002
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6741
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7151
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:33:10.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6002" for this suite.
STEP: Destroying namespace "nsdeletetest-6741" for this suite.
Mar 30 22:33:10.720: INFO: Namespace nsdeletetest-6741 was already deleted
STEP: Destroying namespace "nsdeletetest-7151" for this suite.

• [SLOW TEST:6.727 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":278,"completed":223,"skipped":3579,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:33:10.745: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6567
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 22:33:10.955: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 30 22:33:14.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 --namespace=crd-publish-openapi-6567 create -f -'
Mar 30 22:33:15.711: INFO: stderr: ""
Mar 30 22:33:15.711: INFO: stdout: "e2e-test-crd-publish-openapi-3979-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 30 22:33:15.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 --namespace=crd-publish-openapi-6567 delete e2e-test-crd-publish-openapi-3979-crds test-cr'
Mar 30 22:33:15.899: INFO: stderr: ""
Mar 30 22:33:15.899: INFO: stdout: "e2e-test-crd-publish-openapi-3979-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar 30 22:33:15.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 --namespace=crd-publish-openapi-6567 apply -f -'
Mar 30 22:33:16.210: INFO: stderr: ""
Mar 30 22:33:16.210: INFO: stdout: "e2e-test-crd-publish-openapi-3979-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 30 22:33:16.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 --namespace=crd-publish-openapi-6567 delete e2e-test-crd-publish-openapi-3979-crds test-cr'
Mar 30 22:33:16.334: INFO: stderr: ""
Mar 30 22:33:16.334: INFO: stdout: "e2e-test-crd-publish-openapi-3979-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar 30 22:33:16.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 explain e2e-test-crd-publish-openapi-3979-crds'
Mar 30 22:33:16.860: INFO: stderr: ""
Mar 30 22:33:16.860: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3979-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:33:20.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6567" for this suite.

• [SLOW TEST:10.002 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":278,"completed":224,"skipped":3579,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:33:20.756: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6687
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-d03c5669-3be4-4e45-875b-de00290b3360
STEP: Creating a pod to test consume secrets
Mar 30 22:33:20.966: INFO: Waiting up to 5m0s for pod "pod-secrets-907297e3-163f-481c-9c44-e744c0b077bf" in namespace "secrets-6687" to be "success or failure"
Mar 30 22:33:20.972: INFO: Pod "pod-secrets-907297e3-163f-481c-9c44-e744c0b077bf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.762108ms
Mar 30 22:33:22.977: INFO: Pod "pod-secrets-907297e3-163f-481c-9c44-e744c0b077bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01045417s
Mar 30 22:33:24.983: INFO: Pod "pod-secrets-907297e3-163f-481c-9c44-e744c0b077bf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016706977s
Mar 30 22:33:26.990: INFO: Pod "pod-secrets-907297e3-163f-481c-9c44-e744c0b077bf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023523692s
Mar 30 22:33:28.996: INFO: Pod "pod-secrets-907297e3-163f-481c-9c44-e744c0b077bf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.029911732s
Mar 30 22:33:31.002: INFO: Pod "pod-secrets-907297e3-163f-481c-9c44-e744c0b077bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.036081835s
STEP: Saw pod success
Mar 30 22:33:31.002: INFO: Pod "pod-secrets-907297e3-163f-481c-9c44-e744c0b077bf" satisfied condition "success or failure"
Mar 30 22:33:31.005: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-secrets-907297e3-163f-481c-9c44-e744c0b077bf container secret-volume-test: <nil>
STEP: delete the pod
Mar 30 22:33:31.038: INFO: Waiting for pod pod-secrets-907297e3-163f-481c-9c44-e744c0b077bf to disappear
Mar 30 22:33:31.041: INFO: Pod pod-secrets-907297e3-163f-481c-9c44-e744c0b077bf no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:33:31.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6687" for this suite.

• [SLOW TEST:10.308 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":278,"completed":225,"skipped":3642,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:33:31.067: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-2324
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Mar 30 22:33:39.352: INFO: &Pod{ObjectMeta:{send-events-b5857d27-d128-4e5c-b115-e4c955c04059  events-2324 /api/v1/namespaces/events-2324/pods/send-events-b5857d27-d128-4e5c-b115-e4c955c04059 a47bb113-06e8-42ef-8bdd-402dfc069e58 47605 0 2020-03-30 22:33:31 +0000 UTC <nil> <nil> map[name:foo time:296895031] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.14.127"
    ],
    "dns": {}
}] kubernetes.io/psp:ccd-privileged] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vmx62,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vmx62,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vmx62,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-tjim0te5-eccd-ci-os-12-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 22:33:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 22:33:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 22:33:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-30 22:33:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.5,PodIP:192.168.14.127,StartTime:2020-03-30 22:33:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-30 22:33:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://4550f0f4f395daf108882667f323043fd225e9e7c83f67b4d3ad3a01c04cf187,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.14.127,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Mar 30 22:33:41.363: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Mar 30 22:33:43.370: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:33:43.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2324" for this suite.

• [SLOW TEST:12.339 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":278,"completed":226,"skipped":3659,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:33:43.408: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5338
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 30 22:33:43.603: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f1a5437d-24f5-4548-a42f-94308de0cbd9" in namespace "projected-5338" to be "success or failure"
Mar 30 22:33:43.608: INFO: Pod "downwardapi-volume-f1a5437d-24f5-4548-a42f-94308de0cbd9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.194685ms
Mar 30 22:33:45.614: INFO: Pod "downwardapi-volume-f1a5437d-24f5-4548-a42f-94308de0cbd9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010479923s
Mar 30 22:33:47.621: INFO: Pod "downwardapi-volume-f1a5437d-24f5-4548-a42f-94308de0cbd9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018067337s
Mar 30 22:33:49.627: INFO: Pod "downwardapi-volume-f1a5437d-24f5-4548-a42f-94308de0cbd9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024007891s
Mar 30 22:33:51.642: INFO: Pod "downwardapi-volume-f1a5437d-24f5-4548-a42f-94308de0cbd9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.038593609s
Mar 30 22:33:53.660: INFO: Pod "downwardapi-volume-f1a5437d-24f5-4548-a42f-94308de0cbd9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.056880618s
STEP: Saw pod success
Mar 30 22:33:53.660: INFO: Pod "downwardapi-volume-f1a5437d-24f5-4548-a42f-94308de0cbd9" satisfied condition "success or failure"
Mar 30 22:33:53.668: INFO: Trying to get logs from node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins pod downwardapi-volume-f1a5437d-24f5-4548-a42f-94308de0cbd9 container client-container: <nil>
STEP: delete the pod
Mar 30 22:33:53.710: INFO: Waiting for pod downwardapi-volume-f1a5437d-24f5-4548-a42f-94308de0cbd9 to disappear
Mar 30 22:33:53.718: INFO: Pod downwardapi-volume-f1a5437d-24f5-4548-a42f-94308de0cbd9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:33:53.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5338" for this suite.

• [SLOW TEST:10.325 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":278,"completed":227,"skipped":3664,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:33:53.737: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1866
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 30 22:33:53.938: INFO: Waiting up to 5m0s for pod "downwardapi-volume-07947ecd-a2f1-4241-b334-a453c6b1a4c0" in namespace "projected-1866" to be "success or failure"
Mar 30 22:33:53.943: INFO: Pod "downwardapi-volume-07947ecd-a2f1-4241-b334-a453c6b1a4c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.524399ms
Mar 30 22:33:55.949: INFO: Pod "downwardapi-volume-07947ecd-a2f1-4241-b334-a453c6b1a4c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010503548s
Mar 30 22:33:57.954: INFO: Pod "downwardapi-volume-07947ecd-a2f1-4241-b334-a453c6b1a4c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015968574s
Mar 30 22:33:59.960: INFO: Pod "downwardapi-volume-07947ecd-a2f1-4241-b334-a453c6b1a4c0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021774701s
Mar 30 22:34:01.967: INFO: Pod "downwardapi-volume-07947ecd-a2f1-4241-b334-a453c6b1a4c0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.029295695s
Mar 30 22:34:03.974: INFO: Pod "downwardapi-volume-07947ecd-a2f1-4241-b334-a453c6b1a4c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.036347169s
STEP: Saw pod success
Mar 30 22:34:03.974: INFO: Pod "downwardapi-volume-07947ecd-a2f1-4241-b334-a453c6b1a4c0" satisfied condition "success or failure"
Mar 30 22:34:03.978: INFO: Trying to get logs from node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins pod downwardapi-volume-07947ecd-a2f1-4241-b334-a453c6b1a4c0 container client-container: <nil>
STEP: delete the pod
Mar 30 22:34:04.046: INFO: Waiting for pod downwardapi-volume-07947ecd-a2f1-4241-b334-a453c6b1a4c0 to disappear
Mar 30 22:34:04.052: INFO: Pod downwardapi-volume-07947ecd-a2f1-4241-b334-a453c6b1a4c0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:34:04.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1866" for this suite.

• [SLOW TEST:10.347 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":278,"completed":228,"skipped":3666,"failed":0}
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:34:04.084: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9410
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-mdmg
STEP: Creating a pod to test atomic-volume-subpath
Mar 30 22:34:04.322: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-mdmg" in namespace "subpath-9410" to be "success or failure"
Mar 30 22:34:04.333: INFO: Pod "pod-subpath-test-configmap-mdmg": Phase="Pending", Reason="", readiness=false. Elapsed: 10.491436ms
Mar 30 22:34:06.341: INFO: Pod "pod-subpath-test-configmap-mdmg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018380896s
Mar 30 22:34:08.349: INFO: Pod "pod-subpath-test-configmap-mdmg": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026631294s
Mar 30 22:34:10.354: INFO: Pod "pod-subpath-test-configmap-mdmg": Phase="Pending", Reason="", readiness=false. Elapsed: 6.031754093s
Mar 30 22:34:12.360: INFO: Pod "pod-subpath-test-configmap-mdmg": Phase="Running", Reason="", readiness=true. Elapsed: 8.037282303s
Mar 30 22:34:14.366: INFO: Pod "pod-subpath-test-configmap-mdmg": Phase="Running", Reason="", readiness=true. Elapsed: 10.043608196s
Mar 30 22:34:16.373: INFO: Pod "pod-subpath-test-configmap-mdmg": Phase="Running", Reason="", readiness=true. Elapsed: 12.050852911s
Mar 30 22:34:18.381: INFO: Pod "pod-subpath-test-configmap-mdmg": Phase="Running", Reason="", readiness=true. Elapsed: 14.059049292s
Mar 30 22:34:20.389: INFO: Pod "pod-subpath-test-configmap-mdmg": Phase="Running", Reason="", readiness=true. Elapsed: 16.066730308s
Mar 30 22:34:22.397: INFO: Pod "pod-subpath-test-configmap-mdmg": Phase="Running", Reason="", readiness=true. Elapsed: 18.075181699s
Mar 30 22:34:24.405: INFO: Pod "pod-subpath-test-configmap-mdmg": Phase="Running", Reason="", readiness=true. Elapsed: 20.082504971s
Mar 30 22:34:26.410: INFO: Pod "pod-subpath-test-configmap-mdmg": Phase="Running", Reason="", readiness=true. Elapsed: 22.087964374s
Mar 30 22:34:28.416: INFO: Pod "pod-subpath-test-configmap-mdmg": Phase="Running", Reason="", readiness=true. Elapsed: 24.093972718s
Mar 30 22:34:30.424: INFO: Pod "pod-subpath-test-configmap-mdmg": Phase="Running", Reason="", readiness=true. Elapsed: 26.10142645s
Mar 30 22:34:32.429: INFO: Pod "pod-subpath-test-configmap-mdmg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.106423766s
STEP: Saw pod success
Mar 30 22:34:32.429: INFO: Pod "pod-subpath-test-configmap-mdmg" satisfied condition "success or failure"
Mar 30 22:34:32.434: INFO: Trying to get logs from node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins pod pod-subpath-test-configmap-mdmg container test-container-subpath-configmap-mdmg: <nil>
STEP: delete the pod
Mar 30 22:34:32.469: INFO: Waiting for pod pod-subpath-test-configmap-mdmg to disappear
Mar 30 22:34:32.475: INFO: Pod pod-subpath-test-configmap-mdmg no longer exists
STEP: Deleting pod pod-subpath-test-configmap-mdmg
Mar 30 22:34:32.475: INFO: Deleting pod "pod-subpath-test-configmap-mdmg" in namespace "subpath-9410"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:34:32.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9410" for this suite.

• [SLOW TEST:28.420 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":278,"completed":229,"skipped":3666,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:34:32.519: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1456
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:34:40.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1456" for this suite.

• [SLOW TEST:8.282 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":230,"skipped":3730,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:34:40.809: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-823
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating api versions
Mar 30 22:34:41.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 api-versions'
Mar 30 22:34:41.245: INFO: stderr: ""
Mar 30 22:34:41.245: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nk8s.cni.cncf.io/v1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsnapshot.storage.k8s.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:34:41.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-823" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":278,"completed":231,"skipped":3756,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:34:41.268: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1097
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 30 22:34:49.728: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:34:49.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1097" for this suite.

• [SLOW TEST:8.526 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:131
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":278,"completed":232,"skipped":3765,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:34:49.795: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-315
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-315.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-315.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-315.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-315.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 30 22:35:00.038: INFO: DNS probes using dns-test-5297c4fa-0c62-4233-8b33-94ae1f36340b succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-315.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-315.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-315.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-315.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 30 22:35:10.120: INFO: File wheezy_udp@dns-test-service-3.dns-315.svc.cluster.local from pod  dns-315/dns-test-d0ec233c-d08e-4c14-9407-6bbb56dcab30 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 30 22:35:10.125: INFO: File jessie_udp@dns-test-service-3.dns-315.svc.cluster.local from pod  dns-315/dns-test-d0ec233c-d08e-4c14-9407-6bbb56dcab30 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 30 22:35:10.126: INFO: Lookups using dns-315/dns-test-d0ec233c-d08e-4c14-9407-6bbb56dcab30 failed for: [wheezy_udp@dns-test-service-3.dns-315.svc.cluster.local jessie_udp@dns-test-service-3.dns-315.svc.cluster.local]

Mar 30 22:35:15.134: INFO: File wheezy_udp@dns-test-service-3.dns-315.svc.cluster.local from pod  dns-315/dns-test-d0ec233c-d08e-4c14-9407-6bbb56dcab30 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 30 22:35:15.139: INFO: File jessie_udp@dns-test-service-3.dns-315.svc.cluster.local from pod  dns-315/dns-test-d0ec233c-d08e-4c14-9407-6bbb56dcab30 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 30 22:35:15.139: INFO: Lookups using dns-315/dns-test-d0ec233c-d08e-4c14-9407-6bbb56dcab30 failed for: [wheezy_udp@dns-test-service-3.dns-315.svc.cluster.local jessie_udp@dns-test-service-3.dns-315.svc.cluster.local]

Mar 30 22:35:20.135: INFO: File wheezy_udp@dns-test-service-3.dns-315.svc.cluster.local from pod  dns-315/dns-test-d0ec233c-d08e-4c14-9407-6bbb56dcab30 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 30 22:35:20.139: INFO: File jessie_udp@dns-test-service-3.dns-315.svc.cluster.local from pod  dns-315/dns-test-d0ec233c-d08e-4c14-9407-6bbb56dcab30 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 30 22:35:20.139: INFO: Lookups using dns-315/dns-test-d0ec233c-d08e-4c14-9407-6bbb56dcab30 failed for: [wheezy_udp@dns-test-service-3.dns-315.svc.cluster.local jessie_udp@dns-test-service-3.dns-315.svc.cluster.local]

Mar 30 22:35:25.143: INFO: File wheezy_udp@dns-test-service-3.dns-315.svc.cluster.local from pod  dns-315/dns-test-d0ec233c-d08e-4c14-9407-6bbb56dcab30 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 30 22:35:25.149: INFO: File jessie_udp@dns-test-service-3.dns-315.svc.cluster.local from pod  dns-315/dns-test-d0ec233c-d08e-4c14-9407-6bbb56dcab30 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 30 22:35:25.149: INFO: Lookups using dns-315/dns-test-d0ec233c-d08e-4c14-9407-6bbb56dcab30 failed for: [wheezy_udp@dns-test-service-3.dns-315.svc.cluster.local jessie_udp@dns-test-service-3.dns-315.svc.cluster.local]

Mar 30 22:35:30.147: INFO: DNS probes using dns-test-d0ec233c-d08e-4c14-9407-6bbb56dcab30 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-315.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-315.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-315.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-315.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 30 22:35:40.296: INFO: DNS probes using dns-test-ec2481fa-46b1-46a0-8995-5f00b3ffd3aa succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:35:40.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-315" for this suite.

• [SLOW TEST:50.624 seconds]
[sig-network] DNS
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":278,"completed":233,"skipped":3782,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:35:40.421: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-911
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-7000
STEP: Creating secret with name secret-test-9501912e-6383-4d0f-8627-704db8e551b0
STEP: Creating a pod to test consume secrets
Mar 30 22:35:40.853: INFO: Waiting up to 5m0s for pod "pod-secrets-0f049645-4343-4164-b8a3-ae6ee39651f7" in namespace "secrets-911" to be "success or failure"
Mar 30 22:35:40.861: INFO: Pod "pod-secrets-0f049645-4343-4164-b8a3-ae6ee39651f7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.39357ms
Mar 30 22:35:42.866: INFO: Pod "pod-secrets-0f049645-4343-4164-b8a3-ae6ee39651f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012525785s
Mar 30 22:35:44.872: INFO: Pod "pod-secrets-0f049645-4343-4164-b8a3-ae6ee39651f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018812074s
Mar 30 22:35:46.878: INFO: Pod "pod-secrets-0f049645-4343-4164-b8a3-ae6ee39651f7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024142952s
Mar 30 22:35:48.885: INFO: Pod "pod-secrets-0f049645-4343-4164-b8a3-ae6ee39651f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.03157484s
STEP: Saw pod success
Mar 30 22:35:48.885: INFO: Pod "pod-secrets-0f049645-4343-4164-b8a3-ae6ee39651f7" satisfied condition "success or failure"
Mar 30 22:35:48.890: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-secrets-0f049645-4343-4164-b8a3-ae6ee39651f7 container secret-volume-test: <nil>
STEP: delete the pod
Mar 30 22:35:48.924: INFO: Waiting for pod pod-secrets-0f049645-4343-4164-b8a3-ae6ee39651f7 to disappear
Mar 30 22:35:48.928: INFO: Pod pod-secrets-0f049645-4343-4164-b8a3-ae6ee39651f7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:35:48.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-911" for this suite.
STEP: Destroying namespace "secret-namespace-7000" for this suite.

• [SLOW TEST:8.533 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":278,"completed":234,"skipped":3792,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:35:48.956: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5153
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service nodeport-test with type=NodePort in namespace services-5153
STEP: creating replication controller nodeport-test in namespace services-5153
I0330 22:35:49.183939      20 runners.go:189] Created replication controller with name: nodeport-test, namespace: services-5153, replica count: 2
I0330 22:35:52.237081      20 runners.go:189] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0330 22:35:55.237454      20 runners.go:189] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0330 22:35:58.237887      20 runners.go:189] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 30 22:35:58.238: INFO: Creating new exec pod
Mar 30 22:36:07.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=services-5153 execpod9mglt -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Mar 30 22:36:07.565: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar 30 22:36:07.565: INFO: stdout: ""
Mar 30 22:36:07.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=services-5153 execpod9mglt -- /bin/sh -x -c nc -zv -t -w 2 10.100.96.49 80'
Mar 30 22:36:07.825: INFO: stderr: "+ nc -zv -t -w 2 10.100.96.49 80\nConnection to 10.100.96.49 80 port [tcp/http] succeeded!\n"
Mar 30 22:36:07.825: INFO: stdout: ""
Mar 30 22:36:07.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=services-5153 execpod9mglt -- /bin/sh -x -c nc -zv -t -w 2 10.0.10.5 32688'
Mar 30 22:36:08.081: INFO: stderr: "+ nc -zv -t -w 2 10.0.10.5 32688\nConnection to 10.0.10.5 32688 port [tcp/32688] succeeded!\n"
Mar 30 22:36:08.081: INFO: stdout: ""
Mar 30 22:36:08.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 exec --namespace=services-5153 execpod9mglt -- /bin/sh -x -c nc -zv -t -w 2 10.0.10.4 32688'
Mar 30 22:36:08.329: INFO: stderr: "+ nc -zv -t -w 2 10.0.10.4 32688\nConnection to 10.0.10.4 32688 port [tcp/32688] succeeded!\n"
Mar 30 22:36:08.329: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:36:08.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5153" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:19.396 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":278,"completed":235,"skipped":3799,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:36:08.354: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8533
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Mar 30 22:36:08.541: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:36:19.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8533" for this suite.

• [SLOW TEST:10.987 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":278,"completed":236,"skipped":3809,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:36:19.344: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6747
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 30 22:36:19.520: INFO: Waiting up to 5m0s for pod "pod-ee6fe3f8-9c52-45a3-8349-4079e3ca1e5e" in namespace "emptydir-6747" to be "success or failure"
Mar 30 22:36:19.524: INFO: Pod "pod-ee6fe3f8-9c52-45a3-8349-4079e3ca1e5e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.111049ms
Mar 30 22:36:21.529: INFO: Pod "pod-ee6fe3f8-9c52-45a3-8349-4079e3ca1e5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009447631s
Mar 30 22:36:23.536: INFO: Pod "pod-ee6fe3f8-9c52-45a3-8349-4079e3ca1e5e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015978579s
Mar 30 22:36:25.543: INFO: Pod "pod-ee6fe3f8-9c52-45a3-8349-4079e3ca1e5e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023077882s
Mar 30 22:36:27.550: INFO: Pod "pod-ee6fe3f8-9c52-45a3-8349-4079e3ca1e5e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.030123505s
Mar 30 22:36:29.559: INFO: Pod "pod-ee6fe3f8-9c52-45a3-8349-4079e3ca1e5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.039112784s
STEP: Saw pod success
Mar 30 22:36:29.559: INFO: Pod "pod-ee6fe3f8-9c52-45a3-8349-4079e3ca1e5e" satisfied condition "success or failure"
Mar 30 22:36:29.564: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-ee6fe3f8-9c52-45a3-8349-4079e3ca1e5e container test-container: <nil>
STEP: delete the pod
Mar 30 22:36:29.600: INFO: Waiting for pod pod-ee6fe3f8-9c52-45a3-8349-4079e3ca1e5e to disappear
Mar 30 22:36:29.604: INFO: Pod pod-ee6fe3f8-9c52-45a3-8349-4079e3ca1e5e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:36:29.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6747" for this suite.

• [SLOW TEST:10.283 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":237,"skipped":3811,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:36:29.630: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8911
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-bc8af7b3-3d62-46bc-b880-a81a6287b055
STEP: Creating configMap with name cm-test-opt-upd-e4584694-33ef-4752-9234-28aacb263db5
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-bc8af7b3-3d62-46bc-b880-a81a6287b055
STEP: Updating configmap cm-test-opt-upd-e4584694-33ef-4752-9234-28aacb263db5
STEP: Creating configMap with name cm-test-opt-create-6ba777c5-b090-48ae-a64d-df6810a609e7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:38:02.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8911" for this suite.

• [SLOW TEST:92.947 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":238,"skipped":3822,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:38:02.579: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6569
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 22:38:02.764: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:38:12.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6569" for this suite.

• [SLOW TEST:10.278 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":278,"completed":239,"skipped":3831,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:38:12.859: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9258
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Mar 30 22:38:21.634: INFO: Successfully updated pod "annotationupdatea906668a-afcc-4cd2-8f9c-a188d824ab5b"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:38:25.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9258" for this suite.

• [SLOW TEST:12.843 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":278,"completed":240,"skipped":3838,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:38:25.708: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6188
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-projected-all-test-volume-1afd0b8b-8972-4cbb-9923-b0b079043ec2
STEP: Creating secret with name secret-projected-all-test-volume-3d5f627f-d4fa-4641-88ba-ea633ca2ff6c
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar 30 22:38:25.944: INFO: Waiting up to 5m0s for pod "projected-volume-34d584ba-403a-4d1e-839c-1d13bd1559c3" in namespace "projected-6188" to be "success or failure"
Mar 30 22:38:25.948: INFO: Pod "projected-volume-34d584ba-403a-4d1e-839c-1d13bd1559c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006598ms
Mar 30 22:38:27.954: INFO: Pod "projected-volume-34d584ba-403a-4d1e-839c-1d13bd1559c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009492168s
Mar 30 22:38:29.960: INFO: Pod "projected-volume-34d584ba-403a-4d1e-839c-1d13bd1559c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01551775s
Mar 30 22:38:31.966: INFO: Pod "projected-volume-34d584ba-403a-4d1e-839c-1d13bd1559c3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021670729s
Mar 30 22:38:33.973: INFO: Pod "projected-volume-34d584ba-403a-4d1e-839c-1d13bd1559c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.02846337s
STEP: Saw pod success
Mar 30 22:38:33.973: INFO: Pod "projected-volume-34d584ba-403a-4d1e-839c-1d13bd1559c3" satisfied condition "success or failure"
Mar 30 22:38:33.985: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod projected-volume-34d584ba-403a-4d1e-839c-1d13bd1559c3 container projected-all-volume-test: <nil>
STEP: delete the pod
Mar 30 22:38:34.027: INFO: Waiting for pod projected-volume-34d584ba-403a-4d1e-839c-1d13bd1559c3 to disappear
Mar 30 22:38:34.031: INFO: Pod projected-volume-34d584ba-403a-4d1e-839c-1d13bd1559c3 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:38:34.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6188" for this suite.

• [SLOW TEST:8.340 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":278,"completed":241,"skipped":3911,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:38:34.051: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4182
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 30 22:38:34.264: INFO: Waiting up to 5m0s for pod "downwardapi-volume-36be2a4f-e2c0-4fa4-b974-dbb0ce9b4184" in namespace "projected-4182" to be "success or failure"
Mar 30 22:38:34.273: INFO: Pod "downwardapi-volume-36be2a4f-e2c0-4fa4-b974-dbb0ce9b4184": Phase="Pending", Reason="", readiness=false. Elapsed: 8.687696ms
Mar 30 22:38:36.279: INFO: Pod "downwardapi-volume-36be2a4f-e2c0-4fa4-b974-dbb0ce9b4184": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014697929s
Mar 30 22:38:38.284: INFO: Pod "downwardapi-volume-36be2a4f-e2c0-4fa4-b974-dbb0ce9b4184": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020533367s
Mar 30 22:38:40.289: INFO: Pod "downwardapi-volume-36be2a4f-e2c0-4fa4-b974-dbb0ce9b4184": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025615598s
Mar 30 22:38:42.297: INFO: Pod "downwardapi-volume-36be2a4f-e2c0-4fa4-b974-dbb0ce9b4184": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.03275988s
STEP: Saw pod success
Mar 30 22:38:42.297: INFO: Pod "downwardapi-volume-36be2a4f-e2c0-4fa4-b974-dbb0ce9b4184" satisfied condition "success or failure"
Mar 30 22:38:42.301: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod downwardapi-volume-36be2a4f-e2c0-4fa4-b974-dbb0ce9b4184 container client-container: <nil>
STEP: delete the pod
Mar 30 22:38:42.337: INFO: Waiting for pod downwardapi-volume-36be2a4f-e2c0-4fa4-b974-dbb0ce9b4184 to disappear
Mar 30 22:38:42.340: INFO: Pod downwardapi-volume-36be2a4f-e2c0-4fa4-b974-dbb0ce9b4184 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:38:42.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4182" for this suite.

• [SLOW TEST:8.306 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":278,"completed":242,"skipped":3943,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:38:42.358: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6349
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-c0bcc2de-8117-41f4-b918-8d90d827d6af
STEP: Creating a pod to test consume configMaps
Mar 30 22:38:42.567: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5ff181a9-5606-4aa0-bb5c-5bece5b868d0" in namespace "projected-6349" to be "success or failure"
Mar 30 22:38:42.581: INFO: Pod "pod-projected-configmaps-5ff181a9-5606-4aa0-bb5c-5bece5b868d0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.695567ms
Mar 30 22:38:44.588: INFO: Pod "pod-projected-configmaps-5ff181a9-5606-4aa0-bb5c-5bece5b868d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021042429s
Mar 30 22:38:46.603: INFO: Pod "pod-projected-configmaps-5ff181a9-5606-4aa0-bb5c-5bece5b868d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035398846s
Mar 30 22:38:48.610: INFO: Pod "pod-projected-configmaps-5ff181a9-5606-4aa0-bb5c-5bece5b868d0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.042782553s
Mar 30 22:38:50.617: INFO: Pod "pod-projected-configmaps-5ff181a9-5606-4aa0-bb5c-5bece5b868d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.049243854s
STEP: Saw pod success
Mar 30 22:38:50.617: INFO: Pod "pod-projected-configmaps-5ff181a9-5606-4aa0-bb5c-5bece5b868d0" satisfied condition "success or failure"
Mar 30 22:38:50.620: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-projected-configmaps-5ff181a9-5606-4aa0-bb5c-5bece5b868d0 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 30 22:38:50.661: INFO: Waiting for pod pod-projected-configmaps-5ff181a9-5606-4aa0-bb5c-5bece5b868d0 to disappear
Mar 30 22:38:50.664: INFO: Pod pod-projected-configmaps-5ff181a9-5606-4aa0-bb5c-5bece5b868d0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:38:50.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6349" for this suite.

• [SLOW TEST:8.336 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":243,"skipped":3949,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:38:50.699: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6592
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 22:38:50.890: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:38:52.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6592" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":278,"completed":244,"skipped":3961,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:38:52.280: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5499
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 30 22:38:52.627: INFO: Waiting up to 5m0s for pod "downwardapi-volume-308cb5a0-01af-4887-8822-4fc80def3d25" in namespace "projected-5499" to be "success or failure"
Mar 30 22:38:52.632: INFO: Pod "downwardapi-volume-308cb5a0-01af-4887-8822-4fc80def3d25": Phase="Pending", Reason="", readiness=false. Elapsed: 5.255121ms
Mar 30 22:38:54.640: INFO: Pod "downwardapi-volume-308cb5a0-01af-4887-8822-4fc80def3d25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012753476s
Mar 30 22:38:56.647: INFO: Pod "downwardapi-volume-308cb5a0-01af-4887-8822-4fc80def3d25": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02023148s
Mar 30 22:38:58.654: INFO: Pod "downwardapi-volume-308cb5a0-01af-4887-8822-4fc80def3d25": Phase="Pending", Reason="", readiness=false. Elapsed: 6.0264204s
Mar 30 22:39:00.660: INFO: Pod "downwardapi-volume-308cb5a0-01af-4887-8822-4fc80def3d25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.032900348s
STEP: Saw pod success
Mar 30 22:39:00.660: INFO: Pod "downwardapi-volume-308cb5a0-01af-4887-8822-4fc80def3d25" satisfied condition "success or failure"
Mar 30 22:39:00.665: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod downwardapi-volume-308cb5a0-01af-4887-8822-4fc80def3d25 container client-container: <nil>
STEP: delete the pod
Mar 30 22:39:00.701: INFO: Waiting for pod downwardapi-volume-308cb5a0-01af-4887-8822-4fc80def3d25 to disappear
Mar 30 22:39:00.706: INFO: Pod downwardapi-volume-308cb5a0-01af-4887-8822-4fc80def3d25 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:39:00.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5499" for this suite.

• [SLOW TEST:8.449 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":278,"completed":245,"skipped":3990,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:39:00.730: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4774
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-42dd1bc1-0c95-49f8-8cf9-6a8e8acef399
STEP: Creating a pod to test consume secrets
Mar 30 22:39:00.916: INFO: Waiting up to 5m0s for pod "pod-secrets-18fb215f-ef7e-41f2-9e10-9a57deb635af" in namespace "secrets-4774" to be "success or failure"
Mar 30 22:39:00.925: INFO: Pod "pod-secrets-18fb215f-ef7e-41f2-9e10-9a57deb635af": Phase="Pending", Reason="", readiness=false. Elapsed: 8.140845ms
Mar 30 22:39:02.930: INFO: Pod "pod-secrets-18fb215f-ef7e-41f2-9e10-9a57deb635af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013739898s
Mar 30 22:39:04.936: INFO: Pod "pod-secrets-18fb215f-ef7e-41f2-9e10-9a57deb635af": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020018081s
Mar 30 22:39:06.944: INFO: Pod "pod-secrets-18fb215f-ef7e-41f2-9e10-9a57deb635af": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027430617s
Mar 30 22:39:08.951: INFO: Pod "pod-secrets-18fb215f-ef7e-41f2-9e10-9a57deb635af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.034187428s
STEP: Saw pod success
Mar 30 22:39:08.951: INFO: Pod "pod-secrets-18fb215f-ef7e-41f2-9e10-9a57deb635af" satisfied condition "success or failure"
Mar 30 22:39:08.955: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-secrets-18fb215f-ef7e-41f2-9e10-9a57deb635af container secret-volume-test: <nil>
STEP: delete the pod
Mar 30 22:39:09.001: INFO: Waiting for pod pod-secrets-18fb215f-ef7e-41f2-9e10-9a57deb635af to disappear
Mar 30 22:39:09.005: INFO: Pod pod-secrets-18fb215f-ef7e-41f2-9e10-9a57deb635af no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:39:09.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4774" for this suite.

• [SLOW TEST:8.293 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":278,"completed":246,"skipped":3993,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:39:09.027: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5230
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's command
Mar 30 22:39:09.220: INFO: Waiting up to 5m0s for pod "var-expansion-cb0c1e2e-0691-4205-ae3c-f971b25b9576" in namespace "var-expansion-5230" to be "success or failure"
Mar 30 22:39:09.224: INFO: Pod "var-expansion-cb0c1e2e-0691-4205-ae3c-f971b25b9576": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068074ms
Mar 30 22:39:11.229: INFO: Pod "var-expansion-cb0c1e2e-0691-4205-ae3c-f971b25b9576": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009353661s
Mar 30 22:39:13.234: INFO: Pod "var-expansion-cb0c1e2e-0691-4205-ae3c-f971b25b9576": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014170951s
Mar 30 22:39:15.239: INFO: Pod "var-expansion-cb0c1e2e-0691-4205-ae3c-f971b25b9576": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019215278s
Mar 30 22:39:17.247: INFO: Pod "var-expansion-cb0c1e2e-0691-4205-ae3c-f971b25b9576": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.027412345s
STEP: Saw pod success
Mar 30 22:39:17.248: INFO: Pod "var-expansion-cb0c1e2e-0691-4205-ae3c-f971b25b9576" satisfied condition "success or failure"
Mar 30 22:39:17.251: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod var-expansion-cb0c1e2e-0691-4205-ae3c-f971b25b9576 container dapi-container: <nil>
STEP: delete the pod
Mar 30 22:39:17.290: INFO: Waiting for pod var-expansion-cb0c1e2e-0691-4205-ae3c-f971b25b9576 to disappear
Mar 30 22:39:17.294: INFO: Pod var-expansion-cb0c1e2e-0691-4205-ae3c-f971b25b9576 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:39:17.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5230" for this suite.

• [SLOW TEST:8.285 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":278,"completed":247,"skipped":4002,"failed":0}
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:39:17.313: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-5977
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Mar 30 22:39:17.497: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the sample API server.
Mar 30 22:39:18.478: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Mar 30 22:39:20.587: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:39:22.599: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:39:24.594: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:39:26.594: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:39:28.595: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:39:30.592: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:39:32.594: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:39:34.593: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:39:36.593: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204758, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:39:40.156: INFO: Waited 1.537804686s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:39:40.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5977" for this suite.

• [SLOW TEST:23.651 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]","total":278,"completed":248,"skipped":4002,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:39:40.964: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4841
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 30 22:39:41.173: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9ad4bdeb-0222-4638-a1b7-8ea24f0b0b15" in namespace "downward-api-4841" to be "success or failure"
Mar 30 22:39:41.184: INFO: Pod "downwardapi-volume-9ad4bdeb-0222-4638-a1b7-8ea24f0b0b15": Phase="Pending", Reason="", readiness=false. Elapsed: 11.048536ms
Mar 30 22:39:43.190: INFO: Pod "downwardapi-volume-9ad4bdeb-0222-4638-a1b7-8ea24f0b0b15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017451048s
Mar 30 22:39:45.195: INFO: Pod "downwardapi-volume-9ad4bdeb-0222-4638-a1b7-8ea24f0b0b15": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022315766s
Mar 30 22:39:47.200: INFO: Pod "downwardapi-volume-9ad4bdeb-0222-4638-a1b7-8ea24f0b0b15": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027741863s
Mar 30 22:39:49.206: INFO: Pod "downwardapi-volume-9ad4bdeb-0222-4638-a1b7-8ea24f0b0b15": Phase="Pending", Reason="", readiness=false. Elapsed: 8.033238946s
Mar 30 22:39:51.212: INFO: Pod "downwardapi-volume-9ad4bdeb-0222-4638-a1b7-8ea24f0b0b15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.039662419s
STEP: Saw pod success
Mar 30 22:39:51.212: INFO: Pod "downwardapi-volume-9ad4bdeb-0222-4638-a1b7-8ea24f0b0b15" satisfied condition "success or failure"
Mar 30 22:39:51.218: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod downwardapi-volume-9ad4bdeb-0222-4638-a1b7-8ea24f0b0b15 container client-container: <nil>
STEP: delete the pod
Mar 30 22:39:51.262: INFO: Waiting for pod downwardapi-volume-9ad4bdeb-0222-4638-a1b7-8ea24f0b0b15 to disappear
Mar 30 22:39:51.266: INFO: Pod downwardapi-volume-9ad4bdeb-0222-4638-a1b7-8ea24f0b0b15 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:39:51.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4841" for this suite.

• [SLOW TEST:10.329 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":278,"completed":249,"skipped":4011,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:39:51.295: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4308
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:40:07.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4308" for this suite.

• [SLOW TEST:16.282 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":278,"completed":250,"skipped":4012,"failed":0}
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:40:07.578: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-124
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:178
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:40:07.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-124" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":278,"completed":251,"skipped":4012,"failed":0}
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:40:07.822: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4374
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-4374
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 30 22:40:08.021: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 30 22:40:44.270: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.14.86:8080/dial?request=hostname&protocol=http&host=192.168.125.32&port=8080&tries=1'] Namespace:pod-network-test-4374 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 30 22:40:44.270: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 22:40:44.482: INFO: Waiting for responses: map[]
Mar 30 22:40:44.487: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.14.86:8080/dial?request=hostname&protocol=http&host=192.168.14.87&port=8080&tries=1'] Namespace:pod-network-test-4374 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 30 22:40:44.487: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 22:40:44.677: INFO: Waiting for responses: map[]
Mar 30 22:40:44.683: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.14.86:8080/dial?request=hostname&protocol=http&host=192.168.185.219&port=8080&tries=1'] Namespace:pod-network-test-4374 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 30 22:40:44.683: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 22:40:44.893: INFO: Waiting for responses: map[]
Mar 30 22:40:44.903: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.14.86:8080/dial?request=hostname&protocol=http&host=192.168.10.241&port=8080&tries=1'] Namespace:pod-network-test-4374 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 30 22:40:44.903: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
Mar 30 22:40:45.081: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:40:45.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4374" for this suite.

• [SLOW TEST:37.282 seconds]
[sig-network] Networking
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":252,"skipped":4016,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:40:45.116: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-1218
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 22:40:45.301: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Creating first CR 
Mar 30 22:40:45.967: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-30T22:40:45Z generation:1 name:name1 resourceVersion:50720 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:b731dfd4-ac00-42ed-8fe1-56395541997c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Mar 30 22:40:55.980: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-30T22:40:55Z generation:1 name:name2 resourceVersion:50824 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:f4212703-94ed-45db-8a1c-222539fb5bec] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Mar 30 22:41:05.997: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-30T22:40:45Z generation:2 name:name1 resourceVersion:50864 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:b731dfd4-ac00-42ed-8fe1-56395541997c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Mar 30 22:41:16.009: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-30T22:40:55Z generation:2 name:name2 resourceVersion:50905 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:f4212703-94ed-45db-8a1c-222539fb5bec] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Mar 30 22:41:26.094: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-30T22:40:45Z generation:2 name:name1 resourceVersion:50947 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:b731dfd4-ac00-42ed-8fe1-56395541997c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Mar 30 22:41:36.111: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-30T22:40:55Z generation:2 name:name2 resourceVersion:50987 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:f4212703-94ed-45db-8a1c-222539fb5bec] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:41:46.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-1218" for this suite.

• [SLOW TEST:61.561 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:41
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":278,"completed":253,"skipped":4042,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:41:46.678: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3273
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:41:55.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3273" for this suite.

• [SLOW TEST:9.306 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":278,"completed":254,"skipped":4065,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:41:55.992: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4357
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:42:12.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4357" for this suite.

• [SLOW TEST:16.382 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":278,"completed":255,"skipped":4099,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:42:12.381: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6914
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Mar 30 22:42:12.571: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:42:35.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6914" for this suite.

• [SLOW TEST:23.062 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":278,"completed":256,"skipped":4110,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:42:35.443: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-4363
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:42:43.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4363" for this suite.

• [SLOW TEST:8.320 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":278,"completed":257,"skipped":4113,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:42:43.765: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8282
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 30 22:42:44.650: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 30 22:42:46.665: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204964, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204964, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204964, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204964, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:42:48.677: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204964, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204964, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204964, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204964, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:42:50.672: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204964, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204964, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204964, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204964, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:42:52.672: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204964, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204964, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204964, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721204964, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 30 22:42:55.700: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:42:55.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8282" for this suite.
STEP: Destroying namespace "webhook-8282-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:12.211 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":278,"completed":258,"skipped":4142,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:42:55.981: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-3894
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:43:16.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3894" for this suite.

• [SLOW TEST:20.364 seconds]
[sig-apps] Job
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":278,"completed":259,"skipped":4153,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:43:16.346: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6866
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Mar 30 22:43:16.540: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 30 22:43:16.571: INFO: Waiting for terminating namespaces to be deleted...
Mar 30 22:43:16.581: INFO: 
Logging pods the kubelet thinks is on node worker-pool1-9jnl5j0t-eccd-ci-os-12-jenkins before test
Mar 30 22:43:16.597: INFO: kube-multus-ds-amd64-vm7r6 from kube-system started at 2020-03-30 20:48:53 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.597: INFO: 	Container kube-multus ready: true, restart count 0
Mar 30 22:43:16.597: INFO: eric-pm-server-eric-pm-server-7b4dd54bc5-m7fjg from monitoring started at 2020-03-30 20:51:51 +0000 UTC (2 container statuses recorded)
Mar 30 22:43:16.597: INFO: 	Container eric-pm-server-eric-pm-server ready: true, restart count 0
Mar 30 22:43:16.597: INFO: 	Container eric-pm-server-eric-pm-server-eric-pm-configmap-reload ready: true, restart count 0
Mar 30 22:43:16.597: INFO: eric-pm-server-node-exporter-m4kjc from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.597: INFO: 	Container eric-pm-server-node-exporter ready: true, restart count 0
Mar 30 22:43:16.597: INFO: sonobuoy from sonobuoy started at 2020-03-30 21:04:25 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.597: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 30 22:43:16.597: INFO: sonobuoy-systemd-logs-daemon-set-4495c60740044637-62zgj from sonobuoy started at 2020-03-30 21:04:37 +0000 UTC (2 container statuses recorded)
Mar 30 22:43:16.597: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 30 22:43:16.597: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 30 22:43:16.597: INFO: eric-tm-external-connectivity-frontend-speaker-vmndg from kube-system started at 2020-03-30 20:50:36 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.597: INFO: 	Container speaker ready: true, restart count 0
Mar 30 22:43:16.597: INFO: eric-pm-server-pushgateway-7798d479ff-fcnk9 from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.597: INFO: 	Container eric-pm-server-pushgateway ready: true, restart count 0
Mar 30 22:43:16.597: INFO: calico-node-kswgg from kube-system started at 2020-03-30 20:48:23 +0000 UTC (2 container statuses recorded)
Mar 30 22:43:16.597: INFO: 	Container calico-node ready: true, restart count 0
Mar 30 22:43:16.597: INFO: 	Container install-cni ready: true, restart count 0
Mar 30 22:43:16.597: INFO: kube-proxy-746bb from kube-system started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.597: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 30 22:43:16.597: INFO: csi-cinder-nodeplugin-ftdf2 from kube-system started at 2020-03-30 20:48:53 +0000 UTC (2 container statuses recorded)
Mar 30 22:43:16.597: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 30 22:43:16.597: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 30 22:43:16.597: INFO: eric-lm-combined-server-license-consumer-handler-58597bc9679zxp from kube-system started at 2020-03-30 20:54:37 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.597: INFO: 	Container eric-lm-license-consumer-handler ready: true, restart count 0
Mar 30 22:43:16.597: INFO: ccd-license-consumer-5c7ff9fc96-b2hn2 from kube-system started at 2020-03-30 20:55:33 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.597: INFO: 	Container ccd-license-consumer ready: true, restart count 0
Mar 30 22:43:16.597: INFO: 
Logging pods the kubelet thinks is on node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins before test
Mar 30 22:43:16.618: INFO: kube-multus-ds-amd64-q9869 from kube-system started at 2020-03-30 20:48:57 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.618: INFO: 	Container kube-multus ready: true, restart count 0
Mar 30 22:43:16.618: INFO: eric-pm-server-alertmanager-648979cdd-z57mc from monitoring started at 2020-03-30 20:51:51 +0000 UTC (2 container statuses recorded)
Mar 30 22:43:16.618: INFO: 	Container eric-pm-server-alertmanager ready: true, restart count 0
Mar 30 22:43:16.618: INFO: 	Container eric-pm-server-alertmanager-configmap-reload-for-alertmanager ready: true, restart count 0
Mar 30 22:43:16.618: INFO: sonobuoy-systemd-logs-daemon-set-4495c60740044637-gqxkr from sonobuoy started at 2020-03-30 21:04:37 +0000 UTC (2 container statuses recorded)
Mar 30 22:43:16.618: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 30 22:43:16.618: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 30 22:43:16.618: INFO: fail-once-local-tltqw from job-3894 started at 2020-03-30 22:42:56 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.618: INFO: 	Container c ready: false, restart count 1
Mar 30 22:43:16.618: INFO: calico-node-lrw4b from kube-system started at 2020-03-30 20:48:27 +0000 UTC (2 container statuses recorded)
Mar 30 22:43:16.618: INFO: 	Container calico-node ready: true, restart count 0
Mar 30 22:43:16.618: INFO: 	Container install-cni ready: true, restart count 0
Mar 30 22:43:16.618: INFO: csi-cinder-nodeplugin-wnrkx from kube-system started at 2020-03-30 20:48:57 +0000 UTC (2 container statuses recorded)
Mar 30 22:43:16.618: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 30 22:43:16.618: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 30 22:43:16.618: INFO: eric-pm-server-node-exporter-v5bmh from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.618: INFO: 	Container eric-pm-server-node-exporter ready: true, restart count 0
Mar 30 22:43:16.618: INFO: kube-proxy-9w8kt from kube-system started at 2020-03-30 20:48:27 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.618: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 30 22:43:16.618: INFO: fail-once-local-2k78x from job-3894 started at 2020-03-30 22:43:04 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.618: INFO: 	Container c ready: false, restart count 1
Mar 30 22:43:16.618: INFO: eric-tm-external-connectivity-frontend-controller-78f9fd87bvc6l from kube-system started at 2020-03-30 20:50:36 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.618: INFO: 	Container controller ready: true, restart count 0
Mar 30 22:43:16.618: INFO: eric-tm-external-connectivity-frontend-speaker-m4f4p from kube-system started at 2020-03-30 20:50:36 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.618: INFO: 	Container speaker ready: true, restart count 0
Mar 30 22:43:16.618: INFO: metrics-server-8666db6c57-znld5 from kube-system started at 2020-03-30 20:53:13 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.618: INFO: 	Container metrics-server ready: true, restart count 0
Mar 30 22:43:16.618: INFO: eric-lm-combined-server-license-server-client-945b6bc4c-5phm5 from kube-system started at 2020-03-30 20:54:37 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.618: INFO: 	Container eric-lm-license-server-client ready: true, restart count 0
Mar 30 22:43:16.618: INFO: 
Logging pods the kubelet thinks is on node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins before test
Mar 30 22:43:16.635: INFO: calico-node-hf278 from kube-system started at 2020-03-30 20:48:23 +0000 UTC (2 container statuses recorded)
Mar 30 22:43:16.635: INFO: 	Container calico-node ready: true, restart count 0
Mar 30 22:43:16.635: INFO: 	Container install-cni ready: true, restart count 0
Mar 30 22:43:16.635: INFO: eric-pm-server-node-exporter-9cwgm from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.635: INFO: 	Container eric-pm-server-node-exporter ready: true, restart count 0
Mar 30 22:43:16.635: INFO: eric-pm-server-kube-state-metrics-66f7fbfd44-k7krp from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.635: INFO: 	Container eric-pm-server-kube-state-metrics ready: true, restart count 0
Mar 30 22:43:16.635: INFO: fail-once-local-c9vmw from job-3894 started at 2020-03-30 22:42:56 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.635: INFO: 	Container c ready: false, restart count 1
Mar 30 22:43:16.635: INFO: eric-tm-external-connectivity-frontend-speaker-8tmb7 from kube-system started at 2020-03-30 20:50:36 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.635: INFO: 	Container speaker ready: true, restart count 0
Mar 30 22:43:16.635: INFO: kube-proxy-4mzsc from kube-system started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.635: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 30 22:43:16.635: INFO: kube-multus-ds-amd64-9mjsk from kube-system started at 2020-03-30 20:48:53 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.635: INFO: 	Container kube-multus ready: true, restart count 0
Mar 30 22:43:16.635: INFO: csi-cinder-nodeplugin-cfp5q from kube-system started at 2020-03-30 20:48:53 +0000 UTC (2 container statuses recorded)
Mar 30 22:43:16.635: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 30 22:43:16.635: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 30 22:43:16.635: INFO: eric-lcm-container-registry-registry-0 from kube-system started at 2020-03-30 20:49:39 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.635: INFO: 	Container registry ready: true, restart count 0
Mar 30 22:43:16.635: INFO: postgresql-postgresql-0 from kube-system started at 2020-03-30 20:53:52 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.635: INFO: 	Container postgresql ready: true, restart count 0
Mar 30 22:43:16.635: INFO: sonobuoy-systemd-logs-daemon-set-4495c60740044637-5rfld from sonobuoy started at 2020-03-30 21:04:37 +0000 UTC (2 container statuses recorded)
Mar 30 22:43:16.635: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 30 22:43:16.635: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 30 22:43:16.635: INFO: fail-once-local-wmf2c from job-3894 started at 2020-03-30 22:43:05 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.635: INFO: 	Container c ready: false, restart count 1
Mar 30 22:43:16.635: INFO: 
Logging pods the kubelet thinks is on node worker-pool1-zo88v95j-eccd-ci-os-12-jenkins before test
Mar 30 22:43:16.649: INFO: default-http-backend-6664c884c9-dxczc from ingress-nginx started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.649: INFO: 	Container default-http-backend ready: true, restart count 0
Mar 30 22:43:16.649: INFO: sonobuoy-systemd-logs-daemon-set-4495c60740044637-v7t76 from sonobuoy started at 2020-03-30 21:04:37 +0000 UTC (2 container statuses recorded)
Mar 30 22:43:16.649: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 30 22:43:16.649: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 30 22:43:16.649: INFO: calico-node-xk89l from kube-system started at 2020-03-30 20:47:51 +0000 UTC (2 container statuses recorded)
Mar 30 22:43:16.649: INFO: 	Container calico-node ready: true, restart count 0
Mar 30 22:43:16.649: INFO: 	Container install-cni ready: true, restart count 0
Mar 30 22:43:16.649: INFO: csi-cinder-nodeplugin-qgrf4 from kube-system started at 2020-03-30 20:48:21 +0000 UTC (2 container statuses recorded)
Mar 30 22:43:16.649: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar 30 22:43:16.650: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 30 22:43:16.650: INFO: nginx-ingress-controller-78f766b979-bfx4q from ingress-nginx started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.650: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 30 22:43:16.650: INFO: kube-proxy-pcd5r from kube-system started at 2020-03-30 20:47:51 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.650: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 30 22:43:16.650: INFO: nginx-ingress-controller-78f766b979-wf7qb from ingress-nginx started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.650: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 30 22:43:16.650: INFO: eric-tm-external-connectivity-frontend-speaker-zwnfr from kube-system started at 2020-03-30 20:50:36 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.650: INFO: 	Container speaker ready: true, restart count 0
Mar 30 22:43:16.650: INFO: tiller-deploy-cd77547bd-2pv7h from kube-system started at 2020-03-30 20:48:23 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.650: INFO: 	Container tiller ready: true, restart count 0
Mar 30 22:43:16.650: INFO: kube-multus-ds-amd64-6t2pw from kube-system started at 2020-03-30 20:48:21 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.650: INFO: 	Container kube-multus ready: true, restart count 0
Mar 30 22:43:16.650: INFO: eric-pm-server-node-exporter-ktbpn from monitoring started at 2020-03-30 20:51:51 +0000 UTC (1 container statuses recorded)
Mar 30 22:43:16.650: INFO: 	Container eric-pm-server-node-exporter ready: true, restart count 0
Mar 30 22:43:16.650: INFO: sonobuoy-e2e-job-14477a96f0f5465c from sonobuoy started at 2020-03-30 21:04:37 +0000 UTC (2 container statuses recorded)
Mar 30 22:43:16.650: INFO: 	Container e2e ready: true, restart count 0
Mar 30 22:43:16.650: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-06305852-7701-4912-866e-31adcc823f46 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-06305852-7701-4912-866e-31adcc823f46 off the node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins
STEP: verifying the node doesn't have the label kubernetes.io/e2e-06305852-7701-4912-866e-31adcc823f46
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:43:54.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6866" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:38.612 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":278,"completed":260,"skipped":4158,"failed":0}
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:43:54.959: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5768
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar 30 22:43:55.185: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5768 /api/v1/namespaces/watch-5768/configmaps/e2e-watch-test-label-changed feaf6c8b-06f1-412a-a924-063c55806129 52051 0 2020-03-30 22:43:55 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 30 22:43:55.185: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5768 /api/v1/namespaces/watch-5768/configmaps/e2e-watch-test-label-changed feaf6c8b-06f1-412a-a924-063c55806129 52052 0 2020-03-30 22:43:55 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar 30 22:43:55.185: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5768 /api/v1/namespaces/watch-5768/configmaps/e2e-watch-test-label-changed feaf6c8b-06f1-412a-a924-063c55806129 52053 0 2020-03-30 22:43:55 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar 30 22:44:05.238: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5768 /api/v1/namespaces/watch-5768/configmaps/e2e-watch-test-label-changed feaf6c8b-06f1-412a-a924-063c55806129 52144 0 2020-03-30 22:43:55 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 30 22:44:05.239: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5768 /api/v1/namespaces/watch-5768/configmaps/e2e-watch-test-label-changed feaf6c8b-06f1-412a-a924-063c55806129 52145 0 2020-03-30 22:43:55 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Mar 30 22:44:05.239: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5768 /api/v1/namespaces/watch-5768/configmaps/e2e-watch-test-label-changed feaf6c8b-06f1-412a-a924-063c55806129 52146 0 2020-03-30 22:43:55 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:44:05.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5768" for this suite.

• [SLOW TEST:10.303 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":278,"completed":261,"skipped":4158,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:44:05.264: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1568
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run default
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1596
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 30 22:44:05.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-1568'
Mar 30 22:44:05.838: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 30 22:44:05.838: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1602
Mar 30 22:44:07.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-408641372 delete deployment e2e-test-httpd-deployment --namespace=kubectl-1568'
Mar 30 22:44:07.974: INFO: stderr: ""
Mar 30 22:44:07.974: INFO: stdout: "deployment.extensions \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:44:07.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1568" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run default should create an rc or deployment from an image  [Conformance]","total":278,"completed":262,"skipped":4171,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:44:08.033: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8499
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 22:44:34.279: INFO: Container started at 2020-03-30 22:44:15 +0000 UTC, pod became ready at 2020-03-30 22:44:32 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:44:34.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8499" for this suite.

• [SLOW TEST:26.266 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":278,"completed":263,"skipped":4185,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:44:34.303: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9672
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Mar 30 22:44:34.491: INFO: PodSpec: initContainers in spec.initContainers
Mar 30 22:45:25.749: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-eb36269a-9277-4985-85d1-5f6d7bd0d44e", GenerateName:"", Namespace:"init-container-9672", SelfLink:"/api/v1/namespaces/init-container-9672/pods/pod-init-eb36269a-9277-4985-85d1-5f6d7bd0d44e", UID:"d7aba678-4b88-46c6-86d2-7b7147bf7ceb", ResourceVersion:"52605", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63721205074, loc:(*time.Location)(0x7db7bc0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"490970817"}, Annotations:map[string]string{"k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.14.97\"\n    ],\n    \"dns\": {}\n}]", "kubernetes.io/psp":"ccd-privileged"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-8j9mn", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc004b34300), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-8j9mn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-8j9mn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-8j9mn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc006463fa8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"worker-pool1-tjim0te5-eccd-ci-os-12-jenkins", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002e301e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003d4a060)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003d4a080)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003d4a088), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003d4a08c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205074, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205074, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205074, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205074, loc:(*time.Location)(0x7db7bc0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.10.5", PodIP:"192.168.14.97", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.14.97"}}, StartTime:(*v1.Time)(0xc00345e340), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000722310)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000722380)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://3e7e1d76642066a4b0ad9577aa95bf4196ae0890783cc6718326e1b1c7f860a3", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00345e380), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00345e360), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc003d4a10f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:45:25.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9672" for this suite.

• [SLOW TEST:51.471 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":278,"completed":264,"skipped":4213,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:45:25.776: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9266
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar 30 22:45:26.033: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6d7d19b5-ca47-448e-86ee-c895b74b33b1" in namespace "downward-api-9266" to be "success or failure"
Mar 30 22:45:26.044: INFO: Pod "downwardapi-volume-6d7d19b5-ca47-448e-86ee-c895b74b33b1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.293885ms
Mar 30 22:45:28.052: INFO: Pod "downwardapi-volume-6d7d19b5-ca47-448e-86ee-c895b74b33b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01875209s
Mar 30 22:45:30.057: INFO: Pod "downwardapi-volume-6d7d19b5-ca47-448e-86ee-c895b74b33b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023575274s
Mar 30 22:45:32.061: INFO: Pod "downwardapi-volume-6d7d19b5-ca47-448e-86ee-c895b74b33b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028091923s
Mar 30 22:45:34.071: INFO: Pod "downwardapi-volume-6d7d19b5-ca47-448e-86ee-c895b74b33b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.037731779s
STEP: Saw pod success
Mar 30 22:45:34.071: INFO: Pod "downwardapi-volume-6d7d19b5-ca47-448e-86ee-c895b74b33b1" satisfied condition "success or failure"
Mar 30 22:45:34.076: INFO: Trying to get logs from node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins pod downwardapi-volume-6d7d19b5-ca47-448e-86ee-c895b74b33b1 container client-container: <nil>
STEP: delete the pod
Mar 30 22:45:34.129: INFO: Waiting for pod downwardapi-volume-6d7d19b5-ca47-448e-86ee-c895b74b33b1 to disappear
Mar 30 22:45:34.139: INFO: Pod downwardapi-volume-6d7d19b5-ca47-448e-86ee-c895b74b33b1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:45:34.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9266" for this suite.

• [SLOW TEST:8.393 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":278,"completed":265,"skipped":4236,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:45:34.171: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5649
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-5cd02080-ac95-48bb-bb75-887bd1531370
STEP: Creating a pod to test consume secrets
Mar 30 22:45:34.390: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b6bba006-4759-4ea7-9269-fdc34223d35b" in namespace "projected-5649" to be "success or failure"
Mar 30 22:45:34.396: INFO: Pod "pod-projected-secrets-b6bba006-4759-4ea7-9269-fdc34223d35b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.118149ms
Mar 30 22:45:36.408: INFO: Pod "pod-projected-secrets-b6bba006-4759-4ea7-9269-fdc34223d35b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017714102s
Mar 30 22:45:38.412: INFO: Pod "pod-projected-secrets-b6bba006-4759-4ea7-9269-fdc34223d35b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021650176s
Mar 30 22:45:40.418: INFO: Pod "pod-projected-secrets-b6bba006-4759-4ea7-9269-fdc34223d35b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028025125s
Mar 30 22:45:42.426: INFO: Pod "pod-projected-secrets-b6bba006-4759-4ea7-9269-fdc34223d35b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.036353062s
Mar 30 22:45:44.432: INFO: Pod "pod-projected-secrets-b6bba006-4759-4ea7-9269-fdc34223d35b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.041551542s
STEP: Saw pod success
Mar 30 22:45:44.432: INFO: Pod "pod-projected-secrets-b6bba006-4759-4ea7-9269-fdc34223d35b" satisfied condition "success or failure"
Mar 30 22:45:44.436: INFO: Trying to get logs from node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins pod pod-projected-secrets-b6bba006-4759-4ea7-9269-fdc34223d35b container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 30 22:45:44.477: INFO: Waiting for pod pod-projected-secrets-b6bba006-4759-4ea7-9269-fdc34223d35b to disappear
Mar 30 22:45:44.481: INFO: Pod pod-projected-secrets-b6bba006-4759-4ea7-9269-fdc34223d35b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:45:44.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5649" for this suite.

• [SLOW TEST:10.330 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":266,"skipped":4258,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:45:44.512: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8993
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 30 22:45:45.341: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 30 22:45:47.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205145, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205145, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205145, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205145, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:45:49.360: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205145, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205145, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205145, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205145, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:45:51.359: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205145, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205145, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205145, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205145, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:45:53.363: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205145, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205145, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205145, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205145, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 30 22:45:56.439: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:45:56.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8993" for this suite.
STEP: Destroying namespace "webhook-8993-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:12.441 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":278,"completed":267,"skipped":4291,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:45:56.955: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-7134
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:46
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:45:57.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7134" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":278,"completed":268,"skipped":4304,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:45:57.174: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8107
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-67b3ac19-0591-47de-8cdf-5b6d79c86d43
STEP: Creating a pod to test consume secrets
Mar 30 22:45:57.376: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9c7e57fa-8fa6-4ac0-aa61-9691158d9cba" in namespace "projected-8107" to be "success or failure"
Mar 30 22:45:57.384: INFO: Pod "pod-projected-secrets-9c7e57fa-8fa6-4ac0-aa61-9691158d9cba": Phase="Pending", Reason="", readiness=false. Elapsed: 7.798022ms
Mar 30 22:45:59.389: INFO: Pod "pod-projected-secrets-9c7e57fa-8fa6-4ac0-aa61-9691158d9cba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012892142s
Mar 30 22:46:01.396: INFO: Pod "pod-projected-secrets-9c7e57fa-8fa6-4ac0-aa61-9691158d9cba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019331932s
Mar 30 22:46:03.401: INFO: Pod "pod-projected-secrets-9c7e57fa-8fa6-4ac0-aa61-9691158d9cba": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024807483s
Mar 30 22:46:05.407: INFO: Pod "pod-projected-secrets-9c7e57fa-8fa6-4ac0-aa61-9691158d9cba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.0305977s
STEP: Saw pod success
Mar 30 22:46:05.407: INFO: Pod "pod-projected-secrets-9c7e57fa-8fa6-4ac0-aa61-9691158d9cba" satisfied condition "success or failure"
Mar 30 22:46:05.411: INFO: Trying to get logs from node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins pod pod-projected-secrets-9c7e57fa-8fa6-4ac0-aa61-9691158d9cba container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 30 22:46:05.446: INFO: Waiting for pod pod-projected-secrets-9c7e57fa-8fa6-4ac0-aa61-9691158d9cba to disappear
Mar 30 22:46:05.451: INFO: Pod pod-projected-secrets-9c7e57fa-8fa6-4ac0-aa61-9691158d9cba no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:46:05.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8107" for this suite.

• [SLOW TEST:8.296 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":269,"skipped":4312,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:46:05.471: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4797
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 30 22:46:05.994: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 30 22:46:08.008: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205166, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205166, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205166, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205165, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:46:10.015: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205166, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205166, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205166, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205165, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:46:12.017: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205166, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205166, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205166, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205165, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 30 22:46:15.078: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 22:46:15.083: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9821-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:46:16.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4797" for this suite.
STEP: Destroying namespace "webhook-4797-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.034 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":278,"completed":270,"skipped":4320,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:46:16.507: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1559
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:46:22.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1559" for this suite.

• [SLOW TEST:5.865 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":278,"completed":271,"skipped":4371,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:46:22.375: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6831
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 22:46:22.556: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:46:28.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6831" for this suite.

• [SLOW TEST:6.226 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:47
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":278,"completed":272,"skipped":4389,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:46:28.603: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4062
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-e5dcd204-229e-4507-8529-967ba72e878b
STEP: Creating a pod to test consume secrets
Mar 30 22:46:28.791: INFO: Waiting up to 5m0s for pod "pod-secrets-e58f8fe0-c7bd-4e2e-b4e9-aba8de90f5d2" in namespace "secrets-4062" to be "success or failure"
Mar 30 22:46:28.796: INFO: Pod "pod-secrets-e58f8fe0-c7bd-4e2e-b4e9-aba8de90f5d2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.259691ms
Mar 30 22:46:30.800: INFO: Pod "pod-secrets-e58f8fe0-c7bd-4e2e-b4e9-aba8de90f5d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009732245s
Mar 30 22:46:32.807: INFO: Pod "pod-secrets-e58f8fe0-c7bd-4e2e-b4e9-aba8de90f5d2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016311739s
Mar 30 22:46:34.813: INFO: Pod "pod-secrets-e58f8fe0-c7bd-4e2e-b4e9-aba8de90f5d2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022225959s
Mar 30 22:46:36.820: INFO: Pod "pod-secrets-e58f8fe0-c7bd-4e2e-b4e9-aba8de90f5d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.029681145s
STEP: Saw pod success
Mar 30 22:46:36.820: INFO: Pod "pod-secrets-e58f8fe0-c7bd-4e2e-b4e9-aba8de90f5d2" satisfied condition "success or failure"
Mar 30 22:46:36.824: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-secrets-e58f8fe0-c7bd-4e2e-b4e9-aba8de90f5d2 container secret-volume-test: <nil>
STEP: delete the pod
Mar 30 22:46:36.858: INFO: Waiting for pod pod-secrets-e58f8fe0-c7bd-4e2e-b4e9-aba8de90f5d2 to disappear
Mar 30 22:46:36.863: INFO: Pod pod-secrets-e58f8fe0-c7bd-4e2e-b4e9-aba8de90f5d2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:46:36.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4062" for this suite.

• [SLOW TEST:8.277 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":273,"skipped":4424,"failed":0}
SSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:46:36.881: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-931
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test hostPath mode
Mar 30 22:46:37.131: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-931" to be "success or failure"
Mar 30 22:46:37.135: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 3.299659ms
Mar 30 22:46:39.141: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009284057s
Mar 30 22:46:41.153: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021051483s
Mar 30 22:46:43.158: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026289061s
Mar 30 22:46:45.164: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.032280625s
STEP: Saw pod success
Mar 30 22:46:45.164: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Mar 30 22:46:45.167: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Mar 30 22:46:45.230: INFO: Waiting for pod pod-host-path-test to disappear
Mar 30 22:46:45.234: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:46:45.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-931" for this suite.

• [SLOW TEST:8.371 seconds]
[sig-storage] HostPath
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":274,"skipped":4430,"failed":0}
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:46:45.256: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6112
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on node default medium
Mar 30 22:46:45.454: INFO: Waiting up to 5m0s for pod "pod-43aa0c66-25bc-4c56-baa2-8663d57331be" in namespace "emptydir-6112" to be "success or failure"
Mar 30 22:46:45.463: INFO: Pod "pod-43aa0c66-25bc-4c56-baa2-8663d57331be": Phase="Pending", Reason="", readiness=false. Elapsed: 8.840353ms
Mar 30 22:46:47.469: INFO: Pod "pod-43aa0c66-25bc-4c56-baa2-8663d57331be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014640637s
Mar 30 22:46:49.474: INFO: Pod "pod-43aa0c66-25bc-4c56-baa2-8663d57331be": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019856336s
Mar 30 22:46:51.481: INFO: Pod "pod-43aa0c66-25bc-4c56-baa2-8663d57331be": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026120088s
Mar 30 22:46:53.490: INFO: Pod "pod-43aa0c66-25bc-4c56-baa2-8663d57331be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.035945961s
STEP: Saw pod success
Mar 30 22:46:53.491: INFO: Pod "pod-43aa0c66-25bc-4c56-baa2-8663d57331be" satisfied condition "success or failure"
Mar 30 22:46:53.495: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod pod-43aa0c66-25bc-4c56-baa2-8663d57331be container test-container: <nil>
STEP: delete the pod
Mar 30 22:46:53.529: INFO: Waiting for pod pod-43aa0c66-25bc-4c56-baa2-8663d57331be to disappear
Mar 30 22:46:53.533: INFO: Pod pod-43aa0c66-25bc-4c56-baa2-8663d57331be no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:46:53.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6112" for this suite.

• [SLOW TEST:8.295 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":275,"skipped":4430,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:46:53.554: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4102
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 30 22:46:54.379: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 30 22:46:56.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205214, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205214, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205214, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205214, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:46:58.401: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205214, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205214, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205214, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205214, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 22:47:00.399: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205214, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205214, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205214, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721205214, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 30 22:47:03.437: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar 30 22:47:03.443: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3726-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:47:04.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4102" for this suite.
STEP: Destroying namespace "webhook-4102-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.413 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":278,"completed":276,"skipped":4442,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:47:04.969: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8233
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Mar 30 22:47:05.194: INFO: Waiting up to 5m0s for pod "downward-api-42516940-7acc-4f49-b13b-559b189f22f7" in namespace "downward-api-8233" to be "success or failure"
Mar 30 22:47:05.201: INFO: Pod "downward-api-42516940-7acc-4f49-b13b-559b189f22f7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.231076ms
Mar 30 22:47:07.207: INFO: Pod "downward-api-42516940-7acc-4f49-b13b-559b189f22f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012476506s
Mar 30 22:47:09.213: INFO: Pod "downward-api-42516940-7acc-4f49-b13b-559b189f22f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018221026s
Mar 30 22:47:11.223: INFO: Pod "downward-api-42516940-7acc-4f49-b13b-559b189f22f7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028600901s
Mar 30 22:47:13.228: INFO: Pod "downward-api-42516940-7acc-4f49-b13b-559b189f22f7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.033685263s
Mar 30 22:47:15.242: INFO: Pod "downward-api-42516940-7acc-4f49-b13b-559b189f22f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.048044494s
STEP: Saw pod success
Mar 30 22:47:15.243: INFO: Pod "downward-api-42516940-7acc-4f49-b13b-559b189f22f7" satisfied condition "success or failure"
Mar 30 22:47:15.247: INFO: Trying to get logs from node worker-pool1-vvs2j292-eccd-ci-os-12-jenkins pod downward-api-42516940-7acc-4f49-b13b-559b189f22f7 container dapi-container: <nil>
STEP: delete the pod
Mar 30 22:47:15.288: INFO: Waiting for pod downward-api-42516940-7acc-4f49-b13b-559b189f22f7 to disappear
Mar 30 22:47:15.292: INFO: Pod downward-api-42516940-7acc-4f49-b13b-559b189f22f7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:47:15.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8233" for this suite.

• [SLOW TEST:10.341 seconds]
[sig-node] Downward API
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:33
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":278,"completed":277,"skipped":4477,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 30 22:47:15.324: INFO: >>> kubeConfig: /tmp/kubeconfig-408641372
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2971
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Mar 30 22:47:15.711: INFO: Waiting up to 5m0s for pod "downward-api-2eff8c64-09ea-4d20-a28b-4a2a47367165" in namespace "downward-api-2971" to be "success or failure"
Mar 30 22:47:15.729: INFO: Pod "downward-api-2eff8c64-09ea-4d20-a28b-4a2a47367165": Phase="Pending", Reason="", readiness=false. Elapsed: 17.845894ms
Mar 30 22:47:17.738: INFO: Pod "downward-api-2eff8c64-09ea-4d20-a28b-4a2a47367165": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027177107s
Mar 30 22:47:19.745: INFO: Pod "downward-api-2eff8c64-09ea-4d20-a28b-4a2a47367165": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033617548s
Mar 30 22:47:21.751: INFO: Pod "downward-api-2eff8c64-09ea-4d20-a28b-4a2a47367165": Phase="Pending", Reason="", readiness=false. Elapsed: 6.040475512s
Mar 30 22:47:23.758: INFO: Pod "downward-api-2eff8c64-09ea-4d20-a28b-4a2a47367165": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.047393533s
STEP: Saw pod success
Mar 30 22:47:23.759: INFO: Pod "downward-api-2eff8c64-09ea-4d20-a28b-4a2a47367165" satisfied condition "success or failure"
Mar 30 22:47:23.762: INFO: Trying to get logs from node worker-pool1-tjim0te5-eccd-ci-os-12-jenkins pod downward-api-2eff8c64-09ea-4d20-a28b-4a2a47367165 container dapi-container: <nil>
STEP: delete the pod
Mar 30 22:47:23.803: INFO: Waiting for pod downward-api-2eff8c64-09ea-4d20-a28b-4a2a47367165 to disappear
Mar 30 22:47:23.808: INFO: Pod downward-api-2eff8c64-09ea-4d20-a28b-4a2a47367165 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 30 22:47:23.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2971" for this suite.

• [SLOW TEST:8.508 seconds]
[sig-node] Downward API
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:33
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":278,"completed":278,"skipped":4544,"failed":0}
SSSSSSSSSSSSSSSSSSSSSMar 30 22:47:23.835: INFO: Running AfterSuite actions on all nodes
Mar 30 22:47:23.836: INFO: Running AfterSuite actions on node 1
Mar 30 22:47:23.836: INFO: Skipping dumping logs from cluster
{"msg":"Test Suite completed","total":278,"completed":278,"skipped":4565,"failed":0}

Ran 278 of 4843 Specs in 6131.178 seconds
SUCCESS! -- 278 Passed | 0 Failed | 0 Pending | 4565 Skipped
PASS

Ginkgo ran 1 suite in 1h42m13.087210506s
Test Suite Passed
