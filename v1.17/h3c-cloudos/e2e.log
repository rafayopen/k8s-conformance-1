I0304 08:58:23.921492      19 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-195674220
I0304 08:58:23.921519      19 test_context.go:419] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0304 08:58:23.921679      19 e2e.go:109] Starting e2e run "f13001ad-ea64-4255-88a4-b4c27ba3a891" on Ginkgo node 1
{"msg":"Test Suite starting","total":278,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1583312301 - Will randomize all specs
Will run 278 of 4843 specs

Mar  4 08:58:23.982: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
Mar  4 08:58:23.987: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar  4 08:58:24.002: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar  4 08:58:24.029: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar  4 08:58:24.029: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Mar  4 08:58:24.029: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar  4 08:58:24.037: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Mar  4 08:58:24.037: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Mar  4 08:58:24.037: INFO: e2e test version: v1.17.3
Mar  4 08:58:24.039: INFO: kube-apiserver version: v1.17.3
Mar  4 08:58:24.039: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
Mar  4 08:58:24.046: INFO: Cluster IP family: ipv4
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 08:58:24.046: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename resourcequota
Mar  4 08:58:24.110: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 08:58:41.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8606" for this suite.

• [SLOW TEST:17.110 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":278,"completed":1,"skipped":5,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 08:58:41.157: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-e4cf00a8-aa26-4b15-88e4-7d1e14d87f4f
STEP: Creating a pod to test consume secrets
Mar  4 08:58:41.208: INFO: Waiting up to 5m0s for pod "pod-secrets-3203bf78-59bc-40a0-80f3-1422510370ba" in namespace "secrets-8970" to be "success or failure"
Mar  4 08:58:41.218: INFO: Pod "pod-secrets-3203bf78-59bc-40a0-80f3-1422510370ba": Phase="Pending", Reason="", readiness=false. Elapsed: 9.802109ms
Mar  4 08:58:43.221: INFO: Pod "pod-secrets-3203bf78-59bc-40a0-80f3-1422510370ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013036274s
Mar  4 08:58:45.224: INFO: Pod "pod-secrets-3203bf78-59bc-40a0-80f3-1422510370ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016277186s
STEP: Saw pod success
Mar  4 08:58:45.224: INFO: Pod "pod-secrets-3203bf78-59bc-40a0-80f3-1422510370ba" satisfied condition "success or failure"
Mar  4 08:58:45.227: INFO: Trying to get logs from node master3 pod pod-secrets-3203bf78-59bc-40a0-80f3-1422510370ba container secret-volume-test: <nil>
STEP: delete the pod
Mar  4 08:58:45.253: INFO: Waiting for pod pod-secrets-3203bf78-59bc-40a0-80f3-1422510370ba to disappear
Mar  4 08:58:45.256: INFO: Pod pod-secrets-3203bf78-59bc-40a0-80f3-1422510370ba no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 08:58:45.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8970" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":2,"skipped":16,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 08:58:45.263: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 08:58:48.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5300" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":278,"completed":3,"skipped":26,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 08:58:48.351: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 08:58:48.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7540" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":278,"completed":4,"skipped":36,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 08:58:48.430: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap that has name configmap-test-emptyKey-5ab8b087-d0d7-4571-9327-2583eb2793ca
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 08:58:48.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4881" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":278,"completed":5,"skipped":47,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 08:58:48.466: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-f0ca7598-0adc-474e-9d73-bef9e6053f4e
STEP: Creating configMap with name cm-test-opt-upd-07a448aa-eec8-44c1-abaf-ae8529113fe6
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-f0ca7598-0adc-474e-9d73-bef9e6053f4e
STEP: Updating configmap cm-test-opt-upd-07a448aa-eec8-44c1-abaf-ae8529113fe6
STEP: Creating configMap with name cm-test-opt-create-bd8a931e-9d88-4d79-8530-e6d523d8a6f0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:00:00.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4619" for this suite.

• [SLOW TEST:72.437 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":6,"skipped":47,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:00:00.904: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  4 09:00:00.937: INFO: Waiting up to 5m0s for pod "pod-e6f8e900-17fb-4c1e-8a5d-64d114933779" in namespace "emptydir-4214" to be "success or failure"
Mar  4 09:00:00.941: INFO: Pod "pod-e6f8e900-17fb-4c1e-8a5d-64d114933779": Phase="Pending", Reason="", readiness=false. Elapsed: 4.002459ms
Mar  4 09:00:02.944: INFO: Pod "pod-e6f8e900-17fb-4c1e-8a5d-64d114933779": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006994335s
STEP: Saw pod success
Mar  4 09:00:02.944: INFO: Pod "pod-e6f8e900-17fb-4c1e-8a5d-64d114933779" satisfied condition "success or failure"
Mar  4 09:00:02.951: INFO: Trying to get logs from node master3 pod pod-e6f8e900-17fb-4c1e-8a5d-64d114933779 container test-container: <nil>
STEP: delete the pod
Mar  4 09:00:02.983: INFO: Waiting for pod pod-e6f8e900-17fb-4c1e-8a5d-64d114933779 to disappear
Mar  4 09:00:02.985: INFO: Pod pod-e6f8e900-17fb-4c1e-8a5d-64d114933779 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:00:02.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4214" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":7,"skipped":53,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:00:02.994: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-5ac7133e-0818-42f3-b6c0-fde9c83fab01
STEP: Creating a pod to test consume configMaps
Mar  4 09:00:03.042: INFO: Waiting up to 5m0s for pod "pod-configmaps-e65284ad-cbeb-40a0-97ae-2ba0cb5c0dec" in namespace "configmap-4873" to be "success or failure"
Mar  4 09:00:03.047: INFO: Pod "pod-configmaps-e65284ad-cbeb-40a0-97ae-2ba0cb5c0dec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.748264ms
Mar  4 09:00:05.052: INFO: Pod "pod-configmaps-e65284ad-cbeb-40a0-97ae-2ba0cb5c0dec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009685082s
STEP: Saw pod success
Mar  4 09:00:05.052: INFO: Pod "pod-configmaps-e65284ad-cbeb-40a0-97ae-2ba0cb5c0dec" satisfied condition "success or failure"
Mar  4 09:00:05.056: INFO: Trying to get logs from node master3 pod pod-configmaps-e65284ad-cbeb-40a0-97ae-2ba0cb5c0dec container configmap-volume-test: <nil>
STEP: delete the pod
Mar  4 09:00:05.077: INFO: Waiting for pod pod-configmaps-e65284ad-cbeb-40a0-97ae-2ba0cb5c0dec to disappear
Mar  4 09:00:05.082: INFO: Pod pod-configmaps-e65284ad-cbeb-40a0-97ae-2ba0cb5c0dec no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:00:05.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4873" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":8,"skipped":55,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:00:05.091: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-l79sz in namespace proxy-5080
I0304 09:00:05.143265      19 runners.go:189] Created replication controller with name: proxy-service-l79sz, namespace: proxy-5080, replica count: 1
I0304 09:00:06.196500      19 runners.go:189] proxy-service-l79sz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0304 09:00:07.196829      19 runners.go:189] proxy-service-l79sz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0304 09:00:08.197066      19 runners.go:189] proxy-service-l79sz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0304 09:00:09.197304      19 runners.go:189] proxy-service-l79sz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0304 09:00:10.197534      19 runners.go:189] proxy-service-l79sz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0304 09:00:11.197789      19 runners.go:189] proxy-service-l79sz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0304 09:00:12.198022      19 runners.go:189] proxy-service-l79sz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0304 09:00:13.198284      19 runners.go:189] proxy-service-l79sz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0304 09:00:14.198534      19 runners.go:189] proxy-service-l79sz Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  4 09:00:14.202: INFO: setup took 9.077883321s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar  4 09:00:14.210: INFO: (0) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:160/proxy/: foo (200; 8.122273ms)
Mar  4 09:00:14.210: INFO: (0) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/rewriteme">... (200; 8.324486ms)
Mar  4 09:00:14.227: INFO: (0) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname2/proxy/: bar (200; 23.991051ms)
Mar  4 09:00:14.227: INFO: (0) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/rewriteme">test</a> (200; 24.025935ms)
Mar  4 09:00:14.227: INFO: (0) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:160/proxy/: foo (200; 24.413853ms)
Mar  4 09:00:14.227: INFO: (0) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname2/proxy/: bar (200; 24.253229ms)
Mar  4 09:00:14.227: INFO: (0) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:162/proxy/: bar (200; 24.439663ms)
Mar  4 09:00:14.227: INFO: (0) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:162/proxy/: bar (200; 24.760306ms)
Mar  4 09:00:14.227: INFO: (0) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/rewriteme">test<... (200; 24.613586ms)
Mar  4 09:00:14.230: INFO: (0) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:462/proxy/: tls qux (200; 27.923935ms)
Mar  4 09:00:14.230: INFO: (0) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname1/proxy/: foo (200; 27.923686ms)
Mar  4 09:00:14.230: INFO: (0) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname1/proxy/: tls baz (200; 28.576165ms)
Mar  4 09:00:14.230: INFO: (0) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:460/proxy/: tls baz (200; 28.772441ms)
Mar  4 09:00:14.230: INFO: (0) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname1/proxy/: foo (200; 28.336983ms)
Mar  4 09:00:14.230: INFO: (0) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/tlsrewritem... (200; 28.149022ms)
Mar  4 09:00:14.230: INFO: (0) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname2/proxy/: tls qux (200; 28.305376ms)
Mar  4 09:00:14.239: INFO: (1) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/rewriteme">test</a> (200; 8.313402ms)
Mar  4 09:00:14.239: INFO: (1) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:462/proxy/: tls qux (200; 8.738323ms)
Mar  4 09:00:14.240: INFO: (1) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:160/proxy/: foo (200; 8.534193ms)
Mar  4 09:00:14.240: INFO: (1) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:460/proxy/: tls baz (200; 9.316539ms)
Mar  4 09:00:14.241: INFO: (1) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/tlsrewritem... (200; 8.993761ms)
Mar  4 09:00:14.241: INFO: (1) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/rewriteme">... (200; 9.451254ms)
Mar  4 09:00:14.241: INFO: (1) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:160/proxy/: foo (200; 9.823686ms)
Mar  4 09:00:14.244: INFO: (1) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:162/proxy/: bar (200; 11.899016ms)
Mar  4 09:00:14.244: INFO: (1) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname1/proxy/: foo (200; 13.112307ms)
Mar  4 09:00:14.244: INFO: (1) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:162/proxy/: bar (200; 13.208778ms)
Mar  4 09:00:14.245: INFO: (1) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/rewriteme">test<... (200; 13.332636ms)
Mar  4 09:00:14.247: INFO: (1) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname1/proxy/: foo (200; 16.333438ms)
Mar  4 09:00:14.247: INFO: (1) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname2/proxy/: bar (200; 16.202813ms)
Mar  4 09:00:14.247: INFO: (1) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname1/proxy/: tls baz (200; 16.165106ms)
Mar  4 09:00:14.247: INFO: (1) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname2/proxy/: bar (200; 16.582302ms)
Mar  4 09:00:14.248: INFO: (1) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname2/proxy/: tls qux (200; 16.83379ms)
Mar  4 09:00:14.257: INFO: (2) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/rewriteme">... (200; 8.882659ms)
Mar  4 09:00:14.258: INFO: (2) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/tlsrewritem... (200; 8.824035ms)
Mar  4 09:00:14.258: INFO: (2) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/rewriteme">test</a> (200; 9.590976ms)
Mar  4 09:00:14.258: INFO: (2) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:162/proxy/: bar (200; 8.878877ms)
Mar  4 09:00:14.258: INFO: (2) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:462/proxy/: tls qux (200; 8.963205ms)
Mar  4 09:00:14.258: INFO: (2) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:460/proxy/: tls baz (200; 8.920397ms)
Mar  4 09:00:14.258: INFO: (2) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:160/proxy/: foo (200; 9.613178ms)
Mar  4 09:00:14.258: INFO: (2) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:160/proxy/: foo (200; 9.15768ms)
Mar  4 09:00:14.258: INFO: (2) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/rewriteme">test<... (200; 9.246717ms)
Mar  4 09:00:14.263: INFO: (2) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname1/proxy/: foo (200; 14.432041ms)
Mar  4 09:00:14.263: INFO: (2) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname2/proxy/: bar (200; 13.827287ms)
Mar  4 09:00:14.263: INFO: (2) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname1/proxy/: foo (200; 13.979052ms)
Mar  4 09:00:14.263: INFO: (2) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname2/proxy/: bar (200; 13.933577ms)
Mar  4 09:00:14.263: INFO: (2) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname1/proxy/: tls baz (200; 14.911595ms)
Mar  4 09:00:14.263: INFO: (2) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname2/proxy/: tls qux (200; 14.475568ms)
Mar  4 09:00:14.263: INFO: (2) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:162/proxy/: bar (200; 14.729156ms)
Mar  4 09:00:14.270: INFO: (3) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/rewriteme">... (200; 6.378115ms)
Mar  4 09:00:14.270: INFO: (3) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/rewriteme">test<... (200; 6.358817ms)
Mar  4 09:00:14.273: INFO: (3) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:160/proxy/: foo (200; 9.762144ms)
Mar  4 09:00:14.273: INFO: (3) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:162/proxy/: bar (200; 9.070255ms)
Mar  4 09:00:14.275: INFO: (3) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/tlsrewritem... (200; 10.967155ms)
Mar  4 09:00:14.275: INFO: (3) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:162/proxy/: bar (200; 10.744168ms)
Mar  4 09:00:14.275: INFO: (3) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:462/proxy/: tls qux (200; 11.016239ms)
Mar  4 09:00:14.276: INFO: (3) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:460/proxy/: tls baz (200; 11.423338ms)
Mar  4 09:00:14.276: INFO: (3) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:160/proxy/: foo (200; 11.801543ms)
Mar  4 09:00:14.277: INFO: (3) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname2/proxy/: bar (200; 12.887221ms)
Mar  4 09:00:14.277: INFO: (3) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/rewriteme">test</a> (200; 12.84818ms)
Mar  4 09:00:14.279: INFO: (3) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname2/proxy/: bar (200; 14.717204ms)
Mar  4 09:00:14.279: INFO: (3) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname2/proxy/: tls qux (200; 15.089201ms)
Mar  4 09:00:14.279: INFO: (3) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname1/proxy/: foo (200; 14.745087ms)
Mar  4 09:00:14.281: INFO: (3) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname1/proxy/: foo (200; 16.810692ms)
Mar  4 09:00:14.281: INFO: (3) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname1/proxy/: tls baz (200; 16.72303ms)
Mar  4 09:00:14.286: INFO: (4) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:162/proxy/: bar (200; 4.434722ms)
Mar  4 09:00:14.286: INFO: (4) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/tlsrewritem... (200; 4.647676ms)
Mar  4 09:00:14.295: INFO: (4) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/rewriteme">... (200; 13.188798ms)
Mar  4 09:00:14.295: INFO: (4) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/rewriteme">test<... (200; 13.032377ms)
Mar  4 09:00:14.295: INFO: (4) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname2/proxy/: bar (200; 13.68043ms)
Mar  4 09:00:14.295: INFO: (4) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:160/proxy/: foo (200; 13.055986ms)
Mar  4 09:00:14.295: INFO: (4) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:462/proxy/: tls qux (200; 13.985958ms)
Mar  4 09:00:14.295: INFO: (4) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname2/proxy/: tls qux (200; 13.223965ms)
Mar  4 09:00:14.295: INFO: (4) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/rewriteme">test</a> (200; 13.681683ms)
Mar  4 09:00:14.296: INFO: (4) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:460/proxy/: tls baz (200; 13.743981ms)
Mar  4 09:00:14.296: INFO: (4) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname1/proxy/: foo (200; 14.01117ms)
Mar  4 09:00:14.296: INFO: (4) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:160/proxy/: foo (200; 13.546506ms)
Mar  4 09:00:14.296: INFO: (4) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname2/proxy/: bar (200; 13.952914ms)
Mar  4 09:00:14.296: INFO: (4) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:162/proxy/: bar (200; 13.507174ms)
Mar  4 09:00:14.296: INFO: (4) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname1/proxy/: tls baz (200; 13.715903ms)
Mar  4 09:00:14.296: INFO: (4) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname1/proxy/: foo (200; 13.490979ms)
Mar  4 09:00:14.304: INFO: (5) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:160/proxy/: foo (200; 8.196296ms)
Mar  4 09:00:14.304: INFO: (5) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname2/proxy/: tls qux (200; 8.310908ms)
Mar  4 09:00:14.304: INFO: (5) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:162/proxy/: bar (200; 8.040059ms)
Mar  4 09:00:14.304: INFO: (5) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/rewriteme">test<... (200; 8.317348ms)
Mar  4 09:00:14.305: INFO: (5) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:160/proxy/: foo (200; 8.654434ms)
Mar  4 09:00:14.306: INFO: (5) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/rewriteme">... (200; 10.029707ms)
Mar  4 09:00:14.307: INFO: (5) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/rewriteme">test</a> (200; 10.673619ms)
Mar  4 09:00:14.307: INFO: (5) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:462/proxy/: tls qux (200; 11.048701ms)
Mar  4 09:00:14.307: INFO: (5) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/tlsrewritem... (200; 10.606353ms)
Mar  4 09:00:14.307: INFO: (5) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:162/proxy/: bar (200; 10.827254ms)
Mar  4 09:00:14.307: INFO: (5) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:460/proxy/: tls baz (200; 10.78227ms)
Mar  4 09:00:14.311: INFO: (5) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname1/proxy/: foo (200; 15.363392ms)
Mar  4 09:00:14.311: INFO: (5) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname2/proxy/: bar (200; 15.610065ms)
Mar  4 09:00:14.311: INFO: (5) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname1/proxy/: tls baz (200; 15.180652ms)
Mar  4 09:00:14.312: INFO: (5) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname1/proxy/: foo (200; 15.702254ms)
Mar  4 09:00:14.312: INFO: (5) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname2/proxy/: bar (200; 15.599688ms)
Mar  4 09:00:14.319: INFO: (6) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/tlsrewritem... (200; 7.559288ms)
Mar  4 09:00:14.321: INFO: (6) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:162/proxy/: bar (200; 8.891874ms)
Mar  4 09:00:14.321: INFO: (6) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:160/proxy/: foo (200; 8.344046ms)
Mar  4 09:00:14.321: INFO: (6) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/rewriteme">... (200; 8.467191ms)
Mar  4 09:00:14.322: INFO: (6) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/rewriteme">test</a> (200; 9.710637ms)
Mar  4 09:00:14.322: INFO: (6) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:462/proxy/: tls qux (200; 10.035938ms)
Mar  4 09:00:14.322: INFO: (6) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:162/proxy/: bar (200; 9.486531ms)
Mar  4 09:00:14.323: INFO: (6) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:460/proxy/: tls baz (200; 11.258009ms)
Mar  4 09:00:14.323: INFO: (6) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/rewriteme">test<... (200; 10.893099ms)
Mar  4 09:00:14.325: INFO: (6) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:160/proxy/: foo (200; 12.186926ms)
Mar  4 09:00:14.326: INFO: (6) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname1/proxy/: tls baz (200; 13.584461ms)
Mar  4 09:00:14.326: INFO: (6) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname2/proxy/: bar (200; 13.90584ms)
Mar  4 09:00:14.326: INFO: (6) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname2/proxy/: bar (200; 14.141996ms)
Mar  4 09:00:14.327: INFO: (6) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname1/proxy/: foo (200; 14.487068ms)
Mar  4 09:00:14.327: INFO: (6) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname1/proxy/: foo (200; 15.090131ms)
Mar  4 09:00:14.327: INFO: (6) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname2/proxy/: tls qux (200; 14.546374ms)
Mar  4 09:00:14.348: INFO: (7) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname2/proxy/: bar (200; 20.613543ms)
Mar  4 09:00:14.348: INFO: (7) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname1/proxy/: foo (200; 20.748156ms)
Mar  4 09:00:14.348: INFO: (7) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:462/proxy/: tls qux (200; 20.838435ms)
Mar  4 09:00:14.348: INFO: (7) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:162/proxy/: bar (200; 20.956387ms)
Mar  4 09:00:14.348: INFO: (7) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/tlsrewritem... (200; 21.129093ms)
Mar  4 09:00:14.348: INFO: (7) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:160/proxy/: foo (200; 20.305889ms)
Mar  4 09:00:14.348: INFO: (7) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/rewriteme">test</a> (200; 20.753108ms)
Mar  4 09:00:14.348: INFO: (7) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/rewriteme">... (200; 20.556124ms)
Mar  4 09:00:14.348: INFO: (7) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:162/proxy/: bar (200; 20.408668ms)
Mar  4 09:00:14.348: INFO: (7) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname1/proxy/: tls baz (200; 20.708201ms)
Mar  4 09:00:14.351: INFO: (7) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:160/proxy/: foo (200; 23.183245ms)
Mar  4 09:00:14.351: INFO: (7) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname2/proxy/: bar (200; 23.935266ms)
Mar  4 09:00:14.352: INFO: (7) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/rewriteme">test<... (200; 23.319776ms)
Mar  4 09:00:14.352: INFO: (7) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname1/proxy/: foo (200; 23.464549ms)
Mar  4 09:00:14.352: INFO: (7) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname2/proxy/: tls qux (200; 23.417703ms)
Mar  4 09:00:14.352: INFO: (7) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:460/proxy/: tls baz (200; 23.883079ms)
Mar  4 09:00:14.361: INFO: (8) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:162/proxy/: bar (200; 9.380969ms)
Mar  4 09:00:14.361: INFO: (8) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/tlsrewritem... (200; 9.524655ms)
Mar  4 09:00:14.361: INFO: (8) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:460/proxy/: tls baz (200; 8.977293ms)
Mar  4 09:00:14.361: INFO: (8) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:462/proxy/: tls qux (200; 9.334814ms)
Mar  4 09:00:14.361: INFO: (8) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/rewriteme">test</a> (200; 9.066194ms)
Mar  4 09:00:14.363: INFO: (8) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:160/proxy/: foo (200; 10.628655ms)
Mar  4 09:00:14.363: INFO: (8) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/rewriteme">... (200; 10.764081ms)
Mar  4 09:00:14.366: INFO: (8) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:160/proxy/: foo (200; 13.254465ms)
Mar  4 09:00:14.366: INFO: (8) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:162/proxy/: bar (200; 13.563278ms)
Mar  4 09:00:14.367: INFO: (8) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname2/proxy/: bar (200; 14.670044ms)
Mar  4 09:00:14.368: INFO: (8) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/rewriteme">test<... (200; 14.814895ms)
Mar  4 09:00:14.370: INFO: (8) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname1/proxy/: foo (200; 18.161647ms)
Mar  4 09:00:14.370: INFO: (8) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname1/proxy/: tls baz (200; 17.935189ms)
Mar  4 09:00:14.370: INFO: (8) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname2/proxy/: tls qux (200; 17.696879ms)
Mar  4 09:00:14.370: INFO: (8) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname1/proxy/: foo (200; 17.78507ms)
Mar  4 09:00:14.370: INFO: (8) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname2/proxy/: bar (200; 18.337905ms)
Mar  4 09:00:14.379: INFO: (9) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:160/proxy/: foo (200; 7.635843ms)
Mar  4 09:00:14.379: INFO: (9) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:162/proxy/: bar (200; 8.010927ms)
Mar  4 09:00:14.379: INFO: (9) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:162/proxy/: bar (200; 8.566515ms)
Mar  4 09:00:14.379: INFO: (9) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:462/proxy/: tls qux (200; 8.505989ms)
Mar  4 09:00:14.382: INFO: (9) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/rewriteme">test<... (200; 10.381367ms)
Mar  4 09:00:14.382: INFO: (9) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/rewriteme">test</a> (200; 10.826028ms)
Mar  4 09:00:14.382: INFO: (9) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/tlsrewritem... (200; 10.411405ms)
Mar  4 09:00:14.382: INFO: (9) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:160/proxy/: foo (200; 10.825209ms)
Mar  4 09:00:14.382: INFO: (9) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/rewriteme">... (200; 11.340941ms)
Mar  4 09:00:14.383: INFO: (9) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:460/proxy/: tls baz (200; 12.614653ms)
Mar  4 09:00:14.384: INFO: (9) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname1/proxy/: tls baz (200; 12.409292ms)
Mar  4 09:00:14.384: INFO: (9) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname1/proxy/: foo (200; 12.582258ms)
Mar  4 09:00:14.386: INFO: (9) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname2/proxy/: bar (200; 15.156062ms)
Mar  4 09:00:14.386: INFO: (9) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname2/proxy/: tls qux (200; 15.798229ms)
Mar  4 09:00:14.386: INFO: (9) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname2/proxy/: bar (200; 15.513441ms)
Mar  4 09:00:14.386: INFO: (9) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname1/proxy/: foo (200; 14.993954ms)
Mar  4 09:00:14.399: INFO: (10) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/tlsrewritem... (200; 11.791014ms)
Mar  4 09:00:14.401: INFO: (10) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname1/proxy/: foo (200; 14.098002ms)
Mar  4 09:00:14.402: INFO: (10) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:460/proxy/: tls baz (200; 14.924068ms)
Mar  4 09:00:14.402: INFO: (10) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:162/proxy/: bar (200; 14.303773ms)
Mar  4 09:00:14.402: INFO: (10) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:462/proxy/: tls qux (200; 15.033205ms)
Mar  4 09:00:14.402: INFO: (10) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:162/proxy/: bar (200; 15.140238ms)
Mar  4 09:00:14.402: INFO: (10) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/rewriteme">test<... (200; 14.083707ms)
Mar  4 09:00:14.402: INFO: (10) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname2/proxy/: bar (200; 14.857996ms)
Mar  4 09:00:14.402: INFO: (10) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname1/proxy/: tls baz (200; 14.603982ms)
Mar  4 09:00:14.402: INFO: (10) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/rewriteme">test</a> (200; 14.786547ms)
Mar  4 09:00:14.402: INFO: (10) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname1/proxy/: foo (200; 15.082211ms)
Mar  4 09:00:14.402: INFO: (10) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:160/proxy/: foo (200; 14.613034ms)
Mar  4 09:00:14.402: INFO: (10) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:160/proxy/: foo (200; 15.529039ms)
Mar  4 09:00:14.402: INFO: (10) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/rewriteme">... (200; 14.753295ms)
Mar  4 09:00:14.405: INFO: (10) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname2/proxy/: tls qux (200; 17.576293ms)
Mar  4 09:00:14.405: INFO: (10) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname2/proxy/: bar (200; 18.209934ms)
Mar  4 09:00:14.431: INFO: (11) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:162/proxy/: bar (200; 26.205486ms)
Mar  4 09:00:14.438: INFO: (11) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:160/proxy/: foo (200; 32.709235ms)
Mar  4 09:00:14.438: INFO: (11) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname1/proxy/: foo (200; 32.313148ms)
Mar  4 09:00:14.438: INFO: (11) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname1/proxy/: tls baz (200; 32.678214ms)
Mar  4 09:00:14.438: INFO: (11) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname1/proxy/: foo (200; 32.989576ms)
Mar  4 09:00:14.444: INFO: (11) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:160/proxy/: foo (200; 38.541236ms)
Mar  4 09:00:14.444: INFO: (11) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname2/proxy/: bar (200; 38.852824ms)
Mar  4 09:00:14.444: INFO: (11) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/rewriteme">test</a> (200; 38.816468ms)
Mar  4 09:00:14.444: INFO: (11) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/rewriteme">... (200; 38.695534ms)
Mar  4 09:00:14.444: INFO: (11) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/rewriteme">test<... (200; 38.355914ms)
Mar  4 09:00:14.444: INFO: (11) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:162/proxy/: bar (200; 38.621082ms)
Mar  4 09:00:14.447: INFO: (11) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname2/proxy/: tls qux (200; 40.46539ms)
Mar  4 09:00:14.447: INFO: (11) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname2/proxy/: bar (200; 41.088953ms)
Mar  4 09:00:14.447: INFO: (11) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:460/proxy/: tls baz (200; 41.223587ms)
Mar  4 09:00:14.447: INFO: (11) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:462/proxy/: tls qux (200; 41.311159ms)
Mar  4 09:00:14.458: INFO: (11) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/tlsrewritem... (200; 52.414382ms)
Mar  4 09:00:14.474: INFO: (12) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/rewriteme">test</a> (200; 13.809549ms)
Mar  4 09:00:14.474: INFO: (12) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/rewriteme">test<... (200; 14.488522ms)
Mar  4 09:00:14.474: INFO: (12) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:160/proxy/: foo (200; 14.42828ms)
Mar  4 09:00:14.477: INFO: (12) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:162/proxy/: bar (200; 17.868212ms)
Mar  4 09:00:14.477: INFO: (12) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/tlsrewritem... (200; 18.16882ms)
Mar  4 09:00:14.478: INFO: (12) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname2/proxy/: tls qux (200; 18.008545ms)
Mar  4 09:00:14.478: INFO: (12) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:462/proxy/: tls qux (200; 17.892501ms)
Mar  4 09:00:14.484: INFO: (12) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/rewriteme">... (200; 25.220766ms)
Mar  4 09:00:14.484: INFO: (12) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:160/proxy/: foo (200; 25.550278ms)
Mar  4 09:00:14.484: INFO: (12) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname1/proxy/: foo (200; 24.777753ms)
Mar  4 09:00:14.484: INFO: (12) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname2/proxy/: bar (200; 25.762391ms)
Mar  4 09:00:14.486: INFO: (12) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:162/proxy/: bar (200; 27.022772ms)
Mar  4 09:00:14.486: INFO: (12) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname1/proxy/: foo (200; 21.231724ms)
Mar  4 09:00:14.495: INFO: (12) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname1/proxy/: tls baz (200; 36.513463ms)
Mar  4 09:00:14.495: INFO: (12) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname2/proxy/: bar (200; 35.600707ms)
Mar  4 09:00:14.495: INFO: (12) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:460/proxy/: tls baz (200; 35.48428ms)
Mar  4 09:00:14.504: INFO: (13) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/rewriteme">test<... (200; 7.812887ms)
Mar  4 09:00:14.509: INFO: (13) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/tlsrewritem... (200; 13.052683ms)
Mar  4 09:00:14.509: INFO: (13) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:160/proxy/: foo (200; 12.030434ms)
Mar  4 09:00:14.509: INFO: (13) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:162/proxy/: bar (200; 13.279925ms)
Mar  4 09:00:14.514: INFO: (13) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname2/proxy/: bar (200; 17.651107ms)
Mar  4 09:00:14.514: INFO: (13) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname2/proxy/: bar (200; 18.095433ms)
Mar  4 09:00:14.514: INFO: (13) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/rewriteme">... (200; 17.611783ms)
Mar  4 09:00:14.514: INFO: (13) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:162/proxy/: bar (200; 18.022962ms)
Mar  4 09:00:14.514: INFO: (13) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname1/proxy/: foo (200; 17.85717ms)
Mar  4 09:00:14.514: INFO: (13) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:160/proxy/: foo (200; 18.400554ms)
Mar  4 09:00:14.520: INFO: (13) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname2/proxy/: tls qux (200; 24.771751ms)
Mar  4 09:00:14.521: INFO: (13) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname1/proxy/: tls baz (200; 24.087856ms)
Mar  4 09:00:14.521: INFO: (13) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname1/proxy/: foo (200; 24.917498ms)
Mar  4 09:00:14.521: INFO: (13) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:460/proxy/: tls baz (200; 24.212826ms)
Mar  4 09:00:14.521: INFO: (13) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:462/proxy/: tls qux (200; 24.284426ms)
Mar  4 09:00:14.521: INFO: (13) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/rewriteme">test</a> (200; 24.207056ms)
Mar  4 09:00:14.533: INFO: (14) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:162/proxy/: bar (200; 11.525644ms)
Mar  4 09:00:14.533: INFO: (14) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/rewriteme">test<... (200; 11.716003ms)
Mar  4 09:00:14.533: INFO: (14) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:160/proxy/: foo (200; 12.263285ms)
Mar  4 09:00:14.533: INFO: (14) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:460/proxy/: tls baz (200; 11.298423ms)
Mar  4 09:00:14.533: INFO: (14) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:462/proxy/: tls qux (200; 11.470939ms)
Mar  4 09:00:14.533: INFO: (14) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname2/proxy/: tls qux (200; 11.866538ms)
Mar  4 09:00:14.533: INFO: (14) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/tlsrewritem... (200; 12.067998ms)
Mar  4 09:00:14.534: INFO: (14) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/rewriteme">test</a> (200; 11.875089ms)
Mar  4 09:00:14.534: INFO: (14) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname1/proxy/: foo (200; 12.503546ms)
Mar  4 09:00:14.534: INFO: (14) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/rewriteme">... (200; 5.148313ms)
Mar  4 09:00:14.534: INFO: (14) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:160/proxy/: foo (200; 13.333788ms)
Mar  4 09:00:14.534: INFO: (14) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:162/proxy/: bar (200; 6.306476ms)
Mar  4 09:00:14.536: INFO: (14) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname1/proxy/: tls baz (200; 7.689298ms)
Mar  4 09:00:14.536: INFO: (14) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname2/proxy/: bar (200; 7.798658ms)
Mar  4 09:00:14.536: INFO: (14) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname2/proxy/: bar (200; 7.935773ms)
Mar  4 09:00:14.539: INFO: (14) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname1/proxy/: foo (200; 17.167128ms)
Mar  4 09:00:14.548: INFO: (15) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:462/proxy/: tls qux (200; 8.589585ms)
Mar  4 09:00:14.549: INFO: (15) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:460/proxy/: tls baz (200; 9.755372ms)
Mar  4 09:00:14.549: INFO: (15) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/rewriteme">test<... (200; 9.343032ms)
Mar  4 09:00:14.549: INFO: (15) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/rewriteme">... (200; 9.644691ms)
Mar  4 09:00:14.549: INFO: (15) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:160/proxy/: foo (200; 9.42021ms)
Mar  4 09:00:14.550: INFO: (15) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:162/proxy/: bar (200; 10.778798ms)
Mar  4 09:00:14.551: INFO: (15) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:162/proxy/: bar (200; 11.106238ms)
Mar  4 09:00:14.551: INFO: (15) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/rewriteme">test</a> (200; 10.632348ms)
Mar  4 09:00:14.551: INFO: (15) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:160/proxy/: foo (200; 11.195919ms)
Mar  4 09:00:14.551: INFO: (15) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/tlsrewritem... (200; 10.947612ms)
Mar  4 09:00:14.554: INFO: (15) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname1/proxy/: tls baz (200; 15.1043ms)
Mar  4 09:00:14.555: INFO: (15) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname1/proxy/: foo (200; 15.825011ms)
Mar  4 09:00:14.555: INFO: (15) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname2/proxy/: tls qux (200; 15.820377ms)
Mar  4 09:00:14.557: INFO: (15) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname2/proxy/: bar (200; 17.275485ms)
Mar  4 09:00:14.557: INFO: (15) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname1/proxy/: foo (200; 17.358634ms)
Mar  4 09:00:14.557: INFO: (15) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname2/proxy/: bar (200; 17.622786ms)
Mar  4 09:00:14.563: INFO: (16) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/rewriteme">... (200; 5.26921ms)
Mar  4 09:00:14.569: INFO: (16) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:162/proxy/: bar (200; 10.478358ms)
Mar  4 09:00:14.569: INFO: (16) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname1/proxy/: foo (200; 11.419378ms)
Mar  4 09:00:14.569: INFO: (16) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:460/proxy/: tls baz (200; 10.300163ms)
Mar  4 09:00:14.569: INFO: (16) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:160/proxy/: foo (200; 11.207068ms)
Mar  4 09:00:14.571: INFO: (16) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname1/proxy/: tls baz (200; 13.50054ms)
Mar  4 09:00:14.571: INFO: (16) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/tlsrewritem... (200; 13.264359ms)
Mar  4 09:00:14.572: INFO: (16) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/rewriteme">test<... (200; 13.428949ms)
Mar  4 09:00:14.572: INFO: (16) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/rewriteme">test</a> (200; 12.922938ms)
Mar  4 09:00:14.572: INFO: (16) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:462/proxy/: tls qux (200; 13.2014ms)
Mar  4 09:00:14.572: INFO: (16) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:162/proxy/: bar (200; 13.72524ms)
Mar  4 09:00:14.572: INFO: (16) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:160/proxy/: foo (200; 13.432155ms)
Mar  4 09:00:14.572: INFO: (16) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname2/proxy/: bar (200; 13.056677ms)
Mar  4 09:00:14.572: INFO: (16) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname2/proxy/: tls qux (200; 13.608549ms)
Mar  4 09:00:14.572: INFO: (16) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname2/proxy/: bar (200; 13.158304ms)
Mar  4 09:00:14.573: INFO: (16) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname1/proxy/: foo (200; 14.695706ms)
Mar  4 09:00:14.581: INFO: (17) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:160/proxy/: foo (200; 6.568431ms)
Mar  4 09:00:14.581: INFO: (17) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/tlsrewritem... (200; 7.657698ms)
Mar  4 09:00:14.582: INFO: (17) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname2/proxy/: bar (200; 7.653318ms)
Mar  4 09:00:14.582: INFO: (17) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname1/proxy/: foo (200; 8.709654ms)
Mar  4 09:00:14.583: INFO: (17) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname1/proxy/: tls baz (200; 8.923053ms)
Mar  4 09:00:14.583: INFO: (17) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname2/proxy/: tls qux (200; 9.731484ms)
Mar  4 09:00:14.583: INFO: (17) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname2/proxy/: bar (200; 9.409958ms)
Mar  4 09:00:14.586: INFO: (17) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/rewriteme">test</a> (200; 12.354294ms)
Mar  4 09:00:14.586: INFO: (17) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/rewriteme">test<... (200; 13.018949ms)
Mar  4 09:00:14.587: INFO: (17) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:160/proxy/: foo (200; 12.99868ms)
Mar  4 09:00:14.587: INFO: (17) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:162/proxy/: bar (200; 13.516195ms)
Mar  4 09:00:14.587: INFO: (17) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/rewriteme">... (200; 12.35658ms)
Mar  4 09:00:14.587: INFO: (17) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:460/proxy/: tls baz (200; 12.869016ms)
Mar  4 09:00:14.587: INFO: (17) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname1/proxy/: foo (200; 13.874142ms)
Mar  4 09:00:14.587: INFO: (17) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:162/proxy/: bar (200; 13.636957ms)
Mar  4 09:00:14.587: INFO: (17) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:462/proxy/: tls qux (200; 13.594635ms)
Mar  4 09:00:14.597: INFO: (18) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:160/proxy/: foo (200; 8.757897ms)
Mar  4 09:00:14.597: INFO: (18) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:160/proxy/: foo (200; 9.435515ms)
Mar  4 09:00:14.597: INFO: (18) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/rewriteme">test</a> (200; 8.315547ms)
Mar  4 09:00:14.597: INFO: (18) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:462/proxy/: tls qux (200; 8.624816ms)
Mar  4 09:00:14.597: INFO: (18) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/rewriteme">... (200; 9.565581ms)
Mar  4 09:00:14.597: INFO: (18) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:460/proxy/: tls baz (200; 9.869645ms)
Mar  4 09:00:14.597: INFO: (18) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:162/proxy/: bar (200; 8.738595ms)
Mar  4 09:00:14.599: INFO: (18) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:162/proxy/: bar (200; 11.692218ms)
Mar  4 09:00:14.599: INFO: (18) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/tlsrewritem... (200; 11.129499ms)
Mar  4 09:00:14.599: INFO: (18) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/rewriteme">test<... (200; 11.32809ms)
Mar  4 09:00:14.601: INFO: (18) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname1/proxy/: tls baz (200; 13.922526ms)
Mar  4 09:00:14.601: INFO: (18) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname1/proxy/: foo (200; 12.872078ms)
Mar  4 09:00:14.602: INFO: (18) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname2/proxy/: bar (200; 12.80163ms)
Mar  4 09:00:14.603: INFO: (18) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname2/proxy/: tls qux (200; 14.888901ms)
Mar  4 09:00:14.603: INFO: (18) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname2/proxy/: bar (200; 14.166274ms)
Mar  4 09:00:14.603: INFO: (18) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname1/proxy/: foo (200; 15.018143ms)
Mar  4 09:00:14.610: INFO: (19) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:162/proxy/: bar (200; 6.654969ms)
Mar  4 09:00:14.610: INFO: (19) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:443/proxy/tlsrewritem... (200; 7.374907ms)
Mar  4 09:00:14.613: INFO: (19) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q/proxy/rewriteme">test</a> (200; 9.411591ms)
Mar  4 09:00:14.613: INFO: (19) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:1080/proxy/rewriteme">... (200; 9.777078ms)
Mar  4 09:00:14.614: INFO: (19) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:162/proxy/: bar (200; 9.865599ms)
Mar  4 09:00:14.614: INFO: (19) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/: <a href="/api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:1080/proxy/rewriteme">test<... (200; 9.663787ms)
Mar  4 09:00:14.614: INFO: (19) /api/v1/namespaces/proxy-5080/pods/proxy-service-l79sz-75s7q:160/proxy/: foo (200; 9.976337ms)
Mar  4 09:00:14.614: INFO: (19) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:462/proxy/: tls qux (200; 11.229167ms)
Mar  4 09:00:14.616: INFO: (19) /api/v1/namespaces/proxy-5080/pods/http:proxy-service-l79sz-75s7q:160/proxy/: foo (200; 11.467323ms)
Mar  4 09:00:14.616: INFO: (19) /api/v1/namespaces/proxy-5080/pods/https:proxy-service-l79sz-75s7q:460/proxy/: tls baz (200; 12.230338ms)
Mar  4 09:00:14.619: INFO: (19) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname2/proxy/: tls qux (200; 15.090426ms)
Mar  4 09:00:14.620: INFO: (19) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname1/proxy/: foo (200; 15.766184ms)
Mar  4 09:00:14.620: INFO: (19) /api/v1/namespaces/proxy-5080/services/proxy-service-l79sz:portname2/proxy/: bar (200; 16.464878ms)
Mar  4 09:00:14.620: INFO: (19) /api/v1/namespaces/proxy-5080/services/https:proxy-service-l79sz:tlsportname1/proxy/: tls baz (200; 16.235297ms)
Mar  4 09:00:14.623: INFO: (19) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname1/proxy/: foo (200; 19.684153ms)
Mar  4 09:00:14.623: INFO: (19) /api/v1/namespaces/proxy-5080/services/http:proxy-service-l79sz:portname2/proxy/: bar (200; 19.573334ms)
STEP: deleting ReplicationController proxy-service-l79sz in namespace proxy-5080, will wait for the garbage collector to delete the pods
Mar  4 09:00:14.682: INFO: Deleting ReplicationController proxy-service-l79sz took: 5.514523ms
Mar  4 09:00:15.082: INFO: Terminating ReplicationController proxy-service-l79sz pods took: 400.17343ms
[AfterEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:00:18.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5080" for this suite.

• [SLOW TEST:13.700 seconds]
[sig-network] Proxy
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":278,"completed":9,"skipped":63,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:00:18.792: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:00:18.854: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:00:26.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1553" for this suite.

• [SLOW TEST:8.199 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:47
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":278,"completed":10,"skipped":71,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:00:26.991: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-325f6e79-72b1-4a12-aa0e-f0520371f40d
STEP: Creating a pod to test consume secrets
Mar  4 09:00:27.030: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3b8e3b7b-aa66-4838-a729-70bfe6316d4d" in namespace "projected-9695" to be "success or failure"
Mar  4 09:00:27.035: INFO: Pod "pod-projected-secrets-3b8e3b7b-aa66-4838-a729-70bfe6316d4d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.922006ms
Mar  4 09:00:29.041: INFO: Pod "pod-projected-secrets-3b8e3b7b-aa66-4838-a729-70bfe6316d4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010505071s
Mar  4 09:00:31.044: INFO: Pod "pod-projected-secrets-3b8e3b7b-aa66-4838-a729-70bfe6316d4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013662804s
STEP: Saw pod success
Mar  4 09:00:31.044: INFO: Pod "pod-projected-secrets-3b8e3b7b-aa66-4838-a729-70bfe6316d4d" satisfied condition "success or failure"
Mar  4 09:00:31.046: INFO: Trying to get logs from node master3 pod pod-projected-secrets-3b8e3b7b-aa66-4838-a729-70bfe6316d4d container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  4 09:00:31.063: INFO: Waiting for pod pod-projected-secrets-3b8e3b7b-aa66-4838-a729-70bfe6316d4d to disappear
Mar  4 09:00:31.066: INFO: Pod pod-projected-secrets-3b8e3b7b-aa66-4838-a729-70bfe6316d4d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:00:31.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9695" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":278,"completed":11,"skipped":74,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:00:31.074: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:00:31.139: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"e8f67e20-aa66-48ab-8e09-2965b256517b", Controller:(*bool)(0xc002fb4566), BlockOwnerDeletion:(*bool)(0xc002fb4567)}}
Mar  4 09:00:31.150: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"855a2545-ee8a-422f-9781-eb14db98b4d0", Controller:(*bool)(0xc002fb4886), BlockOwnerDeletion:(*bool)(0xc002fb4887)}}
Mar  4 09:00:31.158: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a273581e-6f91-4604-b773-3eba61594ccc", Controller:(*bool)(0xc002fb4b56), BlockOwnerDeletion:(*bool)(0xc002fb4b57)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:01:01.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-709" for this suite.

• [SLOW TEST:30.102 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":278,"completed":12,"skipped":98,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:01:01.176: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name projected-secret-test-c6b1cb73-656c-45f2-834f-c786de139743
STEP: Creating a pod to test consume secrets
Mar  4 09:01:01.275: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5c64c27d-17a2-4413-bd38-7f499ebde241" in namespace "projected-6783" to be "success or failure"
Mar  4 09:01:01.288: INFO: Pod "pod-projected-secrets-5c64c27d-17a2-4413-bd38-7f499ebde241": Phase="Pending", Reason="", readiness=false. Elapsed: 12.828864ms
Mar  4 09:01:03.291: INFO: Pod "pod-projected-secrets-5c64c27d-17a2-4413-bd38-7f499ebde241": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016098364s
STEP: Saw pod success
Mar  4 09:01:03.291: INFO: Pod "pod-projected-secrets-5c64c27d-17a2-4413-bd38-7f499ebde241" satisfied condition "success or failure"
Mar  4 09:01:03.293: INFO: Trying to get logs from node master3 pod pod-projected-secrets-5c64c27d-17a2-4413-bd38-7f499ebde241 container secret-volume-test: <nil>
STEP: delete the pod
Mar  4 09:01:03.312: INFO: Waiting for pod pod-projected-secrets-5c64c27d-17a2-4413-bd38-7f499ebde241 to disappear
Mar  4 09:01:03.314: INFO: Pod pod-projected-secrets-5c64c27d-17a2-4413-bd38-7f499ebde241 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:01:03.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6783" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":278,"completed":13,"skipped":103,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:01:03.331: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:01:05.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8941" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":14,"skipped":111,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:01:05.394: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar  4 09:01:05.427: INFO: Waiting up to 5m0s for pod "downwardapi-volume-85d876a9-5f98-4096-95a1-1edec567cc8a" in namespace "projected-576" to be "success or failure"
Mar  4 09:01:05.438: INFO: Pod "downwardapi-volume-85d876a9-5f98-4096-95a1-1edec567cc8a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.684207ms
Mar  4 09:01:07.441: INFO: Pod "downwardapi-volume-85d876a9-5f98-4096-95a1-1edec567cc8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013913404s
STEP: Saw pod success
Mar  4 09:01:07.441: INFO: Pod "downwardapi-volume-85d876a9-5f98-4096-95a1-1edec567cc8a" satisfied condition "success or failure"
Mar  4 09:01:07.444: INFO: Trying to get logs from node master3 pod downwardapi-volume-85d876a9-5f98-4096-95a1-1edec567cc8a container client-container: <nil>
STEP: delete the pod
Mar  4 09:01:07.461: INFO: Waiting for pod downwardapi-volume-85d876a9-5f98-4096-95a1-1edec567cc8a to disappear
Mar  4 09:01:07.465: INFO: Pod downwardapi-volume-85d876a9-5f98-4096-95a1-1edec567cc8a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:01:07.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-576" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":278,"completed":15,"skipped":128,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:01:07.476: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-b1624b84-fee8-4eb7-a2f6-24e351a14015
STEP: Creating a pod to test consume secrets
Mar  4 09:01:07.516: INFO: Waiting up to 5m0s for pod "pod-secrets-754b9016-5109-4bd1-ada3-2ff1af72bb12" in namespace "secrets-589" to be "success or failure"
Mar  4 09:01:07.519: INFO: Pod "pod-secrets-754b9016-5109-4bd1-ada3-2ff1af72bb12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.432345ms
Mar  4 09:01:09.521: INFO: Pod "pod-secrets-754b9016-5109-4bd1-ada3-2ff1af72bb12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005299771s
STEP: Saw pod success
Mar  4 09:01:09.522: INFO: Pod "pod-secrets-754b9016-5109-4bd1-ada3-2ff1af72bb12" satisfied condition "success or failure"
Mar  4 09:01:09.525: INFO: Trying to get logs from node master2 pod pod-secrets-754b9016-5109-4bd1-ada3-2ff1af72bb12 container secret-volume-test: <nil>
STEP: delete the pod
Mar  4 09:01:09.541: INFO: Waiting for pod pod-secrets-754b9016-5109-4bd1-ada3-2ff1af72bb12 to disappear
Mar  4 09:01:09.545: INFO: Pod pod-secrets-754b9016-5109-4bd1-ada3-2ff1af72bb12 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:01:09.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-589" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":278,"completed":16,"skipped":142,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:01:09.553: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  4 09:01:10.396: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  4 09:01:12.405: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718909270, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718909270, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718909270, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718909270, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  4 09:01:15.419: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:01:15.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1824" for this suite.
STEP: Destroying namespace "webhook-1824-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.001 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":278,"completed":17,"skipped":175,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:01:15.554: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar  4 09:01:15.605: INFO: Waiting up to 5m0s for pod "downwardapi-volume-89d22dea-1804-4bfe-9cea-2beeb63cf132" in namespace "projected-6667" to be "success or failure"
Mar  4 09:01:15.609: INFO: Pod "downwardapi-volume-89d22dea-1804-4bfe-9cea-2beeb63cf132": Phase="Pending", Reason="", readiness=false. Elapsed: 3.883066ms
Mar  4 09:01:17.612: INFO: Pod "downwardapi-volume-89d22dea-1804-4bfe-9cea-2beeb63cf132": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007069068s
STEP: Saw pod success
Mar  4 09:01:17.612: INFO: Pod "downwardapi-volume-89d22dea-1804-4bfe-9cea-2beeb63cf132" satisfied condition "success or failure"
Mar  4 09:01:17.615: INFO: Trying to get logs from node master2 pod downwardapi-volume-89d22dea-1804-4bfe-9cea-2beeb63cf132 container client-container: <nil>
STEP: delete the pod
Mar  4 09:01:17.642: INFO: Waiting for pod downwardapi-volume-89d22dea-1804-4bfe-9cea-2beeb63cf132 to disappear
Mar  4 09:01:17.645: INFO: Pod downwardapi-volume-89d22dea-1804-4bfe-9cea-2beeb63cf132 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:01:17.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6667" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":18,"skipped":180,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:01:17.652: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-7281
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating stateful set ss in namespace statefulset-7281
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7281
Mar  4 09:01:17.699: INFO: Found 0 stateful pods, waiting for 1
Mar  4 09:01:27.703: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar  4 09:01:27.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=statefulset-7281 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  4 09:01:28.059: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  4 09:01:28.059: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  4 09:01:28.059: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  4 09:01:28.062: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  4 09:01:38.065: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  4 09:01:38.065: INFO: Waiting for statefulset status.replicas updated to 0
Mar  4 09:01:38.080: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  4 09:01:38.080: INFO: ss-0  master2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:17 +0000 UTC  }]
Mar  4 09:01:38.080: INFO: ss-1           Pending         []
Mar  4 09:01:38.080: INFO: 
Mar  4 09:01:38.080: INFO: StatefulSet ss has not reached scale 3, at 2
Mar  4 09:01:39.084: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992707283s
Mar  4 09:01:40.089: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.98915705s
Mar  4 09:01:41.096: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984009061s
Mar  4 09:01:42.100: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.976735861s
Mar  4 09:01:43.103: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.973038199s
Mar  4 09:01:44.107: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.969532941s
Mar  4 09:01:45.111: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.966060604s
Mar  4 09:01:46.115: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.961983317s
Mar  4 09:01:47.118: INFO: Verifying statefulset ss doesn't scale past 3 for another 958.472245ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7281
Mar  4 09:01:48.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=statefulset-7281 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  4 09:01:48.342: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  4 09:01:48.342: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  4 09:01:48.342: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  4 09:01:48.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=statefulset-7281 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  4 09:01:48.583: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  4 09:01:48.583: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  4 09:01:48.583: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  4 09:01:48.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=statefulset-7281 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  4 09:01:48.812: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  4 09:01:48.812: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  4 09:01:48.812: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  4 09:01:48.815: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  4 09:01:48.815: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  4 09:01:48.815: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar  4 09:01:48.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=statefulset-7281 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  4 09:01:49.050: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  4 09:01:49.050: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  4 09:01:49.050: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  4 09:01:49.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=statefulset-7281 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  4 09:01:49.274: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  4 09:01:49.274: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  4 09:01:49.274: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  4 09:01:49.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=statefulset-7281 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  4 09:01:49.501: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  4 09:01:49.501: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  4 09:01:49.501: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  4 09:01:49.501: INFO: Waiting for statefulset status.replicas updated to 0
Mar  4 09:01:49.504: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar  4 09:01:59.510: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  4 09:01:59.510: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  4 09:01:59.510: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  4 09:01:59.517: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  4 09:01:59.517: INFO: ss-0  master2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:17 +0000 UTC  }]
Mar  4 09:01:59.517: INFO: ss-1  master3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:38 +0000 UTC  }]
Mar  4 09:01:59.517: INFO: ss-2  master2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:38 +0000 UTC  }]
Mar  4 09:01:59.518: INFO: 
Mar  4 09:01:59.518: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  4 09:02:00.521: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  4 09:02:00.521: INFO: ss-0  master2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:17 +0000 UTC  }]
Mar  4 09:02:00.521: INFO: ss-1  master3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:38 +0000 UTC  }]
Mar  4 09:02:00.521: INFO: ss-2  master2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:38 +0000 UTC  }]
Mar  4 09:02:00.521: INFO: 
Mar  4 09:02:00.521: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  4 09:02:01.525: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  4 09:02:01.525: INFO: ss-0  master2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:17 +0000 UTC  }]
Mar  4 09:02:01.525: INFO: ss-1  master3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:38 +0000 UTC  }]
Mar  4 09:02:01.525: INFO: ss-2  master2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:38 +0000 UTC  }]
Mar  4 09:02:01.525: INFO: 
Mar  4 09:02:01.525: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  4 09:02:02.528: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  4 09:02:02.528: INFO: ss-1  master3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:38 +0000 UTC  }]
Mar  4 09:02:02.528: INFO: 
Mar  4 09:02:02.528: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  4 09:02:03.531: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  4 09:02:03.531: INFO: ss-1  master3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:38 +0000 UTC  }]
Mar  4 09:02:03.531: INFO: 
Mar  4 09:02:03.531: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  4 09:02:04.535: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  4 09:02:04.535: INFO: ss-1  master3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:38 +0000 UTC  }]
Mar  4 09:02:04.535: INFO: 
Mar  4 09:02:04.535: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  4 09:02:05.538: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  4 09:02:05.538: INFO: ss-1  master3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:38 +0000 UTC  }]
Mar  4 09:02:05.538: INFO: 
Mar  4 09:02:05.538: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  4 09:02:06.541: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  4 09:02:06.541: INFO: ss-1  master3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:38 +0000 UTC  }]
Mar  4 09:02:06.541: INFO: 
Mar  4 09:02:06.541: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  4 09:02:07.544: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  4 09:02:07.545: INFO: ss-1  master3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:38 +0000 UTC  }]
Mar  4 09:02:07.545: INFO: 
Mar  4 09:02:07.545: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  4 09:02:08.548: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  4 09:02:08.548: INFO: ss-1  master3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-04 09:01:38 +0000 UTC  }]
Mar  4 09:02:08.548: INFO: 
Mar  4 09:02:08.548: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7281
Mar  4 09:02:09.551: INFO: Scaling statefulset ss to 0
Mar  4 09:02:09.558: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Mar  4 09:02:09.560: INFO: Deleting all statefulset in ns statefulset-7281
Mar  4 09:02:09.562: INFO: Scaling statefulset ss to 0
Mar  4 09:02:09.568: INFO: Waiting for statefulset status.replicas updated to 0
Mar  4 09:02:09.570: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:02:09.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7281" for this suite.

• [SLOW TEST:51.939 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":278,"completed":19,"skipped":198,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:02:09.591: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  4 09:02:09.675: INFO: Waiting up to 5m0s for pod "pod-e311631e-5b66-4894-8e23-0e226f8f859e" in namespace "emptydir-5805" to be "success or failure"
Mar  4 09:02:09.680: INFO: Pod "pod-e311631e-5b66-4894-8e23-0e226f8f859e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.084417ms
Mar  4 09:02:11.683: INFO: Pod "pod-e311631e-5b66-4894-8e23-0e226f8f859e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008016372s
STEP: Saw pod success
Mar  4 09:02:11.683: INFO: Pod "pod-e311631e-5b66-4894-8e23-0e226f8f859e" satisfied condition "success or failure"
Mar  4 09:02:11.686: INFO: Trying to get logs from node master3 pod pod-e311631e-5b66-4894-8e23-0e226f8f859e container test-container: <nil>
STEP: delete the pod
Mar  4 09:02:11.705: INFO: Waiting for pod pod-e311631e-5b66-4894-8e23-0e226f8f859e to disappear
Mar  4 09:02:11.707: INFO: Pod pod-e311631e-5b66-4894-8e23-0e226f8f859e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:02:11.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5805" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":20,"skipped":220,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:02:11.714: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-projected-all-test-volume-db402bd1-eebb-4f4d-ad46-a109f30d7696
STEP: Creating secret with name secret-projected-all-test-volume-ac6d3db2-9548-4c6a-ba88-e4a05bed129a
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar  4 09:02:11.754: INFO: Waiting up to 5m0s for pod "projected-volume-9c251074-9701-4690-8ced-cc571ba70a5e" in namespace "projected-2846" to be "success or failure"
Mar  4 09:02:11.758: INFO: Pod "projected-volume-9c251074-9701-4690-8ced-cc571ba70a5e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.99558ms
Mar  4 09:02:13.761: INFO: Pod "projected-volume-9c251074-9701-4690-8ced-cc571ba70a5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007140589s
STEP: Saw pod success
Mar  4 09:02:13.761: INFO: Pod "projected-volume-9c251074-9701-4690-8ced-cc571ba70a5e" satisfied condition "success or failure"
Mar  4 09:02:13.764: INFO: Trying to get logs from node master3 pod projected-volume-9c251074-9701-4690-8ced-cc571ba70a5e container projected-all-volume-test: <nil>
STEP: delete the pod
Mar  4 09:02:13.778: INFO: Waiting for pod projected-volume-9c251074-9701-4690-8ced-cc571ba70a5e to disappear
Mar  4 09:02:13.781: INFO: Pod projected-volume-9c251074-9701-4690-8ced-cc571ba70a5e no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:02:13.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2846" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":278,"completed":21,"skipped":258,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:02:13.790: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:330
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the initial replication controller
Mar  4 09:02:13.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 create -f - --namespace=kubectl-8165'
Mar  4 09:02:14.141: INFO: stderr: ""
Mar  4 09:02:14.141: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  4 09:02:14.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8165'
Mar  4 09:02:14.252: INFO: stderr: ""
Mar  4 09:02:14.252: INFO: stdout: "update-demo-nautilus-mbt45 update-demo-nautilus-t7vn6 "
Mar  4 09:02:14.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods update-demo-nautilus-mbt45 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8165'
Mar  4 09:02:14.349: INFO: stderr: ""
Mar  4 09:02:14.349: INFO: stdout: ""
Mar  4 09:02:14.349: INFO: update-demo-nautilus-mbt45 is created but not running
Mar  4 09:02:19.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8165'
Mar  4 09:02:19.444: INFO: stderr: ""
Mar  4 09:02:19.444: INFO: stdout: "update-demo-nautilus-mbt45 update-demo-nautilus-t7vn6 "
Mar  4 09:02:19.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods update-demo-nautilus-mbt45 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8165'
Mar  4 09:02:19.562: INFO: stderr: ""
Mar  4 09:02:19.562: INFO: stdout: "true"
Mar  4 09:02:19.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods update-demo-nautilus-mbt45 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8165'
Mar  4 09:02:19.662: INFO: stderr: ""
Mar  4 09:02:19.662: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  4 09:02:19.662: INFO: validating pod update-demo-nautilus-mbt45
Mar  4 09:02:19.667: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  4 09:02:19.667: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  4 09:02:19.668: INFO: update-demo-nautilus-mbt45 is verified up and running
Mar  4 09:02:19.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods update-demo-nautilus-t7vn6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8165'
Mar  4 09:02:19.762: INFO: stderr: ""
Mar  4 09:02:19.762: INFO: stdout: "true"
Mar  4 09:02:19.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods update-demo-nautilus-t7vn6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8165'
Mar  4 09:02:19.856: INFO: stderr: ""
Mar  4 09:02:19.856: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  4 09:02:19.856: INFO: validating pod update-demo-nautilus-t7vn6
Mar  4 09:02:19.862: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  4 09:02:19.862: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  4 09:02:19.862: INFO: update-demo-nautilus-t7vn6 is verified up and running
STEP: rolling-update to new replication controller
Mar  4 09:02:19.864: INFO: scanned /root for discovery docs: <nil>
Mar  4 09:02:19.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-8165'
Mar  4 09:02:42.323: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar  4 09:02:42.324: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  4 09:02:42.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8165'
Mar  4 09:02:42.424: INFO: stderr: ""
Mar  4 09:02:42.424: INFO: stdout: "update-demo-kitten-7w4cl update-demo-kitten-q5nml update-demo-nautilus-t7vn6 "
STEP: Replicas for name=update-demo: expected=2 actual=3
Mar  4 09:02:47.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8165'
Mar  4 09:02:47.516: INFO: stderr: ""
Mar  4 09:02:47.516: INFO: stdout: "update-demo-kitten-7w4cl update-demo-kitten-q5nml "
Mar  4 09:02:47.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods update-demo-kitten-7w4cl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8165'
Mar  4 09:02:47.605: INFO: stderr: ""
Mar  4 09:02:47.605: INFO: stdout: "true"
Mar  4 09:02:47.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods update-demo-kitten-7w4cl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8165'
Mar  4 09:02:47.693: INFO: stderr: ""
Mar  4 09:02:47.693: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar  4 09:02:47.693: INFO: validating pod update-demo-kitten-7w4cl
Mar  4 09:02:47.699: INFO: got data: {
  "image": "kitten.jpg"
}

Mar  4 09:02:47.699: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar  4 09:02:47.699: INFO: update-demo-kitten-7w4cl is verified up and running
Mar  4 09:02:47.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods update-demo-kitten-q5nml -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8165'
Mar  4 09:02:47.797: INFO: stderr: ""
Mar  4 09:02:47.797: INFO: stdout: "true"
Mar  4 09:02:47.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods update-demo-kitten-q5nml -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8165'
Mar  4 09:02:47.887: INFO: stderr: ""
Mar  4 09:02:47.887: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar  4 09:02:47.887: INFO: validating pod update-demo-kitten-q5nml
Mar  4 09:02:47.892: INFO: got data: {
  "image": "kitten.jpg"
}

Mar  4 09:02:47.892: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar  4 09:02:47.892: INFO: update-demo-kitten-q5nml is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:02:47.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8165" for this suite.

• [SLOW TEST:34.109 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:328
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should do a rolling update of a replication controller  [Conformance]","total":278,"completed":22,"skipped":277,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:02:47.899: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:02:47.926: INFO: Creating deployment "webserver-deployment"
Mar  4 09:02:47.929: INFO: Waiting for observed generation 1
Mar  4 09:02:49.945: INFO: Waiting for all required pods to come up
Mar  4 09:02:49.950: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar  4 09:02:53.961: INFO: Waiting for deployment "webserver-deployment" to complete
Mar  4 09:02:53.967: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar  4 09:02:53.972: INFO: Updating deployment webserver-deployment
Mar  4 09:02:53.972: INFO: Waiting for observed generation 2
Mar  4 09:02:55.991: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar  4 09:02:55.996: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar  4 09:02:55.998: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar  4 09:02:56.008: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar  4 09:02:56.008: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar  4 09:02:56.011: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar  4 09:02:56.019: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar  4 09:02:56.019: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar  4 09:02:56.032: INFO: Updating deployment webserver-deployment
Mar  4 09:02:56.032: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar  4 09:02:56.076: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar  4 09:02:58.107: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Mar  4 09:02:58.117: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-581 /apis/apps/v1/namespaces/deployment-581/deployments/webserver-deployment dfe7d600-56ce-45a3-bc83-9340924f7f55 44821 3 2020-03-04 09:02:47 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002501d58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-03-04 09:02:56 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-03-04 09:02:56 +0000 UTC,LastTransitionTime:2020-03-04 09:02:47 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar  4 09:02:58.121: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-581 /apis/apps/v1/namespaces/deployment-581/replicasets/webserver-deployment-c7997dcc8 21998a18-4a2e-4d99-a452-f0adb64edb23 44820 3 2020-03-04 09:02:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment dfe7d600-56ce-45a3-bc83-9340924f7f55 0xc00248d3e7 0xc00248d3e8}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00248d478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  4 09:02:58.121: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar  4 09:02:58.121: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-581 /apis/apps/v1/namespaces/deployment-581/replicasets/webserver-deployment-595b5b9587 41a50ce4-7af0-4d9d-9c5d-299f41768828 44774 3 2020-03-04 09:02:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment dfe7d600-56ce-45a3-bc83-9340924f7f55 0xc00248d237 0xc00248d238}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00248d378 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar  4 09:02:58.135: INFO: Pod "webserver-deployment-595b5b9587-2zzd8" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2zzd8 webserver-deployment-595b5b9587- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-595b5b9587-2zzd8 3f4c0fe0-ab51-4c43-b69e-533cefcc6b33 44766 0 2020-03-04 09:02:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 41a50ce4-7af0-4d9d-9c5d-299f41768828 0xc002480177 0xc002480178}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.135: INFO: Pod "webserver-deployment-595b5b9587-4gfr9" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-4gfr9 webserver-deployment-595b5b9587- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-595b5b9587-4gfr9 d0c04155-24fb-42e9-8ff7-2a4807d82128 44582 0 2020-03-04 09:02:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.136.31/32 cni.projectcalico.org/podIPs:192.168.136.31/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 41a50ce4-7af0-4d9d-9c5d-299f41768828 0xc002480690 0xc002480691}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.4,PodIP:192.168.136.31,StartTime:2020-03-04 09:02:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-04 09:02:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://e74d9627c4523d2370b036535d8c3306442de970440ddc0437a58c36b5db0b8a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.136.31,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.136: INFO: Pod "webserver-deployment-595b5b9587-5c8wm" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-5c8wm webserver-deployment-595b5b9587- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-595b5b9587-5c8wm 7849fdb5-6e96-427c-b701-a733b59f943f 44838 0 2020-03-04 09:02:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 41a50ce4-7af0-4d9d-9c5d-299f41768828 0xc002480cc0 0xc002480cc1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.3,PodIP:,StartTime:2020-03-04 09:02:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.136: INFO: Pod "webserver-deployment-595b5b9587-6r9gd" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-6r9gd webserver-deployment-595b5b9587- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-595b5b9587-6r9gd 5eebe63d-f136-4a59-9de4-94efc1024369 44762 0 2020-03-04 09:02:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 41a50ce4-7af0-4d9d-9c5d-299f41768828 0xc002481700 0xc002481701}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.136: INFO: Pod "webserver-deployment-595b5b9587-7vhv6" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-7vhv6 webserver-deployment-595b5b9587- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-595b5b9587-7vhv6 34cac607-656a-47ed-8a61-9ab9849cfbc3 44555 0 2020-03-04 09:02:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.180.58/32 cni.projectcalico.org/podIPs:192.168.180.58/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 41a50ce4-7af0-4d9d-9c5d-299f41768828 0xc002481820 0xc002481821}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.3,PodIP:192.168.180.58,StartTime:2020-03-04 09:02:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-04 09:02:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://1ff3049653ca5214f44a73c045b6d3803dfd82fe4b9cb679469c4db587df2a24,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.180.58,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.136: INFO: Pod "webserver-deployment-595b5b9587-bbhbk" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bbhbk webserver-deployment-595b5b9587- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-595b5b9587-bbhbk 74b3d29f-0e48-4605-9677-7dbd689c2a89 44562 0 2020-03-04 09:02:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.180.60/32 cni.projectcalico.org/podIPs:192.168.180.60/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 41a50ce4-7af0-4d9d-9c5d-299f41768828 0xc002481bc0 0xc002481bc1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.3,PodIP:192.168.180.60,StartTime:2020-03-04 09:02:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-04 09:02:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://62aad87bb8d5a74d2e232726a48aed6b7d8c0274b741e91c4aa778af92ec84eb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.180.60,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.136: INFO: Pod "webserver-deployment-595b5b9587-cw58t" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-cw58t webserver-deployment-595b5b9587- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-595b5b9587-cw58t 505b7f51-6c06-4b2f-94fe-f212b122c400 44552 0 2020-03-04 09:02:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.136.38/32 cni.projectcalico.org/podIPs:192.168.136.38/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 41a50ce4-7af0-4d9d-9c5d-299f41768828 0xc002481fe0 0xc002481fe1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.4,PodIP:192.168.136.38,StartTime:2020-03-04 09:02:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-04 09:02:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://e641b9b65cf66d91ca2ac471b852bd8f8ac59d08681dd801a6f193c85d53c60d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.136.38,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.136: INFO: Pod "webserver-deployment-595b5b9587-d96rg" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-d96rg webserver-deployment-595b5b9587- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-595b5b9587-d96rg 6bf8ff0c-c8b9-4ad9-95be-ed79e91df593 44573 0 2020-03-04 09:02:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.180.59/32 cni.projectcalico.org/podIPs:192.168.180.59/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 41a50ce4-7af0-4d9d-9c5d-299f41768828 0xc002384150 0xc002384151}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.3,PodIP:192.168.180.59,StartTime:2020-03-04 09:02:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-04 09:02:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://8f6c78d08eac5b23affda57d0053044e18e470796d38f3b2e1178c62e71e6d5a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.180.59,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.137: INFO: Pod "webserver-deployment-595b5b9587-fvjdw" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-fvjdw webserver-deployment-595b5b9587- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-595b5b9587-fvjdw b9895d1c-e3af-4988-b375-5c4e3e17f0e2 44788 0 2020-03-04 09:02:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 41a50ce4-7af0-4d9d-9c5d-299f41768828 0xc0023842e0 0xc0023842e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.3,PodIP:,StartTime:2020-03-04 09:02:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.137: INFO: Pod "webserver-deployment-595b5b9587-k2pzl" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-k2pzl webserver-deployment-595b5b9587- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-595b5b9587-k2pzl 27c12c03-3748-403d-9de6-e186fdc725ab 44849 0 2020-03-04 09:02:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.136.27/32 cni.projectcalico.org/podIPs:192.168.136.27/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 41a50ce4-7af0-4d9d-9c5d-299f41768828 0xc002384430 0xc002384431}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.4,PodIP:,StartTime:2020-03-04 09:02:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.144: INFO: Pod "webserver-deployment-595b5b9587-lstzz" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-lstzz webserver-deployment-595b5b9587- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-595b5b9587-lstzz be74bdf5-be23-46f7-a613-d8d00f8b59a7 44558 0 2020-03-04 09:02:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.180.62/32 cni.projectcalico.org/podIPs:192.168.180.62/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 41a50ce4-7af0-4d9d-9c5d-299f41768828 0xc002384580 0xc002384581}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.3,PodIP:192.168.180.62,StartTime:2020-03-04 09:02:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-04 09:02:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://29702865bdbd16147289b06b60007b4bd2076a7b59bf01365a71c1c087b73024,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.180.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.144: INFO: Pod "webserver-deployment-595b5b9587-lxx9l" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-lxx9l webserver-deployment-595b5b9587- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-595b5b9587-lxx9l c54895d5-4fb2-4f5d-9092-0ff6b6fd82e3 44565 0 2020-03-04 09:02:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.180.61/32 cni.projectcalico.org/podIPs:192.168.180.61/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 41a50ce4-7af0-4d9d-9c5d-299f41768828 0xc0023846f0 0xc0023846f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.3,PodIP:192.168.180.61,StartTime:2020-03-04 09:02:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-04 09:02:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://114097c8aa61d3604fb6b0fabb40ed3a92310598bd6d5b1e0d86e0586f7ba22f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.180.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.144: INFO: Pod "webserver-deployment-595b5b9587-mznn8" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mznn8 webserver-deployment-595b5b9587- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-595b5b9587-mznn8 4b17fdc2-20ad-44d8-8d06-51baa2eb8e57 44830 0 2020-03-04 09:02:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 41a50ce4-7af0-4d9d-9c5d-299f41768828 0xc002384860 0xc002384861}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.3,PodIP:,StartTime:2020-03-04 09:02:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.144: INFO: Pod "webserver-deployment-595b5b9587-pdhpp" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-pdhpp webserver-deployment-595b5b9587- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-595b5b9587-pdhpp 1bac91b8-c82d-42a6-bfc1-845fc011bfa6 44851 0 2020-03-04 09:02:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 41a50ce4-7af0-4d9d-9c5d-299f41768828 0xc0023849b0 0xc0023849b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.4,PodIP:,StartTime:2020-03-04 09:02:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.144: INFO: Pod "webserver-deployment-595b5b9587-qq6d5" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qq6d5 webserver-deployment-595b5b9587- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-595b5b9587-qq6d5 f888346c-1f7f-414b-9547-23f0b2fcaa7d 44548 0 2020-03-04 09:02:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.136.36/32 cni.projectcalico.org/podIPs:192.168.136.36/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 41a50ce4-7af0-4d9d-9c5d-299f41768828 0xc002384b00 0xc002384b01}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.4,PodIP:192.168.136.36,StartTime:2020-03-04 09:02:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-04 09:02:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://80dd79a2c548ae3be3d814c285ca79d0ca4bf1535260ccbfee70273c9c9c2d8b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.136.36,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.145: INFO: Pod "webserver-deployment-595b5b9587-srvbv" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-srvbv webserver-deployment-595b5b9587- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-595b5b9587-srvbv 71e164dc-5aea-496f-a23b-f734bc289e0f 44772 0 2020-03-04 09:02:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 41a50ce4-7af0-4d9d-9c5d-299f41768828 0xc002384c70 0xc002384c71}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.4,PodIP:,StartTime:2020-03-04 09:02:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.145: INFO: Pod "webserver-deployment-595b5b9587-vp4tk" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-vp4tk webserver-deployment-595b5b9587- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-595b5b9587-vp4tk 7d7bf031-6ab0-4f40-a81e-42d4e0f098c0 44784 0 2020-03-04 09:02:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 41a50ce4-7af0-4d9d-9c5d-299f41768828 0xc002384dc0 0xc002384dc1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.4,PodIP:,StartTime:2020-03-04 09:02:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.145: INFO: Pod "webserver-deployment-595b5b9587-wpskl" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-wpskl webserver-deployment-595b5b9587- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-595b5b9587-wpskl d72f072b-86f7-4d4b-ae59-a78d8c596565 44794 0 2020-03-04 09:02:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 41a50ce4-7af0-4d9d-9c5d-299f41768828 0xc002384f10 0xc002384f11}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.3,PodIP:,StartTime:2020-03-04 09:02:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.145: INFO: Pod "webserver-deployment-595b5b9587-xnjhd" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xnjhd webserver-deployment-595b5b9587- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-595b5b9587-xnjhd 859bf9b7-f110-4fdb-aae2-68fdf83ffe57 44827 0 2020-03-04 09:02:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 41a50ce4-7af0-4d9d-9c5d-299f41768828 0xc002385060 0xc002385061}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.4,PodIP:,StartTime:2020-03-04 09:02:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.150: INFO: Pod "webserver-deployment-595b5b9587-zfgsz" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-zfgsz webserver-deployment-595b5b9587- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-595b5b9587-zfgsz cdd78282-8d30-4e50-8d9c-add0acbd5ec9 44846 0 2020-03-04 09:02:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 41a50ce4-7af0-4d9d-9c5d-299f41768828 0xc0023851b0 0xc0023851b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.3,PodIP:,StartTime:2020-03-04 09:02:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.151: INFO: Pod "webserver-deployment-c7997dcc8-2n7hn" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-2n7hn webserver-deployment-c7997dcc8- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-c7997dcc8-2n7hn 8bdaf840-0388-44f3-a0e4-516b9630c8f5 44779 0 2020-03-04 09:02:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 21998a18-4a2e-4d99-a452-f0adb64edb23 0xc002385300 0xc002385301}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.151: INFO: Pod "webserver-deployment-c7997dcc8-4qcz7" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-4qcz7 webserver-deployment-c7997dcc8- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-c7997dcc8-4qcz7 0a5802a1-3308-437e-8d48-32b5d1c34bf6 44710 0 2020-03-04 09:02:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.136.28/32 cni.projectcalico.org/podIPs:192.168.136.28/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 21998a18-4a2e-4d99-a452-f0adb64edb23 0xc002385440 0xc002385441}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.4,PodIP:,StartTime:2020-03-04 09:02:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.151: INFO: Pod "webserver-deployment-c7997dcc8-4w49t" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-4w49t webserver-deployment-c7997dcc8- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-c7997dcc8-4w49t 9b15ef4b-0a17-4f1b-9e53-f71de45cf08e 44845 0 2020-03-04 09:02:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 21998a18-4a2e-4d99-a452-f0adb64edb23 0xc0023855b0 0xc0023855b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.4,PodIP:,StartTime:2020-03-04 09:02:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.151: INFO: Pod "webserver-deployment-c7997dcc8-6srhl" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-6srhl webserver-deployment-c7997dcc8- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-c7997dcc8-6srhl 57ac4ef6-bd77-434b-b2c2-e547fefbc871 44769 0 2020-03-04 09:02:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 21998a18-4a2e-4d99-a452-f0adb64edb23 0xc002385720 0xc002385721}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.152: INFO: Pod "webserver-deployment-c7997dcc8-8nkwx" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-8nkwx webserver-deployment-c7997dcc8- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-c7997dcc8-8nkwx 83f5c545-eea8-48a1-8962-dd04d19cd8b3 44818 0 2020-03-04 09:02:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.180.63/32 cni.projectcalico.org/podIPs:192.168.180.63/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 21998a18-4a2e-4d99-a452-f0adb64edb23 0xc002385840 0xc002385841}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.3,PodIP:192.168.180.63,StartTime:2020-03-04 09:02:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.180.63,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.152: INFO: Pod "webserver-deployment-c7997dcc8-b5hrg" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-b5hrg webserver-deployment-c7997dcc8- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-c7997dcc8-b5hrg 976a63f7-097f-4fcd-a61c-99c340453e1b 44780 0 2020-03-04 09:02:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 21998a18-4a2e-4d99-a452-f0adb64edb23 0xc0023859e0 0xc0023859e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.3,PodIP:,StartTime:2020-03-04 09:02:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.152: INFO: Pod "webserver-deployment-c7997dcc8-h5vd6" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-h5vd6 webserver-deployment-c7997dcc8- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-c7997dcc8-h5vd6 8fbfe6c4-d591-4a42-8723-57c02d222321 44809 0 2020-03-04 09:02:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.136.35/32 cni.projectcalico.org/podIPs:192.168.136.35/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 21998a18-4a2e-4d99-a452-f0adb64edb23 0xc002385b50 0xc002385b51}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.4,PodIP:,StartTime:2020-03-04 09:02:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.152: INFO: Pod "webserver-deployment-c7997dcc8-j5bvr" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-j5bvr webserver-deployment-c7997dcc8- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-c7997dcc8-j5bvr 87b1eb25-f49d-4824-826e-ec354d0fbda5 44813 0 2020-03-04 09:02:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 21998a18-4a2e-4d99-a452-f0adb64edb23 0xc002385cd0 0xc002385cd1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.4,PodIP:,StartTime:2020-03-04 09:02:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.152: INFO: Pod "webserver-deployment-c7997dcc8-wmr7t" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-wmr7t webserver-deployment-c7997dcc8- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-c7997dcc8-wmr7t 2ba5c775-137c-4fd5-a6cb-a0bf389e4e21 44765 0 2020-03-04 09:02:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.136.29/32 cni.projectcalico.org/podIPs:192.168.136.29/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 21998a18-4a2e-4d99-a452-f0adb64edb23 0xc002385e50 0xc002385e51}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.4,PodIP:,StartTime:2020-03-04 09:02:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.153: INFO: Pod "webserver-deployment-c7997dcc8-z4pxf" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-z4pxf webserver-deployment-c7997dcc8- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-c7997dcc8-z4pxf 97ea969e-47fe-4303-81ee-951669f79e3f 44801 0 2020-03-04 09:02:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.180.4/32 cni.projectcalico.org/podIPs:192.168.180.4/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 21998a18-4a2e-4d99-a452-f0adb64edb23 0xc002385fe0 0xc002385fe1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.3,PodIP:,StartTime:2020-03-04 09:02:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.153: INFO: Pod "webserver-deployment-c7997dcc8-z95nz" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-z95nz webserver-deployment-c7997dcc8- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-c7997dcc8-z95nz 9a7ae78c-4643-4d40-9c78-b41319e8524d 44852 0 2020-03-04 09:02:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 21998a18-4a2e-4d99-a452-f0adb64edb23 0xc002374160 0xc002374161}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.3,PodIP:,StartTime:2020-03-04 09:02:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.159: INFO: Pod "webserver-deployment-c7997dcc8-zf5dk" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-zf5dk webserver-deployment-c7997dcc8- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-c7997dcc8-zf5dk 58b1c4eb-814a-4b33-8ca6-d4db2d4a107c 44761 0 2020-03-04 09:02:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 21998a18-4a2e-4d99-a452-f0adb64edb23 0xc0023742d0 0xc0023742d1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.3,PodIP:,StartTime:2020-03-04 09:02:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  4 09:02:58.160: INFO: Pod "webserver-deployment-c7997dcc8-zpzz7" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-zpzz7 webserver-deployment-c7997dcc8- deployment-581 /api/v1/namespaces/deployment-581/pods/webserver-deployment-c7997dcc8-zpzz7 363e8bcb-7615-44c4-88d0-43a2f83f3b2c 44837 0 2020-03-04 09:02:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 21998a18-4a2e-4d99-a452-f0adb64edb23 0xc002374440 0xc002374441}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nbq5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nbq5v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nbq5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:02:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.4,PodIP:,StartTime:2020-03-04 09:02:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:02:58.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-581" for this suite.

• [SLOW TEST:10.280 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":278,"completed":23,"skipped":299,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:02:58.180: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar  4 09:02:58.230: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aba94882-47ef-4bf3-a428-5b41e9a9301f" in namespace "downward-api-3834" to be "success or failure"
Mar  4 09:02:58.238: INFO: Pod "downwardapi-volume-aba94882-47ef-4bf3-a428-5b41e9a9301f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.965989ms
Mar  4 09:03:00.245: INFO: Pod "downwardapi-volume-aba94882-47ef-4bf3-a428-5b41e9a9301f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014910573s
Mar  4 09:03:02.254: INFO: Pod "downwardapi-volume-aba94882-47ef-4bf3-a428-5b41e9a9301f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024053765s
Mar  4 09:03:04.302: INFO: Pod "downwardapi-volume-aba94882-47ef-4bf3-a428-5b41e9a9301f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072007417s
Mar  4 09:03:06.315: INFO: Pod "downwardapi-volume-aba94882-47ef-4bf3-a428-5b41e9a9301f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.084895776s
STEP: Saw pod success
Mar  4 09:03:06.315: INFO: Pod "downwardapi-volume-aba94882-47ef-4bf3-a428-5b41e9a9301f" satisfied condition "success or failure"
Mar  4 09:03:06.323: INFO: Trying to get logs from node master3 pod downwardapi-volume-aba94882-47ef-4bf3-a428-5b41e9a9301f container client-container: <nil>
STEP: delete the pod
Mar  4 09:03:06.365: INFO: Waiting for pod downwardapi-volume-aba94882-47ef-4bf3-a428-5b41e9a9301f to disappear
Mar  4 09:03:06.372: INFO: Pod downwardapi-volume-aba94882-47ef-4bf3-a428-5b41e9a9301f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:03:06.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3834" for this suite.

• [SLOW TEST:8.216 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":278,"completed":24,"skipped":307,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:03:06.396: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-7473dd29-db15-4f71-a7fc-c3430f51f6ca
STEP: Creating a pod to test consume configMaps
Mar  4 09:03:06.489: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1371bf1f-007e-4a78-8e06-5d0e92f82010" in namespace "projected-3527" to be "success or failure"
Mar  4 09:03:06.500: INFO: Pod "pod-projected-configmaps-1371bf1f-007e-4a78-8e06-5d0e92f82010": Phase="Pending", Reason="", readiness=false. Elapsed: 11.035973ms
Mar  4 09:03:08.504: INFO: Pod "pod-projected-configmaps-1371bf1f-007e-4a78-8e06-5d0e92f82010": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014682497s
Mar  4 09:03:10.507: INFO: Pod "pod-projected-configmaps-1371bf1f-007e-4a78-8e06-5d0e92f82010": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018097213s
STEP: Saw pod success
Mar  4 09:03:10.507: INFO: Pod "pod-projected-configmaps-1371bf1f-007e-4a78-8e06-5d0e92f82010" satisfied condition "success or failure"
Mar  4 09:03:10.510: INFO: Trying to get logs from node master3 pod pod-projected-configmaps-1371bf1f-007e-4a78-8e06-5d0e92f82010 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  4 09:03:10.532: INFO: Waiting for pod pod-projected-configmaps-1371bf1f-007e-4a78-8e06-5d0e92f82010 to disappear
Mar  4 09:03:10.537: INFO: Pod pod-projected-configmaps-1371bf1f-007e-4a78-8e06-5d0e92f82010 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:03:10.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3527" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":25,"skipped":314,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:03:10.550: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  4 09:03:11.139: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  4 09:03:14.163: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:03:14.166: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8748-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:03:15.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7845" for this suite.
STEP: Destroying namespace "webhook-7845-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.202 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":278,"completed":26,"skipped":347,"failed":0}
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:03:15.752: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-b2c91fbd-fea8-44a2-b683-cd4dbb228b16
STEP: Creating a pod to test consume configMaps
Mar  4 09:03:15.868: INFO: Waiting up to 5m0s for pod "pod-configmaps-02c6d481-00de-42c8-98f6-9bb96b90587c" in namespace "configmap-7995" to be "success or failure"
Mar  4 09:03:15.883: INFO: Pod "pod-configmaps-02c6d481-00de-42c8-98f6-9bb96b90587c": Phase="Pending", Reason="", readiness=false. Elapsed: 15.518138ms
Mar  4 09:03:17.887: INFO: Pod "pod-configmaps-02c6d481-00de-42c8-98f6-9bb96b90587c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018917915s
STEP: Saw pod success
Mar  4 09:03:17.887: INFO: Pod "pod-configmaps-02c6d481-00de-42c8-98f6-9bb96b90587c" satisfied condition "success or failure"
Mar  4 09:03:17.889: INFO: Trying to get logs from node master3 pod pod-configmaps-02c6d481-00de-42c8-98f6-9bb96b90587c container configmap-volume-test: <nil>
STEP: delete the pod
Mar  4 09:03:17.915: INFO: Waiting for pod pod-configmaps-02c6d481-00de-42c8-98f6-9bb96b90587c to disappear
Mar  4 09:03:17.920: INFO: Pod pod-configmaps-02c6d481-00de-42c8-98f6-9bb96b90587c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:03:17.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7995" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":27,"skipped":347,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:03:17.929: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating all guestbook components
Mar  4 09:03:17.970: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Mar  4 09:03:17.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 create -f - --namespace=kubectl-8978'
Mar  4 09:03:18.349: INFO: stderr: ""
Mar  4 09:03:18.349: INFO: stdout: "service/agnhost-slave created\n"
Mar  4 09:03:18.350: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Mar  4 09:03:18.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 create -f - --namespace=kubectl-8978'
Mar  4 09:03:18.661: INFO: stderr: ""
Mar  4 09:03:18.661: INFO: stdout: "service/agnhost-master created\n"
Mar  4 09:03:18.661: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar  4 09:03:18.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 create -f - --namespace=kubectl-8978'
Mar  4 09:03:19.004: INFO: stderr: ""
Mar  4 09:03:19.004: INFO: stdout: "service/frontend created\n"
Mar  4 09:03:19.004: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar  4 09:03:19.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 create -f - --namespace=kubectl-8978'
Mar  4 09:03:19.264: INFO: stderr: ""
Mar  4 09:03:19.264: INFO: stdout: "deployment.apps/frontend created\n"
Mar  4 09:03:19.264: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  4 09:03:19.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 create -f - --namespace=kubectl-8978'
Mar  4 09:03:19.539: INFO: stderr: ""
Mar  4 09:03:19.539: INFO: stdout: "deployment.apps/agnhost-master created\n"
Mar  4 09:03:19.539: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  4 09:03:19.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 create -f - --namespace=kubectl-8978'
Mar  4 09:03:20.165: INFO: stderr: ""
Mar  4 09:03:20.165: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Mar  4 09:03:20.165: INFO: Waiting for all frontend pods to be Running.
Mar  4 09:03:25.216: INFO: Waiting for frontend to serve content.
Mar  4 09:03:25.225: INFO: Trying to add a new entry to the guestbook.
Mar  4 09:03:25.235: INFO: Verifying that added entry can be retrieved.
Mar  4 09:03:25.245: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Mar  4 09:03:30.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 delete --grace-period=0 --force -f - --namespace=kubectl-8978'
Mar  4 09:03:30.400: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  4 09:03:30.400: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Mar  4 09:03:30.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 delete --grace-period=0 --force -f - --namespace=kubectl-8978'
Mar  4 09:03:30.573: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  4 09:03:30.573: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Mar  4 09:03:30.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 delete --grace-period=0 --force -f - --namespace=kubectl-8978'
Mar  4 09:03:30.762: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  4 09:03:30.762: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  4 09:03:30.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 delete --grace-period=0 --force -f - --namespace=kubectl-8978'
Mar  4 09:03:30.900: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  4 09:03:30.900: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  4 09:03:30.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 delete --grace-period=0 --force -f - --namespace=kubectl-8978'
Mar  4 09:03:31.130: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  4 09:03:31.130: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Mar  4 09:03:31.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 delete --grace-period=0 --force -f - --namespace=kubectl-8978'
Mar  4 09:03:31.264: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  4 09:03:31.264: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:03:31.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8978" for this suite.

• [SLOW TEST:13.352 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:386
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":278,"completed":28,"skipped":349,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:03:31.282: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7195 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7195;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7195 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7195;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7195.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7195.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7195.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7195.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7195.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7195.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7195.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7195.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7195.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7195.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7195.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7195.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7195.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 247.143.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.143.247_udp@PTR;check="$$(dig +tcp +noall +answer +search 247.143.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.143.247_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7195 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7195;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7195 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7195;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7195.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7195.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7195.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7195.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7195.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7195.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7195.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7195.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7195.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7195.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7195.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7195.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7195.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 247.143.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.143.247_udp@PTR;check="$$(dig +tcp +noall +answer +search 247.143.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.143.247_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  4 09:03:35.464: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:35.468: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:35.472: INFO: Unable to read wheezy_udp@dns-test-service.dns-7195 from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:35.475: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7195 from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:35.479: INFO: Unable to read wheezy_udp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:35.482: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:35.488: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:35.492: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:35.524: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:35.528: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:35.531: INFO: Unable to read jessie_udp@dns-test-service.dns-7195 from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:35.535: INFO: Unable to read jessie_tcp@dns-test-service.dns-7195 from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:35.538: INFO: Unable to read jessie_udp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:35.542: INFO: Unable to read jessie_tcp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:35.546: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:35.549: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:35.570: INFO: Lookups using dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7195 wheezy_tcp@dns-test-service.dns-7195 wheezy_udp@dns-test-service.dns-7195.svc wheezy_tcp@dns-test-service.dns-7195.svc wheezy_udp@_http._tcp.dns-test-service.dns-7195.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7195.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7195 jessie_tcp@dns-test-service.dns-7195 jessie_udp@dns-test-service.dns-7195.svc jessie_tcp@dns-test-service.dns-7195.svc jessie_udp@_http._tcp.dns-test-service.dns-7195.svc jessie_tcp@_http._tcp.dns-test-service.dns-7195.svc]

Mar  4 09:03:40.574: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:40.580: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:40.586: INFO: Unable to read wheezy_udp@dns-test-service.dns-7195 from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:40.593: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7195 from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:40.598: INFO: Unable to read wheezy_udp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:40.601: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:40.605: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:40.609: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:40.634: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:40.638: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:40.643: INFO: Unable to read jessie_udp@dns-test-service.dns-7195 from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:40.649: INFO: Unable to read jessie_tcp@dns-test-service.dns-7195 from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:40.655: INFO: Unable to read jessie_udp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:40.659: INFO: Unable to read jessie_tcp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:40.662: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:40.666: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:40.685: INFO: Lookups using dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7195 wheezy_tcp@dns-test-service.dns-7195 wheezy_udp@dns-test-service.dns-7195.svc wheezy_tcp@dns-test-service.dns-7195.svc wheezy_udp@_http._tcp.dns-test-service.dns-7195.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7195.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7195 jessie_tcp@dns-test-service.dns-7195 jessie_udp@dns-test-service.dns-7195.svc jessie_tcp@dns-test-service.dns-7195.svc jessie_udp@_http._tcp.dns-test-service.dns-7195.svc jessie_tcp@_http._tcp.dns-test-service.dns-7195.svc]

Mar  4 09:03:45.574: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:45.577: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:45.581: INFO: Unable to read wheezy_udp@dns-test-service.dns-7195 from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:45.584: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7195 from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:45.588: INFO: Unable to read wheezy_udp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:45.592: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:45.595: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:45.598: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:45.620: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:45.623: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:45.626: INFO: Unable to read jessie_udp@dns-test-service.dns-7195 from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:45.630: INFO: Unable to read jessie_tcp@dns-test-service.dns-7195 from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:45.633: INFO: Unable to read jessie_udp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:45.637: INFO: Unable to read jessie_tcp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:45.640: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:45.644: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:45.664: INFO: Lookups using dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7195 wheezy_tcp@dns-test-service.dns-7195 wheezy_udp@dns-test-service.dns-7195.svc wheezy_tcp@dns-test-service.dns-7195.svc wheezy_udp@_http._tcp.dns-test-service.dns-7195.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7195.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7195 jessie_tcp@dns-test-service.dns-7195 jessie_udp@dns-test-service.dns-7195.svc jessie_tcp@dns-test-service.dns-7195.svc jessie_udp@_http._tcp.dns-test-service.dns-7195.svc jessie_tcp@_http._tcp.dns-test-service.dns-7195.svc]

Mar  4 09:03:50.574: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:50.578: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:50.581: INFO: Unable to read wheezy_udp@dns-test-service.dns-7195 from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:50.585: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7195 from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:50.588: INFO: Unable to read wheezy_udp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:50.591: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:50.595: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:50.599: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:50.622: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:50.626: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:50.630: INFO: Unable to read jessie_udp@dns-test-service.dns-7195 from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:50.633: INFO: Unable to read jessie_tcp@dns-test-service.dns-7195 from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:50.637: INFO: Unable to read jessie_udp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:50.642: INFO: Unable to read jessie_tcp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:50.646: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:50.650: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:50.674: INFO: Lookups using dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7195 wheezy_tcp@dns-test-service.dns-7195 wheezy_udp@dns-test-service.dns-7195.svc wheezy_tcp@dns-test-service.dns-7195.svc wheezy_udp@_http._tcp.dns-test-service.dns-7195.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7195.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7195 jessie_tcp@dns-test-service.dns-7195 jessie_udp@dns-test-service.dns-7195.svc jessie_tcp@dns-test-service.dns-7195.svc jessie_udp@_http._tcp.dns-test-service.dns-7195.svc jessie_tcp@_http._tcp.dns-test-service.dns-7195.svc]

Mar  4 09:03:55.574: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:55.578: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:55.583: INFO: Unable to read wheezy_udp@dns-test-service.dns-7195 from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:55.588: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7195 from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:55.592: INFO: Unable to read wheezy_udp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:55.594: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:55.597: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:55.604: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:55.627: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:55.632: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:55.637: INFO: Unable to read jessie_udp@dns-test-service.dns-7195 from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:55.640: INFO: Unable to read jessie_tcp@dns-test-service.dns-7195 from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:55.645: INFO: Unable to read jessie_udp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:55.649: INFO: Unable to read jessie_tcp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:55.655: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:55.660: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:03:55.679: INFO: Lookups using dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7195 wheezy_tcp@dns-test-service.dns-7195 wheezy_udp@dns-test-service.dns-7195.svc wheezy_tcp@dns-test-service.dns-7195.svc wheezy_udp@_http._tcp.dns-test-service.dns-7195.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7195.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7195 jessie_tcp@dns-test-service.dns-7195 jessie_udp@dns-test-service.dns-7195.svc jessie_tcp@dns-test-service.dns-7195.svc jessie_udp@_http._tcp.dns-test-service.dns-7195.svc jessie_tcp@_http._tcp.dns-test-service.dns-7195.svc]

Mar  4 09:04:00.574: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:04:00.578: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:04:00.581: INFO: Unable to read wheezy_udp@dns-test-service.dns-7195 from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:04:00.585: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7195 from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:04:00.588: INFO: Unable to read wheezy_udp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:04:00.592: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:04:00.595: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:04:00.599: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:04:00.623: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:04:00.627: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:04:00.632: INFO: Unable to read jessie_udp@dns-test-service.dns-7195 from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:04:00.635: INFO: Unable to read jessie_tcp@dns-test-service.dns-7195 from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:04:00.638: INFO: Unable to read jessie_udp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:04:00.641: INFO: Unable to read jessie_tcp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:04:00.647: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:04:00.652: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7195.svc from pod dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e: the server could not find the requested resource (get pods dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e)
Mar  4 09:04:00.678: INFO: Lookups using dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7195 wheezy_tcp@dns-test-service.dns-7195 wheezy_udp@dns-test-service.dns-7195.svc wheezy_tcp@dns-test-service.dns-7195.svc wheezy_udp@_http._tcp.dns-test-service.dns-7195.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7195.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7195 jessie_tcp@dns-test-service.dns-7195 jessie_udp@dns-test-service.dns-7195.svc jessie_tcp@dns-test-service.dns-7195.svc jessie_udp@_http._tcp.dns-test-service.dns-7195.svc jessie_tcp@_http._tcp.dns-test-service.dns-7195.svc]

Mar  4 09:04:05.657: INFO: DNS probes using dns-7195/dns-test-e4b8d4ef-2750-45e3-b354-f92a68f3329e succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:04:05.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7195" for this suite.

• [SLOW TEST:34.470 seconds]
[sig-network] DNS
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":278,"completed":29,"skipped":355,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:04:05.770: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-2dcb6627-8c33-4549-a28c-85c510b67482
STEP: Creating a pod to test consume secrets
Mar  4 09:04:05.886: INFO: Waiting up to 5m0s for pod "pod-secrets-2531edb6-6bbc-4f5c-91bf-709ac8860c24" in namespace "secrets-8690" to be "success or failure"
Mar  4 09:04:05.894: INFO: Pod "pod-secrets-2531edb6-6bbc-4f5c-91bf-709ac8860c24": Phase="Pending", Reason="", readiness=false. Elapsed: 7.908263ms
Mar  4 09:04:07.897: INFO: Pod "pod-secrets-2531edb6-6bbc-4f5c-91bf-709ac8860c24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011048302s
STEP: Saw pod success
Mar  4 09:04:07.897: INFO: Pod "pod-secrets-2531edb6-6bbc-4f5c-91bf-709ac8860c24" satisfied condition "success or failure"
Mar  4 09:04:07.903: INFO: Trying to get logs from node master3 pod pod-secrets-2531edb6-6bbc-4f5c-91bf-709ac8860c24 container secret-env-test: <nil>
STEP: delete the pod
Mar  4 09:04:07.955: INFO: Waiting for pod pod-secrets-2531edb6-6bbc-4f5c-91bf-709ac8860c24 to disappear
Mar  4 09:04:07.961: INFO: Pod pod-secrets-2531edb6-6bbc-4f5c-91bf-709ac8860c24 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:04:07.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8690" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":278,"completed":30,"skipped":382,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:04:07.970: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Mar  4 09:04:48.046: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:04:48.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0304 09:04:48.046347      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-5687" for this suite.

• [SLOW TEST:40.083 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":278,"completed":31,"skipped":413,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:04:48.054: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-e1959473-384f-439d-9360-b72aa6b0dbbf
STEP: Creating a pod to test consume configMaps
Mar  4 09:04:48.093: INFO: Waiting up to 5m0s for pod "pod-configmaps-fde70165-3907-4b68-bb73-ca669d37db3d" in namespace "configmap-9937" to be "success or failure"
Mar  4 09:04:48.097: INFO: Pod "pod-configmaps-fde70165-3907-4b68-bb73-ca669d37db3d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.907992ms
Mar  4 09:04:50.101: INFO: Pod "pod-configmaps-fde70165-3907-4b68-bb73-ca669d37db3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007380851s
STEP: Saw pod success
Mar  4 09:04:50.101: INFO: Pod "pod-configmaps-fde70165-3907-4b68-bb73-ca669d37db3d" satisfied condition "success or failure"
Mar  4 09:04:50.106: INFO: Trying to get logs from node master3 pod pod-configmaps-fde70165-3907-4b68-bb73-ca669d37db3d container configmap-volume-test: <nil>
STEP: delete the pod
Mar  4 09:04:50.126: INFO: Waiting for pod pod-configmaps-fde70165-3907-4b68-bb73-ca669d37db3d to disappear
Mar  4 09:04:50.132: INFO: Pod pod-configmaps-fde70165-3907-4b68-bb73-ca669d37db3d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:04:50.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9937" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":278,"completed":32,"skipped":423,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:04:50.141: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:04:50.182: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar  4 09:04:50.190: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:04:50.194: INFO: Number of nodes with available pods: 0
Mar  4 09:04:50.194: INFO: Node master2 is running more than one daemon pod
Mar  4 09:04:51.200: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:04:51.203: INFO: Number of nodes with available pods: 0
Mar  4 09:04:51.203: INFO: Node master2 is running more than one daemon pod
Mar  4 09:04:52.198: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:04:52.201: INFO: Number of nodes with available pods: 2
Mar  4 09:04:52.201: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar  4 09:04:52.224: INFO: Wrong image for pod: daemon-set-dbn2q. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar  4 09:04:52.224: INFO: Wrong image for pod: daemon-set-m9g4l. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar  4 09:04:52.231: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:04:53.235: INFO: Wrong image for pod: daemon-set-dbn2q. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar  4 09:04:53.235: INFO: Wrong image for pod: daemon-set-m9g4l. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar  4 09:04:53.239: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:04:54.236: INFO: Wrong image for pod: daemon-set-dbn2q. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar  4 09:04:54.236: INFO: Wrong image for pod: daemon-set-m9g4l. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar  4 09:04:54.239: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:04:55.235: INFO: Wrong image for pod: daemon-set-dbn2q. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar  4 09:04:55.235: INFO: Wrong image for pod: daemon-set-m9g4l. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar  4 09:04:55.239: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:04:56.235: INFO: Wrong image for pod: daemon-set-dbn2q. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar  4 09:04:56.235: INFO: Wrong image for pod: daemon-set-m9g4l. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar  4 09:04:56.239: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:04:57.236: INFO: Wrong image for pod: daemon-set-dbn2q. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar  4 09:04:57.236: INFO: Pod daemon-set-dbn2q is not available
Mar  4 09:04:57.236: INFO: Wrong image for pod: daemon-set-m9g4l. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar  4 09:04:57.239: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:04:58.235: INFO: Wrong image for pod: daemon-set-m9g4l. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar  4 09:04:58.235: INFO: Pod daemon-set-rf8jt is not available
Mar  4 09:04:58.237: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:04:59.235: INFO: Wrong image for pod: daemon-set-m9g4l. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar  4 09:04:59.235: INFO: Pod daemon-set-rf8jt is not available
Mar  4 09:04:59.237: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:05:00.236: INFO: Wrong image for pod: daemon-set-m9g4l. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar  4 09:05:00.240: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:05:01.235: INFO: Wrong image for pod: daemon-set-m9g4l. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar  4 09:05:01.235: INFO: Pod daemon-set-m9g4l is not available
Mar  4 09:05:01.238: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:05:02.235: INFO: Wrong image for pod: daemon-set-m9g4l. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar  4 09:05:02.235: INFO: Pod daemon-set-m9g4l is not available
Mar  4 09:05:02.238: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:05:03.235: INFO: Wrong image for pod: daemon-set-m9g4l. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar  4 09:05:03.235: INFO: Pod daemon-set-m9g4l is not available
Mar  4 09:05:03.238: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:05:04.235: INFO: Wrong image for pod: daemon-set-m9g4l. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar  4 09:05:04.235: INFO: Pod daemon-set-m9g4l is not available
Mar  4 09:05:04.245: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:05:05.235: INFO: Wrong image for pod: daemon-set-m9g4l. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar  4 09:05:05.235: INFO: Pod daemon-set-m9g4l is not available
Mar  4 09:05:05.238: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:05:06.235: INFO: Wrong image for pod: daemon-set-m9g4l. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar  4 09:05:06.235: INFO: Pod daemon-set-m9g4l is not available
Mar  4 09:05:06.238: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:05:07.238: INFO: Wrong image for pod: daemon-set-m9g4l. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar  4 09:05:07.238: INFO: Pod daemon-set-m9g4l is not available
Mar  4 09:05:07.250: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:05:08.235: INFO: Wrong image for pod: daemon-set-m9g4l. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Mar  4 09:05:08.235: INFO: Pod daemon-set-m9g4l is not available
Mar  4 09:05:08.239: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:05:09.239: INFO: Pod daemon-set-9fcxc is not available
Mar  4 09:05:09.242: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Mar  4 09:05:09.245: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:05:09.249: INFO: Number of nodes with available pods: 1
Mar  4 09:05:09.249: INFO: Node master3 is running more than one daemon pod
Mar  4 09:05:10.253: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:05:10.255: INFO: Number of nodes with available pods: 1
Mar  4 09:05:10.256: INFO: Node master3 is running more than one daemon pod
Mar  4 09:05:11.253: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:05:11.256: INFO: Number of nodes with available pods: 2
Mar  4 09:05:11.256: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-183, will wait for the garbage collector to delete the pods
Mar  4 09:05:11.326: INFO: Deleting DaemonSet.extensions daemon-set took: 5.008693ms
Mar  4 09:05:11.727: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.750867ms
Mar  4 09:05:18.733: INFO: Number of nodes with available pods: 0
Mar  4 09:05:18.733: INFO: Number of running nodes: 0, number of available pods: 0
Mar  4 09:05:18.740: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-183/daemonsets","resourceVersion":"46490"},"items":null}

Mar  4 09:05:18.743: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-183/pods","resourceVersion":"46490"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:05:18.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-183" for this suite.

• [SLOW TEST:28.619 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":278,"completed":33,"skipped":426,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:05:18.761: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:05:20.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7718" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":278,"completed":34,"skipped":458,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:05:20.833: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:46
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:05:20.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-6326" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":278,"completed":35,"skipped":464,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:05:20.873: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar  4 09:05:23.943: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:05:23.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7789" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":278,"completed":36,"skipped":466,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:05:23.987: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-083276e3-d514-443a-ad39-4d0afee4713b in namespace container-probe-2030
Mar  4 09:05:26.045: INFO: Started pod busybox-083276e3-d514-443a-ad39-4d0afee4713b in namespace container-probe-2030
STEP: checking the pod's current state and verifying that restartCount is present
Mar  4 09:05:26.048: INFO: Initial restart count of pod busybox-083276e3-d514-443a-ad39-4d0afee4713b is 0
Mar  4 09:06:18.135: INFO: Restart count of pod container-probe-2030/busybox-083276e3-d514-443a-ad39-4d0afee4713b is now 1 (52.086513839s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:06:18.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2030" for this suite.

• [SLOW TEST:54.165 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":278,"completed":37,"skipped":477,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:06:18.153: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:06:18.234: INFO: (0) /api/v1/nodes/master2/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 49.694013ms)
Mar  4 09:06:18.242: INFO: (1) /api/v1/nodes/master2/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 7.412017ms)
Mar  4 09:06:18.246: INFO: (2) /api/v1/nodes/master2/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 4.678706ms)
Mar  4 09:06:18.250: INFO: (3) /api/v1/nodes/master2/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 4.140876ms)
Mar  4 09:06:18.255: INFO: (4) /api/v1/nodes/master2/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 4.18671ms)
Mar  4 09:06:18.259: INFO: (5) /api/v1/nodes/master2/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.895736ms)
Mar  4 09:06:18.262: INFO: (6) /api/v1/nodes/master2/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.493091ms)
Mar  4 09:06:18.266: INFO: (7) /api/v1/nodes/master2/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.974908ms)
Mar  4 09:06:18.270: INFO: (8) /api/v1/nodes/master2/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.703255ms)
Mar  4 09:06:18.274: INFO: (9) /api/v1/nodes/master2/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.63364ms)
Mar  4 09:06:18.277: INFO: (10) /api/v1/nodes/master2/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.595476ms)
Mar  4 09:06:18.281: INFO: (11) /api/v1/nodes/master2/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.774647ms)
Mar  4 09:06:18.285: INFO: (12) /api/v1/nodes/master2/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 4.023176ms)
Mar  4 09:06:18.289: INFO: (13) /api/v1/nodes/master2/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.491442ms)
Mar  4 09:06:18.293: INFO: (14) /api/v1/nodes/master2/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 4.133549ms)
Mar  4 09:06:18.296: INFO: (15) /api/v1/nodes/master2/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.546962ms)
Mar  4 09:06:18.300: INFO: (16) /api/v1/nodes/master2/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.819616ms)
Mar  4 09:06:18.307: INFO: (17) /api/v1/nodes/master2/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 6.505754ms)
Mar  4 09:06:18.311: INFO: (18) /api/v1/nodes/master2/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.883637ms)
Mar  4 09:06:18.315: INFO: (19) /api/v1/nodes/master2/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.953225ms)
[AfterEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:06:18.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7426" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":278,"completed":38,"skipped":486,"failed":0}
SSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:06:18.321: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod test-webserver-c994cb32-27a8-4f2f-967c-243fd8361cd3 in namespace container-probe-8952
Mar  4 09:06:20.363: INFO: Started pod test-webserver-c994cb32-27a8-4f2f-967c-243fd8361cd3 in namespace container-probe-8952
STEP: checking the pod's current state and verifying that restartCount is present
Mar  4 09:06:20.366: INFO: Initial restart count of pod test-webserver-c994cb32-27a8-4f2f-967c-243fd8361cd3 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:10:20.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8952" for this suite.

• [SLOW TEST:242.471 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":278,"completed":39,"skipped":490,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:10:20.793: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating pod
Mar  4 09:10:22.864: INFO: Pod pod-hostip-b74d5607-561c-43fc-9cbb-a0a53e44624d has hostIP: 10.128.0.4
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:10:22.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9564" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":278,"completed":40,"skipped":502,"failed":0}
SSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:10:22.871: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:10:30.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2812" for this suite.

• [SLOW TEST:8.043 seconds]
[sig-apps] Job
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":278,"completed":41,"skipped":508,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:10:30.915: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-4c4c04f8-20bc-417b-b8ee-cb9da8e2f4ae
STEP: Creating a pod to test consume configMaps
Mar  4 09:10:30.957: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-55bf9fad-725d-4df0-b692-127216b3ebcd" in namespace "projected-1434" to be "success or failure"
Mar  4 09:10:30.960: INFO: Pod "pod-projected-configmaps-55bf9fad-725d-4df0-b692-127216b3ebcd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.66367ms
Mar  4 09:10:32.963: INFO: Pod "pod-projected-configmaps-55bf9fad-725d-4df0-b692-127216b3ebcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005827799s
STEP: Saw pod success
Mar  4 09:10:32.963: INFO: Pod "pod-projected-configmaps-55bf9fad-725d-4df0-b692-127216b3ebcd" satisfied condition "success or failure"
Mar  4 09:10:32.968: INFO: Trying to get logs from node master3 pod pod-projected-configmaps-55bf9fad-725d-4df0-b692-127216b3ebcd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  4 09:10:33.009: INFO: Waiting for pod pod-projected-configmaps-55bf9fad-725d-4df0-b692-127216b3ebcd to disappear
Mar  4 09:10:33.014: INFO: Pod pod-projected-configmaps-55bf9fad-725d-4df0-b692-127216b3ebcd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:10:33.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1434" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":278,"completed":42,"skipped":513,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:10:33.023: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9255.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9255.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9255.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9255.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9255.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9255.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9255.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9255.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9255.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9255.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  4 09:10:37.131: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:37.135: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:37.138: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:37.141: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:37.151: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:37.155: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:37.159: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:37.163: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:37.170: INFO: Lookups using dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9255.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9255.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local jessie_udp@dns-test-service-2.dns-9255.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9255.svc.cluster.local]

Mar  4 09:10:42.174: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:42.177: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:42.181: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:42.185: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:42.205: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:42.210: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:42.215: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:42.224: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:42.233: INFO: Lookups using dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9255.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9255.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local jessie_udp@dns-test-service-2.dns-9255.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9255.svc.cluster.local]

Mar  4 09:10:47.173: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:47.176: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:47.179: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:47.183: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:47.192: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:47.200: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:47.205: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:47.210: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:47.220: INFO: Lookups using dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9255.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9255.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local jessie_udp@dns-test-service-2.dns-9255.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9255.svc.cluster.local]

Mar  4 09:10:52.174: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:52.177: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:52.180: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:52.183: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:52.193: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:52.196: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:52.199: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:52.203: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:52.209: INFO: Lookups using dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9255.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9255.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local jessie_udp@dns-test-service-2.dns-9255.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9255.svc.cluster.local]

Mar  4 09:10:57.174: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:57.178: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:57.182: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:57.185: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:57.195: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:57.198: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:57.201: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:57.204: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:10:57.213: INFO: Lookups using dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9255.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9255.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local jessie_udp@dns-test-service-2.dns-9255.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9255.svc.cluster.local]

Mar  4 09:11:02.174: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:11:02.177: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:11:02.180: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:11:02.183: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:11:02.194: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:11:02.197: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:11:02.200: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:11:02.204: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9255.svc.cluster.local from pod dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d: the server could not find the requested resource (get pods dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d)
Mar  4 09:11:02.210: INFO: Lookups using dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9255.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9255.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9255.svc.cluster.local jessie_udp@dns-test-service-2.dns-9255.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9255.svc.cluster.local]

Mar  4 09:11:07.207: INFO: DNS probes using dns-9255/dns-test-1ab46540-a27b-4008-b78a-fa74d04cef0d succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:11:07.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9255" for this suite.

• [SLOW TEST:34.323 seconds]
[sig-network] DNS
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":278,"completed":43,"skipped":514,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:11:07.347: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:11:29.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6738" for this suite.

• [SLOW TEST:22.294 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":278,"completed":44,"skipped":536,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:11:29.640: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar  4 09:11:29.692: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8a1b6ba0-7626-4e74-aeba-96c16432056c" in namespace "downward-api-2826" to be "success or failure"
Mar  4 09:11:29.695: INFO: Pod "downwardapi-volume-8a1b6ba0-7626-4e74-aeba-96c16432056c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.782208ms
Mar  4 09:11:31.698: INFO: Pod "downwardapi-volume-8a1b6ba0-7626-4e74-aeba-96c16432056c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006734076s
STEP: Saw pod success
Mar  4 09:11:31.698: INFO: Pod "downwardapi-volume-8a1b6ba0-7626-4e74-aeba-96c16432056c" satisfied condition "success or failure"
Mar  4 09:11:31.701: INFO: Trying to get logs from node master3 pod downwardapi-volume-8a1b6ba0-7626-4e74-aeba-96c16432056c container client-container: <nil>
STEP: delete the pod
Mar  4 09:11:31.718: INFO: Waiting for pod downwardapi-volume-8a1b6ba0-7626-4e74-aeba-96c16432056c to disappear
Mar  4 09:11:31.724: INFO: Pod downwardapi-volume-8a1b6ba0-7626-4e74-aeba-96c16432056c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:11:31.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2826" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":278,"completed":45,"skipped":543,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:11:31.743: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1692
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar  4 09:11:31.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-2711'
Mar  4 09:11:32.212: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  4 09:11:32.212: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Mar  4 09:11:32.225: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Mar  4 09:11:32.231: INFO: scanned /root for discovery docs: <nil>
Mar  4 09:11:32.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-2711'
Mar  4 09:11:48.058: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar  4 09:11:48.058: INFO: stdout: "Created e2e-test-httpd-rc-b1047e176717430f78a48ad33ddded40\nScaling up e2e-test-httpd-rc-b1047e176717430f78a48ad33ddded40 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-b1047e176717430f78a48ad33ddded40 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-b1047e176717430f78a48ad33ddded40 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Mar  4 09:11:48.058: INFO: stdout: "Created e2e-test-httpd-rc-b1047e176717430f78a48ad33ddded40\nScaling up e2e-test-httpd-rc-b1047e176717430f78a48ad33ddded40 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-b1047e176717430f78a48ad33ddded40 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-b1047e176717430f78a48ad33ddded40 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Mar  4 09:11:48.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-2711'
Mar  4 09:11:48.155: INFO: stderr: ""
Mar  4 09:11:48.155: INFO: stdout: "e2e-test-httpd-rc-9stzr e2e-test-httpd-rc-b1047e176717430f78a48ad33ddded40-5zc29 "
STEP: Replicas for run=e2e-test-httpd-rc: expected=1 actual=2
Mar  4 09:11:53.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-2711'
Mar  4 09:11:53.248: INFO: stderr: ""
Mar  4 09:11:53.248: INFO: stdout: "e2e-test-httpd-rc-b1047e176717430f78a48ad33ddded40-5zc29 "
Mar  4 09:11:53.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods e2e-test-httpd-rc-b1047e176717430f78a48ad33ddded40-5zc29 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2711'
Mar  4 09:11:53.338: INFO: stderr: ""
Mar  4 09:11:53.338: INFO: stdout: "true"
Mar  4 09:11:53.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods e2e-test-httpd-rc-b1047e176717430f78a48ad33ddded40-5zc29 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2711'
Mar  4 09:11:53.429: INFO: stderr: ""
Mar  4 09:11:53.429: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Mar  4 09:11:53.429: INFO: e2e-test-httpd-rc-b1047e176717430f78a48ad33ddded40-5zc29 is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1698
Mar  4 09:11:53.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 delete rc e2e-test-httpd-rc --namespace=kubectl-2711'
Mar  4 09:11:53.525: INFO: stderr: ""
Mar  4 09:11:53.525: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:11:53.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2711" for this suite.

• [SLOW TEST:21.796 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl rolling-update should support rolling-update to same image  [Conformance]","total":278,"completed":46,"skipped":545,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:11:53.539: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Mar  4 09:11:53.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 create -f - --namespace=kubectl-5885'
Mar  4 09:11:53.804: INFO: stderr: ""
Mar  4 09:11:53.804: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Mar  4 09:11:54.811: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  4 09:11:54.811: INFO: Found 0 / 1
Mar  4 09:11:55.807: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  4 09:11:55.807: INFO: Found 0 / 1
Mar  4 09:11:56.807: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  4 09:11:56.807: INFO: Found 1 / 1
Mar  4 09:11:56.807: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar  4 09:11:56.810: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  4 09:11:56.810: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  4 09:11:56.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 patch pod agnhost-master-s52vs --namespace=kubectl-5885 -p {"metadata":{"annotations":{"x":"y"}}}'
Mar  4 09:11:56.914: INFO: stderr: ""
Mar  4 09:11:56.914: INFO: stdout: "pod/agnhost-master-s52vs patched\n"
STEP: checking annotations
Mar  4 09:11:56.918: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  4 09:11:56.918: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:11:56.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5885" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":278,"completed":47,"skipped":546,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:11:56.927: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar  4 09:11:56.963: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c84a0937-82ee-4f0b-a53b-694e1640564e" in namespace "projected-5943" to be "success or failure"
Mar  4 09:11:56.968: INFO: Pod "downwardapi-volume-c84a0937-82ee-4f0b-a53b-694e1640564e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.293004ms
Mar  4 09:11:58.973: INFO: Pod "downwardapi-volume-c84a0937-82ee-4f0b-a53b-694e1640564e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010098943s
Mar  4 09:12:00.977: INFO: Pod "downwardapi-volume-c84a0937-82ee-4f0b-a53b-694e1640564e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013211093s
STEP: Saw pod success
Mar  4 09:12:00.977: INFO: Pod "downwardapi-volume-c84a0937-82ee-4f0b-a53b-694e1640564e" satisfied condition "success or failure"
Mar  4 09:12:00.979: INFO: Trying to get logs from node master2 pod downwardapi-volume-c84a0937-82ee-4f0b-a53b-694e1640564e container client-container: <nil>
STEP: delete the pod
Mar  4 09:12:01.008: INFO: Waiting for pod downwardapi-volume-c84a0937-82ee-4f0b-a53b-694e1640564e to disappear
Mar  4 09:12:01.010: INFO: Pod downwardapi-volume-c84a0937-82ee-4f0b-a53b-694e1640564e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:12:01.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5943" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":278,"completed":48,"skipped":555,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:12:01.016: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-915dc642-841e-4dfa-9a79-413c310e70ef
STEP: Creating a pod to test consume secrets
Mar  4 09:12:01.059: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e8e3ddf1-188b-4892-9375-1071a7356198" in namespace "projected-7073" to be "success or failure"
Mar  4 09:12:01.067: INFO: Pod "pod-projected-secrets-e8e3ddf1-188b-4892-9375-1071a7356198": Phase="Pending", Reason="", readiness=false. Elapsed: 7.526507ms
Mar  4 09:12:03.070: INFO: Pod "pod-projected-secrets-e8e3ddf1-188b-4892-9375-1071a7356198": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011018413s
Mar  4 09:12:05.074: INFO: Pod "pod-projected-secrets-e8e3ddf1-188b-4892-9375-1071a7356198": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014363421s
STEP: Saw pod success
Mar  4 09:12:05.074: INFO: Pod "pod-projected-secrets-e8e3ddf1-188b-4892-9375-1071a7356198" satisfied condition "success or failure"
Mar  4 09:12:05.076: INFO: Trying to get logs from node master3 pod pod-projected-secrets-e8e3ddf1-188b-4892-9375-1071a7356198 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  4 09:12:05.095: INFO: Waiting for pod pod-projected-secrets-e8e3ddf1-188b-4892-9375-1071a7356198 to disappear
Mar  4 09:12:05.105: INFO: Pod pod-projected-secrets-e8e3ddf1-188b-4892-9375-1071a7356198 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:12:05.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7073" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":278,"completed":49,"skipped":563,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:12:05.113: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-8d95b27d-dc11-4abd-a69f-41c12fb9b523 in namespace container-probe-7060
Mar  4 09:12:09.159: INFO: Started pod liveness-8d95b27d-dc11-4abd-a69f-41c12fb9b523 in namespace container-probe-7060
STEP: checking the pod's current state and verifying that restartCount is present
Mar  4 09:12:09.162: INFO: Initial restart count of pod liveness-8d95b27d-dc11-4abd-a69f-41c12fb9b523 is 0
Mar  4 09:12:29.195: INFO: Restart count of pod container-probe-7060/liveness-8d95b27d-dc11-4abd-a69f-41c12fb9b523 is now 1 (20.033658392s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:12:29.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7060" for this suite.

• [SLOW TEST:24.098 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":278,"completed":50,"skipped":606,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:12:29.211: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:172
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating server pod server in namespace prestop-3960
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3960
STEP: Deleting pre-stop pod
Mar  4 09:12:38.313: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:12:38.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3960" for this suite.

• [SLOW TEST:9.125 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":278,"completed":51,"skipped":614,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:12:38.337: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar  4 09:12:38.399: INFO: Waiting up to 5m0s for pod "downwardapi-volume-248f17d8-3be7-416b-b22e-0f37b9c48ba6" in namespace "projected-7112" to be "success or failure"
Mar  4 09:12:38.403: INFO: Pod "downwardapi-volume-248f17d8-3be7-416b-b22e-0f37b9c48ba6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.708788ms
Mar  4 09:12:40.406: INFO: Pod "downwardapi-volume-248f17d8-3be7-416b-b22e-0f37b9c48ba6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007026179s
STEP: Saw pod success
Mar  4 09:12:40.406: INFO: Pod "downwardapi-volume-248f17d8-3be7-416b-b22e-0f37b9c48ba6" satisfied condition "success or failure"
Mar  4 09:12:40.409: INFO: Trying to get logs from node master3 pod downwardapi-volume-248f17d8-3be7-416b-b22e-0f37b9c48ba6 container client-container: <nil>
STEP: delete the pod
Mar  4 09:12:40.427: INFO: Waiting for pod downwardapi-volume-248f17d8-3be7-416b-b22e-0f37b9c48ba6 to disappear
Mar  4 09:12:40.430: INFO: Pod downwardapi-volume-248f17d8-3be7-416b-b22e-0f37b9c48ba6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:12:40.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7112" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":278,"completed":52,"skipped":650,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:12:40.439: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  4 09:12:40.529: INFO: Waiting up to 5m0s for pod "pod-f38e6c27-7867-4f88-b77d-17aa2ef8a336" in namespace "emptydir-3126" to be "success or failure"
Mar  4 09:12:40.533: INFO: Pod "pod-f38e6c27-7867-4f88-b77d-17aa2ef8a336": Phase="Pending", Reason="", readiness=false. Elapsed: 3.828185ms
Mar  4 09:12:42.536: INFO: Pod "pod-f38e6c27-7867-4f88-b77d-17aa2ef8a336": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006949668s
STEP: Saw pod success
Mar  4 09:12:42.536: INFO: Pod "pod-f38e6c27-7867-4f88-b77d-17aa2ef8a336" satisfied condition "success or failure"
Mar  4 09:12:42.539: INFO: Trying to get logs from node master3 pod pod-f38e6c27-7867-4f88-b77d-17aa2ef8a336 container test-container: <nil>
STEP: delete the pod
Mar  4 09:12:42.561: INFO: Waiting for pod pod-f38e6c27-7867-4f88-b77d-17aa2ef8a336 to disappear
Mar  4 09:12:42.564: INFO: Pod pod-f38e6c27-7867-4f88-b77d-17aa2ef8a336 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:12:42.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3126" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":53,"skipped":733,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:12:42.573: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-fgx2
STEP: Creating a pod to test atomic-volume-subpath
Mar  4 09:12:42.623: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-fgx2" in namespace "subpath-8281" to be "success or failure"
Mar  4 09:12:42.627: INFO: Pod "pod-subpath-test-configmap-fgx2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053071ms
Mar  4 09:12:44.630: INFO: Pod "pod-subpath-test-configmap-fgx2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007246135s
Mar  4 09:12:46.633: INFO: Pod "pod-subpath-test-configmap-fgx2": Phase="Running", Reason="", readiness=true. Elapsed: 4.010308731s
Mar  4 09:12:48.640: INFO: Pod "pod-subpath-test-configmap-fgx2": Phase="Running", Reason="", readiness=true. Elapsed: 6.017016099s
Mar  4 09:12:50.643: INFO: Pod "pod-subpath-test-configmap-fgx2": Phase="Running", Reason="", readiness=true. Elapsed: 8.020296089s
Mar  4 09:12:52.646: INFO: Pod "pod-subpath-test-configmap-fgx2": Phase="Running", Reason="", readiness=true. Elapsed: 10.023180754s
Mar  4 09:12:54.649: INFO: Pod "pod-subpath-test-configmap-fgx2": Phase="Running", Reason="", readiness=true. Elapsed: 12.026325028s
Mar  4 09:12:56.652: INFO: Pod "pod-subpath-test-configmap-fgx2": Phase="Running", Reason="", readiness=true. Elapsed: 14.029269559s
Mar  4 09:12:58.656: INFO: Pod "pod-subpath-test-configmap-fgx2": Phase="Running", Reason="", readiness=true. Elapsed: 16.032373055s
Mar  4 09:13:00.659: INFO: Pod "pod-subpath-test-configmap-fgx2": Phase="Running", Reason="", readiness=true. Elapsed: 18.035465229s
Mar  4 09:13:02.662: INFO: Pod "pod-subpath-test-configmap-fgx2": Phase="Running", Reason="", readiness=true. Elapsed: 20.038559493s
Mar  4 09:13:04.665: INFO: Pod "pod-subpath-test-configmap-fgx2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.041881007s
STEP: Saw pod success
Mar  4 09:13:04.665: INFO: Pod "pod-subpath-test-configmap-fgx2" satisfied condition "success or failure"
Mar  4 09:13:04.668: INFO: Trying to get logs from node master3 pod pod-subpath-test-configmap-fgx2 container test-container-subpath-configmap-fgx2: <nil>
STEP: delete the pod
Mar  4 09:13:04.686: INFO: Waiting for pod pod-subpath-test-configmap-fgx2 to disappear
Mar  4 09:13:04.700: INFO: Pod pod-subpath-test-configmap-fgx2 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-fgx2
Mar  4 09:13:04.700: INFO: Deleting pod "pod-subpath-test-configmap-fgx2" in namespace "subpath-8281"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:13:04.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8281" for this suite.

• [SLOW TEST:22.148 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":278,"completed":54,"skipped":741,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:13:04.721: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-1a982ee9-5a88-45c8-ba72-a9ec187dad7e
STEP: Creating a pod to test consume secrets
Mar  4 09:13:04.772: INFO: Waiting up to 5m0s for pod "pod-secrets-21802caf-d31c-4e3a-bd3e-bee1fedc3a76" in namespace "secrets-1284" to be "success or failure"
Mar  4 09:13:04.776: INFO: Pod "pod-secrets-21802caf-d31c-4e3a-bd3e-bee1fedc3a76": Phase="Pending", Reason="", readiness=false. Elapsed: 3.901314ms
Mar  4 09:13:06.779: INFO: Pod "pod-secrets-21802caf-d31c-4e3a-bd3e-bee1fedc3a76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006970574s
STEP: Saw pod success
Mar  4 09:13:06.779: INFO: Pod "pod-secrets-21802caf-d31c-4e3a-bd3e-bee1fedc3a76" satisfied condition "success or failure"
Mar  4 09:13:06.782: INFO: Trying to get logs from node master3 pod pod-secrets-21802caf-d31c-4e3a-bd3e-bee1fedc3a76 container secret-volume-test: <nil>
STEP: delete the pod
Mar  4 09:13:06.824: INFO: Waiting for pod pod-secrets-21802caf-d31c-4e3a-bd3e-bee1fedc3a76 to disappear
Mar  4 09:13:06.832: INFO: Pod pod-secrets-21802caf-d31c-4e3a-bd3e-bee1fedc3a76 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:13:06.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1284" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":55,"skipped":746,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:13:06.842: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar  4 09:13:06.882: INFO: Waiting up to 5m0s for pod "downwardapi-volume-70c5d18e-11f7-42f4-ace4-ae77e4890adb" in namespace "downward-api-1856" to be "success or failure"
Mar  4 09:13:06.886: INFO: Pod "downwardapi-volume-70c5d18e-11f7-42f4-ace4-ae77e4890adb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.878447ms
Mar  4 09:13:08.890: INFO: Pod "downwardapi-volume-70c5d18e-11f7-42f4-ace4-ae77e4890adb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008454932s
STEP: Saw pod success
Mar  4 09:13:08.890: INFO: Pod "downwardapi-volume-70c5d18e-11f7-42f4-ace4-ae77e4890adb" satisfied condition "success or failure"
Mar  4 09:13:08.895: INFO: Trying to get logs from node master3 pod downwardapi-volume-70c5d18e-11f7-42f4-ace4-ae77e4890adb container client-container: <nil>
STEP: delete the pod
Mar  4 09:13:08.920: INFO: Waiting for pod downwardapi-volume-70c5d18e-11f7-42f4-ace4-ae77e4890adb to disappear
Mar  4 09:13:08.922: INFO: Pod downwardapi-volume-70c5d18e-11f7-42f4-ace4-ae77e4890adb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:13:08.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1856" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":278,"completed":56,"skipped":746,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:13:08.930: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  4 09:13:10.054: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  4 09:13:13.073: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:13:23.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7914" for this suite.
STEP: Destroying namespace "webhook-7914-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:14.322 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":278,"completed":57,"skipped":752,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:13:23.252: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:13:23.295: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar  4 09:13:23.306: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  4 09:13:28.313: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  4 09:13:28.313: INFO: Creating deployment "test-rolling-update-deployment"
Mar  4 09:13:28.320: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar  4 09:13:28.344: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
Mar  4 09:13:30.351: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar  4 09:13:30.355: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Mar  4 09:13:30.369: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3880 /apis/apps/v1/namespaces/deployment-3880/deployments/test-rolling-update-deployment bf936b64-779c-4382-be6e-3ff611ed3e22 48866 1 2020-03-04 09:13:28 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002faf4a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-03-04 09:13:28 +0000 UTC,LastTransitionTime:2020-03-04 09:13:28 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67cf4f6444" has successfully progressed.,LastUpdateTime:2020-03-04 09:13:30 +0000 UTC,LastTransitionTime:2020-03-04 09:13:28 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  4 09:13:30.377: INFO: New ReplicaSet "test-rolling-update-deployment-67cf4f6444" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67cf4f6444  deployment-3880 /apis/apps/v1/namespaces/deployment-3880/replicasets/test-rolling-update-deployment-67cf4f6444 9e7badfe-9749-4048-972d-fa501dc9c197 48855 1 2020-03-04 09:13:28 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment bf936b64-779c-4382-be6e-3ff611ed3e22 0xc0030228e7 0xc0030228e8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67cf4f6444,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003022958 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  4 09:13:30.377: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar  4 09:13:30.378: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3880 /apis/apps/v1/namespaces/deployment-3880/replicasets/test-rolling-update-controller 9480afe6-083d-4fb9-a7cd-65236c3aafc2 48864 2 2020-03-04 09:13:23 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment bf936b64-779c-4382-be6e-3ff611ed3e22 0xc003022817 0xc003022818}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003022878 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  4 09:13:30.383: INFO: Pod "test-rolling-update-deployment-67cf4f6444-5vwhm" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67cf4f6444-5vwhm test-rolling-update-deployment-67cf4f6444- deployment-3880 /api/v1/namespaces/deployment-3880/pods/test-rolling-update-deployment-67cf4f6444-5vwhm 7eba5b0a-f02b-470c-a65a-5764fe9f0537 48854 0 2020-03-04 09:13:28 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[cni.projectcalico.org/podIP:192.168.180.36/32 cni.projectcalico.org/podIPs:192.168.180.36/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-67cf4f6444 9e7badfe-9749-4048-972d-fa501dc9c197 0xc002faf897 0xc002faf898}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kpp9g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kpp9g,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kpp9g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:13:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:13:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:13:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:13:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.3,PodIP:192.168.180.36,StartTime:2020-03-04 09:13:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-04 09:13:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://4f920cf2fef5b57fa91fac3892fff8ff867e161efa2afd567f661e46185efcc4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.180.36,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:13:30.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3880" for this suite.

• [SLOW TEST:7.141 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":278,"completed":58,"skipped":770,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:13:30.393: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Mar  4 09:13:30.442: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:13:38.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8780" for this suite.

• [SLOW TEST:8.339 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":278,"completed":59,"skipped":773,"failed":0}
S
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:13:38.732: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test hostPath mode
Mar  4 09:13:38.826: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-155" to be "success or failure"
Mar  4 09:13:38.828: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.439134ms
Mar  4 09:13:40.832: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005841241s
Mar  4 09:13:42.835: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008899069s
STEP: Saw pod success
Mar  4 09:13:42.835: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Mar  4 09:13:42.837: INFO: Trying to get logs from node master3 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Mar  4 09:13:42.868: INFO: Waiting for pod pod-host-path-test to disappear
Mar  4 09:13:42.872: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:13:42.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-155" for this suite.
•{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":60,"skipped":774,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:13:42.882: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-9766
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Mar  4 09:13:42.929: INFO: Found 0 stateful pods, waiting for 3
Mar  4 09:13:52.933: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  4 09:13:52.933: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  4 09:13:52.933: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar  4 09:13:52.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=statefulset-9766 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  4 09:13:53.171: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  4 09:13:53.171: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  4 09:13:53.171: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar  4 09:14:03.201: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar  4 09:14:13.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=statefulset-9766 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  4 09:14:13.440: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  4 09:14:13.440: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  4 09:14:13.440: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Mar  4 09:14:33.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=statefulset-9766 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  4 09:14:33.686: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  4 09:14:33.686: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  4 09:14:33.686: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  4 09:14:43.719: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar  4 09:14:53.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=statefulset-9766 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  4 09:14:53.951: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  4 09:14:53.951: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  4 09:14:53.951: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Mar  4 09:15:23.966: INFO: Deleting all statefulset in ns statefulset-9766
Mar  4 09:15:23.968: INFO: Scaling statefulset ss2 to 0
Mar  4 09:15:33.980: INFO: Waiting for statefulset status.replicas updated to 0
Mar  4 09:15:33.983: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:15:33.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9766" for this suite.

• [SLOW TEST:111.122 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":278,"completed":61,"skipped":830,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:15:34.004: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:15:38.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4739" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":62,"skipped":851,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:15:38.081: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on node default medium
Mar  4 09:15:38.118: INFO: Waiting up to 5m0s for pod "pod-b086b50e-f7ed-4a35-a2f0-08b91817cd1c" in namespace "emptydir-6939" to be "success or failure"
Mar  4 09:15:38.122: INFO: Pod "pod-b086b50e-f7ed-4a35-a2f0-08b91817cd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.306022ms
Mar  4 09:15:40.125: INFO: Pod "pod-b086b50e-f7ed-4a35-a2f0-08b91817cd1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007409455s
STEP: Saw pod success
Mar  4 09:15:40.126: INFO: Pod "pod-b086b50e-f7ed-4a35-a2f0-08b91817cd1c" satisfied condition "success or failure"
Mar  4 09:15:40.128: INFO: Trying to get logs from node master2 pod pod-b086b50e-f7ed-4a35-a2f0-08b91817cd1c container test-container: <nil>
STEP: delete the pod
Mar  4 09:15:40.156: INFO: Waiting for pod pod-b086b50e-f7ed-4a35-a2f0-08b91817cd1c to disappear
Mar  4 09:15:40.158: INFO: Pod pod-b086b50e-f7ed-4a35-a2f0-08b91817cd1c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:15:40.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6939" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":63,"skipped":873,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:15:40.170: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar  4 09:15:40.204: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7686 /api/v1/namespaces/watch-7686/configmaps/e2e-watch-test-watch-closed 679c3ef7-3e25-4bee-8808-5a52e8d05eed 49744 0 2020-03-04 09:15:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  4 09:15:40.204: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7686 /api/v1/namespaces/watch-7686/configmaps/e2e-watch-test-watch-closed 679c3ef7-3e25-4bee-8808-5a52e8d05eed 49745 0 2020-03-04 09:15:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar  4 09:15:40.214: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7686 /api/v1/namespaces/watch-7686/configmaps/e2e-watch-test-watch-closed 679c3ef7-3e25-4bee-8808-5a52e8d05eed 49746 0 2020-03-04 09:15:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  4 09:15:40.215: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7686 /api/v1/namespaces/watch-7686/configmaps/e2e-watch-test-watch-closed 679c3ef7-3e25-4bee-8808-5a52e8d05eed 49747 0 2020-03-04 09:15:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:15:40.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7686" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":278,"completed":64,"skipped":881,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:15:40.223: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar  4 09:15:44.342: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  4 09:15:44.346: INFO: Pod pod-with-prestop-http-hook still exists
Mar  4 09:15:46.346: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  4 09:15:46.350: INFO: Pod pod-with-prestop-http-hook still exists
Mar  4 09:15:48.346: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  4 09:15:48.351: INFO: Pod pod-with-prestop-http-hook still exists
Mar  4 09:15:50.346: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  4 09:15:50.354: INFO: Pod pod-with-prestop-http-hook still exists
Mar  4 09:15:52.346: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  4 09:15:52.349: INFO: Pod pod-with-prestop-http-hook still exists
Mar  4 09:15:54.346: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  4 09:15:54.349: INFO: Pod pod-with-prestop-http-hook still exists
Mar  4 09:15:56.346: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  4 09:15:56.350: INFO: Pod pod-with-prestop-http-hook still exists
Mar  4 09:15:58.346: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  4 09:15:58.350: INFO: Pod pod-with-prestop-http-hook still exists
Mar  4 09:16:00.346: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  4 09:16:00.351: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:16:00.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1196" for this suite.

• [SLOW TEST:20.146 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":278,"completed":65,"skipped":935,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:16:00.369: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar  4 09:16:00.411: INFO: Waiting up to 5m0s for pod "pod-0efe5fda-ee39-4e5f-8d10-494d2170c70c" in namespace "emptydir-206" to be "success or failure"
Mar  4 09:16:00.422: INFO: Pod "pod-0efe5fda-ee39-4e5f-8d10-494d2170c70c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.093612ms
Mar  4 09:16:02.426: INFO: Pod "pod-0efe5fda-ee39-4e5f-8d10-494d2170c70c": Phase="Running", Reason="", readiness=true. Elapsed: 2.014237309s
Mar  4 09:16:04.437: INFO: Pod "pod-0efe5fda-ee39-4e5f-8d10-494d2170c70c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025890365s
STEP: Saw pod success
Mar  4 09:16:04.438: INFO: Pod "pod-0efe5fda-ee39-4e5f-8d10-494d2170c70c" satisfied condition "success or failure"
Mar  4 09:16:04.442: INFO: Trying to get logs from node master3 pod pod-0efe5fda-ee39-4e5f-8d10-494d2170c70c container test-container: <nil>
STEP: delete the pod
Mar  4 09:16:04.469: INFO: Waiting for pod pod-0efe5fda-ee39-4e5f-8d10-494d2170c70c to disappear
Mar  4 09:16:04.475: INFO: Pod pod-0efe5fda-ee39-4e5f-8d10-494d2170c70c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:16:04.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-206" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":66,"skipped":941,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:16:04.495: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-b115966f-8968-47af-a370-941e98708801
STEP: Creating a pod to test consume secrets
Mar  4 09:16:04.553: INFO: Waiting up to 5m0s for pod "pod-secrets-c8b383be-41ea-478b-a373-32217b44fc19" in namespace "secrets-8048" to be "success or failure"
Mar  4 09:16:04.560: INFO: Pod "pod-secrets-c8b383be-41ea-478b-a373-32217b44fc19": Phase="Pending", Reason="", readiness=false. Elapsed: 7.238026ms
Mar  4 09:16:06.563: INFO: Pod "pod-secrets-c8b383be-41ea-478b-a373-32217b44fc19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01020335s
Mar  4 09:16:08.566: INFO: Pod "pod-secrets-c8b383be-41ea-478b-a373-32217b44fc19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013355627s
STEP: Saw pod success
Mar  4 09:16:08.566: INFO: Pod "pod-secrets-c8b383be-41ea-478b-a373-32217b44fc19" satisfied condition "success or failure"
Mar  4 09:16:08.569: INFO: Trying to get logs from node master3 pod pod-secrets-c8b383be-41ea-478b-a373-32217b44fc19 container secret-volume-test: <nil>
STEP: delete the pod
Mar  4 09:16:08.586: INFO: Waiting for pod pod-secrets-c8b383be-41ea-478b-a373-32217b44fc19 to disappear
Mar  4 09:16:08.590: INFO: Pod pod-secrets-c8b383be-41ea-478b-a373-32217b44fc19 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:16:08.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8048" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":278,"completed":67,"skipped":960,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:16:08.597: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2540.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2540.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  4 09:16:12.679: INFO: DNS probes using dns-2540/dns-test-d4aeb5ad-4b8f-488f-8e73-99c2716626cd succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:16:12.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2540" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":278,"completed":68,"skipped":975,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:16:12.734: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl replace
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1897
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar  4 09:16:12.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-3821'
Mar  4 09:16:12.981: INFO: stderr: ""
Mar  4 09:16:12.981: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Mar  4 09:16:18.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pod e2e-test-httpd-pod --namespace=kubectl-3821 -o json'
Mar  4 09:16:18.129: INFO: stderr: ""
Mar  4 09:16:18.129: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"192.168.136.30/32\",\n            \"cni.projectcalico.org/podIPs\": \"192.168.136.30/32\"\n        },\n        \"creationTimestamp\": \"2020-03-04T09:16:12Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3821\",\n        \"resourceVersion\": \"50013\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-3821/pods/e2e-test-httpd-pod\",\n        \"uid\": \"eca9b664-bb02-4b3d-bdd5-c9e769ea6811\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-q47f9\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"master3\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-q47f9\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-q47f9\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-04T09:16:12Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-04T09:16:15Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-04T09:16:15Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-04T09:16:12Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://89c0bb605a917ba8849f987e8e6e28241c4b9f203dbdf69ad8bc3f72e6249a07\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-03-04T09:16:14Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.128.0.4\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.136.30\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.136.30\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-03-04T09:16:12Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar  4 09:16:18.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 replace -f - --namespace=kubectl-3821'
Mar  4 09:16:18.386: INFO: stderr: ""
Mar  4 09:16:18.386: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1902
Mar  4 09:16:18.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 delete pods e2e-test-httpd-pod --namespace=kubectl-3821'
Mar  4 09:16:28.722: INFO: stderr: ""
Mar  4 09:16:28.722: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:16:28.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3821" for this suite.

• [SLOW TEST:16.004 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1893
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":278,"completed":69,"skipped":999,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:16:28.738: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name secret-emptykey-test-8e5022bd-821a-432f-80c7-56f0143b416c
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:16:28.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2319" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":278,"completed":70,"skipped":1027,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:16:28.786: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:16:28.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 version'
Mar  4 09:16:28.898: INFO: stderr: ""
Mar  4 09:16:28.898: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.3\", GitCommit:\"06ad960bfd03b39c8310aaf92d1e7c12ce618213\", GitTreeState:\"clean\", BuildDate:\"2020-02-11T18:14:22Z\", GoVersion:\"go1.13.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.3\", GitCommit:\"06ad960bfd03b39c8310aaf92d1e7c12ce618213\", GitTreeState:\"clean\", BuildDate:\"2020-02-11T18:07:13Z\", GoVersion:\"go1.13.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:16:28.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2854" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":278,"completed":71,"skipped":1033,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:16:28.907: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:16:36.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-239" for this suite.

• [SLOW TEST:7.115 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":278,"completed":72,"skipped":1037,"failed":0}
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:16:36.021: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run default
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1596
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar  4 09:16:36.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-8056'
Mar  4 09:16:36.151: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  4 09:16:36.151: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1602
Mar  4 09:16:38.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 delete deployment e2e-test-httpd-deployment --namespace=kubectl-8056'
Mar  4 09:16:38.262: INFO: stderr: ""
Mar  4 09:16:38.262: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:16:38.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8056" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run default should create an rc or deployment from an image  [Conformance]","total":278,"completed":73,"skipped":1037,"failed":0}
S
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:16:38.275: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:16:38.336: INFO: (0) /api/v1/nodes/master2:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 7.467763ms)
Mar  4 09:16:38.340: INFO: (1) /api/v1/nodes/master2:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.757789ms)
Mar  4 09:16:38.344: INFO: (2) /api/v1/nodes/master2:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.816789ms)
Mar  4 09:16:38.348: INFO: (3) /api/v1/nodes/master2:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 4.18389ms)
Mar  4 09:16:38.352: INFO: (4) /api/v1/nodes/master2:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.746641ms)
Mar  4 09:16:38.355: INFO: (5) /api/v1/nodes/master2:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.551362ms)
Mar  4 09:16:38.359: INFO: (6) /api/v1/nodes/master2:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.703448ms)
Mar  4 09:16:38.364: INFO: (7) /api/v1/nodes/master2:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 4.42552ms)
Mar  4 09:16:38.368: INFO: (8) /api/v1/nodes/master2:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.8888ms)
Mar  4 09:16:38.371: INFO: (9) /api/v1/nodes/master2:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.391116ms)
Mar  4 09:16:38.375: INFO: (10) /api/v1/nodes/master2:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.731084ms)
Mar  4 09:16:38.380: INFO: (11) /api/v1/nodes/master2:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 5.475361ms)
Mar  4 09:16:38.384: INFO: (12) /api/v1/nodes/master2:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.858017ms)
Mar  4 09:16:38.388: INFO: (13) /api/v1/nodes/master2:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.65821ms)
Mar  4 09:16:38.392: INFO: (14) /api/v1/nodes/master2:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.593419ms)
Mar  4 09:16:38.395: INFO: (15) /api/v1/nodes/master2:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.715557ms)
Mar  4 09:16:38.399: INFO: (16) /api/v1/nodes/master2:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.463838ms)
Mar  4 09:16:38.403: INFO: (17) /api/v1/nodes/master2:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.579951ms)
Mar  4 09:16:38.406: INFO: (18) /api/v1/nodes/master2:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.664505ms)
Mar  4 09:16:38.413: INFO: (19) /api/v1/nodes/master2:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 6.515734ms)
[AfterEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:16:38.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7028" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":278,"completed":74,"skipped":1038,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:16:38.422: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-9e013db7-e4da-45b1-ab98-222706ec6cb9
STEP: Creating a pod to test consume configMaps
Mar  4 09:16:38.458: INFO: Waiting up to 5m0s for pod "pod-configmaps-e879c8aa-9810-40b9-a06a-015beb1aecdf" in namespace "configmap-880" to be "success or failure"
Mar  4 09:16:38.463: INFO: Pod "pod-configmaps-e879c8aa-9810-40b9-a06a-015beb1aecdf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.135215ms
Mar  4 09:16:40.472: INFO: Pod "pod-configmaps-e879c8aa-9810-40b9-a06a-015beb1aecdf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01399271s
STEP: Saw pod success
Mar  4 09:16:40.472: INFO: Pod "pod-configmaps-e879c8aa-9810-40b9-a06a-015beb1aecdf" satisfied condition "success or failure"
Mar  4 09:16:40.475: INFO: Trying to get logs from node master2 pod pod-configmaps-e879c8aa-9810-40b9-a06a-015beb1aecdf container configmap-volume-test: <nil>
STEP: delete the pod
Mar  4 09:16:40.496: INFO: Waiting for pod pod-configmaps-e879c8aa-9810-40b9-a06a-015beb1aecdf to disappear
Mar  4 09:16:40.499: INFO: Pod pod-configmaps-e879c8aa-9810-40b9-a06a-015beb1aecdf no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:16:40.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-880" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":278,"completed":75,"skipped":1054,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:16:40.515: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:16:40.556: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:16:40.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7915" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":278,"completed":76,"skipped":1068,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:16:40.620: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-cd546686-089c-401e-81f6-8d1fac99d635 in namespace container-probe-3532
Mar  4 09:16:42.686: INFO: Started pod busybox-cd546686-089c-401e-81f6-8d1fac99d635 in namespace container-probe-3532
STEP: checking the pod's current state and verifying that restartCount is present
Mar  4 09:16:42.689: INFO: Initial restart count of pod busybox-cd546686-089c-401e-81f6-8d1fac99d635 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:20:43.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3532" for this suite.

• [SLOW TEST:242.555 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":278,"completed":77,"skipped":1166,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:20:43.175: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:20:43.219: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Creating first CR 
Mar  4 09:20:43.785: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-04T09:20:43Z generation:1 name:name1 resourceVersion:50887 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:2aff17e9-40ab-4580-861e-f2b8a848fefc] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Mar  4 09:20:53.789: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-04T09:20:53Z generation:1 name:name2 resourceVersion:50926 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:c6c6fdeb-915d-4af2-b0cb-bcb5830576ec] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Mar  4 09:21:03.794: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-04T09:20:43Z generation:2 name:name1 resourceVersion:50951 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:2aff17e9-40ab-4580-861e-f2b8a848fefc] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Mar  4 09:21:13.799: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-04T09:20:53Z generation:2 name:name2 resourceVersion:50976 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:c6c6fdeb-915d-4af2-b0cb-bcb5830576ec] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Mar  4 09:21:23.805: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-04T09:20:43Z generation:2 name:name1 resourceVersion:50999 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:2aff17e9-40ab-4580-861e-f2b8a848fefc] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Mar  4 09:21:33.811: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-04T09:20:53Z generation:2 name:name2 resourceVersion:51024 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:c6c6fdeb-915d-4af2-b0cb-bcb5830576ec] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:21:44.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-7383" for this suite.

• [SLOW TEST:61.167 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:41
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":278,"completed":78,"skipped":1168,"failed":0}
SSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:21:44.342: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar  4 09:21:48.694: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5100 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  4 09:21:48.694: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
Mar  4 09:21:48.850: INFO: Exec stderr: ""
Mar  4 09:21:48.850: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5100 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  4 09:21:48.850: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
Mar  4 09:21:48.971: INFO: Exec stderr: ""
Mar  4 09:21:48.971: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5100 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  4 09:21:48.971: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
Mar  4 09:21:49.109: INFO: Exec stderr: ""
Mar  4 09:21:49.109: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5100 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  4 09:21:49.109: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
Mar  4 09:21:49.230: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar  4 09:21:49.230: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5100 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  4 09:21:49.230: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
Mar  4 09:21:49.369: INFO: Exec stderr: ""
Mar  4 09:21:49.369: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5100 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  4 09:21:49.369: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
Mar  4 09:21:49.495: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar  4 09:21:49.496: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5100 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  4 09:21:49.496: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
Mar  4 09:21:49.631: INFO: Exec stderr: ""
Mar  4 09:21:49.631: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5100 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  4 09:21:49.632: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
Mar  4 09:21:49.754: INFO: Exec stderr: ""
Mar  4 09:21:49.754: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5100 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  4 09:21:49.754: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
Mar  4 09:21:49.913: INFO: Exec stderr: ""
Mar  4 09:21:49.913: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5100 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  4 09:21:49.913: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
Mar  4 09:21:50.034: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:21:50.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5100" for this suite.

• [SLOW TEST:5.702 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":79,"skipped":1175,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:21:50.045: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:21:50.073: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:21:51.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7299" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":278,"completed":80,"skipped":1186,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:21:51.179: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar  4 09:21:51.235: INFO: Pod name pod-release: Found 0 pods out of 1
Mar  4 09:21:56.238: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:21:56.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9421" for this suite.

• [SLOW TEST:5.117 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":278,"completed":81,"skipped":1188,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:21:56.296: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar  4 09:21:56.338: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c529395d-7ff1-498c-9d42-af12b7f13e50" in namespace "downward-api-409" to be "success or failure"
Mar  4 09:21:56.342: INFO: Pod "downwardapi-volume-c529395d-7ff1-498c-9d42-af12b7f13e50": Phase="Pending", Reason="", readiness=false. Elapsed: 4.196213ms
Mar  4 09:21:58.345: INFO: Pod "downwardapi-volume-c529395d-7ff1-498c-9d42-af12b7f13e50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007389727s
STEP: Saw pod success
Mar  4 09:21:58.345: INFO: Pod "downwardapi-volume-c529395d-7ff1-498c-9d42-af12b7f13e50" satisfied condition "success or failure"
Mar  4 09:21:58.349: INFO: Trying to get logs from node master2 pod downwardapi-volume-c529395d-7ff1-498c-9d42-af12b7f13e50 container client-container: <nil>
STEP: delete the pod
Mar  4 09:21:58.376: INFO: Waiting for pod downwardapi-volume-c529395d-7ff1-498c-9d42-af12b7f13e50 to disappear
Mar  4 09:21:58.378: INFO: Pod downwardapi-volume-c529395d-7ff1-498c-9d42-af12b7f13e50 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:21:58.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-409" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":82,"skipped":1199,"failed":0}
S
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:21:58.385: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:21:58.419: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-c63e148d-bb3a-4969-8aea-8f4651cb58d0" in namespace "security-context-test-6968" to be "success or failure"
Mar  4 09:21:58.422: INFO: Pod "busybox-readonly-false-c63e148d-bb3a-4969-8aea-8f4651cb58d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.971733ms
Mar  4 09:22:00.425: INFO: Pod "busybox-readonly-false-c63e148d-bb3a-4969-8aea-8f4651cb58d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005816324s
Mar  4 09:22:00.425: INFO: Pod "busybox-readonly-false-c63e148d-bb3a-4969-8aea-8f4651cb58d0" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:22:00.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6968" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":278,"completed":83,"skipped":1200,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:22:00.437: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-745bc87e-2e84-4c15-a106-f9ea50a36585
STEP: Creating a pod to test consume secrets
Mar  4 09:22:00.526: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d77cb70d-e019-4037-869b-c84d5ab2a611" in namespace "projected-9318" to be "success or failure"
Mar  4 09:22:00.530: INFO: Pod "pod-projected-secrets-d77cb70d-e019-4037-869b-c84d5ab2a611": Phase="Pending", Reason="", readiness=false. Elapsed: 3.433875ms
Mar  4 09:22:02.533: INFO: Pod "pod-projected-secrets-d77cb70d-e019-4037-869b-c84d5ab2a611": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00632526s
STEP: Saw pod success
Mar  4 09:22:02.533: INFO: Pod "pod-projected-secrets-d77cb70d-e019-4037-869b-c84d5ab2a611" satisfied condition "success or failure"
Mar  4 09:22:02.535: INFO: Trying to get logs from node master2 pod pod-projected-secrets-d77cb70d-e019-4037-869b-c84d5ab2a611 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  4 09:22:02.551: INFO: Waiting for pod pod-projected-secrets-d77cb70d-e019-4037-869b-c84d5ab2a611 to disappear
Mar  4 09:22:02.556: INFO: Pod pod-projected-secrets-d77cb70d-e019-4037-869b-c84d5ab2a611 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:22:02.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9318" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":84,"skipped":1278,"failed":0}

------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:22:02.566: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  4 09:22:02.620: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:22:02.622: INFO: Number of nodes with available pods: 0
Mar  4 09:22:02.622: INFO: Node master2 is running more than one daemon pod
Mar  4 09:22:03.627: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:22:03.633: INFO: Number of nodes with available pods: 0
Mar  4 09:22:03.633: INFO: Node master2 is running more than one daemon pod
Mar  4 09:22:04.626: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:22:04.628: INFO: Number of nodes with available pods: 1
Mar  4 09:22:04.628: INFO: Node master3 is running more than one daemon pod
Mar  4 09:22:05.626: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:22:05.629: INFO: Number of nodes with available pods: 2
Mar  4 09:22:05.629: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar  4 09:22:05.651: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:22:05.665: INFO: Number of nodes with available pods: 1
Mar  4 09:22:05.665: INFO: Node master3 is running more than one daemon pod
Mar  4 09:22:06.669: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:22:06.672: INFO: Number of nodes with available pods: 1
Mar  4 09:22:06.672: INFO: Node master3 is running more than one daemon pod
Mar  4 09:22:07.670: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:22:07.673: INFO: Number of nodes with available pods: 1
Mar  4 09:22:07.673: INFO: Node master3 is running more than one daemon pod
Mar  4 09:22:08.669: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:22:08.672: INFO: Number of nodes with available pods: 2
Mar  4 09:22:08.672: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-768, will wait for the garbage collector to delete the pods
Mar  4 09:22:08.734: INFO: Deleting DaemonSet.extensions daemon-set took: 4.884266ms
Mar  4 09:22:09.134: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.18622ms
Mar  4 09:22:18.740: INFO: Number of nodes with available pods: 0
Mar  4 09:22:18.740: INFO: Number of running nodes: 0, number of available pods: 0
Mar  4 09:22:18.746: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-768/daemonsets","resourceVersion":"51437"},"items":null}

Mar  4 09:22:18.750: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-768/pods","resourceVersion":"51437"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:22:18.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-768" for this suite.

• [SLOW TEST:16.202 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":278,"completed":85,"skipped":1278,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:22:18.769: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:22:18.823: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar  4 09:22:22.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 --namespace=crd-publish-openapi-9462 create -f -'
Mar  4 09:22:22.659: INFO: stderr: ""
Mar  4 09:22:22.659: INFO: stdout: "e2e-test-crd-publish-openapi-754-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar  4 09:22:22.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 --namespace=crd-publish-openapi-9462 delete e2e-test-crd-publish-openapi-754-crds test-cr'
Mar  4 09:22:22.770: INFO: stderr: ""
Mar  4 09:22:22.770: INFO: stdout: "e2e-test-crd-publish-openapi-754-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar  4 09:22:22.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 --namespace=crd-publish-openapi-9462 apply -f -'
Mar  4 09:22:23.044: INFO: stderr: ""
Mar  4 09:22:23.044: INFO: stdout: "e2e-test-crd-publish-openapi-754-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar  4 09:22:23.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 --namespace=crd-publish-openapi-9462 delete e2e-test-crd-publish-openapi-754-crds test-cr'
Mar  4 09:22:23.155: INFO: stderr: ""
Mar  4 09:22:23.156: INFO: stdout: "e2e-test-crd-publish-openapi-754-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar  4 09:22:23.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 explain e2e-test-crd-publish-openapi-754-crds'
Mar  4 09:22:23.397: INFO: stderr: ""
Mar  4 09:22:23.397: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-754-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:22:27.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9462" for this suite.

• [SLOW TEST:8.337 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":278,"completed":86,"skipped":1299,"failed":0}
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:22:27.106: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
Mar  4 09:22:27.658: INFO: created pod pod-service-account-defaultsa
Mar  4 09:22:27.658: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar  4 09:22:27.668: INFO: created pod pod-service-account-mountsa
Mar  4 09:22:27.668: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar  4 09:22:27.675: INFO: created pod pod-service-account-nomountsa
Mar  4 09:22:27.675: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar  4 09:22:27.682: INFO: created pod pod-service-account-defaultsa-mountspec
Mar  4 09:22:27.682: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar  4 09:22:27.689: INFO: created pod pod-service-account-mountsa-mountspec
Mar  4 09:22:27.689: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar  4 09:22:27.710: INFO: created pod pod-service-account-nomountsa-mountspec
Mar  4 09:22:27.710: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar  4 09:22:27.730: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar  4 09:22:27.730: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar  4 09:22:27.736: INFO: created pod pod-service-account-mountsa-nomountspec
Mar  4 09:22:27.736: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar  4 09:22:27.754: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar  4 09:22:27.754: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:22:27.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3578" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":278,"completed":87,"skipped":1308,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:22:27.780: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1632
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar  4 09:22:27.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-1071'
Mar  4 09:22:28.033: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  4 09:22:28.033: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Mar  4 09:22:28.043: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-xchj5]
Mar  4 09:22:28.043: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-xchj5" in namespace "kubectl-1071" to be "running and ready"
Mar  4 09:22:28.046: INFO: Pod "e2e-test-httpd-rc-xchj5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.999312ms
Mar  4 09:22:30.052: INFO: Pod "e2e-test-httpd-rc-xchj5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008351618s
Mar  4 09:22:32.055: INFO: Pod "e2e-test-httpd-rc-xchj5": Phase="Running", Reason="", readiness=true. Elapsed: 4.011342637s
Mar  4 09:22:32.055: INFO: Pod "e2e-test-httpd-rc-xchj5" satisfied condition "running and ready"
Mar  4 09:22:32.055: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-xchj5]
Mar  4 09:22:32.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 logs rc/e2e-test-httpd-rc --namespace=kubectl-1071'
Mar  4 09:22:32.194: INFO: stderr: ""
Mar  4 09:22:32.194: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 192.168.136.54. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 192.168.136.54. Set the 'ServerName' directive globally to suppress this message\n[Wed Mar 04 09:22:30.267965 2020] [mpm_event:notice] [pid 1:tid 140017360124776] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Wed Mar 04 09:22:30.268026 2020] [core:notice] [pid 1:tid 140017360124776] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1637
Mar  4 09:22:32.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 delete rc e2e-test-httpd-rc --namespace=kubectl-1071'
Mar  4 09:22:32.324: INFO: stderr: ""
Mar  4 09:22:32.324: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:22:32.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1071" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run rc should create an rc from an image  [Conformance]","total":278,"completed":88,"skipped":1318,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:22:32.348: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar  4 09:22:32.413: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a95b9c1f-651c-4fc1-a571-26e2de62549c" in namespace "projected-1646" to be "success or failure"
Mar  4 09:22:32.417: INFO: Pod "downwardapi-volume-a95b9c1f-651c-4fc1-a571-26e2de62549c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.959577ms
Mar  4 09:22:34.420: INFO: Pod "downwardapi-volume-a95b9c1f-651c-4fc1-a571-26e2de62549c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006712663s
Mar  4 09:22:36.423: INFO: Pod "downwardapi-volume-a95b9c1f-651c-4fc1-a571-26e2de62549c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009630483s
STEP: Saw pod success
Mar  4 09:22:36.423: INFO: Pod "downwardapi-volume-a95b9c1f-651c-4fc1-a571-26e2de62549c" satisfied condition "success or failure"
Mar  4 09:22:36.425: INFO: Trying to get logs from node master3 pod downwardapi-volume-a95b9c1f-651c-4fc1-a571-26e2de62549c container client-container: <nil>
STEP: delete the pod
Mar  4 09:22:36.440: INFO: Waiting for pod downwardapi-volume-a95b9c1f-651c-4fc1-a571-26e2de62549c to disappear
Mar  4 09:22:36.447: INFO: Pod downwardapi-volume-a95b9c1f-651c-4fc1-a571-26e2de62549c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:22:36.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1646" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":89,"skipped":1333,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:22:36.456: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  4 09:22:36.492: INFO: Waiting up to 5m0s for pod "pod-1228d6c5-6633-460a-8b67-f220d57a0e5b" in namespace "emptydir-9561" to be "success or failure"
Mar  4 09:22:36.498: INFO: Pod "pod-1228d6c5-6633-460a-8b67-f220d57a0e5b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.18237ms
Mar  4 09:22:38.500: INFO: Pod "pod-1228d6c5-6633-460a-8b67-f220d57a0e5b": Phase="Running", Reason="", readiness=true. Elapsed: 2.008849297s
Mar  4 09:22:40.503: INFO: Pod "pod-1228d6c5-6633-460a-8b67-f220d57a0e5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011796852s
STEP: Saw pod success
Mar  4 09:22:40.503: INFO: Pod "pod-1228d6c5-6633-460a-8b67-f220d57a0e5b" satisfied condition "success or failure"
Mar  4 09:22:40.507: INFO: Trying to get logs from node master3 pod pod-1228d6c5-6633-460a-8b67-f220d57a0e5b container test-container: <nil>
STEP: delete the pod
Mar  4 09:22:40.526: INFO: Waiting for pod pod-1228d6c5-6633-460a-8b67-f220d57a0e5b to disappear
Mar  4 09:22:40.533: INFO: Pod pod-1228d6c5-6633-460a-8b67-f220d57a0e5b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:22:40.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9561" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":90,"skipped":1337,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:22:40.550: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-5712
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  4 09:22:40.597: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  4 09:23:00.677: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.180.52 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5712 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  4 09:23:00.677: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
Mar  4 09:23:01.805: INFO: Found all expected endpoints: [netserver-0]
Mar  4 09:23:01.810: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.136.60 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5712 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  4 09:23:01.810: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
Mar  4 09:23:02.939: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:23:02.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5712" for this suite.

• [SLOW TEST:22.397 seconds]
[sig-network] Networking
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":91,"skipped":1346,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:23:02.947: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Mar  4 09:23:02.985: INFO: Waiting up to 5m0s for pod "downward-api-6f18aff3-24ed-4586-bafd-95e9440146b9" in namespace "downward-api-297" to be "success or failure"
Mar  4 09:23:02.990: INFO: Pod "downward-api-6f18aff3-24ed-4586-bafd-95e9440146b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.853837ms
Mar  4 09:23:04.993: INFO: Pod "downward-api-6f18aff3-24ed-4586-bafd-95e9440146b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007964296s
Mar  4 09:23:06.996: INFO: Pod "downward-api-6f18aff3-24ed-4586-bafd-95e9440146b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011421389s
STEP: Saw pod success
Mar  4 09:23:06.997: INFO: Pod "downward-api-6f18aff3-24ed-4586-bafd-95e9440146b9" satisfied condition "success or failure"
Mar  4 09:23:07.000: INFO: Trying to get logs from node master3 pod downward-api-6f18aff3-24ed-4586-bafd-95e9440146b9 container dapi-container: <nil>
STEP: delete the pod
Mar  4 09:23:07.017: INFO: Waiting for pod downward-api-6f18aff3-24ed-4586-bafd-95e9440146b9 to disappear
Mar  4 09:23:07.019: INFO: Pod downward-api-6f18aff3-24ed-4586-bafd-95e9440146b9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:23:07.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-297" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":278,"completed":92,"skipped":1361,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:23:07.026: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service nodeport-test with type=NodePort in namespace services-4564
STEP: creating replication controller nodeport-test in namespace services-4564
I0304 09:23:07.076652      19 runners.go:189] Created replication controller with name: nodeport-test, namespace: services-4564, replica count: 2
Mar  4 09:23:10.129: INFO: Creating new exec pod
I0304 09:23:10.129216      19 runners.go:189] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  4 09:23:13.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=services-4564 execpodmlbsx -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Mar  4 09:23:13.368: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar  4 09:23:13.368: INFO: stdout: ""
Mar  4 09:23:13.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=services-4564 execpodmlbsx -- /bin/sh -x -c nc -zv -t -w 2 10.106.20.126 80'
Mar  4 09:23:13.592: INFO: stderr: "+ nc -zv -t -w 2 10.106.20.126 80\nConnection to 10.106.20.126 80 port [tcp/http] succeeded!\n"
Mar  4 09:23:13.592: INFO: stdout: ""
Mar  4 09:23:13.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=services-4564 execpodmlbsx -- /bin/sh -x -c nc -zv -t -w 2 10.128.0.3 31950'
Mar  4 09:23:13.817: INFO: stderr: "+ nc -zv -t -w 2 10.128.0.3 31950\nConnection to 10.128.0.3 31950 port [tcp/31950] succeeded!\n"
Mar  4 09:23:13.817: INFO: stdout: ""
Mar  4 09:23:13.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=services-4564 execpodmlbsx -- /bin/sh -x -c nc -zv -t -w 2 10.128.0.4 31950'
Mar  4 09:23:14.056: INFO: stderr: "+ nc -zv -t -w 2 10.128.0.4 31950\nConnection to 10.128.0.4 31950 port [tcp/31950] succeeded!\n"
Mar  4 09:23:14.056: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:23:14.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4564" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:7.039 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":278,"completed":93,"skipped":1364,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:23:14.065: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating replication controller my-hostname-basic-a3537d14-df52-43bb-9462-dca1980519b4
Mar  4 09:23:14.100: INFO: Pod name my-hostname-basic-a3537d14-df52-43bb-9462-dca1980519b4: Found 0 pods out of 1
Mar  4 09:23:19.105: INFO: Pod name my-hostname-basic-a3537d14-df52-43bb-9462-dca1980519b4: Found 1 pods out of 1
Mar  4 09:23:19.105: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-a3537d14-df52-43bb-9462-dca1980519b4" are running
Mar  4 09:23:19.108: INFO: Pod "my-hostname-basic-a3537d14-df52-43bb-9462-dca1980519b4-79jm8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-04 09:23:14 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-04 09:23:16 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-04 09:23:16 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-04 09:23:14 +0000 UTC Reason: Message:}])
Mar  4 09:23:19.108: INFO: Trying to dial the pod
Mar  4 09:23:24.120: INFO: Controller my-hostname-basic-a3537d14-df52-43bb-9462-dca1980519b4: Got expected result from replica 1 [my-hostname-basic-a3537d14-df52-43bb-9462-dca1980519b4-79jm8]: "my-hostname-basic-a3537d14-df52-43bb-9462-dca1980519b4-79jm8", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:23:24.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-559" for this suite.

• [SLOW TEST:10.063 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":278,"completed":94,"skipped":1383,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:23:24.128: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar  4 09:23:24.161: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6849c1e2-d7f1-4908-bd7c-efb20701f9b0" in namespace "projected-839" to be "success or failure"
Mar  4 09:23:24.167: INFO: Pod "downwardapi-volume-6849c1e2-d7f1-4908-bd7c-efb20701f9b0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.59502ms
Mar  4 09:23:26.170: INFO: Pod "downwardapi-volume-6849c1e2-d7f1-4908-bd7c-efb20701f9b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008561049s
STEP: Saw pod success
Mar  4 09:23:26.170: INFO: Pod "downwardapi-volume-6849c1e2-d7f1-4908-bd7c-efb20701f9b0" satisfied condition "success or failure"
Mar  4 09:23:26.172: INFO: Trying to get logs from node master3 pod downwardapi-volume-6849c1e2-d7f1-4908-bd7c-efb20701f9b0 container client-container: <nil>
STEP: delete the pod
Mar  4 09:23:26.188: INFO: Waiting for pod downwardapi-volume-6849c1e2-d7f1-4908-bd7c-efb20701f9b0 to disappear
Mar  4 09:23:26.193: INFO: Pod downwardapi-volume-6849c1e2-d7f1-4908-bd7c-efb20701f9b0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:23:26.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-839" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":278,"completed":95,"skipped":1385,"failed":0}
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:23:26.201: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-5149
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  4 09:23:26.227: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  4 09:23:50.312: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.136.6:8080/dial?request=hostname&protocol=http&host=192.168.180.54&port=8080&tries=1'] Namespace:pod-network-test-5149 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  4 09:23:50.312: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
Mar  4 09:23:50.481: INFO: Waiting for responses: map[]
Mar  4 09:23:50.488: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.136.6:8080/dial?request=hostname&protocol=http&host=192.168.136.61&port=8080&tries=1'] Namespace:pod-network-test-5149 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  4 09:23:50.488: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
Mar  4 09:23:50.622: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:23:50.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5149" for this suite.

• [SLOW TEST:24.430 seconds]
[sig-network] Networking
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":96,"skipped":1388,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:23:50.631: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:23:55.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8845" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":278,"completed":97,"skipped":1396,"failed":0}
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:23:55.391: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
STEP: reading a file in the container
Mar  4 09:23:59.953: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9284 pod-service-account-7e6e1230-4a17-446a-8051-9ae8f7ab89be -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Mar  4 09:24:00.175: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9284 pod-service-account-7e6e1230-4a17-446a-8051-9ae8f7ab89be -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Mar  4 09:24:00.408: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9284 pod-service-account-7e6e1230-4a17-446a-8051-9ae8f7ab89be -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:24:00.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9284" for this suite.

• [SLOW TEST:5.283 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":278,"completed":98,"skipped":1401,"failed":0}
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:24:00.675: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Mar  4 09:24:01.754: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:24:01.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0304 09:24:01.754204      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-8156" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":278,"completed":99,"skipped":1401,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:24:01.763: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  4 09:24:03.750: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  4 09:24:05.764: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718910643, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718910643, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718910643, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718910643, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  4 09:24:08.784: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:24:08.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3498" for this suite.
STEP: Destroying namespace "webhook-3498-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.120 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":278,"completed":100,"skipped":1420,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:24:08.882: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service endpoint-test2 in namespace services-4066
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4066 to expose endpoints map[]
Mar  4 09:24:08.955: INFO: successfully validated that service endpoint-test2 in namespace services-4066 exposes endpoints map[] (7.997645ms elapsed)
STEP: Creating pod pod1 in namespace services-4066
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4066 to expose endpoints map[pod1:[80]]
Mar  4 09:24:11.997: INFO: successfully validated that service endpoint-test2 in namespace services-4066 exposes endpoints map[pod1:[80]] (3.033062054s elapsed)
STEP: Creating pod pod2 in namespace services-4066
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4066 to expose endpoints map[pod1:[80] pod2:[80]]
Mar  4 09:24:14.054: INFO: successfully validated that service endpoint-test2 in namespace services-4066 exposes endpoints map[pod1:[80] pod2:[80]] (2.049517221s elapsed)
STEP: Deleting pod pod1 in namespace services-4066
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4066 to expose endpoints map[pod2:[80]]
Mar  4 09:24:14.081: INFO: successfully validated that service endpoint-test2 in namespace services-4066 exposes endpoints map[pod2:[80]] (16.135719ms elapsed)
STEP: Deleting pod pod2 in namespace services-4066
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4066 to expose endpoints map[]
Mar  4 09:24:14.109: INFO: successfully validated that service endpoint-test2 in namespace services-4066 exposes endpoints map[] (11.57473ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:24:14.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4066" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:5.290 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":278,"completed":101,"skipped":1427,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:24:14.173: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  4 09:24:14.925: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  4 09:24:16.932: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718910654, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718910654, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718910654, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718910654, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  4 09:24:19.945: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Mar  4 09:24:21.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 attach --namespace=webhook-4476 to-be-attached-pod -i -c=container1'
Mar  4 09:24:22.096: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:24:22.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4476" for this suite.
STEP: Destroying namespace "webhook-4476-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.019 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":278,"completed":102,"skipped":1443,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:24:22.192: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1733
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar  4 09:24:22.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-57'
Mar  4 09:24:22.419: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  4 09:24:22.419: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1738
Mar  4 09:24:26.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 delete deployment e2e-test-httpd-deployment --namespace=kubectl-57'
Mar  4 09:24:26.645: INFO: stderr: ""
Mar  4 09:24:26.645: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:24:26.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-57" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run deployment should create a deployment from an image  [Conformance]","total":278,"completed":103,"skipped":1450,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:24:26.655: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8248
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8248
STEP: creating replication controller externalsvc in namespace services-8248
I0304 09:24:26.752776      19 runners.go:189] Created replication controller with name: externalsvc, namespace: services-8248, replica count: 2
I0304 09:24:29.805937      19 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Mar  4 09:24:29.821: INFO: Creating new exec pod
Mar  4 09:24:31.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=services-8248 execpod7gj9t -- /bin/sh -x -c nslookup clusterip-service'
Mar  4 09:24:32.216: INFO: stderr: "+ nslookup clusterip-service\n"
Mar  4 09:24:32.216: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-8248.svc.cluster.local\tcanonical name = externalsvc.services-8248.svc.cluster.local.\nName:\texternalsvc.services-8248.svc.cluster.local\nAddress: 10.97.115.235\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8248, will wait for the garbage collector to delete the pods
Mar  4 09:24:32.275: INFO: Deleting ReplicationController externalsvc took: 5.453595ms
Mar  4 09:24:32.375: INFO: Terminating ReplicationController externalsvc pods took: 100.169965ms
Mar  4 09:24:38.806: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:24:38.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8248" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:12.205 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":278,"completed":104,"skipped":1459,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:24:38.860: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-016421a0-29c1-41cd-a60d-d43c7ecb2329
STEP: Creating secret with name s-test-opt-upd-1338de6f-392e-48ef-a680-0156c3c93861
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-016421a0-29c1-41cd-a60d-d43c7ecb2329
STEP: Updating secret s-test-opt-upd-1338de6f-392e-48ef-a680-0156c3c93861
STEP: Creating secret with name s-test-opt-create-fc17052e-4f75-4410-866f-96cb1d4ef1b3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:24:43.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9211" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":105,"skipped":1482,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:24:43.035: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar  4 09:24:43.080: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4154 /api/v1/namespaces/watch-4154/configmaps/e2e-watch-test-resource-version 6e509919-1e37-4c41-8f70-d9e9a40de95e 53057 0 2020-03-04 09:24:43 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  4 09:24:43.080: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4154 /api/v1/namespaces/watch-4154/configmaps/e2e-watch-test-resource-version 6e509919-1e37-4c41-8f70-d9e9a40de95e 53058 0 2020-03-04 09:24:43 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:24:43.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4154" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":278,"completed":106,"skipped":1487,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:24:43.089: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Mar  4 09:25:13.646: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:25:13.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0304 09:25:13.646177      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-3666" for this suite.

• [SLOW TEST:30.567 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":278,"completed":107,"skipped":1514,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:25:13.656: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar  4 09:25:13.924: INFO: Pod name wrapped-volume-race-250a6e61-8f03-4ec9-8942-d6a09f4411e3: Found 1 pods out of 5
Mar  4 09:25:18.929: INFO: Pod name wrapped-volume-race-250a6e61-8f03-4ec9-8942-d6a09f4411e3: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-250a6e61-8f03-4ec9-8942-d6a09f4411e3 in namespace emptydir-wrapper-2496, will wait for the garbage collector to delete the pods
Mar  4 09:25:29.012: INFO: Deleting ReplicationController wrapped-volume-race-250a6e61-8f03-4ec9-8942-d6a09f4411e3 took: 6.681138ms
Mar  4 09:25:29.412: INFO: Terminating ReplicationController wrapped-volume-race-250a6e61-8f03-4ec9-8942-d6a09f4411e3 pods took: 400.246155ms
STEP: Creating RC which spawns configmap-volume pods
Mar  4 09:25:36.427: INFO: Pod name wrapped-volume-race-04ba2728-2dc0-4c0a-b58f-4093e4676a37: Found 0 pods out of 5
Mar  4 09:25:41.433: INFO: Pod name wrapped-volume-race-04ba2728-2dc0-4c0a-b58f-4093e4676a37: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-04ba2728-2dc0-4c0a-b58f-4093e4676a37 in namespace emptydir-wrapper-2496, will wait for the garbage collector to delete the pods
Mar  4 09:25:53.513: INFO: Deleting ReplicationController wrapped-volume-race-04ba2728-2dc0-4c0a-b58f-4093e4676a37 took: 6.507521ms
Mar  4 09:25:53.915: INFO: Terminating ReplicationController wrapped-volume-race-04ba2728-2dc0-4c0a-b58f-4093e4676a37 pods took: 401.636266ms
STEP: Creating RC which spawns configmap-volume pods
Mar  4 09:26:09.734: INFO: Pod name wrapped-volume-race-a5d8be06-1367-4a8d-9507-7a37edd74fe5: Found 0 pods out of 5
Mar  4 09:26:14.744: INFO: Pod name wrapped-volume-race-a5d8be06-1367-4a8d-9507-7a37edd74fe5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-a5d8be06-1367-4a8d-9507-7a37edd74fe5 in namespace emptydir-wrapper-2496, will wait for the garbage collector to delete the pods
Mar  4 09:26:26.831: INFO: Deleting ReplicationController wrapped-volume-race-a5d8be06-1367-4a8d-9507-7a37edd74fe5 took: 5.997351ms
Mar  4 09:26:27.231: INFO: Terminating ReplicationController wrapped-volume-race-a5d8be06-1367-4a8d-9507-7a37edd74fe5 pods took: 400.295963ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:26:34.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2496" for this suite.

• [SLOW TEST:80.714 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":278,"completed":108,"skipped":1535,"failed":0}
SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:26:34.370: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Mar  4 09:26:34.403: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:26:38.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3845" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":278,"completed":109,"skipped":1542,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:26:38.268: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:26:38.361: INFO: Create a RollingUpdate DaemonSet
Mar  4 09:26:38.365: INFO: Check that daemon pods launch on every node of the cluster
Mar  4 09:26:38.372: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:26:38.376: INFO: Number of nodes with available pods: 0
Mar  4 09:26:38.376: INFO: Node master2 is running more than one daemon pod
Mar  4 09:26:39.382: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:26:39.385: INFO: Number of nodes with available pods: 0
Mar  4 09:26:39.385: INFO: Node master2 is running more than one daemon pod
Mar  4 09:26:40.380: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:26:40.383: INFO: Number of nodes with available pods: 2
Mar  4 09:26:40.383: INFO: Number of running nodes: 2, number of available pods: 2
Mar  4 09:26:40.383: INFO: Update the DaemonSet to trigger a rollout
Mar  4 09:26:40.388: INFO: Updating DaemonSet daemon-set
Mar  4 09:26:44.404: INFO: Roll back the DaemonSet before rollout is complete
Mar  4 09:26:44.411: INFO: Updating DaemonSet daemon-set
Mar  4 09:26:44.411: INFO: Make sure DaemonSet rollback is complete
Mar  4 09:26:44.414: INFO: Wrong image for pod: daemon-set-2tnmf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  4 09:26:44.414: INFO: Pod daemon-set-2tnmf is not available
Mar  4 09:26:44.418: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:26:45.422: INFO: Wrong image for pod: daemon-set-2tnmf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  4 09:26:45.422: INFO: Pod daemon-set-2tnmf is not available
Mar  4 09:26:45.425: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:26:46.424: INFO: Pod daemon-set-cqj5r is not available
Mar  4 09:26:46.430: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6367, will wait for the garbage collector to delete the pods
Mar  4 09:26:46.497: INFO: Deleting DaemonSet.extensions daemon-set took: 7.343149ms
Mar  4 09:26:46.897: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.400662ms
Mar  4 09:26:53.700: INFO: Number of nodes with available pods: 0
Mar  4 09:26:53.700: INFO: Number of running nodes: 0, number of available pods: 0
Mar  4 09:26:53.702: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6367/daemonsets","resourceVersion":"54591"},"items":null}

Mar  4 09:26:53.704: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6367/pods","resourceVersion":"54591"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:26:53.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6367" for this suite.

• [SLOW TEST:15.450 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":278,"completed":110,"skipped":1555,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:26:53.719: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  4 09:26:54.288: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  4 09:26:56.304: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718910814, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718910814, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718910814, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718910814, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  4 09:26:59.317: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Mar  4 09:26:59.337: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:26:59.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1795" for this suite.
STEP: Destroying namespace "webhook-1795-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.718 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":278,"completed":111,"skipped":1568,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:26:59.439: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:27:10.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5299" for this suite.

• [SLOW TEST:11.102 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":278,"completed":112,"skipped":1590,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:27:10.541: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Mar  4 09:27:10.569: INFO: PodSpec: initContainers in spec.initContainers
Mar  4 09:27:54.452: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-cd07efaa-b675-448b-8d63-35ca75e13020", GenerateName:"", Namespace:"init-container-6216", SelfLink:"/api/v1/namespaces/init-container-6216/pods/pod-init-cd07efaa-b675-448b-8d63-35ca75e13020", UID:"e320ead6-328f-4427-a4f1-9d89aba52085", ResourceVersion:"54891", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63718910830, loc:(*time.Location)(0x7db7bc0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"569925829"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"192.168.136.25/32", "cni.projectcalico.org/podIPs":"192.168.136.25/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-hcg6r", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0033edcc0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-hcg6r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-hcg6r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-hcg6r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004076b48), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"master3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002f34ae0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004076c50)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004076c90)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004076c98), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004076c9c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718910830, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718910830, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718910830, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718910830, loc:(*time.Location)(0x7db7bc0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.128.0.4", PodIP:"192.168.136.25", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.136.25"}}, StartTime:(*v1.Time)(0xc004b3c160), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000f6d9d0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000f6dab0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://2bd4f9d01301645133178fb60b30948b232931dd508cbf50583dc9a567f3040a", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004b3c1a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004b3c180), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc004076d9f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:27:54.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6216" for this suite.

• [SLOW TEST:43.922 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":278,"completed":113,"skipped":1616,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:27:54.463: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1788
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar  4 09:27:54.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-3112'
Mar  4 09:27:54.629: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  4 09:27:54.629: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1793
Mar  4 09:27:54.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 delete jobs e2e-test-httpd-job --namespace=kubectl-3112'
Mar  4 09:27:54.749: INFO: stderr: ""
Mar  4 09:27:54.749: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:27:54.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3112" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run job should create a job from an image when restart is OnFailure  [Conformance]","total":278,"completed":114,"skipped":1618,"failed":0}

------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:27:54.759: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar  4 09:27:55.880: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  4 09:27:58.899: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:27:58.902: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:28:00.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-44" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

• [SLOW TEST:5.945 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":278,"completed":115,"skipped":1618,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:28:00.705: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  4 09:28:02.619: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  4 09:28:05.636: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:28:05.639: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:28:06.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2203" for this suite.
STEP: Destroying namespace "webhook-2203-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.947 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":278,"completed":116,"skipped":1633,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:28:06.652: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-51018e15-d89b-45a9-9c58-5438ea7dc3c6
STEP: Creating secret with name s-test-opt-upd-de36b7d0-96cf-45a6-aeb6-f62b57a43f8a
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-51018e15-d89b-45a9-9c58-5438ea7dc3c6
STEP: Updating secret s-test-opt-upd-de36b7d0-96cf-45a6-aeb6-f62b57a43f8a
STEP: Creating secret with name s-test-opt-create-9c0473e2-a8ef-4665-b181-d0ad238c4100
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:28:10.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-817" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":117,"skipped":1652,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:28:10.823: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:28:14.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9418" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":278,"completed":118,"skipped":1665,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:28:14.929: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Mar  4 09:28:19.505: INFO: Successfully updated pod "labelsupdate98ce1c93-938c-42a0-b4af-39c1dfa2a221"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:28:21.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4404" for this suite.

• [SLOW TEST:6.605 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":278,"completed":119,"skipped":1691,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:28:21.535: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-8277
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Mar  4 09:28:21.579: INFO: Found 0 stateful pods, waiting for 3
Mar  4 09:28:31.582: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  4 09:28:31.582: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  4 09:28:31.582: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar  4 09:28:31.606: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar  4 09:28:41.641: INFO: Updating stateful set ss2
Mar  4 09:28:41.655: INFO: Waiting for Pod statefulset-8277/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Mar  4 09:28:51.709: INFO: Found 2 stateful pods, waiting for 3
Mar  4 09:29:01.713: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  4 09:29:01.713: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  4 09:29:01.713: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar  4 09:29:01.734: INFO: Updating stateful set ss2
Mar  4 09:29:01.752: INFO: Waiting for Pod statefulset-8277/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  4 09:29:11.775: INFO: Updating stateful set ss2
Mar  4 09:29:11.800: INFO: Waiting for StatefulSet statefulset-8277/ss2 to complete update
Mar  4 09:29:11.800: INFO: Waiting for Pod statefulset-8277/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Mar  4 09:29:21.807: INFO: Deleting all statefulset in ns statefulset-8277
Mar  4 09:29:21.810: INFO: Scaling statefulset ss2 to 0
Mar  4 09:29:41.824: INFO: Waiting for statefulset status.replicas updated to 0
Mar  4 09:29:41.829: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:29:41.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8277" for this suite.

• [SLOW TEST:80.315 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":278,"completed":120,"skipped":1701,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:29:41.851: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:29:41.897: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Mar  4 09:29:45.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 --namespace=crd-publish-openapi-1177 create -f -'
Mar  4 09:29:46.013: INFO: stderr: ""
Mar  4 09:29:46.013: INFO: stdout: "e2e-test-crd-publish-openapi-3342-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar  4 09:29:46.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 --namespace=crd-publish-openapi-1177 delete e2e-test-crd-publish-openapi-3342-crds test-foo'
Mar  4 09:29:46.114: INFO: stderr: ""
Mar  4 09:29:46.114: INFO: stdout: "e2e-test-crd-publish-openapi-3342-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar  4 09:29:46.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 --namespace=crd-publish-openapi-1177 apply -f -'
Mar  4 09:29:46.367: INFO: stderr: ""
Mar  4 09:29:46.367: INFO: stdout: "e2e-test-crd-publish-openapi-3342-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar  4 09:29:46.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 --namespace=crd-publish-openapi-1177 delete e2e-test-crd-publish-openapi-3342-crds test-foo'
Mar  4 09:29:46.467: INFO: stderr: ""
Mar  4 09:29:46.467: INFO: stdout: "e2e-test-crd-publish-openapi-3342-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Mar  4 09:29:46.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 --namespace=crd-publish-openapi-1177 create -f -'
Mar  4 09:29:46.679: INFO: rc: 1
Mar  4 09:29:46.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 --namespace=crd-publish-openapi-1177 apply -f -'
Mar  4 09:29:46.900: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Mar  4 09:29:46.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 --namespace=crd-publish-openapi-1177 create -f -'
Mar  4 09:29:47.106: INFO: rc: 1
Mar  4 09:29:47.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 --namespace=crd-publish-openapi-1177 apply -f -'
Mar  4 09:29:47.334: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Mar  4 09:29:47.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 explain e2e-test-crd-publish-openapi-3342-crds'
Mar  4 09:29:47.561: INFO: stderr: ""
Mar  4 09:29:47.561: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3342-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Mar  4 09:29:47.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 explain e2e-test-crd-publish-openapi-3342-crds.metadata'
Mar  4 09:29:47.787: INFO: stderr: ""
Mar  4 09:29:47.787: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3342-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar  4 09:29:47.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 explain e2e-test-crd-publish-openapi-3342-crds.spec'
Mar  4 09:29:47.999: INFO: stderr: ""
Mar  4 09:29:47.999: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3342-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar  4 09:29:48.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 explain e2e-test-crd-publish-openapi-3342-crds.spec.bars'
Mar  4 09:29:48.219: INFO: stderr: ""
Mar  4 09:29:48.220: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3342-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Mar  4 09:29:48.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 explain e2e-test-crd-publish-openapi-3342-crds.spec.bars2'
Mar  4 09:29:48.447: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:29:52.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1177" for this suite.

• [SLOW TEST:10.315 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":278,"completed":121,"skipped":1765,"failed":0}
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:29:52.166: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-4809
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  4 09:29:52.199: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  4 09:30:14.267: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.136.58:8080/dial?request=hostname&protocol=udp&host=192.168.180.18&port=8081&tries=1'] Namespace:pod-network-test-4809 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  4 09:30:14.267: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
Mar  4 09:30:14.400: INFO: Waiting for responses: map[]
Mar  4 09:30:14.405: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.136.58:8080/dial?request=hostname&protocol=udp&host=192.168.136.33&port=8081&tries=1'] Namespace:pod-network-test-4809 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  4 09:30:14.405: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
Mar  4 09:30:14.541: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:30:14.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4809" for this suite.

• [SLOW TEST:22.382 seconds]
[sig-network] Networking
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":122,"skipped":1772,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:30:14.548: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-1768/configmap-test-a55434a1-16aa-481a-a51e-0982d20bd56f
STEP: Creating a pod to test consume configMaps
Mar  4 09:30:14.583: INFO: Waiting up to 5m0s for pod "pod-configmaps-61e90f45-3270-48ef-9c84-5d956c480920" in namespace "configmap-1768" to be "success or failure"
Mar  4 09:30:14.588: INFO: Pod "pod-configmaps-61e90f45-3270-48ef-9c84-5d956c480920": Phase="Pending", Reason="", readiness=false. Elapsed: 4.500078ms
Mar  4 09:30:16.591: INFO: Pod "pod-configmaps-61e90f45-3270-48ef-9c84-5d956c480920": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00759223s
STEP: Saw pod success
Mar  4 09:30:16.591: INFO: Pod "pod-configmaps-61e90f45-3270-48ef-9c84-5d956c480920" satisfied condition "success or failure"
Mar  4 09:30:16.593: INFO: Trying to get logs from node master3 pod pod-configmaps-61e90f45-3270-48ef-9c84-5d956c480920 container env-test: <nil>
STEP: delete the pod
Mar  4 09:30:16.622: INFO: Waiting for pod pod-configmaps-61e90f45-3270-48ef-9c84-5d956c480920 to disappear
Mar  4 09:30:16.623: INFO: Pod pod-configmaps-61e90f45-3270-48ef-9c84-5d956c480920 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:30:16.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1768" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":278,"completed":123,"skipped":1810,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:30:16.634: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  4 09:30:19.205: INFO: Successfully updated pod "pod-update-58be4d35-e9ef-4f60-815c-030308035cd6"
STEP: verifying the updated pod is in kubernetes
Mar  4 09:30:19.215: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:30:19.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9655" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":278,"completed":124,"skipped":1816,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:30:19.225: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Mar  4 09:30:19.268: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  4 09:30:19.279: INFO: Waiting for terminating namespaces to be deleted...
Mar  4 09:30:19.282: INFO: 
Logging pods the kubelet thinks is on node master2 before test
Mar  4 09:30:19.297: INFO: host-test-container-pod from pod-network-test-4809 started at 2020-03-04 09:30:12 +0000 UTC (1 container statuses recorded)
Mar  4 09:30:19.297: INFO: 	Container agnhost ready: true, restart count 0
Mar  4 09:30:19.297: INFO: kube-proxy-xq4nn from kube-system started at 2020-03-04 06:27:13 +0000 UTC (1 container statuses recorded)
Mar  4 09:30:19.297: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  4 09:30:19.297: INFO: sonobuoy-systemd-logs-daemon-set-002e23000b6e4f18-b5sfk from sonobuoy started at 2020-03-04 08:58:19 +0000 UTC (2 container statuses recorded)
Mar  4 09:30:19.297: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  4 09:30:19.297: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  4 09:30:19.297: INFO: calico-kube-controllers-5b644bc49c-knmsz from kube-system started at 2020-03-04 06:30:14 +0000 UTC (1 container statuses recorded)
Mar  4 09:30:19.297: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  4 09:30:19.297: INFO: coredns-6955765f44-npmb6 from kube-system started at 2020-03-04 06:30:15 +0000 UTC (1 container statuses recorded)
Mar  4 09:30:19.297: INFO: 	Container coredns ready: true, restart count 0
Mar  4 09:30:19.297: INFO: calico-node-lzvqd from kube-system started at 2020-03-04 06:30:01 +0000 UTC (1 container statuses recorded)
Mar  4 09:30:19.297: INFO: 	Container calico-node ready: true, restart count 0
Mar  4 09:30:19.297: INFO: netserver-0 from pod-network-test-4809 started at 2020-03-04 09:29:52 +0000 UTC (1 container statuses recorded)
Mar  4 09:30:19.297: INFO: 	Container webserver ready: true, restart count 0
Mar  4 09:30:19.297: INFO: coredns-6955765f44-55pg4 from kube-system started at 2020-03-04 06:30:15 +0000 UTC (1 container statuses recorded)
Mar  4 09:30:19.297: INFO: 	Container coredns ready: true, restart count 0
Mar  4 09:30:19.297: INFO: 
Logging pods the kubelet thinks is on node master3 before test
Mar  4 09:30:19.304: INFO: calico-node-l9qb2 from kube-system started at 2020-03-04 06:30:01 +0000 UTC (1 container statuses recorded)
Mar  4 09:30:19.304: INFO: 	Container calico-node ready: true, restart count 0
Mar  4 09:30:19.304: INFO: sonobuoy-e2e-job-7260dc34846e4a1a from sonobuoy started at 2020-03-04 08:58:19 +0000 UTC (2 container statuses recorded)
Mar  4 09:30:19.304: INFO: 	Container e2e ready: true, restart count 0
Mar  4 09:30:19.304: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  4 09:30:19.304: INFO: sonobuoy-systemd-logs-daemon-set-002e23000b6e4f18-28pjm from sonobuoy started at 2020-03-04 08:58:19 +0000 UTC (2 container statuses recorded)
Mar  4 09:30:19.304: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  4 09:30:19.304: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  4 09:30:19.304: INFO: netserver-1 from pod-network-test-4809 started at 2020-03-04 09:29:52 +0000 UTC (1 container statuses recorded)
Mar  4 09:30:19.304: INFO: 	Container webserver ready: true, restart count 0
Mar  4 09:30:19.304: INFO: kube-proxy-gkd5m from kube-system started at 2020-03-04 06:27:48 +0000 UTC (1 container statuses recorded)
Mar  4 09:30:19.304: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  4 09:30:19.304: INFO: sonobuoy from sonobuoy started at 2020-03-04 08:58:18 +0000 UTC (1 container statuses recorded)
Mar  4 09:30:19.304: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  4 09:30:19.304: INFO: test-container-pod from pod-network-test-4809 started at 2020-03-04 09:30:12 +0000 UTC (1 container statuses recorded)
Mar  4 09:30:19.304: INFO: 	Container webserver ready: true, restart count 0
Mar  4 09:30:19.304: INFO: pod-update-58be4d35-e9ef-4f60-815c-030308035cd6 from pods-9655 started at 2020-03-04 09:30:16 +0000 UTC (1 container statuses recorded)
Mar  4 09:30:19.304: INFO: 	Container nginx ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-a7d5c3fa-da86-4496-9827-642bee5fa64c 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-a7d5c3fa-da86-4496-9827-642bee5fa64c off the node master2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-a7d5c3fa-da86-4496-9827-642bee5fa64c
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:35:27.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2420" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:308.181 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":278,"completed":125,"skipped":1838,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:35:27.406: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:35:29.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9989" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":278,"completed":126,"skipped":1850,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:35:29.492: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Mar  4 09:35:29.555: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  4 09:35:29.568: INFO: Waiting for terminating namespaces to be deleted...
Mar  4 09:35:29.572: INFO: 
Logging pods the kubelet thinks is on node master2 before test
Mar  4 09:35:29.589: INFO: pod4 from sched-pred-2420 started at 2020-03-04 09:30:23 +0000 UTC (1 container statuses recorded)
Mar  4 09:35:29.589: INFO: 	Container pod4 ready: true, restart count 0
Mar  4 09:35:29.589: INFO: calico-node-lzvqd from kube-system started at 2020-03-04 06:30:01 +0000 UTC (1 container statuses recorded)
Mar  4 09:35:29.589: INFO: 	Container calico-node ready: true, restart count 0
Mar  4 09:35:29.589: INFO: calico-kube-controllers-5b644bc49c-knmsz from kube-system started at 2020-03-04 06:30:14 +0000 UTC (1 container statuses recorded)
Mar  4 09:35:29.589: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  4 09:35:29.589: INFO: coredns-6955765f44-npmb6 from kube-system started at 2020-03-04 06:30:15 +0000 UTC (1 container statuses recorded)
Mar  4 09:35:29.589: INFO: 	Container coredns ready: true, restart count 0
Mar  4 09:35:29.589: INFO: coredns-6955765f44-55pg4 from kube-system started at 2020-03-04 06:30:15 +0000 UTC (1 container statuses recorded)
Mar  4 09:35:29.589: INFO: 	Container coredns ready: true, restart count 0
Mar  4 09:35:29.589: INFO: kube-proxy-xq4nn from kube-system started at 2020-03-04 06:27:13 +0000 UTC (1 container statuses recorded)
Mar  4 09:35:29.589: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  4 09:35:29.589: INFO: sonobuoy-systemd-logs-daemon-set-002e23000b6e4f18-b5sfk from sonobuoy started at 2020-03-04 08:58:19 +0000 UTC (2 container statuses recorded)
Mar  4 09:35:29.589: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  4 09:35:29.589: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  4 09:35:29.589: INFO: 
Logging pods the kubelet thinks is on node master3 before test
Mar  4 09:35:29.617: INFO: kube-proxy-gkd5m from kube-system started at 2020-03-04 06:27:48 +0000 UTC (1 container statuses recorded)
Mar  4 09:35:29.617: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  4 09:35:29.617: INFO: sonobuoy from sonobuoy started at 2020-03-04 08:58:18 +0000 UTC (1 container statuses recorded)
Mar  4 09:35:29.617: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  4 09:35:29.617: INFO: sonobuoy-systemd-logs-daemon-set-002e23000b6e4f18-28pjm from sonobuoy started at 2020-03-04 08:58:19 +0000 UTC (2 container statuses recorded)
Mar  4 09:35:29.617: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  4 09:35:29.617: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  4 09:35:29.617: INFO: calico-node-l9qb2 from kube-system started at 2020-03-04 06:30:01 +0000 UTC (1 container statuses recorded)
Mar  4 09:35:29.617: INFO: 	Container calico-node ready: true, restart count 0
Mar  4 09:35:29.617: INFO: sonobuoy-e2e-job-7260dc34846e4a1a from sonobuoy started at 2020-03-04 08:58:19 +0000 UTC (2 container statuses recorded)
Mar  4 09:35:29.617: INFO: 	Container e2e ready: true, restart count 0
Mar  4 09:35:29.617: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-8889432e-79da-4ae5-ba22-9d9efe1ea2d9 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-8889432e-79da-4ae5-ba22-9d9efe1ea2d9 off the node master3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-8889432e-79da-4ae5-ba22-9d9efe1ea2d9
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:35:33.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-635" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":278,"completed":127,"skipped":1892,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:35:33.752: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-30b004b6-a752-49c1-ab34-58afad449a5e
STEP: Creating a pod to test consume configMaps
Mar  4 09:35:33.815: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e8587321-2983-4ff4-86e6-e19db9945aae" in namespace "projected-2632" to be "success or failure"
Mar  4 09:35:33.822: INFO: Pod "pod-projected-configmaps-e8587321-2983-4ff4-86e6-e19db9945aae": Phase="Pending", Reason="", readiness=false. Elapsed: 7.437573ms
Mar  4 09:35:35.826: INFO: Pod "pod-projected-configmaps-e8587321-2983-4ff4-86e6-e19db9945aae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010751359s
Mar  4 09:35:37.829: INFO: Pod "pod-projected-configmaps-e8587321-2983-4ff4-86e6-e19db9945aae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013990448s
STEP: Saw pod success
Mar  4 09:35:37.829: INFO: Pod "pod-projected-configmaps-e8587321-2983-4ff4-86e6-e19db9945aae" satisfied condition "success or failure"
Mar  4 09:35:37.832: INFO: Trying to get logs from node master3 pod pod-projected-configmaps-e8587321-2983-4ff4-86e6-e19db9945aae container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  4 09:35:37.850: INFO: Waiting for pod pod-projected-configmaps-e8587321-2983-4ff4-86e6-e19db9945aae to disappear
Mar  4 09:35:37.855: INFO: Pod pod-projected-configmaps-e8587321-2983-4ff4-86e6-e19db9945aae no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:35:37.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2632" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":278,"completed":128,"skipped":1927,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:35:37.868: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar  4 09:35:37.914: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8473 /api/v1/namespaces/watch-8473/configmaps/e2e-watch-test-label-changed 5571635a-b159-4cc5-9047-0c4fbd0f07af 56990 0 2020-03-04 09:35:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  4 09:35:37.914: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8473 /api/v1/namespaces/watch-8473/configmaps/e2e-watch-test-label-changed 5571635a-b159-4cc5-9047-0c4fbd0f07af 56991 0 2020-03-04 09:35:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar  4 09:35:37.914: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8473 /api/v1/namespaces/watch-8473/configmaps/e2e-watch-test-label-changed 5571635a-b159-4cc5-9047-0c4fbd0f07af 56992 0 2020-03-04 09:35:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar  4 09:35:47.937: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8473 /api/v1/namespaces/watch-8473/configmaps/e2e-watch-test-label-changed 5571635a-b159-4cc5-9047-0c4fbd0f07af 57052 0 2020-03-04 09:35:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  4 09:35:47.937: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8473 /api/v1/namespaces/watch-8473/configmaps/e2e-watch-test-label-changed 5571635a-b159-4cc5-9047-0c4fbd0f07af 57053 0 2020-03-04 09:35:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Mar  4 09:35:47.937: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8473 /api/v1/namespaces/watch-8473/configmaps/e2e-watch-test-label-changed 5571635a-b159-4cc5-9047-0c4fbd0f07af 57054 0 2020-03-04 09:35:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:35:47.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8473" for this suite.

• [SLOW TEST:10.078 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":278,"completed":129,"skipped":1960,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:35:47.946: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:35:47.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7997" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":278,"completed":130,"skipped":1964,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:35:47.983: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Mar  4 09:35:48.015: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
Mar  4 09:35:51.640: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:36:06.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6437" for this suite.

• [SLOW TEST:18.029 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":278,"completed":131,"skipped":1974,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:36:06.012: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Mar  4 09:36:06.041: INFO: namespace kubectl-9870
Mar  4 09:36:06.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 create -f - --namespace=kubectl-9870'
Mar  4 09:36:06.359: INFO: stderr: ""
Mar  4 09:36:06.359: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Mar  4 09:36:07.363: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  4 09:36:07.363: INFO: Found 0 / 1
Mar  4 09:36:08.363: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  4 09:36:08.363: INFO: Found 1 / 1
Mar  4 09:36:08.363: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  4 09:36:08.365: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  4 09:36:08.365: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  4 09:36:08.366: INFO: wait on agnhost-master startup in kubectl-9870 
Mar  4 09:36:08.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 logs agnhost-master-4d4dz agnhost-master --namespace=kubectl-9870'
Mar  4 09:36:08.481: INFO: stderr: ""
Mar  4 09:36:08.481: INFO: stdout: "Paused\n"
STEP: exposing RC
Mar  4 09:36:08.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-9870'
Mar  4 09:36:08.605: INFO: stderr: ""
Mar  4 09:36:08.605: INFO: stdout: "service/rm2 exposed\n"
Mar  4 09:36:08.609: INFO: Service rm2 in namespace kubectl-9870 found.
STEP: exposing service
Mar  4 09:36:10.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-9870'
Mar  4 09:36:10.730: INFO: stderr: ""
Mar  4 09:36:10.730: INFO: stdout: "service/rm3 exposed\n"
Mar  4 09:36:10.733: INFO: Service rm3 in namespace kubectl-9870 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:36:12.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9870" for this suite.

• [SLOW TEST:6.733 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1295
    should create services for rc  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":278,"completed":132,"skipped":1982,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:36:12.746: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar  4 09:36:12.782: INFO: Waiting up to 5m0s for pod "downwardapi-volume-17bb873b-6e9d-446e-aa33-e9c47e9ff5a4" in namespace "downward-api-6479" to be "success or failure"
Mar  4 09:36:12.788: INFO: Pod "downwardapi-volume-17bb873b-6e9d-446e-aa33-e9c47e9ff5a4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.279154ms
Mar  4 09:36:14.792: INFO: Pod "downwardapi-volume-17bb873b-6e9d-446e-aa33-e9c47e9ff5a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009724996s
Mar  4 09:36:16.796: INFO: Pod "downwardapi-volume-17bb873b-6e9d-446e-aa33-e9c47e9ff5a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013376965s
STEP: Saw pod success
Mar  4 09:36:16.796: INFO: Pod "downwardapi-volume-17bb873b-6e9d-446e-aa33-e9c47e9ff5a4" satisfied condition "success or failure"
Mar  4 09:36:16.800: INFO: Trying to get logs from node master2 pod downwardapi-volume-17bb873b-6e9d-446e-aa33-e9c47e9ff5a4 container client-container: <nil>
STEP: delete the pod
Mar  4 09:36:16.819: INFO: Waiting for pod downwardapi-volume-17bb873b-6e9d-446e-aa33-e9c47e9ff5a4 to disappear
Mar  4 09:36:16.822: INFO: Pod downwardapi-volume-17bb873b-6e9d-446e-aa33-e9c47e9ff5a4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:36:16.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6479" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":278,"completed":133,"skipped":1998,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:36:16.830: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:36:29.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4074" for this suite.

• [SLOW TEST:13.090 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":278,"completed":134,"skipped":1999,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:36:29.920: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:36:29.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8823" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":278,"completed":135,"skipped":2049,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:36:29.962: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  4 09:36:30.011: INFO: Waiting up to 5m0s for pod "pod-852511e5-8cf6-4af3-b100-9e6ffb6459f6" in namespace "emptydir-4977" to be "success or failure"
Mar  4 09:36:30.018: INFO: Pod "pod-852511e5-8cf6-4af3-b100-9e6ffb6459f6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.899808ms
Mar  4 09:36:32.028: INFO: Pod "pod-852511e5-8cf6-4af3-b100-9e6ffb6459f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01684375s
STEP: Saw pod success
Mar  4 09:36:32.028: INFO: Pod "pod-852511e5-8cf6-4af3-b100-9e6ffb6459f6" satisfied condition "success or failure"
Mar  4 09:36:32.033: INFO: Trying to get logs from node master3 pod pod-852511e5-8cf6-4af3-b100-9e6ffb6459f6 container test-container: <nil>
STEP: delete the pod
Mar  4 09:36:32.057: INFO: Waiting for pod pod-852511e5-8cf6-4af3-b100-9e6ffb6459f6 to disappear
Mar  4 09:36:32.065: INFO: Pod pod-852511e5-8cf6-4af3-b100-9e6ffb6459f6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:36:32.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4977" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":136,"skipped":2073,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:36:32.072: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  4 09:36:32.510: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  4 09:36:35.544: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:36:35.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2734" for this suite.
STEP: Destroying namespace "webhook-2734-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":278,"completed":137,"skipped":2107,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:36:35.710: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-6819
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-6819
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6819
Mar  4 09:36:35.773: INFO: Found 0 stateful pods, waiting for 1
Mar  4 09:36:45.777: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar  4 09:36:45.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=statefulset-6819 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  4 09:36:46.023: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  4 09:36:46.023: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  4 09:36:46.023: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  4 09:36:46.027: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  4 09:36:56.031: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  4 09:36:56.031: INFO: Waiting for statefulset status.replicas updated to 0
Mar  4 09:36:56.042: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999403s
Mar  4 09:36:57.045: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997236973s
Mar  4 09:36:58.048: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993694242s
Mar  4 09:36:59.052: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.990447489s
Mar  4 09:37:00.055: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.987248629s
Mar  4 09:37:01.058: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.98377687s
Mar  4 09:37:02.062: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.980371507s
Mar  4 09:37:03.065: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.977134552s
Mar  4 09:37:04.068: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.973905012s
Mar  4 09:37:05.072: INFO: Verifying statefulset ss doesn't scale past 1 for another 970.662981ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6819
Mar  4 09:37:06.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=statefulset-6819 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  4 09:37:06.320: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  4 09:37:06.320: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  4 09:37:06.320: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  4 09:37:06.324: INFO: Found 1 stateful pods, waiting for 3
Mar  4 09:37:16.328: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  4 09:37:16.328: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  4 09:37:16.328: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar  4 09:37:16.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=statefulset-6819 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  4 09:37:16.562: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  4 09:37:16.562: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  4 09:37:16.562: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  4 09:37:16.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=statefulset-6819 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  4 09:37:16.808: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  4 09:37:16.808: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  4 09:37:16.808: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  4 09:37:16.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=statefulset-6819 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  4 09:37:17.054: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  4 09:37:17.054: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  4 09:37:17.054: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  4 09:37:17.054: INFO: Waiting for statefulset status.replicas updated to 0
Mar  4 09:37:17.056: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar  4 09:37:27.062: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  4 09:37:27.062: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  4 09:37:27.062: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  4 09:37:27.073: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999543s
Mar  4 09:37:28.077: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994252969s
Mar  4 09:37:29.081: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990244667s
Mar  4 09:37:30.085: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986454432s
Mar  4 09:37:31.088: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.98283766s
Mar  4 09:37:32.092: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.979246473s
Mar  4 09:37:33.095: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.975791315s
Mar  4 09:37:34.099: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.97232857s
Mar  4 09:37:35.102: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.968742132s
Mar  4 09:37:36.107: INFO: Verifying statefulset ss doesn't scale past 3 for another 965.211485ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6819
Mar  4 09:37:37.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=statefulset-6819 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  4 09:37:37.333: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  4 09:37:37.333: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  4 09:37:37.333: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  4 09:37:37.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=statefulset-6819 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  4 09:37:37.550: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  4 09:37:37.550: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  4 09:37:37.550: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  4 09:37:37.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=statefulset-6819 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  4 09:37:37.787: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  4 09:37:37.787: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  4 09:37:37.787: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  4 09:37:37.787: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Mar  4 09:37:57.801: INFO: Deleting all statefulset in ns statefulset-6819
Mar  4 09:37:57.804: INFO: Scaling statefulset ss to 0
Mar  4 09:37:57.811: INFO: Waiting for statefulset status.replicas updated to 0
Mar  4 09:37:57.813: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:37:57.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6819" for this suite.

• [SLOW TEST:82.126 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":278,"completed":138,"skipped":2118,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:37:57.836: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-c24b8e29-d2b3-46bc-9387-3cdf936a9e8e
STEP: Creating a pod to test consume secrets
Mar  4 09:37:57.928: INFO: Waiting up to 5m0s for pod "pod-secrets-1f08e91c-2eba-4c86-b7af-08352b5c697d" in namespace "secrets-9690" to be "success or failure"
Mar  4 09:37:57.931: INFO: Pod "pod-secrets-1f08e91c-2eba-4c86-b7af-08352b5c697d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.366111ms
Mar  4 09:37:59.934: INFO: Pod "pod-secrets-1f08e91c-2eba-4c86-b7af-08352b5c697d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005322475s
STEP: Saw pod success
Mar  4 09:37:59.934: INFO: Pod "pod-secrets-1f08e91c-2eba-4c86-b7af-08352b5c697d" satisfied condition "success or failure"
Mar  4 09:37:59.937: INFO: Trying to get logs from node master3 pod pod-secrets-1f08e91c-2eba-4c86-b7af-08352b5c697d container secret-volume-test: <nil>
STEP: delete the pod
Mar  4 09:37:59.965: INFO: Waiting for pod pod-secrets-1f08e91c-2eba-4c86-b7af-08352b5c697d to disappear
Mar  4 09:37:59.968: INFO: Pod pod-secrets-1f08e91c-2eba-4c86-b7af-08352b5c697d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:37:59.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9690" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":139,"skipped":2124,"failed":0}
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:37:59.986: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Mar  4 09:38:00.021: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:38:03.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6697" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":278,"completed":140,"skipped":2127,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:38:03.968: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:38:04.028: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar  4 09:38:07.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 --namespace=crd-publish-openapi-2450 create -f -'
Mar  4 09:38:08.238: INFO: stderr: ""
Mar  4 09:38:08.238: INFO: stdout: "e2e-test-crd-publish-openapi-1580-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar  4 09:38:08.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 --namespace=crd-publish-openapi-2450 delete e2e-test-crd-publish-openapi-1580-crds test-cr'
Mar  4 09:38:08.341: INFO: stderr: ""
Mar  4 09:38:08.341: INFO: stdout: "e2e-test-crd-publish-openapi-1580-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar  4 09:38:08.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 --namespace=crd-publish-openapi-2450 apply -f -'
Mar  4 09:38:08.570: INFO: stderr: ""
Mar  4 09:38:08.570: INFO: stdout: "e2e-test-crd-publish-openapi-1580-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar  4 09:38:08.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 --namespace=crd-publish-openapi-2450 delete e2e-test-crd-publish-openapi-1580-crds test-cr'
Mar  4 09:38:08.670: INFO: stderr: ""
Mar  4 09:38:08.670: INFO: stdout: "e2e-test-crd-publish-openapi-1580-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar  4 09:38:08.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 explain e2e-test-crd-publish-openapi-1580-crds'
Mar  4 09:38:08.905: INFO: stderr: ""
Mar  4 09:38:08.905: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1580-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:38:12.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2450" for this suite.

• [SLOW TEST:8.663 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":278,"completed":141,"skipped":2129,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:38:12.632: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Mar  4 09:38:12.660: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:38:15.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2764" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":278,"completed":142,"skipped":2182,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:38:15.538: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's args
Mar  4 09:38:15.574: INFO: Waiting up to 5m0s for pod "var-expansion-9c6cf1af-ef82-4490-a962-b57a28dee481" in namespace "var-expansion-6546" to be "success or failure"
Mar  4 09:38:15.578: INFO: Pod "var-expansion-9c6cf1af-ef82-4490-a962-b57a28dee481": Phase="Pending", Reason="", readiness=false. Elapsed: 4.188391ms
Mar  4 09:38:17.581: INFO: Pod "var-expansion-9c6cf1af-ef82-4490-a962-b57a28dee481": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007264004s
Mar  4 09:38:19.585: INFO: Pod "var-expansion-9c6cf1af-ef82-4490-a962-b57a28dee481": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010537013s
STEP: Saw pod success
Mar  4 09:38:19.585: INFO: Pod "var-expansion-9c6cf1af-ef82-4490-a962-b57a28dee481" satisfied condition "success or failure"
Mar  4 09:38:19.587: INFO: Trying to get logs from node master3 pod var-expansion-9c6cf1af-ef82-4490-a962-b57a28dee481 container dapi-container: <nil>
STEP: delete the pod
Mar  4 09:38:19.603: INFO: Waiting for pod var-expansion-9c6cf1af-ef82-4490-a962-b57a28dee481 to disappear
Mar  4 09:38:19.607: INFO: Pod var-expansion-9c6cf1af-ef82-4490-a962-b57a28dee481 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:38:19.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6546" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":278,"completed":143,"skipped":2196,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:38:19.615: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-4704f30f-40e6-483d-b8dd-2a78dd3a9b5f
STEP: Creating a pod to test consume configMaps
Mar  4 09:38:19.655: INFO: Waiting up to 5m0s for pod "pod-configmaps-53cefc7e-eeaa-4261-9a13-96afb3fc4139" in namespace "configmap-8546" to be "success or failure"
Mar  4 09:38:19.659: INFO: Pod "pod-configmaps-53cefc7e-eeaa-4261-9a13-96afb3fc4139": Phase="Pending", Reason="", readiness=false. Elapsed: 3.943953ms
Mar  4 09:38:21.662: INFO: Pod "pod-configmaps-53cefc7e-eeaa-4261-9a13-96afb3fc4139": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007203695s
STEP: Saw pod success
Mar  4 09:38:21.662: INFO: Pod "pod-configmaps-53cefc7e-eeaa-4261-9a13-96afb3fc4139" satisfied condition "success or failure"
Mar  4 09:38:21.665: INFO: Trying to get logs from node master3 pod pod-configmaps-53cefc7e-eeaa-4261-9a13-96afb3fc4139 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  4 09:38:21.682: INFO: Waiting for pod pod-configmaps-53cefc7e-eeaa-4261-9a13-96afb3fc4139 to disappear
Mar  4 09:38:21.684: INFO: Pod pod-configmaps-53cefc7e-eeaa-4261-9a13-96afb3fc4139 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:38:21.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8546" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":144,"skipped":2201,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:38:21.692: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with configMap that has name projected-configmap-test-upd-8279fa29-0d19-4761-8193-5ce927ac3ce7
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-8279fa29-0d19-4761-8193-5ce927ac3ce7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:38:25.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9004" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":145,"skipped":2203,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:38:25.785: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar  4 09:38:25.866: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d9f06bcf-729f-41f6-ba7f-2107ec8c3e11" in namespace "downward-api-8989" to be "success or failure"
Mar  4 09:38:25.871: INFO: Pod "downwardapi-volume-d9f06bcf-729f-41f6-ba7f-2107ec8c3e11": Phase="Pending", Reason="", readiness=false. Elapsed: 5.041232ms
Mar  4 09:38:27.874: INFO: Pod "downwardapi-volume-d9f06bcf-729f-41f6-ba7f-2107ec8c3e11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008360187s
STEP: Saw pod success
Mar  4 09:38:27.874: INFO: Pod "downwardapi-volume-d9f06bcf-729f-41f6-ba7f-2107ec8c3e11" satisfied condition "success or failure"
Mar  4 09:38:27.877: INFO: Trying to get logs from node master2 pod downwardapi-volume-d9f06bcf-729f-41f6-ba7f-2107ec8c3e11 container client-container: <nil>
STEP: delete the pod
Mar  4 09:38:27.905: INFO: Waiting for pod downwardapi-volume-d9f06bcf-729f-41f6-ba7f-2107ec8c3e11 to disappear
Mar  4 09:38:27.909: INFO: Pod downwardapi-volume-d9f06bcf-729f-41f6-ba7f-2107ec8c3e11 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:38:27.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8989" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":278,"completed":146,"skipped":2209,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:38:27.915: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Mar  4 09:38:30.484: INFO: Successfully updated pod "annotationupdateebe5c56d-c780-4562-88b1-975f034d6b3a"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:38:34.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7554" for this suite.

• [SLOW TEST:6.604 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":278,"completed":147,"skipped":2222,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:38:34.519: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-f0b21389-4520-422e-9fc6-574cf24257aa
STEP: Creating a pod to test consume secrets
Mar  4 09:38:34.561: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3ba39b33-d752-4ba7-a83c-545e194b9fa6" in namespace "projected-3396" to be "success or failure"
Mar  4 09:38:34.567: INFO: Pod "pod-projected-secrets-3ba39b33-d752-4ba7-a83c-545e194b9fa6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.640303ms
Mar  4 09:38:36.570: INFO: Pod "pod-projected-secrets-3ba39b33-d752-4ba7-a83c-545e194b9fa6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00876949s
STEP: Saw pod success
Mar  4 09:38:36.570: INFO: Pod "pod-projected-secrets-3ba39b33-d752-4ba7-a83c-545e194b9fa6" satisfied condition "success or failure"
Mar  4 09:38:36.573: INFO: Trying to get logs from node master2 pod pod-projected-secrets-3ba39b33-d752-4ba7-a83c-545e194b9fa6 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  4 09:38:36.588: INFO: Waiting for pod pod-projected-secrets-3ba39b33-d752-4ba7-a83c-545e194b9fa6 to disappear
Mar  4 09:38:36.591: INFO: Pod pod-projected-secrets-3ba39b33-d752-4ba7-a83c-545e194b9fa6 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:38:36.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3396" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":148,"skipped":2230,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:38:36.601: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Mar  4 09:38:36.648: INFO: Waiting up to 5m0s for pod "downward-api-401e1753-8940-4a54-a69b-050d8236cb65" in namespace "downward-api-9604" to be "success or failure"
Mar  4 09:38:36.662: INFO: Pod "downward-api-401e1753-8940-4a54-a69b-050d8236cb65": Phase="Pending", Reason="", readiness=false. Elapsed: 14.657093ms
Mar  4 09:38:38.666: INFO: Pod "downward-api-401e1753-8940-4a54-a69b-050d8236cb65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017929644s
STEP: Saw pod success
Mar  4 09:38:38.666: INFO: Pod "downward-api-401e1753-8940-4a54-a69b-050d8236cb65" satisfied condition "success or failure"
Mar  4 09:38:38.668: INFO: Trying to get logs from node master2 pod downward-api-401e1753-8940-4a54-a69b-050d8236cb65 container dapi-container: <nil>
STEP: delete the pod
Mar  4 09:38:38.698: INFO: Waiting for pod downward-api-401e1753-8940-4a54-a69b-050d8236cb65 to disappear
Mar  4 09:38:38.703: INFO: Pod downward-api-401e1753-8940-4a54-a69b-050d8236cb65 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:38:38.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9604" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":278,"completed":149,"skipped":2274,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:38:38.712: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-projected-s94g
STEP: Creating a pod to test atomic-volume-subpath
Mar  4 09:38:38.763: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-s94g" in namespace "subpath-6976" to be "success or failure"
Mar  4 09:38:38.766: INFO: Pod "pod-subpath-test-projected-s94g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.663691ms
Mar  4 09:38:40.769: INFO: Pod "pod-subpath-test-projected-s94g": Phase="Running", Reason="", readiness=true. Elapsed: 2.006144417s
Mar  4 09:38:42.773: INFO: Pod "pod-subpath-test-projected-s94g": Phase="Running", Reason="", readiness=true. Elapsed: 4.009357695s
Mar  4 09:38:44.779: INFO: Pod "pod-subpath-test-projected-s94g": Phase="Running", Reason="", readiness=true. Elapsed: 6.01553241s
Mar  4 09:38:46.781: INFO: Pod "pod-subpath-test-projected-s94g": Phase="Running", Reason="", readiness=true. Elapsed: 8.018325624s
Mar  4 09:38:48.785: INFO: Pod "pod-subpath-test-projected-s94g": Phase="Running", Reason="", readiness=true. Elapsed: 10.022160028s
Mar  4 09:38:50.789: INFO: Pod "pod-subpath-test-projected-s94g": Phase="Running", Reason="", readiness=true. Elapsed: 12.025521779s
Mar  4 09:38:52.793: INFO: Pod "pod-subpath-test-projected-s94g": Phase="Running", Reason="", readiness=true. Elapsed: 14.02982473s
Mar  4 09:38:54.797: INFO: Pod "pod-subpath-test-projected-s94g": Phase="Running", Reason="", readiness=true. Elapsed: 16.033998997s
Mar  4 09:38:56.804: INFO: Pod "pod-subpath-test-projected-s94g": Phase="Running", Reason="", readiness=true. Elapsed: 18.040866854s
Mar  4 09:38:58.807: INFO: Pod "pod-subpath-test-projected-s94g": Phase="Running", Reason="", readiness=true. Elapsed: 20.044038097s
Mar  4 09:39:00.810: INFO: Pod "pod-subpath-test-projected-s94g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.047150517s
STEP: Saw pod success
Mar  4 09:39:00.810: INFO: Pod "pod-subpath-test-projected-s94g" satisfied condition "success or failure"
Mar  4 09:39:00.814: INFO: Trying to get logs from node master3 pod pod-subpath-test-projected-s94g container test-container-subpath-projected-s94g: <nil>
STEP: delete the pod
Mar  4 09:39:00.874: INFO: Waiting for pod pod-subpath-test-projected-s94g to disappear
Mar  4 09:39:00.880: INFO: Pod pod-subpath-test-projected-s94g no longer exists
STEP: Deleting pod pod-subpath-test-projected-s94g
Mar  4 09:39:00.880: INFO: Deleting pod "pod-subpath-test-projected-s94g" in namespace "subpath-6976"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:39:00.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6976" for this suite.

• [SLOW TEST:22.186 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":278,"completed":150,"skipped":2293,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:39:00.898: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5292.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5292.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5292.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5292.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5292.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5292.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5292.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5292.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5292.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5292.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5292.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 198.250.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.250.198_udp@PTR;check="$$(dig +tcp +noall +answer +search 198.250.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.250.198_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5292.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5292.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5292.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5292.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5292.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5292.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5292.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5292.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5292.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5292.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5292.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 198.250.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.250.198_udp@PTR;check="$$(dig +tcp +noall +answer +search 198.250.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.250.198_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  4 09:39:03.000: INFO: Unable to read wheezy_udp@dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:03.006: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:03.009: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:03.012: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:03.035: INFO: Unable to read jessie_udp@dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:03.040: INFO: Unable to read jessie_tcp@dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:03.043: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:03.048: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:03.067: INFO: Lookups using dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52 failed for: [wheezy_udp@dns-test-service.dns-5292.svc.cluster.local wheezy_tcp@dns-test-service.dns-5292.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local jessie_udp@dns-test-service.dns-5292.svc.cluster.local jessie_tcp@dns-test-service.dns-5292.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local]

Mar  4 09:39:08.071: INFO: Unable to read wheezy_udp@dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:08.075: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:08.080: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:08.084: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:08.106: INFO: Unable to read jessie_udp@dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:08.111: INFO: Unable to read jessie_tcp@dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:08.115: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:08.118: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:08.136: INFO: Lookups using dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52 failed for: [wheezy_udp@dns-test-service.dns-5292.svc.cluster.local wheezy_tcp@dns-test-service.dns-5292.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local jessie_udp@dns-test-service.dns-5292.svc.cluster.local jessie_tcp@dns-test-service.dns-5292.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local]

Mar  4 09:39:13.071: INFO: Unable to read wheezy_udp@dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:13.078: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:13.082: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:13.085: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:13.112: INFO: Unable to read jessie_udp@dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:13.116: INFO: Unable to read jessie_tcp@dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:13.120: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:13.124: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:13.145: INFO: Lookups using dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52 failed for: [wheezy_udp@dns-test-service.dns-5292.svc.cluster.local wheezy_tcp@dns-test-service.dns-5292.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local jessie_udp@dns-test-service.dns-5292.svc.cluster.local jessie_tcp@dns-test-service.dns-5292.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local]

Mar  4 09:39:18.071: INFO: Unable to read wheezy_udp@dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:18.076: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:18.080: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:18.085: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:18.124: INFO: Unable to read jessie_udp@dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:18.129: INFO: Unable to read jessie_tcp@dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:18.133: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:18.137: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:18.156: INFO: Lookups using dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52 failed for: [wheezy_udp@dns-test-service.dns-5292.svc.cluster.local wheezy_tcp@dns-test-service.dns-5292.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local jessie_udp@dns-test-service.dns-5292.svc.cluster.local jessie_tcp@dns-test-service.dns-5292.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local]

Mar  4 09:39:23.071: INFO: Unable to read wheezy_udp@dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:23.075: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:23.078: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:23.082: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:23.114: INFO: Unable to read jessie_udp@dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:23.121: INFO: Unable to read jessie_tcp@dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:23.124: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:23.129: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:23.152: INFO: Lookups using dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52 failed for: [wheezy_udp@dns-test-service.dns-5292.svc.cluster.local wheezy_tcp@dns-test-service.dns-5292.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local jessie_udp@dns-test-service.dns-5292.svc.cluster.local jessie_tcp@dns-test-service.dns-5292.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local]

Mar  4 09:39:28.074: INFO: Unable to read wheezy_udp@dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:28.078: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:28.082: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:28.087: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:28.112: INFO: Unable to read jessie_udp@dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:28.117: INFO: Unable to read jessie_tcp@dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:28.121: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:28.126: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local from pod dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52: the server could not find the requested resource (get pods dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52)
Mar  4 09:39:28.157: INFO: Lookups using dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52 failed for: [wheezy_udp@dns-test-service.dns-5292.svc.cluster.local wheezy_tcp@dns-test-service.dns-5292.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local jessie_udp@dns-test-service.dns-5292.svc.cluster.local jessie_tcp@dns-test-service.dns-5292.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5292.svc.cluster.local]

Mar  4 09:39:33.167: INFO: DNS probes using dns-5292/dns-test-93ae97b3-2515-4bc4-a5e5-26cb9cb1ef52 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:39:33.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5292" for this suite.

• [SLOW TEST:32.456 seconds]
[sig-network] DNS
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":278,"completed":151,"skipped":2307,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:39:33.355: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:39:33.436: INFO: Waiting up to 5m0s for pod "busybox-user-65534-9e2f7803-0c3c-435a-b7f4-efe51a6cfdcb" in namespace "security-context-test-9805" to be "success or failure"
Mar  4 09:39:33.452: INFO: Pod "busybox-user-65534-9e2f7803-0c3c-435a-b7f4-efe51a6cfdcb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.828107ms
Mar  4 09:39:35.458: INFO: Pod "busybox-user-65534-9e2f7803-0c3c-435a-b7f4-efe51a6cfdcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022339402s
Mar  4 09:39:35.458: INFO: Pod "busybox-user-65534-9e2f7803-0c3c-435a-b7f4-efe51a6cfdcb" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:39:35.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9805" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":152,"skipped":2325,"failed":0}
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:39:35.479: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-8pvk
STEP: Creating a pod to test atomic-volume-subpath
Mar  4 09:39:35.594: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8pvk" in namespace "subpath-8822" to be "success or failure"
Mar  4 09:39:35.604: INFO: Pod "pod-subpath-test-configmap-8pvk": Phase="Pending", Reason="", readiness=false. Elapsed: 9.645188ms
Mar  4 09:39:37.607: INFO: Pod "pod-subpath-test-configmap-8pvk": Phase="Running", Reason="", readiness=true. Elapsed: 2.013305868s
Mar  4 09:39:39.611: INFO: Pod "pod-subpath-test-configmap-8pvk": Phase="Running", Reason="", readiness=true. Elapsed: 4.017051147s
Mar  4 09:39:41.615: INFO: Pod "pod-subpath-test-configmap-8pvk": Phase="Running", Reason="", readiness=true. Elapsed: 6.020914866s
Mar  4 09:39:43.619: INFO: Pod "pod-subpath-test-configmap-8pvk": Phase="Running", Reason="", readiness=true. Elapsed: 8.025019133s
Mar  4 09:39:45.623: INFO: Pod "pod-subpath-test-configmap-8pvk": Phase="Running", Reason="", readiness=true. Elapsed: 10.029258899s
Mar  4 09:39:47.627: INFO: Pod "pod-subpath-test-configmap-8pvk": Phase="Running", Reason="", readiness=true. Elapsed: 12.032691815s
Mar  4 09:39:49.630: INFO: Pod "pod-subpath-test-configmap-8pvk": Phase="Running", Reason="", readiness=true. Elapsed: 14.036135169s
Mar  4 09:39:51.634: INFO: Pod "pod-subpath-test-configmap-8pvk": Phase="Running", Reason="", readiness=true. Elapsed: 16.039444611s
Mar  4 09:39:53.637: INFO: Pod "pod-subpath-test-configmap-8pvk": Phase="Running", Reason="", readiness=true. Elapsed: 18.043040093s
Mar  4 09:39:55.641: INFO: Pod "pod-subpath-test-configmap-8pvk": Phase="Running", Reason="", readiness=true. Elapsed: 20.046492698s
Mar  4 09:39:57.645: INFO: Pod "pod-subpath-test-configmap-8pvk": Phase="Running", Reason="", readiness=true. Elapsed: 22.050395308s
Mar  4 09:39:59.648: INFO: Pod "pod-subpath-test-configmap-8pvk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.053945408s
STEP: Saw pod success
Mar  4 09:39:59.648: INFO: Pod "pod-subpath-test-configmap-8pvk" satisfied condition "success or failure"
Mar  4 09:39:59.651: INFO: Trying to get logs from node master3 pod pod-subpath-test-configmap-8pvk container test-container-subpath-configmap-8pvk: <nil>
STEP: delete the pod
Mar  4 09:39:59.674: INFO: Waiting for pod pod-subpath-test-configmap-8pvk to disappear
Mar  4 09:39:59.679: INFO: Pod pod-subpath-test-configmap-8pvk no longer exists
STEP: Deleting pod pod-subpath-test-configmap-8pvk
Mar  4 09:39:59.679: INFO: Deleting pod "pod-subpath-test-configmap-8pvk" in namespace "subpath-8822"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:39:59.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8822" for this suite.

• [SLOW TEST:24.212 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":278,"completed":153,"skipped":2327,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:39:59.692: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar  4 09:39:59.764: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f2c4bca6-5006-47f3-9047-7044f1b341da" in namespace "projected-2642" to be "success or failure"
Mar  4 09:39:59.769: INFO: Pod "downwardapi-volume-f2c4bca6-5006-47f3-9047-7044f1b341da": Phase="Pending", Reason="", readiness=false. Elapsed: 4.793116ms
Mar  4 09:40:01.784: INFO: Pod "downwardapi-volume-f2c4bca6-5006-47f3-9047-7044f1b341da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02035878s
Mar  4 09:40:03.788: INFO: Pod "downwardapi-volume-f2c4bca6-5006-47f3-9047-7044f1b341da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024116989s
STEP: Saw pod success
Mar  4 09:40:03.788: INFO: Pod "downwardapi-volume-f2c4bca6-5006-47f3-9047-7044f1b341da" satisfied condition "success or failure"
Mar  4 09:40:03.792: INFO: Trying to get logs from node master3 pod downwardapi-volume-f2c4bca6-5006-47f3-9047-7044f1b341da container client-container: <nil>
STEP: delete the pod
Mar  4 09:40:03.847: INFO: Waiting for pod downwardapi-volume-f2c4bca6-5006-47f3-9047-7044f1b341da to disappear
Mar  4 09:40:03.854: INFO: Pod downwardapi-volume-f2c4bca6-5006-47f3-9047-7044f1b341da no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:40:03.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2642" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":278,"completed":154,"skipped":2381,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:40:03.864: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:40:07.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4636" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":278,"completed":155,"skipped":2400,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:40:07.975: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  4 09:40:08.024: INFO: Waiting up to 5m0s for pod "pod-97600169-2c39-4bf9-9621-8c9063732bd9" in namespace "emptydir-1130" to be "success or failure"
Mar  4 09:40:08.043: INFO: Pod "pod-97600169-2c39-4bf9-9621-8c9063732bd9": Phase="Pending", Reason="", readiness=false. Elapsed: 18.739636ms
Mar  4 09:40:10.047: INFO: Pod "pod-97600169-2c39-4bf9-9621-8c9063732bd9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022502653s
Mar  4 09:40:12.050: INFO: Pod "pod-97600169-2c39-4bf9-9621-8c9063732bd9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02586593s
STEP: Saw pod success
Mar  4 09:40:12.050: INFO: Pod "pod-97600169-2c39-4bf9-9621-8c9063732bd9" satisfied condition "success or failure"
Mar  4 09:40:12.053: INFO: Trying to get logs from node master3 pod pod-97600169-2c39-4bf9-9621-8c9063732bd9 container test-container: <nil>
STEP: delete the pod
Mar  4 09:40:12.077: INFO: Waiting for pod pod-97600169-2c39-4bf9-9621-8c9063732bd9 to disappear
Mar  4 09:40:12.082: INFO: Pod pod-97600169-2c39-4bf9-9621-8c9063732bd9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:40:12.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1130" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":156,"skipped":2408,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:40:12.093: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Mar  4 09:40:12.146: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  4 09:40:12.159: INFO: Waiting for terminating namespaces to be deleted...
Mar  4 09:40:12.165: INFO: 
Logging pods the kubelet thinks is on node master2 before test
Mar  4 09:40:12.192: INFO: calico-kube-controllers-5b644bc49c-knmsz from kube-system started at 2020-03-04 06:30:14 +0000 UTC (1 container statuses recorded)
Mar  4 09:40:12.192: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  4 09:40:12.192: INFO: coredns-6955765f44-npmb6 from kube-system started at 2020-03-04 06:30:15 +0000 UTC (1 container statuses recorded)
Mar  4 09:40:12.192: INFO: 	Container coredns ready: true, restart count 0
Mar  4 09:40:12.192: INFO: calico-node-lzvqd from kube-system started at 2020-03-04 06:30:01 +0000 UTC (1 container statuses recorded)
Mar  4 09:40:12.192: INFO: 	Container calico-node ready: true, restart count 0
Mar  4 09:40:12.192: INFO: coredns-6955765f44-55pg4 from kube-system started at 2020-03-04 06:30:15 +0000 UTC (1 container statuses recorded)
Mar  4 09:40:12.192: INFO: 	Container coredns ready: true, restart count 0
Mar  4 09:40:12.192: INFO: kube-proxy-xq4nn from kube-system started at 2020-03-04 06:27:13 +0000 UTC (1 container statuses recorded)
Mar  4 09:40:12.192: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  4 09:40:12.192: INFO: sonobuoy-systemd-logs-daemon-set-002e23000b6e4f18-b5sfk from sonobuoy started at 2020-03-04 08:58:19 +0000 UTC (2 container statuses recorded)
Mar  4 09:40:12.192: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  4 09:40:12.192: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  4 09:40:12.192: INFO: 
Logging pods the kubelet thinks is on node master3 before test
Mar  4 09:40:12.200: INFO: sonobuoy-systemd-logs-daemon-set-002e23000b6e4f18-28pjm from sonobuoy started at 2020-03-04 08:58:19 +0000 UTC (2 container statuses recorded)
Mar  4 09:40:12.200: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  4 09:40:12.200: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  4 09:40:12.200: INFO: busybox-scheduling-8004618a-4459-4fcf-9161-149bbf6ab231 from kubelet-test-4636 started at 2020-03-04 09:40:03 +0000 UTC (1 container statuses recorded)
Mar  4 09:40:12.200: INFO: 	Container busybox-scheduling-8004618a-4459-4fcf-9161-149bbf6ab231 ready: true, restart count 0
Mar  4 09:40:12.200: INFO: calico-node-l9qb2 from kube-system started at 2020-03-04 06:30:01 +0000 UTC (1 container statuses recorded)
Mar  4 09:40:12.200: INFO: 	Container calico-node ready: true, restart count 0
Mar  4 09:40:12.200: INFO: sonobuoy-e2e-job-7260dc34846e4a1a from sonobuoy started at 2020-03-04 08:58:19 +0000 UTC (2 container statuses recorded)
Mar  4 09:40:12.200: INFO: 	Container e2e ready: true, restart count 0
Mar  4 09:40:12.200: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  4 09:40:12.200: INFO: kube-proxy-gkd5m from kube-system started at 2020-03-04 06:27:48 +0000 UTC (1 container statuses recorded)
Mar  4 09:40:12.200: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  4 09:40:12.200: INFO: sonobuoy from sonobuoy started at 2020-03-04 08:58:18 +0000 UTC (1 container statuses recorded)
Mar  4 09:40:12.200: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-14247019-af76-4f84-bda1-41e494b287c6 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-14247019-af76-4f84-bda1-41e494b287c6 off the node master2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-14247019-af76-4f84-bda1-41e494b287c6
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:40:26.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5626" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:14.417 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":278,"completed":157,"skipped":2428,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:40:26.510: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:40:42.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8926" for this suite.

• [SLOW TEST:16.215 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":278,"completed":158,"skipped":2466,"failed":0}
SS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:40:42.726: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:40:42.812: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: creating replication controller svc-latency-rc in namespace svc-latency-9927
I0304 09:40:42.830147      19 runners.go:189] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9927, replica count: 1
I0304 09:40:43.881263      19 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0304 09:40:44.881519      19 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  4 09:40:44.992: INFO: Created: latency-svc-ptdfv
Mar  4 09:40:45.003: INFO: Got endpoints: latency-svc-ptdfv [21.594186ms]
Mar  4 09:40:45.094: INFO: Created: latency-svc-fqqb7
Mar  4 09:40:45.106: INFO: Got endpoints: latency-svc-fqqb7 [103.044354ms]
Mar  4 09:40:45.122: INFO: Created: latency-svc-j5zrf
Mar  4 09:40:45.131: INFO: Got endpoints: latency-svc-j5zrf [127.914358ms]
Mar  4 09:40:45.180: INFO: Created: latency-svc-vth8m
Mar  4 09:40:45.180: INFO: Got endpoints: latency-svc-vth8m [175.707591ms]
Mar  4 09:40:45.194: INFO: Created: latency-svc-x7k65
Mar  4 09:40:45.213: INFO: Got endpoints: latency-svc-x7k65 [209.154215ms]
Mar  4 09:40:45.228: INFO: Created: latency-svc-sq46j
Mar  4 09:40:45.240: INFO: Got endpoints: latency-svc-sq46j [236.36914ms]
Mar  4 09:40:45.249: INFO: Created: latency-svc-dxkx4
Mar  4 09:40:45.271: INFO: Got endpoints: latency-svc-dxkx4 [266.675334ms]
Mar  4 09:40:45.291: INFO: Created: latency-svc-lvv4g
Mar  4 09:40:45.300: INFO: Got endpoints: latency-svc-lvv4g [295.899317ms]
Mar  4 09:40:45.312: INFO: Created: latency-svc-q65gb
Mar  4 09:40:45.349: INFO: Got endpoints: latency-svc-q65gb [344.880021ms]
Mar  4 09:40:45.379: INFO: Created: latency-svc-mkczl
Mar  4 09:40:45.389: INFO: Got endpoints: latency-svc-mkczl [384.711438ms]
Mar  4 09:40:45.406: INFO: Created: latency-svc-vs8x8
Mar  4 09:40:45.418: INFO: Got endpoints: latency-svc-vs8x8 [413.680862ms]
Mar  4 09:40:45.435: INFO: Created: latency-svc-cfdzr
Mar  4 09:40:45.471: INFO: Got endpoints: latency-svc-cfdzr [466.745391ms]
Mar  4 09:40:45.755: INFO: Created: latency-svc-qlc64
Mar  4 09:40:45.914: INFO: Got endpoints: latency-svc-qlc64 [908.955743ms]
Mar  4 09:40:46.035: INFO: Created: latency-svc-npqb9
Mar  4 09:40:46.043: INFO: Got endpoints: latency-svc-npqb9 [1.038647372s]
Mar  4 09:40:46.197: INFO: Created: latency-svc-tc9ts
Mar  4 09:40:46.215: INFO: Got endpoints: latency-svc-tc9ts [1.210737792s]
Mar  4 09:40:46.264: INFO: Created: latency-svc-z8pd4
Mar  4 09:40:46.280: INFO: Got endpoints: latency-svc-z8pd4 [1.27530507s]
Mar  4 09:40:46.334: INFO: Created: latency-svc-9mpfc
Mar  4 09:40:46.347: INFO: Got endpoints: latency-svc-9mpfc [1.240644531s]
Mar  4 09:40:46.394: INFO: Created: latency-svc-bzk48
Mar  4 09:40:46.410: INFO: Got endpoints: latency-svc-bzk48 [1.278023211s]
Mar  4 09:40:46.460: INFO: Created: latency-svc-z2bb6
Mar  4 09:40:46.502: INFO: Got endpoints: latency-svc-z2bb6 [1.322030512s]
Mar  4 09:40:46.520: INFO: Created: latency-svc-nxh9c
Mar  4 09:40:46.534: INFO: Got endpoints: latency-svc-nxh9c [1.321405714s]
Mar  4 09:40:46.575: INFO: Created: latency-svc-tjftk
Mar  4 09:40:46.590: INFO: Got endpoints: latency-svc-tjftk [1.35030024s]
Mar  4 09:40:46.664: INFO: Created: latency-svc-wxwdz
Mar  4 09:40:46.675: INFO: Got endpoints: latency-svc-wxwdz [1.404205677s]
Mar  4 09:40:46.700: INFO: Created: latency-svc-q8pql
Mar  4 09:40:46.718: INFO: Got endpoints: latency-svc-q8pql [1.418231581s]
Mar  4 09:40:46.828: INFO: Created: latency-svc-mphbq
Mar  4 09:40:46.850: INFO: Got endpoints: latency-svc-mphbq [1.501321901s]
Mar  4 09:40:46.916: INFO: Created: latency-svc-4w8qb
Mar  4 09:40:46.937: INFO: Got endpoints: latency-svc-4w8qb [1.5481439s]
Mar  4 09:40:46.972: INFO: Created: latency-svc-pqzwq
Mar  4 09:40:46.984: INFO: Got endpoints: latency-svc-pqzwq [1.566325916s]
Mar  4 09:40:47.034: INFO: Created: latency-svc-v8gv7
Mar  4 09:40:47.053: INFO: Got endpoints: latency-svc-v8gv7 [1.581242732s]
Mar  4 09:40:47.075: INFO: Created: latency-svc-ttvbs
Mar  4 09:40:47.080: INFO: Got endpoints: latency-svc-ttvbs [1.166128013s]
Mar  4 09:40:47.096: INFO: Created: latency-svc-zj7qq
Mar  4 09:40:47.119: INFO: Got endpoints: latency-svc-zj7qq [1.076106967s]
Mar  4 09:40:47.135: INFO: Created: latency-svc-t922f
Mar  4 09:40:47.158: INFO: Got endpoints: latency-svc-t922f [942.624273ms]
Mar  4 09:40:47.184: INFO: Created: latency-svc-dnt69
Mar  4 09:40:47.191: INFO: Got endpoints: latency-svc-dnt69 [910.859284ms]
Mar  4 09:40:47.273: INFO: Created: latency-svc-ddnjz
Mar  4 09:40:47.286: INFO: Got endpoints: latency-svc-ddnjz [939.22419ms]
Mar  4 09:40:47.346: INFO: Created: latency-svc-6mllm
Mar  4 09:40:47.382: INFO: Got endpoints: latency-svc-6mllm [972.720667ms]
Mar  4 09:40:47.424: INFO: Created: latency-svc-mhm8g
Mar  4 09:40:47.436: INFO: Got endpoints: latency-svc-mhm8g [933.251373ms]
Mar  4 09:40:47.542: INFO: Created: latency-svc-xkm6k
Mar  4 09:40:47.583: INFO: Got endpoints: latency-svc-xkm6k [1.048270958s]
Mar  4 09:40:47.621: INFO: Created: latency-svc-f66m6
Mar  4 09:40:47.626: INFO: Got endpoints: latency-svc-f66m6 [1.035740157s]
Mar  4 09:40:47.669: INFO: Created: latency-svc-dmflx
Mar  4 09:40:47.678: INFO: Got endpoints: latency-svc-dmflx [1.002738934s]
Mar  4 09:40:47.713: INFO: Created: latency-svc-wvs8r
Mar  4 09:40:47.725: INFO: Got endpoints: latency-svc-wvs8r [1.007002211s]
Mar  4 09:40:47.774: INFO: Created: latency-svc-t8kc7
Mar  4 09:40:47.792: INFO: Got endpoints: latency-svc-t8kc7 [941.596037ms]
Mar  4 09:40:47.848: INFO: Created: latency-svc-hj5bw
Mar  4 09:40:47.863: INFO: Got endpoints: latency-svc-hj5bw [926.004039ms]
Mar  4 09:40:47.882: INFO: Created: latency-svc-n44hl
Mar  4 09:40:47.927: INFO: Got endpoints: latency-svc-n44hl [942.285563ms]
Mar  4 09:40:47.945: INFO: Created: latency-svc-8xlkj
Mar  4 09:40:47.958: INFO: Got endpoints: latency-svc-8xlkj [905.883463ms]
Mar  4 09:40:47.998: INFO: Created: latency-svc-rg667
Mar  4 09:40:48.082: INFO: Got endpoints: latency-svc-rg667 [1.001794843s]
Mar  4 09:40:48.130: INFO: Created: latency-svc-cdt9h
Mar  4 09:40:48.148: INFO: Got endpoints: latency-svc-cdt9h [1.028681658s]
Mar  4 09:40:48.186: INFO: Created: latency-svc-vdc78
Mar  4 09:40:48.197: INFO: Got endpoints: latency-svc-vdc78 [1.039364405s]
Mar  4 09:40:48.317: INFO: Created: latency-svc-jsjfl
Mar  4 09:40:48.334: INFO: Got endpoints: latency-svc-jsjfl [1.142783458s]
Mar  4 09:40:48.464: INFO: Created: latency-svc-jbjfz
Mar  4 09:40:48.503: INFO: Created: latency-svc-tg5nb
Mar  4 09:40:48.510: INFO: Got endpoints: latency-svc-jbjfz [1.224014516s]
Mar  4 09:40:48.527: INFO: Got endpoints: latency-svc-tg5nb [1.144455207s]
Mar  4 09:40:48.574: INFO: Created: latency-svc-p7fmw
Mar  4 09:40:48.591: INFO: Got endpoints: latency-svc-p7fmw [1.155451287s]
Mar  4 09:40:48.632: INFO: Created: latency-svc-m2b5g
Mar  4 09:40:48.647: INFO: Got endpoints: latency-svc-m2b5g [1.064526067s]
Mar  4 09:40:48.683: INFO: Created: latency-svc-bg8ld
Mar  4 09:40:48.683: INFO: Got endpoints: latency-svc-bg8ld [1.056916249s]
Mar  4 09:40:48.704: INFO: Created: latency-svc-5zjtc
Mar  4 09:40:48.734: INFO: Created: latency-svc-h5p2f
Mar  4 09:40:48.735: INFO: Got endpoints: latency-svc-5zjtc [1.057010282s]
Mar  4 09:40:48.750: INFO: Got endpoints: latency-svc-h5p2f [1.024755912s]
Mar  4 09:40:48.847: INFO: Created: latency-svc-r8cgb
Mar  4 09:40:48.887: INFO: Got endpoints: latency-svc-r8cgb [1.094348396s]
Mar  4 09:40:48.920: INFO: Created: latency-svc-84n2f
Mar  4 09:40:48.927: INFO: Got endpoints: latency-svc-84n2f [1.063937858s]
Mar  4 09:40:48.977: INFO: Created: latency-svc-z2pls
Mar  4 09:40:48.996: INFO: Got endpoints: latency-svc-z2pls [1.068992309s]
Mar  4 09:40:49.008: INFO: Created: latency-svc-zbr75
Mar  4 09:40:49.017: INFO: Got endpoints: latency-svc-zbr75 [1.058691438s]
Mar  4 09:40:49.034: INFO: Created: latency-svc-bzwqd
Mar  4 09:40:49.045: INFO: Got endpoints: latency-svc-bzwqd [962.928976ms]
Mar  4 09:40:49.057: INFO: Created: latency-svc-g5ds9
Mar  4 09:40:49.065: INFO: Got endpoints: latency-svc-g5ds9 [917.163671ms]
Mar  4 09:40:49.104: INFO: Created: latency-svc-nlrcm
Mar  4 09:40:49.104: INFO: Got endpoints: latency-svc-nlrcm [906.877475ms]
Mar  4 09:40:49.146: INFO: Created: latency-svc-6j976
Mar  4 09:40:49.146: INFO: Got endpoints: latency-svc-6j976 [812.28843ms]
Mar  4 09:40:49.171: INFO: Created: latency-svc-ztrrt
Mar  4 09:40:49.187: INFO: Got endpoints: latency-svc-ztrrt [676.204108ms]
Mar  4 09:40:49.202: INFO: Created: latency-svc-x6pl7
Mar  4 09:40:49.231: INFO: Got endpoints: latency-svc-x6pl7 [704.381385ms]
Mar  4 09:40:49.250: INFO: Created: latency-svc-qxvcv
Mar  4 09:40:49.256: INFO: Got endpoints: latency-svc-qxvcv [665.275051ms]
Mar  4 09:40:49.298: INFO: Created: latency-svc-65gqs
Mar  4 09:40:49.306: INFO: Got endpoints: latency-svc-65gqs [659.186192ms]
Mar  4 09:40:49.350: INFO: Created: latency-svc-vpkvx
Mar  4 09:40:49.353: INFO: Got endpoints: latency-svc-vpkvx [669.795541ms]
Mar  4 09:40:49.371: INFO: Created: latency-svc-4bfg6
Mar  4 09:40:49.381: INFO: Got endpoints: latency-svc-4bfg6 [646.711316ms]
Mar  4 09:40:49.407: INFO: Created: latency-svc-4cn56
Mar  4 09:40:49.422: INFO: Got endpoints: latency-svc-4cn56 [671.904213ms]
Mar  4 09:40:49.482: INFO: Created: latency-svc-dk5h7
Mar  4 09:40:49.495: INFO: Got endpoints: latency-svc-dk5h7 [608.438346ms]
Mar  4 09:40:49.514: INFO: Created: latency-svc-p29zd
Mar  4 09:40:49.528: INFO: Got endpoints: latency-svc-p29zd [600.124826ms]
Mar  4 09:40:49.546: INFO: Created: latency-svc-rwtl2
Mar  4 09:40:49.558: INFO: Got endpoints: latency-svc-rwtl2 [562.327043ms]
Mar  4 09:40:49.586: INFO: Created: latency-svc-b9k8r
Mar  4 09:40:49.590: INFO: Got endpoints: latency-svc-b9k8r [573.143203ms]
Mar  4 09:40:49.618: INFO: Created: latency-svc-4c7bv
Mar  4 09:40:49.632: INFO: Got endpoints: latency-svc-4c7bv [587.357515ms]
Mar  4 09:40:49.650: INFO: Created: latency-svc-25txh
Mar  4 09:40:49.738: INFO: Created: latency-svc-jbwmn
Mar  4 09:40:49.763: INFO: Got endpoints: latency-svc-jbwmn [658.405657ms]
Mar  4 09:40:49.763: INFO: Got endpoints: latency-svc-25txh [697.802842ms]
Mar  4 09:40:49.768: INFO: Created: latency-svc-82ksz
Mar  4 09:40:49.778: INFO: Got endpoints: latency-svc-82ksz [632.119665ms]
Mar  4 09:40:49.801: INFO: Created: latency-svc-7snmj
Mar  4 09:40:49.839: INFO: Got endpoints: latency-svc-7snmj [652.666261ms]
Mar  4 09:40:49.855: INFO: Created: latency-svc-twh6m
Mar  4 09:40:49.861: INFO: Got endpoints: latency-svc-twh6m [629.939198ms]
Mar  4 09:40:49.883: INFO: Created: latency-svc-s5pw2
Mar  4 09:40:50.034: INFO: Got endpoints: latency-svc-s5pw2 [777.09497ms]
Mar  4 09:40:50.051: INFO: Created: latency-svc-5dw4j
Mar  4 09:40:50.071: INFO: Got endpoints: latency-svc-5dw4j [764.454733ms]
Mar  4 09:40:50.162: INFO: Created: latency-svc-vzp48
Mar  4 09:40:50.171: INFO: Got endpoints: latency-svc-vzp48 [818.202501ms]
Mar  4 09:40:50.193: INFO: Created: latency-svc-qzckh
Mar  4 09:40:50.203: INFO: Got endpoints: latency-svc-qzckh [821.854862ms]
Mar  4 09:40:50.282: INFO: Created: latency-svc-fzvwr
Mar  4 09:40:50.422: INFO: Got endpoints: latency-svc-fzvwr [1.000216152s]
Mar  4 09:40:50.440: INFO: Created: latency-svc-nrwfd
Mar  4 09:40:50.485: INFO: Got endpoints: latency-svc-nrwfd [989.58364ms]
Mar  4 09:40:50.526: INFO: Created: latency-svc-w9h6g
Mar  4 09:40:50.549: INFO: Got endpoints: latency-svc-w9h6g [1.021100504s]
Mar  4 09:40:50.585: INFO: Created: latency-svc-nxfnx
Mar  4 09:40:50.613: INFO: Got endpoints: latency-svc-nxfnx [1.054979435s]
Mar  4 09:40:50.635: INFO: Created: latency-svc-54vsg
Mar  4 09:40:50.650: INFO: Got endpoints: latency-svc-54vsg [1.059627769s]
Mar  4 09:40:50.769: INFO: Created: latency-svc-ht6kh
Mar  4 09:40:50.866: INFO: Got endpoints: latency-svc-ht6kh [1.234150583s]
Mar  4 09:40:50.920: INFO: Created: latency-svc-8d8c4
Mar  4 09:40:50.933: INFO: Got endpoints: latency-svc-8d8c4 [1.170198382s]
Mar  4 09:40:51.007: INFO: Created: latency-svc-c54qr
Mar  4 09:40:51.031: INFO: Got endpoints: latency-svc-c54qr [1.26784036s]
Mar  4 09:40:51.043: INFO: Created: latency-svc-6sl5f
Mar  4 09:40:51.069: INFO: Got endpoints: latency-svc-6sl5f [1.290417351s]
Mar  4 09:40:51.101: INFO: Created: latency-svc-n46rs
Mar  4 09:40:51.120: INFO: Got endpoints: latency-svc-n46rs [1.281020261s]
Mar  4 09:40:51.207: INFO: Created: latency-svc-cjwwg
Mar  4 09:40:51.207: INFO: Got endpoints: latency-svc-cjwwg [1.345783285s]
Mar  4 09:40:51.243: INFO: Created: latency-svc-qpwhs
Mar  4 09:40:51.257: INFO: Got endpoints: latency-svc-qpwhs [1.223284371s]
Mar  4 09:40:51.280: INFO: Created: latency-svc-86bzt
Mar  4 09:40:51.280: INFO: Got endpoints: latency-svc-86bzt [1.208581164s]
Mar  4 09:40:51.295: INFO: Created: latency-svc-t7v48
Mar  4 09:40:51.307: INFO: Got endpoints: latency-svc-t7v48 [1.135479677s]
Mar  4 09:40:51.318: INFO: Created: latency-svc-wjh4n
Mar  4 09:40:51.323: INFO: Got endpoints: latency-svc-wjh4n [1.120140105s]
Mar  4 09:40:51.352: INFO: Created: latency-svc-cxqgj
Mar  4 09:40:51.601: INFO: Got endpoints: latency-svc-cxqgj [1.179070805s]
Mar  4 09:40:51.691: INFO: Created: latency-svc-5r29b
Mar  4 09:40:51.709: INFO: Got endpoints: latency-svc-5r29b [1.224312287s]
Mar  4 09:40:51.753: INFO: Created: latency-svc-vjrz6
Mar  4 09:40:51.760: INFO: Got endpoints: latency-svc-vjrz6 [1.211265654s]
Mar  4 09:40:51.786: INFO: Created: latency-svc-bfm74
Mar  4 09:40:51.803: INFO: Got endpoints: latency-svc-bfm74 [1.19031548s]
Mar  4 09:40:51.812: INFO: Created: latency-svc-2kdsg
Mar  4 09:40:51.818: INFO: Got endpoints: latency-svc-2kdsg [1.167686342s]
Mar  4 09:40:51.848: INFO: Created: latency-svc-nqw4q
Mar  4 09:40:51.852: INFO: Got endpoints: latency-svc-nqw4q [986.029251ms]
Mar  4 09:40:51.884: INFO: Created: latency-svc-r5rbp
Mar  4 09:40:51.893: INFO: Got endpoints: latency-svc-r5rbp [960.204845ms]
Mar  4 09:40:51.915: INFO: Created: latency-svc-5rprb
Mar  4 09:40:51.932: INFO: Got endpoints: latency-svc-5rprb [901.118051ms]
Mar  4 09:40:51.967: INFO: Created: latency-svc-pldbk
Mar  4 09:40:51.982: INFO: Created: latency-svc-4jtcw
Mar  4 09:40:51.987: INFO: Got endpoints: latency-svc-pldbk [918.918182ms]
Mar  4 09:40:52.001: INFO: Got endpoints: latency-svc-4jtcw [880.964335ms]
Mar  4 09:40:52.027: INFO: Created: latency-svc-jh455
Mar  4 09:40:52.032: INFO: Got endpoints: latency-svc-jh455 [824.449595ms]
Mar  4 09:40:52.067: INFO: Created: latency-svc-pfg5z
Mar  4 09:40:52.084: INFO: Got endpoints: latency-svc-pfg5z [827.267936ms]
Mar  4 09:40:52.109: INFO: Created: latency-svc-58gvs
Mar  4 09:40:52.155: INFO: Got endpoints: latency-svc-58gvs [875.56079ms]
Mar  4 09:40:52.163: INFO: Created: latency-svc-5mkpd
Mar  4 09:40:52.175: INFO: Got endpoints: latency-svc-5mkpd [868.061595ms]
Mar  4 09:40:52.205: INFO: Created: latency-svc-6wbhd
Mar  4 09:40:52.210: INFO: Got endpoints: latency-svc-6wbhd [886.548309ms]
Mar  4 09:40:52.242: INFO: Created: latency-svc-b4mfr
Mar  4 09:40:52.263: INFO: Got endpoints: latency-svc-b4mfr [661.931577ms]
Mar  4 09:40:52.266: INFO: Created: latency-svc-qtvtr
Mar  4 09:40:52.276: INFO: Got endpoints: latency-svc-qtvtr [566.680103ms]
Mar  4 09:40:52.302: INFO: Created: latency-svc-tbwxm
Mar  4 09:40:52.305: INFO: Got endpoints: latency-svc-tbwxm [545.398869ms]
Mar  4 09:40:52.323: INFO: Created: latency-svc-mbmcq
Mar  4 09:40:52.409: INFO: Got endpoints: latency-svc-mbmcq [605.908046ms]
Mar  4 09:40:52.449: INFO: Created: latency-svc-n6wjc
Mar  4 09:40:52.462: INFO: Got endpoints: latency-svc-n6wjc [644.506793ms]
Mar  4 09:40:52.492: INFO: Created: latency-svc-28975
Mar  4 09:40:52.495: INFO: Got endpoints: latency-svc-28975 [642.980962ms]
Mar  4 09:40:52.519: INFO: Created: latency-svc-l59pn
Mar  4 09:40:52.534: INFO: Got endpoints: latency-svc-l59pn [641.154135ms]
Mar  4 09:40:52.553: INFO: Created: latency-svc-cl52b
Mar  4 09:40:52.564: INFO: Got endpoints: latency-svc-cl52b [631.717009ms]
Mar  4 09:40:52.647: INFO: Created: latency-svc-v7x86
Mar  4 09:40:52.661: INFO: Got endpoints: latency-svc-v7x86 [673.486187ms]
Mar  4 09:40:52.675: INFO: Created: latency-svc-ldlft
Mar  4 09:40:52.688: INFO: Got endpoints: latency-svc-ldlft [686.986926ms]
Mar  4 09:40:52.699: INFO: Created: latency-svc-pxlxv
Mar  4 09:40:52.737: INFO: Got endpoints: latency-svc-pxlxv [705.218537ms]
Mar  4 09:40:52.759: INFO: Created: latency-svc-gnkzv
Mar  4 09:40:52.768: INFO: Got endpoints: latency-svc-gnkzv [683.274071ms]
Mar  4 09:40:52.812: INFO: Created: latency-svc-bc8z5
Mar  4 09:40:52.853: INFO: Got endpoints: latency-svc-bc8z5 [698.159901ms]
Mar  4 09:40:52.903: INFO: Created: latency-svc-brkzx
Mar  4 09:40:52.931: INFO: Got endpoints: latency-svc-brkzx [756.380433ms]
Mar  4 09:40:52.959: INFO: Created: latency-svc-8kb8x
Mar  4 09:40:52.972: INFO: Got endpoints: latency-svc-8kb8x [119.075539ms]
Mar  4 09:40:52.996: INFO: Created: latency-svc-cdjlr
Mar  4 09:40:53.024: INFO: Got endpoints: latency-svc-cdjlr [813.907744ms]
Mar  4 09:40:53.034: INFO: Created: latency-svc-sx7mk
Mar  4 09:40:53.045: INFO: Got endpoints: latency-svc-sx7mk [781.358755ms]
Mar  4 09:40:53.070: INFO: Created: latency-svc-zc444
Mar  4 09:40:53.105: INFO: Got endpoints: latency-svc-zc444 [828.91251ms]
Mar  4 09:40:53.117: INFO: Created: latency-svc-grmhf
Mar  4 09:40:53.151: INFO: Got endpoints: latency-svc-grmhf [845.783916ms]
Mar  4 09:40:53.165: INFO: Created: latency-svc-8kbtf
Mar  4 09:40:53.174: INFO: Got endpoints: latency-svc-8kbtf [764.870982ms]
Mar  4 09:40:53.201: INFO: Created: latency-svc-l2q4n
Mar  4 09:40:53.284: INFO: Got endpoints: latency-svc-l2q4n [821.355297ms]
Mar  4 09:40:53.303: INFO: Created: latency-svc-g7kf4
Mar  4 09:40:53.312: INFO: Got endpoints: latency-svc-g7kf4 [816.212136ms]
Mar  4 09:40:53.325: INFO: Created: latency-svc-6skjs
Mar  4 09:40:53.331: INFO: Got endpoints: latency-svc-6skjs [796.474852ms]
Mar  4 09:40:53.342: INFO: Created: latency-svc-wmf8l
Mar  4 09:40:53.360: INFO: Got endpoints: latency-svc-wmf8l [795.875967ms]
Mar  4 09:40:53.385: INFO: Created: latency-svc-9xjnx
Mar  4 09:40:53.404: INFO: Got endpoints: latency-svc-9xjnx [743.140097ms]
Mar  4 09:40:53.433: INFO: Created: latency-svc-c24sn
Mar  4 09:40:53.443: INFO: Got endpoints: latency-svc-c24sn [754.623297ms]
Mar  4 09:40:53.458: INFO: Created: latency-svc-s6rcb
Mar  4 09:40:53.470: INFO: Got endpoints: latency-svc-s6rcb [733.416412ms]
Mar  4 09:40:53.488: INFO: Created: latency-svc-pqht5
Mar  4 09:40:53.501: INFO: Got endpoints: latency-svc-pqht5 [733.196978ms]
Mar  4 09:40:53.545: INFO: Created: latency-svc-47zj2
Mar  4 09:40:53.557: INFO: Got endpoints: latency-svc-47zj2 [625.779839ms]
Mar  4 09:40:53.592: INFO: Created: latency-svc-2mx7h
Mar  4 09:40:53.656: INFO: Got endpoints: latency-svc-2mx7h [683.484049ms]
Mar  4 09:40:53.706: INFO: Created: latency-svc-t9vgd
Mar  4 09:40:53.706: INFO: Got endpoints: latency-svc-t9vgd [682.331213ms]
Mar  4 09:40:53.729: INFO: Created: latency-svc-8kjvf
Mar  4 09:40:53.748: INFO: Got endpoints: latency-svc-8kjvf [703.254339ms]
Mar  4 09:40:53.767: INFO: Created: latency-svc-lmjb6
Mar  4 09:40:53.787: INFO: Got endpoints: latency-svc-lmjb6 [682.080885ms]
Mar  4 09:40:53.968: INFO: Created: latency-svc-8v7j2
Mar  4 09:40:54.055: INFO: Got endpoints: latency-svc-8v7j2 [903.685385ms]
Mar  4 09:40:54.072: INFO: Created: latency-svc-28xv6
Mar  4 09:40:54.090: INFO: Got endpoints: latency-svc-28xv6 [915.748476ms]
Mar  4 09:40:54.095: INFO: Created: latency-svc-hx4w6
Mar  4 09:40:54.099: INFO: Got endpoints: latency-svc-hx4w6 [814.735386ms]
Mar  4 09:40:54.128: INFO: Created: latency-svc-mdmhb
Mar  4 09:40:54.141: INFO: Got endpoints: latency-svc-mdmhb [829.809989ms]
Mar  4 09:40:54.185: INFO: Created: latency-svc-42v2z
Mar  4 09:40:54.238: INFO: Got endpoints: latency-svc-42v2z [907.294606ms]
Mar  4 09:40:54.251: INFO: Created: latency-svc-5qmmh
Mar  4 09:40:54.279: INFO: Got endpoints: latency-svc-5qmmh [918.713342ms]
Mar  4 09:40:54.289: INFO: Created: latency-svc-x5ldj
Mar  4 09:40:54.295: INFO: Got endpoints: latency-svc-x5ldj [890.807115ms]
Mar  4 09:40:54.316: INFO: Created: latency-svc-q468x
Mar  4 09:40:54.327: INFO: Got endpoints: latency-svc-q468x [884.096613ms]
Mar  4 09:40:54.342: INFO: Created: latency-svc-6vkdc
Mar  4 09:40:54.360: INFO: Got endpoints: latency-svc-6vkdc [889.398044ms]
Mar  4 09:40:54.391: INFO: Created: latency-svc-b9f2h
Mar  4 09:40:54.403: INFO: Got endpoints: latency-svc-b9f2h [902.356985ms]
Mar  4 09:40:54.432: INFO: Created: latency-svc-l594r
Mar  4 09:40:54.449: INFO: Got endpoints: latency-svc-l594r [892.002133ms]
Mar  4 09:40:54.473: INFO: Created: latency-svc-bczgt
Mar  4 09:40:54.544: INFO: Got endpoints: latency-svc-bczgt [888.072855ms]
Mar  4 09:40:54.579: INFO: Created: latency-svc-6q87m
Mar  4 09:40:54.579: INFO: Created: latency-svc-hj6qm
Mar  4 09:40:54.607: INFO: Got endpoints: latency-svc-6q87m [900.830187ms]
Mar  4 09:40:54.612: INFO: Got endpoints: latency-svc-hj6qm [864.248522ms]
Mar  4 09:40:54.623: INFO: Created: latency-svc-6ht24
Mar  4 09:40:54.631: INFO: Got endpoints: latency-svc-6ht24 [844.438543ms]
Mar  4 09:40:54.702: INFO: Created: latency-svc-ptn4s
Mar  4 09:40:54.739: INFO: Got endpoints: latency-svc-ptn4s [683.790164ms]
Mar  4 09:40:54.780: INFO: Created: latency-svc-8g8jt
Mar  4 09:40:54.849: INFO: Got endpoints: latency-svc-8g8jt [758.511515ms]
Mar  4 09:40:54.885: INFO: Created: latency-svc-g65h5
Mar  4 09:40:54.894: INFO: Got endpoints: latency-svc-g65h5 [795.225444ms]
Mar  4 09:40:54.915: INFO: Created: latency-svc-q5tcd
Mar  4 09:40:54.938: INFO: Got endpoints: latency-svc-q5tcd [796.5116ms]
Mar  4 09:40:55.034: INFO: Created: latency-svc-htsrv
Mar  4 09:40:55.043: INFO: Got endpoints: latency-svc-htsrv [804.763854ms]
Mar  4 09:40:55.082: INFO: Created: latency-svc-5n76w
Mar  4 09:40:55.119: INFO: Got endpoints: latency-svc-5n76w [840.721137ms]
Mar  4 09:40:55.136: INFO: Created: latency-svc-qgjxc
Mar  4 09:40:55.147: INFO: Got endpoints: latency-svc-qgjxc [852.365401ms]
Mar  4 09:40:55.175: INFO: Created: latency-svc-dgcpn
Mar  4 09:40:55.185: INFO: Got endpoints: latency-svc-dgcpn [858.178843ms]
Mar  4 09:40:55.192: INFO: Created: latency-svc-s8js8
Mar  4 09:40:55.203: INFO: Got endpoints: latency-svc-s8js8 [843.245634ms]
Mar  4 09:40:55.237: INFO: Created: latency-svc-689gg
Mar  4 09:40:55.247: INFO: Got endpoints: latency-svc-689gg [843.674977ms]
Mar  4 09:40:55.284: INFO: Created: latency-svc-bc4wz
Mar  4 09:40:55.312: INFO: Got endpoints: latency-svc-bc4wz [863.129637ms]
Mar  4 09:40:55.343: INFO: Created: latency-svc-vsqc6
Mar  4 09:40:55.356: INFO: Got endpoints: latency-svc-vsqc6 [812.064986ms]
Mar  4 09:40:55.377: INFO: Created: latency-svc-gh8ms
Mar  4 09:40:55.393: INFO: Got endpoints: latency-svc-gh8ms [785.309877ms]
Mar  4 09:40:55.406: INFO: Created: latency-svc-2lxxr
Mar  4 09:40:55.420: INFO: Got endpoints: latency-svc-2lxxr [807.146931ms]
Mar  4 09:40:55.432: INFO: Created: latency-svc-88sww
Mar  4 09:40:55.439: INFO: Got endpoints: latency-svc-88sww [807.859297ms]
Mar  4 09:40:55.477: INFO: Created: latency-svc-df4b4
Mar  4 09:40:55.477: INFO: Got endpoints: latency-svc-df4b4 [737.712782ms]
Mar  4 09:40:55.505: INFO: Created: latency-svc-mr7ct
Mar  4 09:40:55.523: INFO: Got endpoints: latency-svc-mr7ct [673.970798ms]
Mar  4 09:40:55.546: INFO: Created: latency-svc-qtw2r
Mar  4 09:40:55.605: INFO: Got endpoints: latency-svc-qtw2r [711.039276ms]
Mar  4 09:40:55.625: INFO: Created: latency-svc-4z8qj
Mar  4 09:40:55.640: INFO: Got endpoints: latency-svc-4z8qj [702.251206ms]
Mar  4 09:40:55.661: INFO: Created: latency-svc-mhr5z
Mar  4 09:40:55.684: INFO: Got endpoints: latency-svc-mhr5z [640.563117ms]
Mar  4 09:40:55.696: INFO: Created: latency-svc-6xldv
Mar  4 09:40:55.743: INFO: Got endpoints: latency-svc-6xldv [623.865121ms]
Mar  4 09:40:55.757: INFO: Created: latency-svc-cqvrp
Mar  4 09:40:55.785: INFO: Got endpoints: latency-svc-cqvrp [637.714394ms]
Mar  4 09:40:55.800: INFO: Created: latency-svc-6vr2h
Mar  4 09:40:55.820: INFO: Got endpoints: latency-svc-6vr2h [634.133077ms]
Mar  4 09:40:55.839: INFO: Created: latency-svc-hqzs8
Mar  4 09:40:55.899: INFO: Got endpoints: latency-svc-hqzs8 [695.992692ms]
Mar  4 09:40:55.932: INFO: Created: latency-svc-svtwk
Mar  4 09:40:55.971: INFO: Got endpoints: latency-svc-svtwk [724.197808ms]
Mar  4 09:40:55.999: INFO: Created: latency-svc-rl66x
Mar  4 09:40:56.016: INFO: Got endpoints: latency-svc-rl66x [703.741431ms]
Mar  4 09:40:56.032: INFO: Created: latency-svc-4lg72
Mar  4 09:40:56.048: INFO: Got endpoints: latency-svc-4lg72 [691.475148ms]
Mar  4 09:40:56.091: INFO: Created: latency-svc-bpctt
Mar  4 09:40:56.401: INFO: Got endpoints: latency-svc-bpctt [1.008814742s]
Mar  4 09:40:56.437: INFO: Created: latency-svc-2qkq5
Mar  4 09:40:56.443: INFO: Got endpoints: latency-svc-2qkq5 [1.023592713s]
Mar  4 09:40:56.475: INFO: Created: latency-svc-f44zt
Mar  4 09:40:56.563: INFO: Got endpoints: latency-svc-f44zt [1.123262165s]
Mar  4 09:40:56.581: INFO: Created: latency-svc-jr6c9
Mar  4 09:40:56.593: INFO: Got endpoints: latency-svc-jr6c9 [1.116530134s]
Mar  4 09:40:56.612: INFO: Created: latency-svc-r4qtc
Mar  4 09:40:56.632: INFO: Got endpoints: latency-svc-r4qtc [1.109736158s]
Mar  4 09:40:56.644: INFO: Created: latency-svc-c9x2w
Mar  4 09:40:56.660: INFO: Got endpoints: latency-svc-c9x2w [1.055280662s]
Mar  4 09:40:56.692: INFO: Created: latency-svc-xfwdx
Mar  4 09:40:56.714: INFO: Got endpoints: latency-svc-xfwdx [1.074021548s]
Mar  4 09:40:56.723: INFO: Created: latency-svc-k6sqk
Mar  4 09:40:56.738: INFO: Got endpoints: latency-svc-k6sqk [1.053774904s]
Mar  4 09:40:56.744: INFO: Created: latency-svc-2tf95
Mar  4 09:40:56.862: INFO: Got endpoints: latency-svc-2tf95 [1.118760944s]
Mar  4 09:40:56.885: INFO: Created: latency-svc-f66wq
Mar  4 09:40:56.904: INFO: Got endpoints: latency-svc-f66wq [1.118644554s]
Mar  4 09:40:56.949: INFO: Created: latency-svc-bvdgx
Mar  4 09:40:56.980: INFO: Got endpoints: latency-svc-bvdgx [1.160006665s]
Mar  4 09:40:56.988: INFO: Created: latency-svc-vxmlb
Mar  4 09:40:56.989: INFO: Got endpoints: latency-svc-vxmlb [1.089588241s]
Mar  4 09:40:57.045: INFO: Created: latency-svc-94mrq
Mar  4 09:40:57.059: INFO: Got endpoints: latency-svc-94mrq [1.087474793s]
Mar  4 09:40:57.070: INFO: Created: latency-svc-p5dvs
Mar  4 09:40:57.116: INFO: Got endpoints: latency-svc-p5dvs [1.099421029s]
Mar  4 09:40:57.116: INFO: Latencies: [103.044354ms 119.075539ms 127.914358ms 175.707591ms 209.154215ms 236.36914ms 266.675334ms 295.899317ms 344.880021ms 384.711438ms 413.680862ms 466.745391ms 545.398869ms 562.327043ms 566.680103ms 573.143203ms 587.357515ms 600.124826ms 605.908046ms 608.438346ms 623.865121ms 625.779839ms 629.939198ms 631.717009ms 632.119665ms 634.133077ms 637.714394ms 640.563117ms 641.154135ms 642.980962ms 644.506793ms 646.711316ms 652.666261ms 658.405657ms 659.186192ms 661.931577ms 665.275051ms 669.795541ms 671.904213ms 673.486187ms 673.970798ms 676.204108ms 682.080885ms 682.331213ms 683.274071ms 683.484049ms 683.790164ms 686.986926ms 691.475148ms 695.992692ms 697.802842ms 698.159901ms 702.251206ms 703.254339ms 703.741431ms 704.381385ms 705.218537ms 711.039276ms 724.197808ms 733.196978ms 733.416412ms 737.712782ms 743.140097ms 754.623297ms 756.380433ms 758.511515ms 764.454733ms 764.870982ms 777.09497ms 781.358755ms 785.309877ms 795.225444ms 795.875967ms 796.474852ms 796.5116ms 804.763854ms 807.146931ms 807.859297ms 812.064986ms 812.28843ms 813.907744ms 814.735386ms 816.212136ms 818.202501ms 821.355297ms 821.854862ms 824.449595ms 827.267936ms 828.91251ms 829.809989ms 840.721137ms 843.245634ms 843.674977ms 844.438543ms 845.783916ms 852.365401ms 858.178843ms 863.129637ms 864.248522ms 868.061595ms 875.56079ms 880.964335ms 884.096613ms 886.548309ms 888.072855ms 889.398044ms 890.807115ms 892.002133ms 900.830187ms 901.118051ms 902.356985ms 903.685385ms 905.883463ms 906.877475ms 907.294606ms 908.955743ms 910.859284ms 915.748476ms 917.163671ms 918.713342ms 918.918182ms 926.004039ms 933.251373ms 939.22419ms 941.596037ms 942.285563ms 942.624273ms 960.204845ms 962.928976ms 972.720667ms 986.029251ms 989.58364ms 1.000216152s 1.001794843s 1.002738934s 1.007002211s 1.008814742s 1.021100504s 1.023592713s 1.024755912s 1.028681658s 1.035740157s 1.038647372s 1.039364405s 1.048270958s 1.053774904s 1.054979435s 1.055280662s 1.056916249s 1.057010282s 1.058691438s 1.059627769s 1.063937858s 1.064526067s 1.068992309s 1.074021548s 1.076106967s 1.087474793s 1.089588241s 1.094348396s 1.099421029s 1.109736158s 1.116530134s 1.118644554s 1.118760944s 1.120140105s 1.123262165s 1.135479677s 1.142783458s 1.144455207s 1.155451287s 1.160006665s 1.166128013s 1.167686342s 1.170198382s 1.179070805s 1.19031548s 1.208581164s 1.210737792s 1.211265654s 1.223284371s 1.224014516s 1.224312287s 1.234150583s 1.240644531s 1.26784036s 1.27530507s 1.278023211s 1.281020261s 1.290417351s 1.321405714s 1.322030512s 1.345783285s 1.35030024s 1.404205677s 1.418231581s 1.501321901s 1.5481439s 1.566325916s 1.581242732s]
Mar  4 09:40:57.116: INFO: 50 %ile: 875.56079ms
Mar  4 09:40:57.116: INFO: 90 %ile: 1.223284371s
Mar  4 09:40:57.116: INFO: 99 %ile: 1.566325916s
Mar  4 09:40:57.116: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:40:57.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-9927" for this suite.

• [SLOW TEST:14.476 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":278,"completed":159,"skipped":2468,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:40:57.202: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:40:57.294: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:41:01.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9612" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":278,"completed":160,"skipped":2476,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:41:01.589: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  4 09:41:01.667: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:41:01.681: INFO: Number of nodes with available pods: 0
Mar  4 09:41:01.681: INFO: Node master2 is running more than one daemon pod
Mar  4 09:41:02.688: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:41:02.847: INFO: Number of nodes with available pods: 0
Mar  4 09:41:02.847: INFO: Node master2 is running more than one daemon pod
Mar  4 09:41:03.699: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:41:03.702: INFO: Number of nodes with available pods: 0
Mar  4 09:41:03.702: INFO: Node master2 is running more than one daemon pod
Mar  4 09:41:04.687: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:41:04.693: INFO: Number of nodes with available pods: 2
Mar  4 09:41:04.693: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar  4 09:41:04.740: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:41:04.774: INFO: Number of nodes with available pods: 1
Mar  4 09:41:04.774: INFO: Node master2 is running more than one daemon pod
Mar  4 09:41:05.790: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:41:05.796: INFO: Number of nodes with available pods: 1
Mar  4 09:41:05.796: INFO: Node master2 is running more than one daemon pod
Mar  4 09:41:06.782: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:41:06.786: INFO: Number of nodes with available pods: 1
Mar  4 09:41:06.786: INFO: Node master2 is running more than one daemon pod
Mar  4 09:41:07.781: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:41:07.788: INFO: Number of nodes with available pods: 1
Mar  4 09:41:07.788: INFO: Node master2 is running more than one daemon pod
Mar  4 09:41:08.807: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:41:08.816: INFO: Number of nodes with available pods: 1
Mar  4 09:41:08.816: INFO: Node master2 is running more than one daemon pod
Mar  4 09:41:09.783: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:41:09.787: INFO: Number of nodes with available pods: 1
Mar  4 09:41:09.787: INFO: Node master2 is running more than one daemon pod
Mar  4 09:41:10.806: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:41:10.811: INFO: Number of nodes with available pods: 1
Mar  4 09:41:10.811: INFO: Node master2 is running more than one daemon pod
Mar  4 09:41:11.779: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:41:11.786: INFO: Number of nodes with available pods: 1
Mar  4 09:41:11.786: INFO: Node master2 is running more than one daemon pod
Mar  4 09:41:12.811: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:41:12.859: INFO: Number of nodes with available pods: 1
Mar  4 09:41:12.859: INFO: Node master2 is running more than one daemon pod
Mar  4 09:41:13.863: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:41:14.034: INFO: Number of nodes with available pods: 1
Mar  4 09:41:14.034: INFO: Node master2 is running more than one daemon pod
Mar  4 09:41:14.783: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:41:14.792: INFO: Number of nodes with available pods: 1
Mar  4 09:41:14.792: INFO: Node master2 is running more than one daemon pod
Mar  4 09:41:15.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:41:15.825: INFO: Number of nodes with available pods: 1
Mar  4 09:41:15.825: INFO: Node master2 is running more than one daemon pod
Mar  4 09:41:18.024: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  4 09:41:18.072: INFO: Number of nodes with available pods: 2
Mar  4 09:41:18.072: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8008, will wait for the garbage collector to delete the pods
Mar  4 09:41:18.567: INFO: Deleting DaemonSet.extensions daemon-set took: 14.516147ms
Mar  4 09:41:18.867: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.478773ms
Mar  4 09:41:28.999: INFO: Number of nodes with available pods: 0
Mar  4 09:41:28.999: INFO: Number of running nodes: 0, number of available pods: 0
Mar  4 09:41:29.013: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8008/daemonsets","resourceVersion":"60488"},"items":null}

Mar  4 09:41:29.020: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8008/pods","resourceVersion":"60490"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:41:29.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8008" for this suite.

• [SLOW TEST:27.448 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":278,"completed":161,"skipped":2486,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:41:29.037: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  4 09:41:29.095: INFO: Waiting up to 5m0s for pod "pod-ebf955bd-4523-4ccb-8562-9e32f0c8288d" in namespace "emptydir-6376" to be "success or failure"
Mar  4 09:41:29.105: INFO: Pod "pod-ebf955bd-4523-4ccb-8562-9e32f0c8288d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.876554ms
Mar  4 09:41:31.263: INFO: Pod "pod-ebf955bd-4523-4ccb-8562-9e32f0c8288d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.167988353s
Mar  4 09:41:33.273: INFO: Pod "pod-ebf955bd-4523-4ccb-8562-9e32f0c8288d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.177756387s
STEP: Saw pod success
Mar  4 09:41:33.273: INFO: Pod "pod-ebf955bd-4523-4ccb-8562-9e32f0c8288d" satisfied condition "success or failure"
Mar  4 09:41:33.304: INFO: Trying to get logs from node master3 pod pod-ebf955bd-4523-4ccb-8562-9e32f0c8288d container test-container: <nil>
STEP: delete the pod
Mar  4 09:41:33.351: INFO: Waiting for pod pod-ebf955bd-4523-4ccb-8562-9e32f0c8288d to disappear
Mar  4 09:41:33.363: INFO: Pod pod-ebf955bd-4523-4ccb-8562-9e32f0c8288d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:41:33.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6376" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":162,"skipped":2486,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:41:33.504: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar  4 09:41:33.582: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6790 /api/v1/namespaces/watch-6790/configmaps/e2e-watch-test-configmap-a 5fe9e21a-ef99-4708-97fd-4d8518c7eb16 60530 0 2020-03-04 09:41:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  4 09:41:33.582: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6790 /api/v1/namespaces/watch-6790/configmaps/e2e-watch-test-configmap-a 5fe9e21a-ef99-4708-97fd-4d8518c7eb16 60530 0 2020-03-04 09:41:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar  4 09:41:43.589: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6790 /api/v1/namespaces/watch-6790/configmaps/e2e-watch-test-configmap-a 5fe9e21a-ef99-4708-97fd-4d8518c7eb16 60587 0 2020-03-04 09:41:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar  4 09:41:43.589: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6790 /api/v1/namespaces/watch-6790/configmaps/e2e-watch-test-configmap-a 5fe9e21a-ef99-4708-97fd-4d8518c7eb16 60587 0 2020-03-04 09:41:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar  4 09:41:53.596: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6790 /api/v1/namespaces/watch-6790/configmaps/e2e-watch-test-configmap-a 5fe9e21a-ef99-4708-97fd-4d8518c7eb16 60616 0 2020-03-04 09:41:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  4 09:41:53.596: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6790 /api/v1/namespaces/watch-6790/configmaps/e2e-watch-test-configmap-a 5fe9e21a-ef99-4708-97fd-4d8518c7eb16 60616 0 2020-03-04 09:41:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar  4 09:42:03.605: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6790 /api/v1/namespaces/watch-6790/configmaps/e2e-watch-test-configmap-a 5fe9e21a-ef99-4708-97fd-4d8518c7eb16 60641 0 2020-03-04 09:41:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  4 09:42:03.605: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6790 /api/v1/namespaces/watch-6790/configmaps/e2e-watch-test-configmap-a 5fe9e21a-ef99-4708-97fd-4d8518c7eb16 60641 0 2020-03-04 09:41:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar  4 09:42:13.613: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6790 /api/v1/namespaces/watch-6790/configmaps/e2e-watch-test-configmap-b 2606cff0-e590-4053-b3b5-cd574069497d 60662 0 2020-03-04 09:42:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  4 09:42:13.613: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6790 /api/v1/namespaces/watch-6790/configmaps/e2e-watch-test-configmap-b 2606cff0-e590-4053-b3b5-cd574069497d 60662 0 2020-03-04 09:42:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar  4 09:42:23.759: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6790 /api/v1/namespaces/watch-6790/configmaps/e2e-watch-test-configmap-b 2606cff0-e590-4053-b3b5-cd574069497d 60687 0 2020-03-04 09:42:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  4 09:42:23.759: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6790 /api/v1/namespaces/watch-6790/configmaps/e2e-watch-test-configmap-b 2606cff0-e590-4053-b3b5-cd574069497d 60687 0 2020-03-04 09:42:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:42:33.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6790" for this suite.

• [SLOW TEST:60.269 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":278,"completed":163,"skipped":2560,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:42:33.774: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:330
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Mar  4 09:42:33.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 create -f - --namespace=kubectl-4564'
Mar  4 09:42:34.263: INFO: stderr: ""
Mar  4 09:42:34.263: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  4 09:42:34.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4564'
Mar  4 09:42:34.547: INFO: stderr: ""
Mar  4 09:42:34.547: INFO: stdout: "update-demo-nautilus-58s9n update-demo-nautilus-m5zct "
Mar  4 09:42:34.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods update-demo-nautilus-58s9n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4564'
Mar  4 09:42:34.767: INFO: stderr: ""
Mar  4 09:42:34.767: INFO: stdout: ""
Mar  4 09:42:34.767: INFO: update-demo-nautilus-58s9n is created but not running
Mar  4 09:42:39.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4564'
Mar  4 09:42:39.870: INFO: stderr: ""
Mar  4 09:42:39.870: INFO: stdout: "update-demo-nautilus-58s9n update-demo-nautilus-m5zct "
Mar  4 09:42:39.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods update-demo-nautilus-58s9n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4564'
Mar  4 09:42:40.011: INFO: stderr: ""
Mar  4 09:42:40.011: INFO: stdout: "true"
Mar  4 09:42:40.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods update-demo-nautilus-58s9n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4564'
Mar  4 09:42:40.113: INFO: stderr: ""
Mar  4 09:42:40.113: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  4 09:42:40.113: INFO: validating pod update-demo-nautilus-58s9n
Mar  4 09:42:40.120: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  4 09:42:40.120: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  4 09:42:40.120: INFO: update-demo-nautilus-58s9n is verified up and running
Mar  4 09:42:40.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods update-demo-nautilus-m5zct -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4564'
Mar  4 09:42:40.278: INFO: stderr: ""
Mar  4 09:42:40.278: INFO: stdout: "true"
Mar  4 09:42:40.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods update-demo-nautilus-m5zct -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4564'
Mar  4 09:42:40.517: INFO: stderr: ""
Mar  4 09:42:40.517: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  4 09:42:40.517: INFO: validating pod update-demo-nautilus-m5zct
Mar  4 09:42:40.542: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  4 09:42:40.542: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  4 09:42:40.542: INFO: update-demo-nautilus-m5zct is verified up and running
STEP: using delete to clean up resources
Mar  4 09:42:40.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 delete --grace-period=0 --force -f - --namespace=kubectl-4564'
Mar  4 09:42:40.841: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  4 09:42:40.841: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  4 09:42:40.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4564'
Mar  4 09:42:41.021: INFO: stderr: "No resources found in kubectl-4564 namespace.\n"
Mar  4 09:42:41.021: INFO: stdout: ""
Mar  4 09:42:41.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods -l name=update-demo --namespace=kubectl-4564 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  4 09:42:41.261: INFO: stderr: ""
Mar  4 09:42:41.261: INFO: stdout: "update-demo-nautilus-58s9n\nupdate-demo-nautilus-m5zct\n"
Mar  4 09:42:41.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4564'
Mar  4 09:42:42.012: INFO: stderr: "No resources found in kubectl-4564 namespace.\n"
Mar  4 09:42:42.013: INFO: stdout: ""
Mar  4 09:42:42.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods -l name=update-demo --namespace=kubectl-4564 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  4 09:42:42.255: INFO: stderr: ""
Mar  4 09:42:42.255: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:42:42.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4564" for this suite.

• [SLOW TEST:8.491 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:328
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":278,"completed":164,"skipped":2586,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:42:42.265: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service nodeport-service with the type=NodePort in namespace services-4864
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4864
STEP: creating replication controller externalsvc in namespace services-4864
I0304 09:42:42.368046      19 runners.go:189] Created replication controller with name: externalsvc, namespace: services-4864, replica count: 2
I0304 09:42:45.418532      19 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Mar  4 09:42:45.520: INFO: Creating new exec pod
Mar  4 09:42:49.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=services-4864 execpodmsgl4 -- /bin/sh -x -c nslookup nodeport-service'
Mar  4 09:42:49.998: INFO: stderr: "+ nslookup nodeport-service\n"
Mar  4 09:42:49.998: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-4864.svc.cluster.local\tcanonical name = externalsvc.services-4864.svc.cluster.local.\nName:\texternalsvc.services-4864.svc.cluster.local\nAddress: 10.96.172.241\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4864, will wait for the garbage collector to delete the pods
Mar  4 09:42:50.065: INFO: Deleting ReplicationController externalsvc took: 13.29824ms
Mar  4 09:42:50.165: INFO: Terminating ReplicationController externalsvc pods took: 100.344833ms
Mar  4 09:43:03.795: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:43:04.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4864" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:21.785 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":278,"completed":165,"skipped":2587,"failed":0}
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:43:04.050: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-7393
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  4 09:43:04.269: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  4 09:43:24.569: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.180.37:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7393 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  4 09:43:24.569: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
Mar  4 09:43:24.746: INFO: Found all expected endpoints: [netserver-0]
Mar  4 09:43:24.749: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.136.5:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7393 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  4 09:43:24.749: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
Mar  4 09:43:24.996: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:43:24.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7393" for this suite.

• [SLOW TEST:20.957 seconds]
[sig-network] Networking
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":166,"skipped":2587,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:43:25.008: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  4 09:43:25.607: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  4 09:43:27.618: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718911805, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718911805, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718911805, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718911805, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  4 09:43:30.758: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:43:30.798: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9082-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:43:33.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3112" for this suite.
STEP: Destroying namespace "webhook-3112-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.585 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":278,"completed":167,"skipped":2605,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:43:33.593: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:43:50.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-13" for this suite.

• [SLOW TEST:16.494 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":278,"completed":168,"skipped":2611,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:43:50.088: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:178
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:43:50.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7038" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":278,"completed":169,"skipped":2642,"failed":0}
SSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:43:50.582: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  4 09:43:53.822: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:43:53.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4513" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":278,"completed":170,"skipped":2645,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:43:54.003: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar  4 09:43:54.072: INFO: Waiting up to 5m0s for pod "downwardapi-volume-467cc3f5-576d-46da-98c7-2f9dd6bf9705" in namespace "downward-api-8868" to be "success or failure"
Mar  4 09:43:54.088: INFO: Pod "downwardapi-volume-467cc3f5-576d-46da-98c7-2f9dd6bf9705": Phase="Pending", Reason="", readiness=false. Elapsed: 16.276643ms
Mar  4 09:43:56.254: INFO: Pod "downwardapi-volume-467cc3f5-576d-46da-98c7-2f9dd6bf9705": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.182121248s
STEP: Saw pod success
Mar  4 09:43:56.254: INFO: Pod "downwardapi-volume-467cc3f5-576d-46da-98c7-2f9dd6bf9705" satisfied condition "success or failure"
Mar  4 09:43:56.264: INFO: Trying to get logs from node master2 pod downwardapi-volume-467cc3f5-576d-46da-98c7-2f9dd6bf9705 container client-container: <nil>
STEP: delete the pod
Mar  4 09:43:56.349: INFO: Waiting for pod downwardapi-volume-467cc3f5-576d-46da-98c7-2f9dd6bf9705 to disappear
Mar  4 09:43:56.356: INFO: Pod downwardapi-volume-467cc3f5-576d-46da-98c7-2f9dd6bf9705 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:43:56.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8868" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":278,"completed":171,"skipped":2654,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:43:56.503: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  4 09:43:57.784: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  4 09:43:59.801: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718911837, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718911837, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718911837, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718911837, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  4 09:44:02.840: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:44:15.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9720" for this suite.
STEP: Destroying namespace "webhook-9720-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.846 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":278,"completed":172,"skipped":2659,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:44:15.350: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override arguments
Mar  4 09:44:15.561: INFO: Waiting up to 5m0s for pod "client-containers-22a4788a-78b0-4e31-8bbb-324d0e18778a" in namespace "containers-6419" to be "success or failure"
Mar  4 09:44:15.585: INFO: Pod "client-containers-22a4788a-78b0-4e31-8bbb-324d0e18778a": Phase="Pending", Reason="", readiness=false. Elapsed: 24.233215ms
Mar  4 09:44:17.595: INFO: Pod "client-containers-22a4788a-78b0-4e31-8bbb-324d0e18778a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034111799s
STEP: Saw pod success
Mar  4 09:44:17.595: INFO: Pod "client-containers-22a4788a-78b0-4e31-8bbb-324d0e18778a" satisfied condition "success or failure"
Mar  4 09:44:17.601: INFO: Trying to get logs from node master3 pod client-containers-22a4788a-78b0-4e31-8bbb-324d0e18778a container test-container: <nil>
STEP: delete the pod
Mar  4 09:44:17.787: INFO: Waiting for pod client-containers-22a4788a-78b0-4e31-8bbb-324d0e18778a to disappear
Mar  4 09:44:17.791: INFO: Pod client-containers-22a4788a-78b0-4e31-8bbb-324d0e18778a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:44:17.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6419" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":278,"completed":173,"skipped":2670,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:44:17.812: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  4 09:44:24.076: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  4 09:44:24.082: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  4 09:44:26.083: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  4 09:44:26.086: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  4 09:44:28.083: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  4 09:44:28.086: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  4 09:44:30.083: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  4 09:44:30.093: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  4 09:44:32.083: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  4 09:44:32.086: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  4 09:44:34.082: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  4 09:44:34.086: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  4 09:44:36.083: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  4 09:44:36.087: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  4 09:44:38.083: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  4 09:44:38.086: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  4 09:44:40.083: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  4 09:44:40.086: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:44:40.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2081" for this suite.

• [SLOW TEST:22.285 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":278,"completed":174,"skipped":2687,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:44:40.097: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:44:40.323: INFO: Creating ReplicaSet my-hostname-basic-fcfca8cb-da62-45b0-87de-cf98b4cbe27d
Mar  4 09:44:40.508: INFO: Pod name my-hostname-basic-fcfca8cb-da62-45b0-87de-cf98b4cbe27d: Found 0 pods out of 1
Mar  4 09:44:45.513: INFO: Pod name my-hostname-basic-fcfca8cb-da62-45b0-87de-cf98b4cbe27d: Found 1 pods out of 1
Mar  4 09:44:45.513: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-fcfca8cb-da62-45b0-87de-cf98b4cbe27d" is running
Mar  4 09:44:45.517: INFO: Pod "my-hostname-basic-fcfca8cb-da62-45b0-87de-cf98b4cbe27d-9pkll" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-04 09:44:40 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-04 09:44:41 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-04 09:44:41 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-04 09:44:40 +0000 UTC Reason: Message:}])
Mar  4 09:44:45.517: INFO: Trying to dial the pod
Mar  4 09:44:50.556: INFO: Controller my-hostname-basic-fcfca8cb-da62-45b0-87de-cf98b4cbe27d: Got expected result from replica 1 [my-hostname-basic-fcfca8cb-da62-45b0-87de-cf98b4cbe27d-9pkll]: "my-hostname-basic-fcfca8cb-da62-45b0-87de-cf98b4cbe27d-9pkll", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:44:50.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5004" for this suite.

• [SLOW TEST:10.468 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":278,"completed":175,"skipped":2694,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:44:50.566: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Mar  4 09:45:01.118: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W0304 09:45:01.118059      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  4 09:45:01.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-930" for this suite.

• [SLOW TEST:10.689 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":278,"completed":176,"skipped":2735,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:45:01.255: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:45:17.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8902" for this suite.

• [SLOW TEST:16.512 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":278,"completed":177,"skipped":2745,"failed":0}
SSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:45:17.767: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Mar  4 09:45:20.510: INFO: Successfully updated pod "adopt-release-j4flz"
STEP: Checking that the Job readopts the Pod
Mar  4 09:45:20.510: INFO: Waiting up to 15m0s for pod "adopt-release-j4flz" in namespace "job-9495" to be "adopted"
Mar  4 09:45:20.567: INFO: Pod "adopt-release-j4flz": Phase="Running", Reason="", readiness=true. Elapsed: 56.64462ms
Mar  4 09:45:22.570: INFO: Pod "adopt-release-j4flz": Phase="Running", Reason="", readiness=true. Elapsed: 2.059861495s
Mar  4 09:45:22.570: INFO: Pod "adopt-release-j4flz" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Mar  4 09:45:23.087: INFO: Successfully updated pod "adopt-release-j4flz"
STEP: Checking that the Job releases the Pod
Mar  4 09:45:23.087: INFO: Waiting up to 15m0s for pod "adopt-release-j4flz" in namespace "job-9495" to be "released"
Mar  4 09:45:23.112: INFO: Pod "adopt-release-j4flz": Phase="Running", Reason="", readiness=true. Elapsed: 25.520073ms
Mar  4 09:45:23.112: INFO: Pod "adopt-release-j4flz" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:45:23.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9495" for this suite.

• [SLOW TEST:5.525 seconds]
[sig-apps] Job
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":278,"completed":178,"skipped":2748,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:45:23.299: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  4 09:45:29.813: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  4 09:45:29.817: INFO: Pod pod-with-poststart-http-hook still exists
Mar  4 09:45:31.817: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  4 09:45:31.821: INFO: Pod pod-with-poststart-http-hook still exists
Mar  4 09:45:33.817: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  4 09:45:33.821: INFO: Pod pod-with-poststart-http-hook still exists
Mar  4 09:45:35.817: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  4 09:45:35.834: INFO: Pod pod-with-poststart-http-hook still exists
Mar  4 09:45:37.817: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  4 09:45:37.825: INFO: Pod pod-with-poststart-http-hook still exists
Mar  4 09:45:39.817: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  4 09:45:39.822: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:45:39.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5871" for this suite.

• [SLOW TEST:16.534 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":278,"completed":179,"skipped":2780,"failed":0}
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:45:39.833: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Mar  4 09:45:42.059: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-6012 PodName:pod-sharedvolume-6c20f876-3b89-4884-9b40-e251395e4e8e ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  4 09:45:42.059: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
Mar  4 09:45:42.252: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:45:42.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6012" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":278,"completed":180,"skipped":2780,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:45:42.262: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  4 09:45:44.341: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:45:44.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6788" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":278,"completed":181,"skipped":2821,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:45:44.533: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-downwardapi-fzx2
STEP: Creating a pod to test atomic-volume-subpath
Mar  4 09:45:44.591: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-fzx2" in namespace "subpath-121" to be "success or failure"
Mar  4 09:45:44.596: INFO: Pod "pod-subpath-test-downwardapi-fzx2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.935245ms
Mar  4 09:45:46.600: INFO: Pod "pod-subpath-test-downwardapi-fzx2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008628647s
Mar  4 09:45:48.603: INFO: Pod "pod-subpath-test-downwardapi-fzx2": Phase="Running", Reason="", readiness=true. Elapsed: 4.011984061s
Mar  4 09:45:50.749: INFO: Pod "pod-subpath-test-downwardapi-fzx2": Phase="Running", Reason="", readiness=true. Elapsed: 6.157593821s
Mar  4 09:45:52.752: INFO: Pod "pod-subpath-test-downwardapi-fzx2": Phase="Running", Reason="", readiness=true. Elapsed: 8.161098794s
Mar  4 09:45:54.756: INFO: Pod "pod-subpath-test-downwardapi-fzx2": Phase="Running", Reason="", readiness=true. Elapsed: 10.16488558s
Mar  4 09:45:56.760: INFO: Pod "pod-subpath-test-downwardapi-fzx2": Phase="Running", Reason="", readiness=true. Elapsed: 12.1684262s
Mar  4 09:45:58.765: INFO: Pod "pod-subpath-test-downwardapi-fzx2": Phase="Running", Reason="", readiness=true. Elapsed: 14.173528631s
Mar  4 09:46:00.768: INFO: Pod "pod-subpath-test-downwardapi-fzx2": Phase="Running", Reason="", readiness=true. Elapsed: 16.176898475s
Mar  4 09:46:02.776: INFO: Pod "pod-subpath-test-downwardapi-fzx2": Phase="Running", Reason="", readiness=true. Elapsed: 18.184609436s
Mar  4 09:46:04.780: INFO: Pod "pod-subpath-test-downwardapi-fzx2": Phase="Running", Reason="", readiness=true. Elapsed: 20.188366399s
Mar  4 09:46:06.784: INFO: Pod "pod-subpath-test-downwardapi-fzx2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.192436069s
STEP: Saw pod success
Mar  4 09:46:06.784: INFO: Pod "pod-subpath-test-downwardapi-fzx2" satisfied condition "success or failure"
Mar  4 09:46:06.791: INFO: Trying to get logs from node master2 pod pod-subpath-test-downwardapi-fzx2 container test-container-subpath-downwardapi-fzx2: <nil>
STEP: delete the pod
Mar  4 09:46:06.832: INFO: Waiting for pod pod-subpath-test-downwardapi-fzx2 to disappear
Mar  4 09:46:06.835: INFO: Pod pod-subpath-test-downwardapi-fzx2 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-fzx2
Mar  4 09:46:06.835: INFO: Deleting pod "pod-subpath-test-downwardapi-fzx2" in namespace "subpath-121"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:46:06.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-121" for this suite.

• [SLOW TEST:22.330 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":278,"completed":182,"skipped":2896,"failed":0}
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:46:06.863: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-be23fb0e-7969-42a7-b614-2fd203f5ec75 in namespace container-probe-4039
Mar  4 09:46:09.113: INFO: Started pod liveness-be23fb0e-7969-42a7-b614-2fd203f5ec75 in namespace container-probe-4039
STEP: checking the pod's current state and verifying that restartCount is present
Mar  4 09:46:09.120: INFO: Initial restart count of pod liveness-be23fb0e-7969-42a7-b614-2fd203f5ec75 is 0
Mar  4 09:46:29.309: INFO: Restart count of pod container-probe-4039/liveness-be23fb0e-7969-42a7-b614-2fd203f5ec75 is now 1 (20.188708902s elapsed)
Mar  4 09:46:49.516: INFO: Restart count of pod container-probe-4039/liveness-be23fb0e-7969-42a7-b614-2fd203f5ec75 is now 2 (40.39586759s elapsed)
Mar  4 09:47:09.575: INFO: Restart count of pod container-probe-4039/liveness-be23fb0e-7969-42a7-b614-2fd203f5ec75 is now 3 (1m0.454973448s elapsed)
Mar  4 09:47:29.609: INFO: Restart count of pod container-probe-4039/liveness-be23fb0e-7969-42a7-b614-2fd203f5ec75 is now 4 (1m20.488571674s elapsed)
Mar  4 09:48:40.007: INFO: Restart count of pod container-probe-4039/liveness-be23fb0e-7969-42a7-b614-2fd203f5ec75 is now 5 (2m30.886809183s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:48:40.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4039" for this suite.

• [SLOW TEST:153.185 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":278,"completed":183,"skipped":2896,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:48:40.048: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1861
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar  4 09:48:40.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-8969'
Mar  4 09:48:40.543: INFO: stderr: ""
Mar  4 09:48:40.543: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1866
Mar  4 09:48:40.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 delete pods e2e-test-httpd-pod --namespace=kubectl-8969'
Mar  4 09:48:48.773: INFO: stderr: ""
Mar  4 09:48:48.773: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:48:48.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8969" for this suite.

• [SLOW TEST:8.738 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1857
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":278,"completed":184,"skipped":2918,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:48:48.787: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  4 09:48:49.866: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  4 09:48:52.002: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912130, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912130, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912130, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912129, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  4 09:48:55.023: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:48:55.027: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3480-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:49:00.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-933" for this suite.
STEP: Destroying namespace "webhook-933-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.802 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":278,"completed":185,"skipped":2921,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:49:04.588: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl logs
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1464
STEP: creating an pod
Mar  4 09:49:04.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.8 --namespace=kubectl-5286 -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar  4 09:49:05.053: INFO: stderr: ""
Mar  4 09:49:05.053: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Waiting for log generator to start.
Mar  4 09:49:05.053: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar  4 09:49:05.053: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5286" to be "running and ready, or succeeded"
Mar  4 09:49:05.070: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 16.8609ms
Mar  4 09:49:07.084: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031025389s
Mar  4 09:49:09.088: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.034557756s
Mar  4 09:49:09.088: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar  4 09:49:09.088: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Mar  4 09:49:09.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 logs logs-generator logs-generator --namespace=kubectl-5286'
Mar  4 09:49:09.291: INFO: stderr: ""
Mar  4 09:49:09.291: INFO: stdout: "I0304 09:49:06.771219       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/q87b 297\nI0304 09:49:06.971442       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/22nj 389\nI0304 09:49:07.171516       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/56d 226\nI0304 09:49:07.371561       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/4l5r 225\nI0304 09:49:07.571396       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/flk 304\nI0304 09:49:07.771481       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/dvxx 288\nI0304 09:49:07.971610       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/lqs 341\nI0304 09:49:08.171466       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/pjs 288\nI0304 09:49:08.371495       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/5tjf 292\nI0304 09:49:08.571450       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/tr8 390\nI0304 09:49:08.771505       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/vl6 281\nI0304 09:49:08.971462       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/75nm 553\nI0304 09:49:09.171450       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/6wj 357\n"
STEP: limiting log lines
Mar  4 09:49:09.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 logs logs-generator logs-generator --namespace=kubectl-5286 --tail=1'
Mar  4 09:49:09.527: INFO: stderr: ""
Mar  4 09:49:09.527: INFO: stdout: "I0304 09:49:09.372507       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/lrcl 545\n"
Mar  4 09:49:09.527: INFO: got output "I0304 09:49:09.372507       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/lrcl 545\n"
STEP: limiting log bytes
Mar  4 09:49:09.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 logs logs-generator logs-generator --namespace=kubectl-5286 --limit-bytes=1'
Mar  4 09:49:09.783: INFO: stderr: ""
Mar  4 09:49:09.783: INFO: stdout: "I"
Mar  4 09:49:09.783: INFO: got output "I"
STEP: exposing timestamps
Mar  4 09:49:09.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 logs logs-generator logs-generator --namespace=kubectl-5286 --tail=1 --timestamps'
Mar  4 09:49:10.026: INFO: stderr: ""
Mar  4 09:49:10.026: INFO: stdout: "2020-03-04T09:49:09.971711745Z I0304 09:49:09.971499       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/xv8r 574\n"
Mar  4 09:49:10.026: INFO: got output "2020-03-04T09:49:09.971711745Z I0304 09:49:09.971499       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/xv8r 574\n"
STEP: restricting to a time range
Mar  4 09:49:12.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 logs logs-generator logs-generator --namespace=kubectl-5286 --since=1s'
Mar  4 09:49:12.755: INFO: stderr: ""
Mar  4 09:49:12.755: INFO: stdout: "I0304 09:49:11.771426       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/kube-system/pods/h7h 458\nI0304 09:49:11.971389       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/gznn 237\nI0304 09:49:12.171474       1 logs_generator.go:76] 27 POST /api/v1/namespaces/default/pods/ckt 203\nI0304 09:49:12.371490       1 logs_generator.go:76] 28 GET /api/v1/namespaces/kube-system/pods/dm2s 596\nI0304 09:49:12.571495       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/ns/pods/xxh9 498\n"
Mar  4 09:49:12.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 logs logs-generator logs-generator --namespace=kubectl-5286 --since=24h'
Mar  4 09:49:13.022: INFO: stderr: ""
Mar  4 09:49:13.022: INFO: stdout: "I0304 09:49:06.771219       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/q87b 297\nI0304 09:49:06.971442       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/22nj 389\nI0304 09:49:07.171516       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/56d 226\nI0304 09:49:07.371561       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/4l5r 225\nI0304 09:49:07.571396       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/flk 304\nI0304 09:49:07.771481       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/dvxx 288\nI0304 09:49:07.971610       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/lqs 341\nI0304 09:49:08.171466       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/pjs 288\nI0304 09:49:08.371495       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/5tjf 292\nI0304 09:49:08.571450       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/tr8 390\nI0304 09:49:08.771505       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/vl6 281\nI0304 09:49:08.971462       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/75nm 553\nI0304 09:49:09.171450       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/6wj 357\nI0304 09:49:09.372507       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/lrcl 545\nI0304 09:49:09.571436       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/hvn 589\nI0304 09:49:09.771427       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/hmw 408\nI0304 09:49:09.971499       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/xv8r 574\nI0304 09:49:10.171451       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/md2n 355\nI0304 09:49:10.371478       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/hxlv 422\nI0304 09:49:10.571475       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/x59g 388\nI0304 09:49:10.771465       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/zg9 249\nI0304 09:49:10.971486       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/6s6 224\nI0304 09:49:11.171470       1 logs_generator.go:76] 22 GET /api/v1/namespaces/ns/pods/zb64 407\nI0304 09:49:11.371503       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/mxl 359\nI0304 09:49:11.571520       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/default/pods/hbkm 552\nI0304 09:49:11.771426       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/kube-system/pods/h7h 458\nI0304 09:49:11.971389       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/gznn 237\nI0304 09:49:12.171474       1 logs_generator.go:76] 27 POST /api/v1/namespaces/default/pods/ckt 203\nI0304 09:49:12.371490       1 logs_generator.go:76] 28 GET /api/v1/namespaces/kube-system/pods/dm2s 596\nI0304 09:49:12.571495       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/ns/pods/xxh9 498\nI0304 09:49:12.771362       1 logs_generator.go:76] 30 POST /api/v1/namespaces/kube-system/pods/wxj6 461\nI0304 09:49:12.971466       1 logs_generator.go:76] 31 PUT /api/v1/namespaces/default/pods/wgq 356\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1470
Mar  4 09:49:13.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 delete pod logs-generator --namespace=kubectl-5286'
Mar  4 09:49:18.790: INFO: stderr: ""
Mar  4 09:49:18.790: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:49:18.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5286" for this suite.

• [SLOW TEST:14.235 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":278,"completed":186,"skipped":2925,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:49:18.824: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar  4 09:49:19.047: INFO: Waiting up to 5m0s for pod "downwardapi-volume-55448890-ed6b-4c0f-85b0-787e8d8325bb" in namespace "projected-7904" to be "success or failure"
Mar  4 09:49:19.055: INFO: Pod "downwardapi-volume-55448890-ed6b-4c0f-85b0-787e8d8325bb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.567948ms
Mar  4 09:49:21.060: INFO: Pod "downwardapi-volume-55448890-ed6b-4c0f-85b0-787e8d8325bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013297086s
STEP: Saw pod success
Mar  4 09:49:21.060: INFO: Pod "downwardapi-volume-55448890-ed6b-4c0f-85b0-787e8d8325bb" satisfied condition "success or failure"
Mar  4 09:49:21.063: INFO: Trying to get logs from node master3 pod downwardapi-volume-55448890-ed6b-4c0f-85b0-787e8d8325bb container client-container: <nil>
STEP: delete the pod
Mar  4 09:49:21.264: INFO: Waiting for pod downwardapi-volume-55448890-ed6b-4c0f-85b0-787e8d8325bb to disappear
Mar  4 09:49:21.270: INFO: Pod downwardapi-volume-55448890-ed6b-4c0f-85b0-787e8d8325bb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:49:21.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7904" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":278,"completed":187,"skipped":2967,"failed":0}
SSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:49:21.288: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:49:21.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-1499" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":278,"completed":188,"skipped":2973,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:49:21.548: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-4302
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating statefulset ss in namespace statefulset-4302
Mar  4 09:49:21.816: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Mar  4 09:49:31.820: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Mar  4 09:49:31.852: INFO: Deleting all statefulset in ns statefulset-4302
Mar  4 09:49:31.868: INFO: Scaling statefulset ss to 0
Mar  4 09:49:52.062: INFO: Waiting for statefulset status.replicas updated to 0
Mar  4 09:49:52.066: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:49:52.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4302" for this suite.

• [SLOW TEST:30.572 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":278,"completed":189,"skipped":2975,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:49:52.121: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-a705984b-0c36-41e7-bfb2-290530cb1468
STEP: Creating configMap with name cm-test-opt-upd-65e2a547-f8b7-4482-a21e-254589b87117
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-a705984b-0c36-41e7-bfb2-290530cb1468
STEP: Updating configmap cm-test-opt-upd-65e2a547-f8b7-4482-a21e-254589b87117
STEP: Creating configMap with name cm-test-opt-create-dc45febe-31ce-4659-8138-4d90564f5a44
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:50:59.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3991" for this suite.

• [SLOW TEST:67.135 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":190,"skipped":3045,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:50:59.256: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-90934711-a597-4089-b3ef-c26c6373373b
STEP: Creating a pod to test consume configMaps
Mar  4 09:50:59.317: INFO: Waiting up to 5m0s for pod "pod-configmaps-394aef75-bf89-4ff0-9b5c-ae63d23d66cd" in namespace "configmap-1499" to be "success or failure"
Mar  4 09:50:59.331: INFO: Pod "pod-configmaps-394aef75-bf89-4ff0-9b5c-ae63d23d66cd": Phase="Pending", Reason="", readiness=false. Elapsed: 13.927771ms
Mar  4 09:51:01.334: INFO: Pod "pod-configmaps-394aef75-bf89-4ff0-9b5c-ae63d23d66cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017151789s
Mar  4 09:51:03.338: INFO: Pod "pod-configmaps-394aef75-bf89-4ff0-9b5c-ae63d23d66cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020795886s
STEP: Saw pod success
Mar  4 09:51:03.338: INFO: Pod "pod-configmaps-394aef75-bf89-4ff0-9b5c-ae63d23d66cd" satisfied condition "success or failure"
Mar  4 09:51:03.341: INFO: Trying to get logs from node master2 pod pod-configmaps-394aef75-bf89-4ff0-9b5c-ae63d23d66cd container configmap-volume-test: <nil>
STEP: delete the pod
Mar  4 09:51:03.504: INFO: Waiting for pod pod-configmaps-394aef75-bf89-4ff0-9b5c-ae63d23d66cd to disappear
Mar  4 09:51:03.509: INFO: Pod pod-configmaps-394aef75-bf89-4ff0-9b5c-ae63d23d66cd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:51:03.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1499" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":278,"completed":191,"skipped":3055,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:51:03.519: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Mar  4 09:51:08.104: INFO: Successfully updated pod "annotationupdate081d0a84-6d30-4cb1-8c48-89c508c070a8"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:51:10.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5713" for this suite.

• [SLOW TEST:6.801 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":278,"completed":192,"skipped":3057,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:51:10.320: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-secret-r8k2
STEP: Creating a pod to test atomic-volume-subpath
Mar  4 09:51:10.574: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-r8k2" in namespace "subpath-6554" to be "success or failure"
Mar  4 09:51:10.604: INFO: Pod "pod-subpath-test-secret-r8k2": Phase="Pending", Reason="", readiness=false. Elapsed: 30.004433ms
Mar  4 09:51:12.607: INFO: Pod "pod-subpath-test-secret-r8k2": Phase="Running", Reason="", readiness=true. Elapsed: 2.033045017s
Mar  4 09:51:14.610: INFO: Pod "pod-subpath-test-secret-r8k2": Phase="Running", Reason="", readiness=true. Elapsed: 4.03596199s
Mar  4 09:51:16.618: INFO: Pod "pod-subpath-test-secret-r8k2": Phase="Running", Reason="", readiness=true. Elapsed: 6.044331385s
Mar  4 09:51:18.751: INFO: Pod "pod-subpath-test-secret-r8k2": Phase="Running", Reason="", readiness=true. Elapsed: 8.177144122s
Mar  4 09:51:20.754: INFO: Pod "pod-subpath-test-secret-r8k2": Phase="Running", Reason="", readiness=true. Elapsed: 10.180358479s
Mar  4 09:51:22.757: INFO: Pod "pod-subpath-test-secret-r8k2": Phase="Running", Reason="", readiness=true. Elapsed: 12.183549081s
Mar  4 09:51:24.760: INFO: Pod "pod-subpath-test-secret-r8k2": Phase="Running", Reason="", readiness=true. Elapsed: 14.186714468s
Mar  4 09:51:26.763: INFO: Pod "pod-subpath-test-secret-r8k2": Phase="Running", Reason="", readiness=true. Elapsed: 16.189752705s
Mar  4 09:51:28.767: INFO: Pod "pod-subpath-test-secret-r8k2": Phase="Running", Reason="", readiness=true. Elapsed: 18.193524767s
Mar  4 09:51:30.770: INFO: Pod "pod-subpath-test-secret-r8k2": Phase="Running", Reason="", readiness=true. Elapsed: 20.196722869s
Mar  4 09:51:32.774: INFO: Pod "pod-subpath-test-secret-r8k2": Phase="Running", Reason="", readiness=true. Elapsed: 22.199917767s
Mar  4 09:51:34.777: INFO: Pod "pod-subpath-test-secret-r8k2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.202992658s
STEP: Saw pod success
Mar  4 09:51:34.777: INFO: Pod "pod-subpath-test-secret-r8k2" satisfied condition "success or failure"
Mar  4 09:51:34.780: INFO: Trying to get logs from node master2 pod pod-subpath-test-secret-r8k2 container test-container-subpath-secret-r8k2: <nil>
STEP: delete the pod
Mar  4 09:51:34.838: INFO: Waiting for pod pod-subpath-test-secret-r8k2 to disappear
Mar  4 09:51:34.841: INFO: Pod pod-subpath-test-secret-r8k2 no longer exists
STEP: Deleting pod pod-subpath-test-secret-r8k2
Mar  4 09:51:34.841: INFO: Deleting pod "pod-subpath-test-secret-r8k2" in namespace "subpath-6554"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:51:34.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6554" for this suite.

• [SLOW TEST:24.537 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":278,"completed":193,"skipped":3061,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:51:34.856: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar  4 09:51:35.061: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c1caf382-41b0-4267-8f17-08508d4cbd39" in namespace "projected-3127" to be "success or failure"
Mar  4 09:51:35.074: INFO: Pod "downwardapi-volume-c1caf382-41b0-4267-8f17-08508d4cbd39": Phase="Pending", Reason="", readiness=false. Elapsed: 13.577226ms
Mar  4 09:51:37.081: INFO: Pod "downwardapi-volume-c1caf382-41b0-4267-8f17-08508d4cbd39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020036124s
STEP: Saw pod success
Mar  4 09:51:37.081: INFO: Pod "downwardapi-volume-c1caf382-41b0-4267-8f17-08508d4cbd39" satisfied condition "success or failure"
Mar  4 09:51:37.084: INFO: Trying to get logs from node master3 pod downwardapi-volume-c1caf382-41b0-4267-8f17-08508d4cbd39 container client-container: <nil>
STEP: delete the pod
Mar  4 09:51:37.115: INFO: Waiting for pod downwardapi-volume-c1caf382-41b0-4267-8f17-08508d4cbd39 to disappear
Mar  4 09:51:37.117: INFO: Pod downwardapi-volume-c1caf382-41b0-4267-8f17-08508d4cbd39 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:51:37.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3127" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":278,"completed":194,"skipped":3083,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:51:37.274: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Mar  4 09:51:37.333: INFO: Waiting up to 5m0s for pod "downward-api-c876c516-f32a-454d-ab53-61e70c7af9fa" in namespace "downward-api-4415" to be "success or failure"
Mar  4 09:51:37.337: INFO: Pod "downward-api-c876c516-f32a-454d-ab53-61e70c7af9fa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.95807ms
Mar  4 09:51:39.344: INFO: Pod "downward-api-c876c516-f32a-454d-ab53-61e70c7af9fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010287563s
STEP: Saw pod success
Mar  4 09:51:39.344: INFO: Pod "downward-api-c876c516-f32a-454d-ab53-61e70c7af9fa" satisfied condition "success or failure"
Mar  4 09:51:39.352: INFO: Trying to get logs from node master3 pod downward-api-c876c516-f32a-454d-ab53-61e70c7af9fa container dapi-container: <nil>
STEP: delete the pod
Mar  4 09:51:39.517: INFO: Waiting for pod downward-api-c876c516-f32a-454d-ab53-61e70c7af9fa to disappear
Mar  4 09:51:39.525: INFO: Pod downward-api-c876c516-f32a-454d-ab53-61e70c7af9fa no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:51:39.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4415" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":278,"completed":195,"skipped":3106,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:51:39.539: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7414
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-7414
I0304 09:51:39.784772      19 runners.go:189] Created replication controller with name: externalname-service, namespace: services-7414, replica count: 2
Mar  4 09:51:42.835: INFO: Creating new exec pod
I0304 09:51:42.835116      19 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  4 09:51:46.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=services-7414 execpodt9xj6 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar  4 09:51:46.498: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  4 09:51:46.498: INFO: stdout: ""
Mar  4 09:51:46.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=services-7414 execpodt9xj6 -- /bin/sh -x -c nc -zv -t -w 2 10.111.145.70 80'
Mar  4 09:51:46.999: INFO: stderr: "+ nc -zv -t -w 2 10.111.145.70 80\nConnection to 10.111.145.70 80 port [tcp/http] succeeded!\n"
Mar  4 09:51:46.999: INFO: stdout: ""
Mar  4 09:51:46.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=services-7414 execpodt9xj6 -- /bin/sh -x -c nc -zv -t -w 2 10.128.0.3 30388'
Mar  4 09:51:47.371: INFO: stderr: "+ nc -zv -t -w 2 10.128.0.3 30388\nConnection to 10.128.0.3 30388 port [tcp/30388] succeeded!\n"
Mar  4 09:51:47.371: INFO: stdout: ""
Mar  4 09:51:47.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=services-7414 execpodt9xj6 -- /bin/sh -x -c nc -zv -t -w 2 10.128.0.4 30388'
Mar  4 09:51:47.754: INFO: stderr: "+ nc -zv -t -w 2 10.128.0.4 30388\nConnection to 10.128.0.4 30388 port [tcp/30388] succeeded!\n"
Mar  4 09:51:47.754: INFO: stdout: ""
Mar  4 09:51:47.754: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:51:47.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7414" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:8.262 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":278,"completed":196,"skipped":3121,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:51:47.801: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Starting the proxy
Mar  4 09:51:48.014: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-195674220 proxy --unix-socket=/tmp/kubectl-proxy-unix005491211/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:51:48.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1458" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":278,"completed":197,"skipped":3142,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:51:48.095: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar  4 09:51:48.292: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5466e64e-cf25-40cb-b267-3c33486d8ef4" in namespace "downward-api-4640" to be "success or failure"
Mar  4 09:51:48.306: INFO: Pod "downwardapi-volume-5466e64e-cf25-40cb-b267-3c33486d8ef4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.365264ms
Mar  4 09:51:50.317: INFO: Pod "downwardapi-volume-5466e64e-cf25-40cb-b267-3c33486d8ef4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025132924s
STEP: Saw pod success
Mar  4 09:51:50.317: INFO: Pod "downwardapi-volume-5466e64e-cf25-40cb-b267-3c33486d8ef4" satisfied condition "success or failure"
Mar  4 09:51:50.320: INFO: Trying to get logs from node master2 pod downwardapi-volume-5466e64e-cf25-40cb-b267-3c33486d8ef4 container client-container: <nil>
STEP: delete the pod
Mar  4 09:51:50.514: INFO: Waiting for pod downwardapi-volume-5466e64e-cf25-40cb-b267-3c33486d8ef4 to disappear
Mar  4 09:51:50.563: INFO: Pod downwardapi-volume-5466e64e-cf25-40cb-b267-3c33486d8ef4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:51:50.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4640" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":198,"skipped":3143,"failed":0}
SSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:51:50.579: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:51:50.769: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-3f862ae1-d182-477e-9e8c-9d8bbc013666" in namespace "security-context-test-4830" to be "success or failure"
Mar  4 09:51:50.801: INFO: Pod "busybox-privileged-false-3f862ae1-d182-477e-9e8c-9d8bbc013666": Phase="Pending", Reason="", readiness=false. Elapsed: 32.040163ms
Mar  4 09:51:52.805: INFO: Pod "busybox-privileged-false-3f862ae1-d182-477e-9e8c-9d8bbc013666": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03674064s
Mar  4 09:51:52.806: INFO: Pod "busybox-privileged-false-3f862ae1-d182-477e-9e8c-9d8bbc013666" satisfied condition "success or failure"
Mar  4 09:51:52.833: INFO: Got logs for pod "busybox-privileged-false-3f862ae1-d182-477e-9e8c-9d8bbc013666": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:51:52.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4830" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":199,"skipped":3147,"failed":0}
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:51:52.848: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:51:53.096: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar  4 09:51:58.100: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  4 09:51:58.100: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Mar  4 09:52:00.300: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9577 /apis/apps/v1/namespaces/deployment-9577/deployments/test-cleanup-deployment 93c75fee-b9d2-414e-bbac-a1f22bc49ba4 64058 1 2020-03-04 09:51:58 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0040272e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-03-04 09:51:58 +0000 UTC,LastTransitionTime:2020-03-04 09:51:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-55ffc6b7b6" has successfully progressed.,LastUpdateTime:2020-03-04 09:52:00 +0000 UTC,LastTransitionTime:2020-03-04 09:51:58 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  4 09:52:00.316: INFO: New ReplicaSet "test-cleanup-deployment-55ffc6b7b6" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-55ffc6b7b6  deployment-9577 /apis/apps/v1/namespaces/deployment-9577/replicasets/test-cleanup-deployment-55ffc6b7b6 9dd621bf-de16-4b34-9eca-cf748861e3de 64047 1 2020-03-04 09:51:58 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 93c75fee-b9d2-414e-bbac-a1f22bc49ba4 0xc0040276d7 0xc0040276d8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55ffc6b7b6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004027748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  4 09:52:00.320: INFO: Pod "test-cleanup-deployment-55ffc6b7b6-x26vc" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-55ffc6b7b6-x26vc test-cleanup-deployment-55ffc6b7b6- deployment-9577 /api/v1/namespaces/deployment-9577/pods/test-cleanup-deployment-55ffc6b7b6-x26vc 607d50d0-664a-4aa9-bce8-e7bedc689de1 64046 0 2020-03-04 09:51:58 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[cni.projectcalico.org/podIP:192.168.136.35/32 cni.projectcalico.org/podIPs:192.168.136.35/32] [{apps/v1 ReplicaSet test-cleanup-deployment-55ffc6b7b6 9dd621bf-de16-4b34-9eca-cf748861e3de 0xc004027ae7 0xc004027ae8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lsk8h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lsk8h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lsk8h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:51:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:52:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:52:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:51:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.4,PodIP:192.168.136.35,StartTime:2020-03-04 09:51:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-04 09:51:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://c75ef3909c2c1f72d3665bd6c4244058d384ac2679ff4eea3bc6c833b8a75c1c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.136.35,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:52:00.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9577" for this suite.

• [SLOW TEST:7.684 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":278,"completed":200,"skipped":3153,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:52:00.532: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:52:11.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2845" for this suite.

• [SLOW TEST:11.305 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":278,"completed":201,"skipped":3157,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:52:11.839: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4298.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4298.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4298.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4298.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  4 09:52:14.077: INFO: DNS probes using dns-test-75a97061-c309-40cb-8982-4f94f03ca017 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4298.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4298.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4298.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4298.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  4 09:52:16.307: INFO: File wheezy_udp@dns-test-service-3.dns-4298.svc.cluster.local from pod  dns-4298/dns-test-de431dfb-217d-48af-94c1-dd7d1d71badd contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  4 09:52:16.312: INFO: Lookups using dns-4298/dns-test-de431dfb-217d-48af-94c1-dd7d1d71badd failed for: [wheezy_udp@dns-test-service-3.dns-4298.svc.cluster.local]

Mar  4 09:52:21.320: INFO: File jessie_udp@dns-test-service-3.dns-4298.svc.cluster.local from pod  dns-4298/dns-test-de431dfb-217d-48af-94c1-dd7d1d71badd contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  4 09:52:21.320: INFO: Lookups using dns-4298/dns-test-de431dfb-217d-48af-94c1-dd7d1d71badd failed for: [jessie_udp@dns-test-service-3.dns-4298.svc.cluster.local]

Mar  4 09:52:26.316: INFO: File wheezy_udp@dns-test-service-3.dns-4298.svc.cluster.local from pod  dns-4298/dns-test-de431dfb-217d-48af-94c1-dd7d1d71badd contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  4 09:52:26.319: INFO: File jessie_udp@dns-test-service-3.dns-4298.svc.cluster.local from pod  dns-4298/dns-test-de431dfb-217d-48af-94c1-dd7d1d71badd contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  4 09:52:26.319: INFO: Lookups using dns-4298/dns-test-de431dfb-217d-48af-94c1-dd7d1d71badd failed for: [wheezy_udp@dns-test-service-3.dns-4298.svc.cluster.local jessie_udp@dns-test-service-3.dns-4298.svc.cluster.local]

Mar  4 09:52:31.320: INFO: DNS probes using dns-test-de431dfb-217d-48af-94c1-dd7d1d71badd succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4298.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4298.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4298.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4298.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  4 09:52:35.798: INFO: DNS probes using dns-test-a8d42c6b-5acf-4a83-a8ca-35fb9349fb03 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:52:35.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4298" for this suite.

• [SLOW TEST:24.178 seconds]
[sig-network] DNS
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":278,"completed":202,"skipped":3195,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:52:36.017: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  4 09:52:36.248: INFO: Waiting up to 5m0s for pod "pod-8afcf900-08aa-4131-bcfb-36a34a8ad1d0" in namespace "emptydir-5176" to be "success or failure"
Mar  4 09:52:36.270: INFO: Pod "pod-8afcf900-08aa-4131-bcfb-36a34a8ad1d0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.447475ms
Mar  4 09:52:38.278: INFO: Pod "pod-8afcf900-08aa-4131-bcfb-36a34a8ad1d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030352499s
STEP: Saw pod success
Mar  4 09:52:38.278: INFO: Pod "pod-8afcf900-08aa-4131-bcfb-36a34a8ad1d0" satisfied condition "success or failure"
Mar  4 09:52:38.281: INFO: Trying to get logs from node master3 pod pod-8afcf900-08aa-4131-bcfb-36a34a8ad1d0 container test-container: <nil>
STEP: delete the pod
Mar  4 09:52:38.333: INFO: Waiting for pod pod-8afcf900-08aa-4131-bcfb-36a34a8ad1d0 to disappear
Mar  4 09:52:38.343: INFO: Pod pod-8afcf900-08aa-4131-bcfb-36a34a8ad1d0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:52:38.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5176" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":203,"skipped":3200,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:52:38.374: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  4 09:52:38.574: INFO: Waiting up to 5m0s for pod "pod-546aa3dc-5fe6-4259-9b69-95ecaabd9d86" in namespace "emptydir-7310" to be "success or failure"
Mar  4 09:52:38.578: INFO: Pod "pod-546aa3dc-5fe6-4259-9b69-95ecaabd9d86": Phase="Pending", Reason="", readiness=false. Elapsed: 3.802557ms
Mar  4 09:52:40.600: INFO: Pod "pod-546aa3dc-5fe6-4259-9b69-95ecaabd9d86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025254988s
STEP: Saw pod success
Mar  4 09:52:40.600: INFO: Pod "pod-546aa3dc-5fe6-4259-9b69-95ecaabd9d86" satisfied condition "success or failure"
Mar  4 09:52:40.602: INFO: Trying to get logs from node master3 pod pod-546aa3dc-5fe6-4259-9b69-95ecaabd9d86 container test-container: <nil>
STEP: delete the pod
Mar  4 09:52:40.834: INFO: Waiting for pod pod-546aa3dc-5fe6-4259-9b69-95ecaabd9d86 to disappear
Mar  4 09:52:40.840: INFO: Pod pod-546aa3dc-5fe6-4259-9b69-95ecaabd9d86 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:52:40.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7310" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":204,"skipped":3215,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:52:40.854: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Mar  4 09:52:45.808: INFO: Successfully updated pod "labelsupdate192c3e97-bd5c-41ca-8dd5-5be23fdbc6bc"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:52:47.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7744" for this suite.

• [SLOW TEST:6.993 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":278,"completed":205,"skipped":3231,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:52:47.848: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:53:48.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3296" for this suite.

• [SLOW TEST:60.189 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":278,"completed":206,"skipped":3273,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:53:48.038: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-3eb1c693-351d-443f-8589-61aad914d343
STEP: Creating a pod to test consume secrets
Mar  4 09:53:48.269: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6f8a483a-04b0-4b5e-b75b-3dd9cc1eb230" in namespace "projected-7487" to be "success or failure"
Mar  4 09:53:48.279: INFO: Pod "pod-projected-secrets-6f8a483a-04b0-4b5e-b75b-3dd9cc1eb230": Phase="Pending", Reason="", readiness=false. Elapsed: 10.221425ms
Mar  4 09:53:50.285: INFO: Pod "pod-projected-secrets-6f8a483a-04b0-4b5e-b75b-3dd9cc1eb230": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015944525s
Mar  4 09:53:52.288: INFO: Pod "pod-projected-secrets-6f8a483a-04b0-4b5e-b75b-3dd9cc1eb230": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019407545s
STEP: Saw pod success
Mar  4 09:53:52.288: INFO: Pod "pod-projected-secrets-6f8a483a-04b0-4b5e-b75b-3dd9cc1eb230" satisfied condition "success or failure"
Mar  4 09:53:52.291: INFO: Trying to get logs from node master3 pod pod-projected-secrets-6f8a483a-04b0-4b5e-b75b-3dd9cc1eb230 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  4 09:53:52.314: INFO: Waiting for pod pod-projected-secrets-6f8a483a-04b0-4b5e-b75b-3dd9cc1eb230 to disappear
Mar  4 09:53:52.317: INFO: Pod pod-projected-secrets-6f8a483a-04b0-4b5e-b75b-3dd9cc1eb230 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:53:52.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7487" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":207,"skipped":3313,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:53:52.328: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Mar  4 09:53:52.367: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:54:21.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5307" for this suite.

• [SLOW TEST:29.262 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":278,"completed":208,"skipped":3313,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:54:21.590: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test env composition
Mar  4 09:54:21.806: INFO: Waiting up to 5m0s for pod "var-expansion-4f36eac9-d87c-4661-ac03-3cdb0a1edb79" in namespace "var-expansion-5213" to be "success or failure"
Mar  4 09:54:21.817: INFO: Pod "var-expansion-4f36eac9-d87c-4661-ac03-3cdb0a1edb79": Phase="Pending", Reason="", readiness=false. Elapsed: 10.418452ms
Mar  4 09:54:23.821: INFO: Pod "var-expansion-4f36eac9-d87c-4661-ac03-3cdb0a1edb79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014180957s
Mar  4 09:54:25.827: INFO: Pod "var-expansion-4f36eac9-d87c-4661-ac03-3cdb0a1edb79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020227627s
STEP: Saw pod success
Mar  4 09:54:25.827: INFO: Pod "var-expansion-4f36eac9-d87c-4661-ac03-3cdb0a1edb79" satisfied condition "success or failure"
Mar  4 09:54:25.829: INFO: Trying to get logs from node master3 pod var-expansion-4f36eac9-d87c-4661-ac03-3cdb0a1edb79 container dapi-container: <nil>
STEP: delete the pod
Mar  4 09:54:25.864: INFO: Waiting for pod var-expansion-4f36eac9-d87c-4661-ac03-3cdb0a1edb79 to disappear
Mar  4 09:54:26.001: INFO: Pod var-expansion-4f36eac9-d87c-4661-ac03-3cdb0a1edb79 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:54:26.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5213" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":278,"completed":209,"skipped":3330,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:54:26.014: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-140def25-9ce8-4926-a390-415a8b7a7396
STEP: Creating a pod to test consume configMaps
Mar  4 09:54:26.073: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cc07e6be-c14c-4709-a7fe-41ba1909f540" in namespace "projected-4586" to be "success or failure"
Mar  4 09:54:26.084: INFO: Pod "pod-projected-configmaps-cc07e6be-c14c-4709-a7fe-41ba1909f540": Phase="Pending", Reason="", readiness=false. Elapsed: 10.442965ms
Mar  4 09:54:28.087: INFO: Pod "pod-projected-configmaps-cc07e6be-c14c-4709-a7fe-41ba1909f540": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014054258s
STEP: Saw pod success
Mar  4 09:54:28.087: INFO: Pod "pod-projected-configmaps-cc07e6be-c14c-4709-a7fe-41ba1909f540" satisfied condition "success or failure"
Mar  4 09:54:28.098: INFO: Trying to get logs from node master3 pod pod-projected-configmaps-cc07e6be-c14c-4709-a7fe-41ba1909f540 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  4 09:54:28.282: INFO: Waiting for pod pod-projected-configmaps-cc07e6be-c14c-4709-a7fe-41ba1909f540 to disappear
Mar  4 09:54:28.294: INFO: Pod pod-projected-configmaps-cc07e6be-c14c-4709-a7fe-41ba1909f540 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:54:28.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4586" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":210,"skipped":3337,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:54:28.321: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Mar  4 09:54:28.538: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Mar  4 09:54:47.794: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
Mar  4 09:54:53.595: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:55:15.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1847" for this suite.

• [SLOW TEST:47.249 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":278,"completed":211,"skipped":3369,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:55:15.570: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  4 09:55:15.759: INFO: Waiting up to 5m0s for pod "pod-b267e72a-6bd0-45b2-be9d-5f722be4204e" in namespace "emptydir-2800" to be "success or failure"
Mar  4 09:55:15.763: INFO: Pod "pod-b267e72a-6bd0-45b2-be9d-5f722be4204e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.624749ms
Mar  4 09:55:17.769: INFO: Pod "pod-b267e72a-6bd0-45b2-be9d-5f722be4204e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010075842s
STEP: Saw pod success
Mar  4 09:55:17.769: INFO: Pod "pod-b267e72a-6bd0-45b2-be9d-5f722be4204e" satisfied condition "success or failure"
Mar  4 09:55:17.773: INFO: Trying to get logs from node master3 pod pod-b267e72a-6bd0-45b2-be9d-5f722be4204e container test-container: <nil>
STEP: delete the pod
Mar  4 09:55:17.821: INFO: Waiting for pod pod-b267e72a-6bd0-45b2-be9d-5f722be4204e to disappear
Mar  4 09:55:17.826: INFO: Pod pod-b267e72a-6bd0-45b2-be9d-5f722be4204e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:55:17.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2800" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":212,"skipped":3369,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:55:17.845: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Mar  4 09:55:18.030: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
Mar  4 09:55:25.004: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:55:48.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7974" for this suite.

• [SLOW TEST:30.915 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":278,"completed":213,"skipped":3392,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:55:48.760: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  4 09:55:49.546: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  4 09:55:51.555: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912549, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912549, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912549, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912549, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  4 09:55:54.580: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:55:54.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-55" for this suite.
STEP: Destroying namespace "webhook-55-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.299 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":278,"completed":214,"skipped":3415,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:55:55.060: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Mar  4 09:55:55.297: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  4 09:55:55.308: INFO: Waiting for terminating namespaces to be deleted...
Mar  4 09:55:55.311: INFO: 
Logging pods the kubelet thinks is on node master2 before test
Mar  4 09:55:55.332: INFO: coredns-6955765f44-55pg4 from kube-system started at 2020-03-04 06:30:15 +0000 UTC (1 container statuses recorded)
Mar  4 09:55:55.332: INFO: 	Container coredns ready: true, restart count 0
Mar  4 09:55:55.332: INFO: kube-proxy-xq4nn from kube-system started at 2020-03-04 06:27:13 +0000 UTC (1 container statuses recorded)
Mar  4 09:55:55.332: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  4 09:55:55.332: INFO: sonobuoy-systemd-logs-daemon-set-002e23000b6e4f18-b5sfk from sonobuoy started at 2020-03-04 08:58:19 +0000 UTC (2 container statuses recorded)
Mar  4 09:55:55.332: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  4 09:55:55.332: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  4 09:55:55.332: INFO: calico-node-lzvqd from kube-system started at 2020-03-04 06:30:01 +0000 UTC (1 container statuses recorded)
Mar  4 09:55:55.332: INFO: 	Container calico-node ready: true, restart count 0
Mar  4 09:55:55.332: INFO: calico-kube-controllers-5b644bc49c-knmsz from kube-system started at 2020-03-04 06:30:14 +0000 UTC (1 container statuses recorded)
Mar  4 09:55:55.332: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  4 09:55:55.332: INFO: coredns-6955765f44-npmb6 from kube-system started at 2020-03-04 06:30:15 +0000 UTC (1 container statuses recorded)
Mar  4 09:55:55.332: INFO: 	Container coredns ready: true, restart count 0
Mar  4 09:55:55.332: INFO: 
Logging pods the kubelet thinks is on node master3 before test
Mar  4 09:55:55.346: INFO: kube-proxy-gkd5m from kube-system started at 2020-03-04 06:27:48 +0000 UTC (1 container statuses recorded)
Mar  4 09:55:55.346: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  4 09:55:55.346: INFO: sonobuoy from sonobuoy started at 2020-03-04 08:58:18 +0000 UTC (1 container statuses recorded)
Mar  4 09:55:55.346: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  4 09:55:55.346: INFO: sonobuoy-systemd-logs-daemon-set-002e23000b6e4f18-28pjm from sonobuoy started at 2020-03-04 08:58:19 +0000 UTC (2 container statuses recorded)
Mar  4 09:55:55.346: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  4 09:55:55.346: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  4 09:55:55.346: INFO: calico-node-l9qb2 from kube-system started at 2020-03-04 06:30:01 +0000 UTC (1 container statuses recorded)
Mar  4 09:55:55.346: INFO: 	Container calico-node ready: true, restart count 0
Mar  4 09:55:55.346: INFO: sonobuoy-e2e-job-7260dc34846e4a1a from sonobuoy started at 2020-03-04 08:58:19 +0000 UTC (2 container statuses recorded)
Mar  4 09:55:55.346: INFO: 	Container e2e ready: true, restart count 0
Mar  4 09:55:55.346: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: verifying the node has the label node master2
STEP: verifying the node has the label node master3
Mar  4 09:55:55.553: INFO: Pod calico-kube-controllers-5b644bc49c-knmsz requesting resource cpu=0m on Node master2
Mar  4 09:55:55.553: INFO: Pod calico-node-l9qb2 requesting resource cpu=250m on Node master3
Mar  4 09:55:55.553: INFO: Pod calico-node-lzvqd requesting resource cpu=250m on Node master2
Mar  4 09:55:55.553: INFO: Pod coredns-6955765f44-55pg4 requesting resource cpu=100m on Node master2
Mar  4 09:55:55.553: INFO: Pod coredns-6955765f44-npmb6 requesting resource cpu=100m on Node master2
Mar  4 09:55:55.553: INFO: Pod kube-proxy-gkd5m requesting resource cpu=0m on Node master3
Mar  4 09:55:55.553: INFO: Pod kube-proxy-xq4nn requesting resource cpu=0m on Node master2
Mar  4 09:55:55.553: INFO: Pod sonobuoy requesting resource cpu=0m on Node master3
Mar  4 09:55:55.553: INFO: Pod sonobuoy-e2e-job-7260dc34846e4a1a requesting resource cpu=0m on Node master3
Mar  4 09:55:55.553: INFO: Pod sonobuoy-systemd-logs-daemon-set-002e23000b6e4f18-28pjm requesting resource cpu=0m on Node master3
Mar  4 09:55:55.553: INFO: Pod sonobuoy-systemd-logs-daemon-set-002e23000b6e4f18-b5sfk requesting resource cpu=0m on Node master2
STEP: Starting Pods to consume most of the cluster CPU.
Mar  4 09:55:55.553: INFO: Creating a pod which consumes cpu=1085m on Node master2
Mar  4 09:55:55.569: INFO: Creating a pod which consumes cpu=1225m on Node master3
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dbca1ca1-971e-4846-bac0-f790a6ea3867.15f9114a7a5b2e02], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6083/filler-pod-dbca1ca1-971e-4846-bac0-f790a6ea3867 to master2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dbca1ca1-971e-4846-bac0-f790a6ea3867.15f9114ab92565f1], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dbca1ca1-971e-4846-bac0-f790a6ea3867.15f9114abc46d9fb], Reason = [Created], Message = [Created container filler-pod-dbca1ca1-971e-4846-bac0-f790a6ea3867]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dbca1ca1-971e-4846-bac0-f790a6ea3867.15f9114ac392aeec], Reason = [Started], Message = [Started container filler-pod-dbca1ca1-971e-4846-bac0-f790a6ea3867]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-df7a08af-559f-4a3b-9c2c-273f33ee673e.15f9114a7cbb2e1b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6083/filler-pod-df7a08af-559f-4a3b-9c2c-273f33ee673e to master3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-df7a08af-559f-4a3b-9c2c-273f33ee673e.15f9114ac5cb2346], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-df7a08af-559f-4a3b-9c2c-273f33ee673e.15f9114ac84fa8e1], Reason = [Created], Message = [Created container filler-pod-df7a08af-559f-4a3b-9c2c-273f33ee673e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-df7a08af-559f-4a3b-9c2c-273f33ee673e.15f9114ad04020cd], Reason = [Started], Message = [Started container filler-pod-df7a08af-559f-4a3b-9c2c-273f33ee673e]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15f9114b75f5df83], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 2 Insufficient cpu.]
STEP: removing the label node off the node master2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node master3
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:56:01.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6083" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:6.283 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":278,"completed":215,"skipped":3458,"failed":0}
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:56:01.343: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar  4 09:56:09.618: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  4 09:56:09.753: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  4 09:56:11.753: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  4 09:56:11.758: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  4 09:56:13.753: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  4 09:56:13.756: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:56:13.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1044" for this suite.

• [SLOW TEST:12.436 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":278,"completed":216,"skipped":3459,"failed":0}
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:56:13.780: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service multi-endpoint-test in namespace services-4750
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4750 to expose endpoints map[]
Mar  4 09:56:13.867: INFO: successfully validated that service multi-endpoint-test in namespace services-4750 exposes endpoints map[] (8.827321ms elapsed)
STEP: Creating pod pod1 in namespace services-4750
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4750 to expose endpoints map[pod1:[100]]
Mar  4 09:56:16.300: INFO: successfully validated that service multi-endpoint-test in namespace services-4750 exposes endpoints map[pod1:[100]] (2.291081947s elapsed)
STEP: Creating pod pod2 in namespace services-4750
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4750 to expose endpoints map[pod1:[100] pod2:[101]]
Mar  4 09:56:18.549: INFO: successfully validated that service multi-endpoint-test in namespace services-4750 exposes endpoints map[pod1:[100] pod2:[101]] (2.231668244s elapsed)
STEP: Deleting pod pod1 in namespace services-4750
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4750 to expose endpoints map[pod2:[101]]
Mar  4 09:56:18.578: INFO: successfully validated that service multi-endpoint-test in namespace services-4750 exposes endpoints map[pod2:[101]] (23.822986ms elapsed)
STEP: Deleting pod pod2 in namespace services-4750
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4750 to expose endpoints map[]
Mar  4 09:56:18.601: INFO: successfully validated that service multi-endpoint-test in namespace services-4750 exposes endpoints map[] (12.142281ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:56:18.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4750" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
•{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":278,"completed":217,"skipped":3459,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:56:18.772: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-583cad97-8257-403e-b97a-d219b9adfd0c
STEP: Creating a pod to test consume configMaps
Mar  4 09:56:19.084: INFO: Waiting up to 5m0s for pod "pod-configmaps-3a5c58ae-4acc-4eb8-aac5-bb3b5ef2f815" in namespace "configmap-9328" to be "success or failure"
Mar  4 09:56:19.116: INFO: Pod "pod-configmaps-3a5c58ae-4acc-4eb8-aac5-bb3b5ef2f815": Phase="Pending", Reason="", readiness=false. Elapsed: 32.162233ms
Mar  4 09:56:21.120: INFO: Pod "pod-configmaps-3a5c58ae-4acc-4eb8-aac5-bb3b5ef2f815": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036135286s
Mar  4 09:56:23.267: INFO: Pod "pod-configmaps-3a5c58ae-4acc-4eb8-aac5-bb3b5ef2f815": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.182631565s
STEP: Saw pod success
Mar  4 09:56:23.267: INFO: Pod "pod-configmaps-3a5c58ae-4acc-4eb8-aac5-bb3b5ef2f815" satisfied condition "success or failure"
Mar  4 09:56:23.275: INFO: Trying to get logs from node master3 pod pod-configmaps-3a5c58ae-4acc-4eb8-aac5-bb3b5ef2f815 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  4 09:56:23.311: INFO: Waiting for pod pod-configmaps-3a5c58ae-4acc-4eb8-aac5-bb3b5ef2f815 to disappear
Mar  4 09:56:23.315: INFO: Pod pod-configmaps-3a5c58ae-4acc-4eb8-aac5-bb3b5ef2f815 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:56:23.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9328" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":218,"skipped":3464,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:56:23.333: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-080bf297-cdf6-4562-9801-2e2597cd88f5
STEP: Creating a pod to test consume configMaps
Mar  4 09:56:23.537: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cb82bacf-f726-460a-9e41-be96b05a239d" in namespace "projected-3984" to be "success or failure"
Mar  4 09:56:23.541: INFO: Pod "pod-projected-configmaps-cb82bacf-f726-460a-9e41-be96b05a239d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.585137ms
Mar  4 09:56:25.555: INFO: Pod "pod-projected-configmaps-cb82bacf-f726-460a-9e41-be96b05a239d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017167208s
STEP: Saw pod success
Mar  4 09:56:25.555: INFO: Pod "pod-projected-configmaps-cb82bacf-f726-460a-9e41-be96b05a239d" satisfied condition "success or failure"
Mar  4 09:56:25.560: INFO: Trying to get logs from node master3 pod pod-projected-configmaps-cb82bacf-f726-460a-9e41-be96b05a239d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  4 09:56:25.611: INFO: Waiting for pod pod-projected-configmaps-cb82bacf-f726-460a-9e41-be96b05a239d to disappear
Mar  4 09:56:25.616: INFO: Pod pod-projected-configmaps-cb82bacf-f726-460a-9e41-be96b05a239d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:56:25.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3984" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":278,"completed":219,"skipped":3517,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:56:25.780: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  4 09:56:27.037: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  4 09:56:29.063: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912587, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912587, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912587, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912587, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  4 09:56:32.090: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:56:32.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3240" for this suite.
STEP: Destroying namespace "webhook-3240-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.811 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":278,"completed":220,"skipped":3522,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:56:32.592: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Mar  4 09:56:42.857: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:56:42.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0304 09:56:42.857726      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-9740" for this suite.

• [SLOW TEST:10.276 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":278,"completed":221,"skipped":3570,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:56:42.868: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:56:43.036: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Mar  4 09:56:44.088: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:56:44.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-82" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":278,"completed":222,"skipped":3607,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:56:44.264: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Mar  4 09:56:44.527: INFO: Waiting up to 5m0s for pod "downward-api-a763f04e-e83e-4464-9e24-9ff3c9e663da" in namespace "downward-api-779" to be "success or failure"
Mar  4 09:56:44.535: INFO: Pod "downward-api-a763f04e-e83e-4464-9e24-9ff3c9e663da": Phase="Pending", Reason="", readiness=false. Elapsed: 7.960742ms
Mar  4 09:56:46.540: INFO: Pod "downward-api-a763f04e-e83e-4464-9e24-9ff3c9e663da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013208144s
Mar  4 09:56:48.544: INFO: Pod "downward-api-a763f04e-e83e-4464-9e24-9ff3c9e663da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016638139s
STEP: Saw pod success
Mar  4 09:56:48.544: INFO: Pod "downward-api-a763f04e-e83e-4464-9e24-9ff3c9e663da" satisfied condition "success or failure"
Mar  4 09:56:48.548: INFO: Trying to get logs from node master3 pod downward-api-a763f04e-e83e-4464-9e24-9ff3c9e663da container dapi-container: <nil>
STEP: delete the pod
Mar  4 09:56:48.604: INFO: Waiting for pod downward-api-a763f04e-e83e-4464-9e24-9ff3c9e663da to disappear
Mar  4 09:56:48.607: INFO: Pod downward-api-a763f04e-e83e-4464-9e24-9ff3c9e663da no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:56:48.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-779" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":278,"completed":223,"skipped":3620,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:56:48.769: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-7a4253cb-6adf-487b-ab08-4cfe50563f58
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:56:55.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-784" for this suite.

• [SLOW TEST:6.265 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":224,"skipped":3680,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:56:55.034: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:56:55.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2766" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":278,"completed":225,"skipped":3696,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:56:55.284: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Mar  4 09:57:02.040: INFO: 0 pods remaining
Mar  4 09:57:02.040: INFO: 0 pods has nil DeletionTimestamp
Mar  4 09:57:02.040: INFO: 
STEP: Gathering metrics
Mar  4 09:57:03.059: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0304 09:57:03.059571      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:57:03.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8459" for this suite.

• [SLOW TEST:8.063 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":278,"completed":226,"skipped":3722,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:57:03.347: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  4 09:57:08.371: INFO: Successfully updated pod "pod-update-activedeadlineseconds-189def54-acda-4287-be08-0a7695353848"
Mar  4 09:57:08.371: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-189def54-acda-4287-be08-0a7695353848" in namespace "pods-9600" to be "terminated due to deadline exceeded"
Mar  4 09:57:08.513: INFO: Pod "pod-update-activedeadlineseconds-189def54-acda-4287-be08-0a7695353848": Phase="Running", Reason="", readiness=true. Elapsed: 142.137314ms
Mar  4 09:57:10.550: INFO: Pod "pod-update-activedeadlineseconds-189def54-acda-4287-be08-0a7695353848": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.179219162s
Mar  4 09:57:10.550: INFO: Pod "pod-update-activedeadlineseconds-189def54-acda-4287-be08-0a7695353848" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:57:10.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9600" for this suite.

• [SLOW TEST:7.261 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":278,"completed":227,"skipped":3742,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:57:10.609: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Mar  4 09:57:11.018: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  4 09:57:11.030: INFO: Waiting for terminating namespaces to be deleted...
Mar  4 09:57:11.033: INFO: 
Logging pods the kubelet thinks is on node master2 before test
Mar  4 09:57:11.040: INFO: kube-proxy-xq4nn from kube-system started at 2020-03-04 06:27:13 +0000 UTC (1 container statuses recorded)
Mar  4 09:57:11.040: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  4 09:57:11.040: INFO: sonobuoy-systemd-logs-daemon-set-002e23000b6e4f18-b5sfk from sonobuoy started at 2020-03-04 08:58:19 +0000 UTC (2 container statuses recorded)
Mar  4 09:57:11.040: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  4 09:57:11.040: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  4 09:57:11.040: INFO: pod-update-activedeadlineseconds-189def54-acda-4287-be08-0a7695353848 from pods-9600 started at 2020-03-04 09:57:03 +0000 UTC (1 container statuses recorded)
Mar  4 09:57:11.040: INFO: 	Container nginx ready: false, restart count 0
Mar  4 09:57:11.040: INFO: calico-node-lzvqd from kube-system started at 2020-03-04 06:30:01 +0000 UTC (1 container statuses recorded)
Mar  4 09:57:11.040: INFO: 	Container calico-node ready: true, restart count 0
Mar  4 09:57:11.040: INFO: calico-kube-controllers-5b644bc49c-knmsz from kube-system started at 2020-03-04 06:30:14 +0000 UTC (1 container statuses recorded)
Mar  4 09:57:11.040: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  4 09:57:11.040: INFO: coredns-6955765f44-npmb6 from kube-system started at 2020-03-04 06:30:15 +0000 UTC (1 container statuses recorded)
Mar  4 09:57:11.040: INFO: 	Container coredns ready: true, restart count 0
Mar  4 09:57:11.040: INFO: coredns-6955765f44-55pg4 from kube-system started at 2020-03-04 06:30:15 +0000 UTC (1 container statuses recorded)
Mar  4 09:57:11.040: INFO: 	Container coredns ready: true, restart count 0
Mar  4 09:57:11.040: INFO: 
Logging pods the kubelet thinks is on node master3 before test
Mar  4 09:57:11.047: INFO: kube-proxy-gkd5m from kube-system started at 2020-03-04 06:27:48 +0000 UTC (1 container statuses recorded)
Mar  4 09:57:11.047: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  4 09:57:11.047: INFO: sonobuoy from sonobuoy started at 2020-03-04 08:58:18 +0000 UTC (1 container statuses recorded)
Mar  4 09:57:11.047: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  4 09:57:11.047: INFO: calico-node-l9qb2 from kube-system started at 2020-03-04 06:30:01 +0000 UTC (1 container statuses recorded)
Mar  4 09:57:11.047: INFO: 	Container calico-node ready: true, restart count 0
Mar  4 09:57:11.047: INFO: sonobuoy-e2e-job-7260dc34846e4a1a from sonobuoy started at 2020-03-04 08:58:19 +0000 UTC (2 container statuses recorded)
Mar  4 09:57:11.047: INFO: 	Container e2e ready: true, restart count 0
Mar  4 09:57:11.047: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  4 09:57:11.047: INFO: sonobuoy-systemd-logs-daemon-set-002e23000b6e4f18-28pjm from sonobuoy started at 2020-03-04 08:58:19 +0000 UTC (2 container statuses recorded)
Mar  4 09:57:11.047: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  4 09:57:11.047: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15f9115c0dfb0abd], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:57:12.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2033" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":278,"completed":228,"skipped":3747,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:57:12.095: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  4 09:57:13.088: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  4 09:57:15.098: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912633, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912633, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912633, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912633, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  4 09:57:18.509: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:57:18.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7143" for this suite.
STEP: Destroying namespace "webhook-7143-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.957 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":278,"completed":229,"skipped":3768,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:57:19.052: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2175
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-2175
I0304 09:57:19.259101      19 runners.go:189] Created replication controller with name: externalname-service, namespace: services-2175, replica count: 2
Mar  4 09:57:22.310: INFO: Creating new exec pod
I0304 09:57:22.310811      19 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  4 09:57:25.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=services-2175 execpodq4r98 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar  4 09:57:25.749: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  4 09:57:25.749: INFO: stdout: ""
Mar  4 09:57:25.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 exec --namespace=services-2175 execpodq4r98 -- /bin/sh -x -c nc -zv -t -w 2 10.107.214.37 80'
Mar  4 09:57:26.106: INFO: stderr: "+ nc -zv -t -w 2 10.107.214.37 80\nConnection to 10.107.214.37 80 port [tcp/http] succeeded!\n"
Mar  4 09:57:26.106: INFO: stdout: ""
Mar  4 09:57:26.106: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:57:26.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2175" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:7.226 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":278,"completed":230,"skipped":3771,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:57:26.278: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar  4 09:57:26.765: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Mar  4 09:57:28.774: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912646, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912646, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912646, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912646, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  4 09:57:31.815: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:57:32.531: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:57:34.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2534" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

• [SLOW TEST:11.567 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":278,"completed":231,"skipped":3775,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:57:37.845: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-f582e4c7-74bf-4bb0-9e6d-42e075447fb5
STEP: Creating a pod to test consume configMaps
Mar  4 09:57:38.343: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b2d24d90-a315-4ae5-b041-ec3cf92c3b09" in namespace "projected-3076" to be "success or failure"
Mar  4 09:57:38.358: INFO: Pod "pod-projected-configmaps-b2d24d90-a315-4ae5-b041-ec3cf92c3b09": Phase="Pending", Reason="", readiness=false. Elapsed: 15.173516ms
Mar  4 09:57:41.081: INFO: Pod "pod-projected-configmaps-b2d24d90-a315-4ae5-b041-ec3cf92c3b09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.738161657s
Mar  4 09:57:43.569: INFO: Pod "pod-projected-configmaps-b2d24d90-a315-4ae5-b041-ec3cf92c3b09": Phase="Pending", Reason="", readiness=false. Elapsed: 5.225594982s
Mar  4 09:57:45.572: INFO: Pod "pod-projected-configmaps-b2d24d90-a315-4ae5-b041-ec3cf92c3b09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.228823348s
STEP: Saw pod success
Mar  4 09:57:45.572: INFO: Pod "pod-projected-configmaps-b2d24d90-a315-4ae5-b041-ec3cf92c3b09" satisfied condition "success or failure"
Mar  4 09:57:45.575: INFO: Trying to get logs from node master3 pod pod-projected-configmaps-b2d24d90-a315-4ae5-b041-ec3cf92c3b09 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  4 09:57:45.613: INFO: Waiting for pod pod-projected-configmaps-b2d24d90-a315-4ae5-b041-ec3cf92c3b09 to disappear
Mar  4 09:57:45.620: INFO: Pod pod-projected-configmaps-b2d24d90-a315-4ae5-b041-ec3cf92c3b09 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:57:45.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3076" for this suite.

• [SLOW TEST:7.936 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":278,"completed":232,"skipped":3786,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:57:45.781: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:57:48.070: INFO: Waiting up to 5m0s for pod "client-envvars-c8605c3c-ff29-4399-a4bc-823b040369a6" in namespace "pods-2768" to be "success or failure"
Mar  4 09:57:48.080: INFO: Pod "client-envvars-c8605c3c-ff29-4399-a4bc-823b040369a6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.146439ms
Mar  4 09:57:50.086: INFO: Pod "client-envvars-c8605c3c-ff29-4399-a4bc-823b040369a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015729898s
STEP: Saw pod success
Mar  4 09:57:50.086: INFO: Pod "client-envvars-c8605c3c-ff29-4399-a4bc-823b040369a6" satisfied condition "success or failure"
Mar  4 09:57:50.090: INFO: Trying to get logs from node master2 pod client-envvars-c8605c3c-ff29-4399-a4bc-823b040369a6 container env3cont: <nil>
STEP: delete the pod
Mar  4 09:57:50.273: INFO: Waiting for pod client-envvars-c8605c3c-ff29-4399-a4bc-823b040369a6 to disappear
Mar  4 09:57:50.277: INFO: Pod client-envvars-c8605c3c-ff29-4399-a4bc-823b040369a6 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:57:50.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2768" for this suite.
•{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":278,"completed":233,"skipped":3815,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:57:50.287: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: executing a command with run --rm and attach with stdin
Mar  4 09:57:50.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 --namespace=kubectl-6810 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Mar  4 09:57:53.264: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Mar  4 09:57:53.264: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:57:55.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6810" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run --rm job should create a job from an image, then delete the job  [Conformance]","total":278,"completed":234,"skipped":3815,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:57:55.279: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:58:03.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8844" for this suite.
STEP: Destroying namespace "nsdeletetest-5782" for this suite.
Mar  4 09:58:03.080: INFO: Namespace nsdeletetest-5782 was already deleted
STEP: Destroying namespace "nsdeletetest-3600" for this suite.

• [SLOW TEST:7.806 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":278,"completed":235,"skipped":3877,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:58:03.085: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  4 09:58:03.858: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  4 09:58:05.869: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912684, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912684, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912684, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912683, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  4 09:58:09.014: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:58:09.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2474" for this suite.
STEP: Destroying namespace "webhook-2474-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.694 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":278,"completed":236,"skipped":3882,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:58:10.779: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:58:33.253: INFO: Container started at 2020-03-04 09:58:12 +0000 UTC, pod became ready at 2020-03-04 09:58:31 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:58:33.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2713" for this suite.

• [SLOW TEST:22.487 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":278,"completed":237,"skipped":3892,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:58:33.267: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-03d581bb-0556-491b-88da-ff04f47e9b7b
STEP: Creating a pod to test consume configMaps
Mar  4 09:58:33.348: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3af69f34-d287-4bef-a8d1-4ceccaf17707" in namespace "projected-8382" to be "success or failure"
Mar  4 09:58:33.363: INFO: Pod "pod-projected-configmaps-3af69f34-d287-4bef-a8d1-4ceccaf17707": Phase="Pending", Reason="", readiness=false. Elapsed: 14.203309ms
Mar  4 09:58:35.366: INFO: Pod "pod-projected-configmaps-3af69f34-d287-4bef-a8d1-4ceccaf17707": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017784887s
STEP: Saw pod success
Mar  4 09:58:35.366: INFO: Pod "pod-projected-configmaps-3af69f34-d287-4bef-a8d1-4ceccaf17707" satisfied condition "success or failure"
Mar  4 09:58:35.526: INFO: Trying to get logs from node master3 pod pod-projected-configmaps-3af69f34-d287-4bef-a8d1-4ceccaf17707 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  4 09:58:35.568: INFO: Waiting for pod pod-projected-configmaps-3af69f34-d287-4bef-a8d1-4ceccaf17707 to disappear
Mar  4 09:58:35.587: INFO: Pod pod-projected-configmaps-3af69f34-d287-4bef-a8d1-4ceccaf17707 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:58:35.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8382" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":278,"completed":238,"skipped":3933,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:58:35.606: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  4 09:58:35.845: INFO: Waiting up to 5m0s for pod "pod-061d8555-169f-42f5-871e-cf829252c387" in namespace "emptydir-7643" to be "success or failure"
Mar  4 09:58:36.008: INFO: Pod "pod-061d8555-169f-42f5-871e-cf829252c387": Phase="Pending", Reason="", readiness=false. Elapsed: 162.808467ms
Mar  4 09:58:38.014: INFO: Pod "pod-061d8555-169f-42f5-871e-cf829252c387": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.168616046s
STEP: Saw pod success
Mar  4 09:58:38.014: INFO: Pod "pod-061d8555-169f-42f5-871e-cf829252c387" satisfied condition "success or failure"
Mar  4 09:58:38.017: INFO: Trying to get logs from node master2 pod pod-061d8555-169f-42f5-871e-cf829252c387 container test-container: <nil>
STEP: delete the pod
Mar  4 09:58:38.046: INFO: Waiting for pod pod-061d8555-169f-42f5-871e-cf829252c387 to disappear
Mar  4 09:58:38.051: INFO: Pod pod-061d8555-169f-42f5-871e-cf829252c387 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:58:38.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7643" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":239,"skipped":4025,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:58:38.060: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating api versions
Mar  4 09:58:38.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 api-versions'
Mar  4 09:58:38.370: INFO: stderr: ""
Mar  4 09:58:38.370: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:58:38.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3268" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":278,"completed":240,"skipped":4035,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:58:38.516: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's command
Mar  4 09:58:38.584: INFO: Waiting up to 5m0s for pod "var-expansion-cdd45937-0ef7-4726-a8f3-dceea21e8b05" in namespace "var-expansion-4084" to be "success or failure"
Mar  4 09:58:38.596: INFO: Pod "var-expansion-cdd45937-0ef7-4726-a8f3-dceea21e8b05": Phase="Pending", Reason="", readiness=false. Elapsed: 12.751837ms
Mar  4 09:58:40.601: INFO: Pod "var-expansion-cdd45937-0ef7-4726-a8f3-dceea21e8b05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017260645s
STEP: Saw pod success
Mar  4 09:58:40.601: INFO: Pod "var-expansion-cdd45937-0ef7-4726-a8f3-dceea21e8b05" satisfied condition "success or failure"
Mar  4 09:58:40.605: INFO: Trying to get logs from node master3 pod var-expansion-cdd45937-0ef7-4726-a8f3-dceea21e8b05 container dapi-container: <nil>
STEP: delete the pod
Mar  4 09:58:40.862: INFO: Waiting for pod var-expansion-cdd45937-0ef7-4726-a8f3-dceea21e8b05 to disappear
Mar  4 09:58:40.868: INFO: Pod var-expansion-cdd45937-0ef7-4726-a8f3-dceea21e8b05 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:58:40.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4084" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":278,"completed":241,"skipped":4071,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:58:41.042: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  4 09:58:42.316: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  4 09:58:44.342: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912722, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912722, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912722, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912722, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  4 09:58:47.368: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:58:47.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7733" for this suite.
STEP: Destroying namespace "webhook-7733-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.022 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":278,"completed":242,"skipped":4090,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:58:48.065: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating secret secrets-3345/secret-test-cb42e677-763d-484a-920d-d3c4b3552a10
STEP: Creating a pod to test consume secrets
Mar  4 09:58:49.279: INFO: Waiting up to 5m0s for pod "pod-configmaps-89d69c8b-beb8-45f1-b5b3-b6b59114ba2e" in namespace "secrets-3345" to be "success or failure"
Mar  4 09:58:49.332: INFO: Pod "pod-configmaps-89d69c8b-beb8-45f1-b5b3-b6b59114ba2e": Phase="Pending", Reason="", readiness=false. Elapsed: 52.773888ms
Mar  4 09:58:51.336: INFO: Pod "pod-configmaps-89d69c8b-beb8-45f1-b5b3-b6b59114ba2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056138983s
Mar  4 09:58:53.342: INFO: Pod "pod-configmaps-89d69c8b-beb8-45f1-b5b3-b6b59114ba2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062635023s
STEP: Saw pod success
Mar  4 09:58:53.342: INFO: Pod "pod-configmaps-89d69c8b-beb8-45f1-b5b3-b6b59114ba2e" satisfied condition "success or failure"
Mar  4 09:58:53.351: INFO: Trying to get logs from node master2 pod pod-configmaps-89d69c8b-beb8-45f1-b5b3-b6b59114ba2e container env-test: <nil>
STEP: delete the pod
Mar  4 09:58:53.601: INFO: Waiting for pod pod-configmaps-89d69c8b-beb8-45f1-b5b3-b6b59114ba2e to disappear
Mar  4 09:58:53.607: INFO: Pod pod-configmaps-89d69c8b-beb8-45f1-b5b3-b6b59114ba2e no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:58:53.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3345" for this suite.

• [SLOW TEST:5.685 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":278,"completed":243,"skipped":4124,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:58:53.750: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-3884/configmap-test-25bb06b8-9efb-4f3f-b277-49a5c55eb228
STEP: Creating a pod to test consume configMaps
Mar  4 09:58:53.834: INFO: Waiting up to 5m0s for pod "pod-configmaps-51301900-b972-442e-b2a3-1e7820300194" in namespace "configmap-3884" to be "success or failure"
Mar  4 09:58:53.842: INFO: Pod "pod-configmaps-51301900-b972-442e-b2a3-1e7820300194": Phase="Pending", Reason="", readiness=false. Elapsed: 7.857844ms
Mar  4 09:58:55.848: INFO: Pod "pod-configmaps-51301900-b972-442e-b2a3-1e7820300194": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013951719s
STEP: Saw pod success
Mar  4 09:58:55.848: INFO: Pod "pod-configmaps-51301900-b972-442e-b2a3-1e7820300194" satisfied condition "success or failure"
Mar  4 09:58:55.856: INFO: Trying to get logs from node master3 pod pod-configmaps-51301900-b972-442e-b2a3-1e7820300194 container env-test: <nil>
STEP: delete the pod
Mar  4 09:58:56.032: INFO: Waiting for pod pod-configmaps-51301900-b972-442e-b2a3-1e7820300194 to disappear
Mar  4 09:58:56.037: INFO: Pod pod-configmaps-51301900-b972-442e-b2a3-1e7820300194 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:58:56.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3884" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":278,"completed":244,"skipped":4142,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:58:56.064: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:58:56.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 create -f - --namespace=kubectl-4911'
Mar  4 09:58:57.027: INFO: stderr: ""
Mar  4 09:58:57.027: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Mar  4 09:58:57.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 create -f - --namespace=kubectl-4911'
Mar  4 09:58:57.624: INFO: stderr: ""
Mar  4 09:58:57.624: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Mar  4 09:58:58.752: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  4 09:58:58.752: INFO: Found 0 / 1
Mar  4 09:58:59.749: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  4 09:58:59.749: INFO: Found 1 / 1
Mar  4 09:58:59.749: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  4 09:58:59.753: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  4 09:58:59.753: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  4 09:58:59.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 describe pod agnhost-master-jn6rg --namespace=kubectl-4911'
Mar  4 09:58:59.869: INFO: stderr: ""
Mar  4 09:58:59.869: INFO: stdout: "Name:         agnhost-master-jn6rg\nNamespace:    kubectl-4911\nPriority:     0\nNode:         master3/10.128.0.4\nStart Time:   Wed, 04 Mar 2020 09:58:57 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 192.168.136.13/32\n              cni.projectcalico.org/podIPs: 192.168.136.13/32\nStatus:       Running\nIP:           192.168.136.13\nIPs:\n  IP:           192.168.136.13\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   docker://cd8144d9471940aef13077a18f58db992cf349874954f492c1eec4cda77a2749\n    Image:          gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 04 Mar 2020 09:58:58 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-p78kv (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-p78kv:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-p78kv\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-4911/agnhost-master-jn6rg to master3\n  Normal  Pulled     1s    kubelet, master3   Container image \"gcr.io/kubernetes-e2e-test-images/agnhost:2.8\" already present on machine\n  Normal  Created    1s    kubelet, master3   Created container agnhost-master\n  Normal  Started    1s    kubelet, master3   Started container agnhost-master\n"
Mar  4 09:58:59.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 describe rc agnhost-master --namespace=kubectl-4911'
Mar  4 09:59:00.034: INFO: stderr: ""
Mar  4 09:59:00.034: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-4911\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-master-jn6rg\n"
Mar  4 09:59:00.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 describe service agnhost-master --namespace=kubectl-4911'
Mar  4 09:59:00.322: INFO: stderr: ""
Mar  4 09:59:00.322: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-4911\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                10.111.136.95\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.136.13:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar  4 09:59:00.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 describe node master1'
Mar  4 09:59:00.603: INFO: stderr: ""
Mar  4 09:59:00.603: INFO: stdout: "Name:               master1\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=master1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.128.0.2/32\n                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.137.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 04 Mar 2020 06:10:29 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  master1\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 04 Mar 2020 09:58:58 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 04 Mar 2020 06:30:24 +0000   Wed, 04 Mar 2020 06:30:24 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 04 Mar 2020 09:58:03 +0000   Wed, 04 Mar 2020 06:10:22 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 04 Mar 2020 09:58:03 +0000   Wed, 04 Mar 2020 06:10:22 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 04 Mar 2020 09:58:03 +0000   Wed, 04 Mar 2020 06:10:22 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 04 Mar 2020 09:58:03 +0000   Wed, 04 Mar 2020 06:30:27 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.128.0.2\n  Hostname:    master1\nCapacity:\n  cpu:                2\n  ephemeral-storage:  41926416Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3880364Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  38639384922\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3777964Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 b5447343d5617cb5f7fd428164927298\n  System UUID:                021202FB-07D2-1BC7-49E4-94837F595538\n  Boot ID:                    197cc7cb-08c2-4ab8-9370-fc4de1b86fc0\n  Kernel Version:             3.10.0-1062.9.1.el7.x86_64\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://19.3.6\n  Kubelet Version:            v1.17.3\n  Kube-Proxy Version:         v1.17.3\nNon-terminated Pods:          (7 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-6znzx                                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         3h28m\n  kube-system                 etcd-master1                                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h48m\n  kube-system                 kube-apiserver-master1                                     250m (12%)    0 (0%)      0 (0%)           0 (0%)         3h48m\n  kube-system                 kube-controller-manager-master1                            200m (10%)    0 (0%)      0 (0%)           0 (0%)         3h48m\n  kube-system                 kube-proxy-vbn4g                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h48m\n  kube-system                 kube-scheduler-master1                                     100m (5%)     0 (0%)      0 (0%)           0 (0%)         3h48m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-002e23000b6e4f18-kv6g7    0 (0%)        0 (0%)      0 (0%)           0 (0%)         60m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                800m (40%)  0 (0%)\n  memory             0 (0%)      0 (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:              <none>\n"
Mar  4 09:59:00.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 describe namespace kubectl-4911'
Mar  4 09:59:00.799: INFO: stderr: ""
Mar  4 09:59:00.799: INFO: stdout: "Name:         kubectl-4911\nLabels:       e2e-framework=kubectl\n              e2e-run=f13001ad-ea64-4255-88a4-b4c27ba3a891\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:59:00.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4911" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":278,"completed":245,"skipped":4153,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:59:00.819: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating cluster-info
Mar  4 09:59:00.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 cluster-info'
Mar  4 09:59:01.017: INFO: stderr: ""
Mar  4 09:59:01.017: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:59:01.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9145" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":278,"completed":246,"skipped":4157,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:59:01.027: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:59:01.071: INFO: Creating deployment "test-recreate-deployment"
Mar  4 09:59:01.101: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar  4 09:59:01.288: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar  4 09:59:01.306: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912741, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912741, loc:(*time.Location)(0x7db7bc0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"test-recreate-deployment-799c574856\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912741, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912741, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Mar  4 09:59:03.311: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar  4 09:59:03.317: INFO: Updating deployment test-recreate-deployment
Mar  4 09:59:03.317: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Mar  4 09:59:03.830: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-6013 /apis/apps/v1/namespaces/deployment-6013/deployments/test-recreate-deployment 03123269-dc31-4b06-984b-b1de49e6a5dc 67329 2 2020-03-04 09:59:01 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006185998 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-03-04 09:59:03 +0000 UTC,LastTransitionTime:2020-03-04 09:59:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-03-04 09:59:03 +0000 UTC,LastTransitionTime:2020-03-04 09:59:01 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar  4 09:59:03.836: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-6013 /apis/apps/v1/namespaces/deployment-6013/replicasets/test-recreate-deployment-5f94c574ff 3d367886-d568-40ba-86c0-a49fafab4398 67326 1 2020-03-04 09:59:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 03123269-dc31-4b06-984b-b1de49e6a5dc 0xc006185d67 0xc006185d68}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006185dc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  4 09:59:03.836: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar  4 09:59:03.836: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-799c574856  deployment-6013 /apis/apps/v1/namespaces/deployment-6013/replicasets/test-recreate-deployment-799c574856 435f8d5d-4f74-415c-9ab8-54a732b82f7b 67317 2 2020-03-04 09:59:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 03123269-dc31-4b06-984b-b1de49e6a5dc 0xc006185e37 0xc006185e38}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 799c574856,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006185ea8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  4 09:59:03.839: INFO: Pod "test-recreate-deployment-5f94c574ff-twd5v" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-twd5v test-recreate-deployment-5f94c574ff- deployment-6013 /api/v1/namespaces/deployment-6013/pods/test-recreate-deployment-5f94c574ff-twd5v df48bdd1-c41d-4dfd-81b4-0ddba6df3d15 67327 0 2020-03-04 09:59:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 3d367886-d568-40ba-86c0-a49fafab4398 0xc00464c327 0xc00464c328}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-x22kp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-x22kp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-x22kp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:59:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:59:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:59:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:59:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.4,PodIP:,StartTime:2020-03-04 09:59:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:59:03.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6013" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":278,"completed":247,"skipped":4168,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:59:03.851: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:59:04.024: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar  4 09:59:11.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 --namespace=crd-publish-openapi-9252 create -f -'
Mar  4 09:59:12.280: INFO: stderr: ""
Mar  4 09:59:12.281: INFO: stdout: "e2e-test-crd-publish-openapi-1169-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar  4 09:59:12.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 --namespace=crd-publish-openapi-9252 delete e2e-test-crd-publish-openapi-1169-crds test-cr'
Mar  4 09:59:12.547: INFO: stderr: ""
Mar  4 09:59:12.547: INFO: stdout: "e2e-test-crd-publish-openapi-1169-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar  4 09:59:12.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 --namespace=crd-publish-openapi-9252 apply -f -'
Mar  4 09:59:13.033: INFO: stderr: ""
Mar  4 09:59:13.033: INFO: stdout: "e2e-test-crd-publish-openapi-1169-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar  4 09:59:13.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 --namespace=crd-publish-openapi-9252 delete e2e-test-crd-publish-openapi-1169-crds test-cr'
Mar  4 09:59:13.267: INFO: stderr: ""
Mar  4 09:59:13.267: INFO: stdout: "e2e-test-crd-publish-openapi-1169-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Mar  4 09:59:13.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 explain e2e-test-crd-publish-openapi-1169-crds'
Mar  4 09:59:13.507: INFO: stderr: ""
Mar  4 09:59:13.507: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1169-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:59:19.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9252" for this suite.

• [SLOW TEST:15.911 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":278,"completed":248,"skipped":4171,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:59:19.761: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 09:59:19.819: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar  4 09:59:24.830: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  4 09:59:24.831: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar  4 09:59:26.834: INFO: Creating deployment "test-rollover-deployment"
Mar  4 09:59:26.843: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar  4 09:59:28.858: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar  4 09:59:28.864: INFO: Ensure that both replica sets have 1 created replica
Mar  4 09:59:28.996: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar  4 09:59:29.003: INFO: Updating deployment test-rollover-deployment
Mar  4 09:59:29.004: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar  4 09:59:31.015: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar  4 09:59:31.022: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar  4 09:59:31.027: INFO: all replica sets need to contain the pod-template-hash label
Mar  4 09:59:31.027: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912767, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912767, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912769, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912766, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  4 09:59:33.056: INFO: all replica sets need to contain the pod-template-hash label
Mar  4 09:59:33.056: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912767, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912767, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912771, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912766, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  4 09:59:35.035: INFO: all replica sets need to contain the pod-template-hash label
Mar  4 09:59:35.035: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912767, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912767, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912771, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912766, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  4 09:59:37.037: INFO: all replica sets need to contain the pod-template-hash label
Mar  4 09:59:37.037: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912767, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912767, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912771, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912766, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  4 09:59:39.041: INFO: all replica sets need to contain the pod-template-hash label
Mar  4 09:59:39.041: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912767, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912767, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912771, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912766, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  4 09:59:41.035: INFO: all replica sets need to contain the pod-template-hash label
Mar  4 09:59:41.035: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912767, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912767, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912771, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912766, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  4 09:59:43.035: INFO: 
Mar  4 09:59:43.035: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Mar  4 09:59:43.045: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-3709 /apis/apps/v1/namespaces/deployment-3709/deployments/test-rollover-deployment 6ba621e8-cfb3-4cb4-aee8-168ca0a18557 67584 2 2020-03-04 09:59:26 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00411af48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-03-04 09:59:27 +0000 UTC,LastTransitionTime:2020-03-04 09:59:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-574d6dfbff" has successfully progressed.,LastUpdateTime:2020-03-04 09:59:41 +0000 UTC,LastTransitionTime:2020-03-04 09:59:26 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  4 09:59:43.052: INFO: New ReplicaSet "test-rollover-deployment-574d6dfbff" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-574d6dfbff  deployment-3709 /apis/apps/v1/namespaces/deployment-3709/replicasets/test-rollover-deployment-574d6dfbff e3749e55-6a82-447d-ad82-183078806f8e 67571 2 2020-03-04 09:59:29 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 6ba621e8-cfb3-4cb4-aee8-168ca0a18557 0xc007174d77 0xc007174d78}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 574d6dfbff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc007174e18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  4 09:59:43.052: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar  4 09:59:43.052: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3709 /apis/apps/v1/namespaces/deployment-3709/replicasets/test-rollover-controller aa6812d5-321e-4a10-a8f7-863f021e0d6b 67580 2 2020-03-04 09:59:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 6ba621e8-cfb3-4cb4-aee8-168ca0a18557 0xc007174c67 0xc007174c68}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc007174cf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  4 09:59:43.052: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-3709 /apis/apps/v1/namespaces/deployment-3709/replicasets/test-rollover-deployment-f6c94f66c 4dde4870-ec7d-4960-aada-cb4c7677f5c2 67523 2 2020-03-04 09:59:26 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 6ba621e8-cfb3-4cb4-aee8-168ca0a18557 0xc007174eb0 0xc007174eb1}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc007174f48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  4 09:59:43.056: INFO: Pod "test-rollover-deployment-574d6dfbff-s2pwc" is available:
&Pod{ObjectMeta:{test-rollover-deployment-574d6dfbff-s2pwc test-rollover-deployment-574d6dfbff- deployment-3709 /api/v1/namespaces/deployment-3709/pods/test-rollover-deployment-574d6dfbff-s2pwc 452ec9c2-8488-4efa-9195-fefb58c799ca 67542 0 2020-03-04 09:59:29 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[cni.projectcalico.org/podIP:192.168.180.21/32 cni.projectcalico.org/podIPs:192.168.180.21/32] [{apps/v1 ReplicaSet test-rollover-deployment-574d6dfbff e3749e55-6a82-447d-ad82-183078806f8e 0xc00411b3b7 0xc00411b3b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5zcpf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5zcpf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5zcpf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:59:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:59:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:59:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 09:59:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.3,PodIP:192.168.180.21,StartTime:2020-03-04 09:59:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-04 09:59:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://3b1542149744acb58462726a83d418ec8b2a329d0eb33252738c25db7bf36416,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.180.21,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 09:59:43.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3709" for this suite.

• [SLOW TEST:23.304 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":278,"completed":249,"skipped":4173,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 09:59:43.066: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Mar  4 09:59:43.115: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 10:00:13.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9787" for this suite.

• [SLOW TEST:29.948 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":278,"completed":250,"skipped":4189,"failed":0}
SSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 10:00:13.015: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:46
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Mar  4 10:00:15.254: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-195674220 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Mar  4 10:00:20.556: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 10:00:20.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1458" for this suite.

• [SLOW TEST:7.593 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should be submitted and removed [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period should be submitted and removed [Conformance]","total":278,"completed":251,"skipped":4193,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 10:00:20.607: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-6010
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6010
STEP: Creating statefulset with conflicting port in namespace statefulset-6010
STEP: Waiting until pod test-pod will start running in namespace statefulset-6010
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6010
Mar  4 10:00:25.077: INFO: Observed stateful pod in namespace: statefulset-6010, name: ss-0, uid: 6b56237d-5e20-4f8b-b166-9e9fc05bbe72, status phase: Pending. Waiting for statefulset controller to delete.
Mar  4 10:00:25.087: INFO: Observed stateful pod in namespace: statefulset-6010, name: ss-0, uid: 6b56237d-5e20-4f8b-b166-9e9fc05bbe72, status phase: Failed. Waiting for statefulset controller to delete.
Mar  4 10:00:25.112: INFO: Observed stateful pod in namespace: statefulset-6010, name: ss-0, uid: 6b56237d-5e20-4f8b-b166-9e9fc05bbe72, status phase: Failed. Waiting for statefulset controller to delete.
Mar  4 10:00:25.247: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6010
STEP: Removing pod with conflicting port in namespace statefulset-6010
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6010 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Mar  4 10:00:29.335: INFO: Deleting all statefulset in ns statefulset-6010
Mar  4 10:00:29.339: INFO: Scaling statefulset ss to 0
Mar  4 10:00:49.351: INFO: Waiting for statefulset status.replicas updated to 0
Mar  4 10:00:49.354: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 10:00:49.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6010" for this suite.

• [SLOW TEST:28.940 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":278,"completed":252,"skipped":4206,"failed":0}
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 10:00:49.547: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override all
Mar  4 10:00:49.601: INFO: Waiting up to 5m0s for pod "client-containers-bd4c700d-cb54-458b-8867-4581555ccb9d" in namespace "containers-225" to be "success or failure"
Mar  4 10:00:49.614: INFO: Pod "client-containers-bd4c700d-cb54-458b-8867-4581555ccb9d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.964931ms
Mar  4 10:00:51.618: INFO: Pod "client-containers-bd4c700d-cb54-458b-8867-4581555ccb9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016143585s
Mar  4 10:00:53.746: INFO: Pod "client-containers-bd4c700d-cb54-458b-8867-4581555ccb9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.145047304s
STEP: Saw pod success
Mar  4 10:00:53.746: INFO: Pod "client-containers-bd4c700d-cb54-458b-8867-4581555ccb9d" satisfied condition "success or failure"
Mar  4 10:00:53.752: INFO: Trying to get logs from node master3 pod client-containers-bd4c700d-cb54-458b-8867-4581555ccb9d container test-container: <nil>
STEP: delete the pod
Mar  4 10:00:53.775: INFO: Waiting for pod client-containers-bd4c700d-cb54-458b-8867-4581555ccb9d to disappear
Mar  4 10:00:53.779: INFO: Pod client-containers-bd4c700d-cb54-458b-8867-4581555ccb9d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 10:00:53.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-225" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":278,"completed":253,"skipped":4207,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 10:00:53.789: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-223.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-223.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-223.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-223.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-223.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-223.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  4 10:00:58.047: INFO: DNS probes using dns-223/dns-test-34f8bd98-ffd3-46c2-809c-b3eb4bc20f25 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 10:00:58.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-223" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":278,"completed":254,"skipped":4222,"failed":0}
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 10:00:58.112: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 10:01:28.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6129" for this suite.
STEP: Destroying namespace "nsdeletetest-2817" for this suite.
Mar  4 10:01:28.607: INFO: Namespace nsdeletetest-2817 was already deleted
STEP: Destroying namespace "nsdeletetest-6954" for this suite.

• [SLOW TEST:30.503 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":278,"completed":255,"skipped":4225,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 10:01:28.615: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Mar  4 10:01:28.798: INFO: Created pod &Pod{ObjectMeta:{dns-3013  dns-3013 /api/v1/namespaces/dns-3013/pods/dns-3013 93b43440-6cc7-42ee-9a38-fdcf57ac12a6 68207 0 2020-03-04 10:01:28 +0000 UTC <nil> <nil> map[] map[] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gn6g6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gn6g6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gn6g6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
STEP: Verifying customized DNS suffix list is configured on pod...
Mar  4 10:01:30.831: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3013 PodName:dns-3013 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  4 10:01:30.831: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Verifying customized DNS server is configured on pod...
Mar  4 10:01:31.247: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3013 PodName:dns-3013 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  4 10:01:31.247: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
Mar  4 10:01:31.496: INFO: Deleting pod dns-3013...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 10:01:31.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3013" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":278,"completed":256,"skipped":4234,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 10:01:31.538: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 10:01:31.609: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar  4 10:01:31.620: INFO: Number of nodes with available pods: 0
Mar  4 10:01:31.620: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Mar  4 10:01:31.786: INFO: Number of nodes with available pods: 0
Mar  4 10:01:31.786: INFO: Node master3 is running more than one daemon pod
Mar  4 10:01:32.813: INFO: Number of nodes with available pods: 0
Mar  4 10:01:32.813: INFO: Node master3 is running more than one daemon pod
Mar  4 10:01:33.829: INFO: Number of nodes with available pods: 1
Mar  4 10:01:33.829: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar  4 10:01:34.010: INFO: Number of nodes with available pods: 1
Mar  4 10:01:34.010: INFO: Number of running nodes: 0, number of available pods: 1
Mar  4 10:01:35.013: INFO: Number of nodes with available pods: 0
Mar  4 10:01:35.013: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar  4 10:01:35.030: INFO: Number of nodes with available pods: 0
Mar  4 10:01:35.030: INFO: Node master3 is running more than one daemon pod
Mar  4 10:01:36.050: INFO: Number of nodes with available pods: 0
Mar  4 10:01:36.050: INFO: Node master3 is running more than one daemon pod
Mar  4 10:01:37.039: INFO: Number of nodes with available pods: 0
Mar  4 10:01:37.039: INFO: Node master3 is running more than one daemon pod
Mar  4 10:01:38.033: INFO: Number of nodes with available pods: 0
Mar  4 10:01:38.033: INFO: Node master3 is running more than one daemon pod
Mar  4 10:01:39.035: INFO: Number of nodes with available pods: 0
Mar  4 10:01:39.035: INFO: Node master3 is running more than one daemon pod
Mar  4 10:01:40.033: INFO: Number of nodes with available pods: 0
Mar  4 10:01:40.033: INFO: Node master3 is running more than one daemon pod
Mar  4 10:01:41.033: INFO: Number of nodes with available pods: 0
Mar  4 10:01:41.033: INFO: Node master3 is running more than one daemon pod
Mar  4 10:01:42.033: INFO: Number of nodes with available pods: 0
Mar  4 10:01:42.033: INFO: Node master3 is running more than one daemon pod
Mar  4 10:01:43.033: INFO: Number of nodes with available pods: 0
Mar  4 10:01:43.033: INFO: Node master3 is running more than one daemon pod
Mar  4 10:01:44.034: INFO: Number of nodes with available pods: 0
Mar  4 10:01:44.034: INFO: Node master3 is running more than one daemon pod
Mar  4 10:01:45.033: INFO: Number of nodes with available pods: 0
Mar  4 10:01:45.033: INFO: Node master3 is running more than one daemon pod
Mar  4 10:01:46.033: INFO: Number of nodes with available pods: 0
Mar  4 10:01:46.033: INFO: Node master3 is running more than one daemon pod
Mar  4 10:01:47.033: INFO: Number of nodes with available pods: 0
Mar  4 10:01:47.033: INFO: Node master3 is running more than one daemon pod
Mar  4 10:01:48.034: INFO: Number of nodes with available pods: 0
Mar  4 10:01:48.034: INFO: Node master3 is running more than one daemon pod
Mar  4 10:01:49.039: INFO: Number of nodes with available pods: 0
Mar  4 10:01:49.039: INFO: Node master3 is running more than one daemon pod
Mar  4 10:01:50.034: INFO: Number of nodes with available pods: 0
Mar  4 10:01:50.034: INFO: Node master3 is running more than one daemon pod
Mar  4 10:01:51.033: INFO: Number of nodes with available pods: 1
Mar  4 10:01:51.033: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3842, will wait for the garbage collector to delete the pods
Mar  4 10:01:51.099: INFO: Deleting DaemonSet.extensions daemon-set took: 6.684137ms
Mar  4 10:01:51.600: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.278773ms
Mar  4 10:01:55.103: INFO: Number of nodes with available pods: 0
Mar  4 10:01:55.103: INFO: Number of running nodes: 0, number of available pods: 0
Mar  4 10:01:55.105: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3842/daemonsets","resourceVersion":"68373"},"items":null}

Mar  4 10:01:55.107: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3842/pods","resourceVersion":"68373"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 10:01:55.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3842" for this suite.

• [SLOW TEST:23.731 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":278,"completed":257,"skipped":4267,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 10:01:55.269: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Mar  4 10:01:55.326: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the sample API server.
Mar  4 10:01:57.058: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Mar  4 10:01:59.260: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912917, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912917, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912917, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718912916, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  4 10:02:03.792: INFO: Waited 225.187868ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 10:02:08.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-1544" for this suite.

• [SLOW TEST:13.264 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]","total":278,"completed":258,"skipped":4271,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 10:02:08.533: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-2165, will wait for the garbage collector to delete the pods
Mar  4 10:02:12.756: INFO: Deleting Job.batch foo took: 106.865374ms
Mar  4 10:02:12.856: INFO: Terminating Job.batch foo pods took: 100.246281ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 10:02:53.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2165" for this suite.

• [SLOW TEST:45.334 seconds]
[sig-apps] Job
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":278,"completed":259,"skipped":4280,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 10:02:53.868: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 10:02:54.052: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-32aa0602-635b-4d54-9aa3-e75d36187e12" in namespace "security-context-test-561" to be "success or failure"
Mar  4 10:02:54.060: INFO: Pod "alpine-nnp-false-32aa0602-635b-4d54-9aa3-e75d36187e12": Phase="Pending", Reason="", readiness=false. Elapsed: 8.049846ms
Mar  4 10:02:56.115: INFO: Pod "alpine-nnp-false-32aa0602-635b-4d54-9aa3-e75d36187e12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.062362295s
Mar  4 10:02:56.115: INFO: Pod "alpine-nnp-false-32aa0602-635b-4d54-9aa3-e75d36187e12" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 10:02:56.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-561" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":260,"skipped":4309,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 10:02:56.746: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting the proxy server
Mar  4 10:02:57.091: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-195674220 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 10:02:57.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9328" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":278,"completed":261,"skipped":4311,"failed":0}
SSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 10:02:57.287: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 10:02:57.334: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 10:02:59.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7921" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":278,"completed":262,"skipped":4315,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 10:02:59.768: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-f75c2cfa-9a35-4302-be99-2597ecdb3062
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-f75c2cfa-9a35-4302-be99-2597ecdb3062
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 10:04:25.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4401" for this suite.

• [SLOW TEST:85.516 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":263,"skipped":4342,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 10:04:25.284: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5211.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5211.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5211.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5211.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5211.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5211.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  4 10:04:29.526: INFO: DNS probes using dns-5211/dns-test-e1b1dd51-d619-44b9-9ea6-8ec67e53efec succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 10:04:29.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5211" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":278,"completed":264,"skipped":4352,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 10:04:29.604: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl label
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1382
STEP: creating the pod
Mar  4 10:04:29.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 create -f - --namespace=kubectl-539'
Mar  4 10:04:30.293: INFO: stderr: ""
Mar  4 10:04:30.293: INFO: stdout: "pod/pause created\n"
Mar  4 10:04:30.293: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar  4 10:04:30.293: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-539" to be "running and ready"
Mar  4 10:04:30.305: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 11.378279ms
Mar  4 10:04:32.310: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.016545657s
Mar  4 10:04:32.310: INFO: Pod "pause" satisfied condition "running and ready"
Mar  4 10:04:32.310: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: adding the label testing-label with value testing-label-value to a pod
Mar  4 10:04:32.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 label pods pause testing-label=testing-label-value --namespace=kubectl-539'
Mar  4 10:04:32.533: INFO: stderr: ""
Mar  4 10:04:32.533: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar  4 10:04:32.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pod pause -L testing-label --namespace=kubectl-539'
Mar  4 10:04:32.773: INFO: stderr: ""
Mar  4 10:04:32.773: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar  4 10:04:32.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 label pods pause testing-label- --namespace=kubectl-539'
Mar  4 10:04:33.024: INFO: stderr: ""
Mar  4 10:04:33.024: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar  4 10:04:33.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pod pause -L testing-label --namespace=kubectl-539'
Mar  4 10:04:33.259: INFO: stderr: ""
Mar  4 10:04:33.259: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
STEP: using delete to clean up resources
Mar  4 10:04:33.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 delete --grace-period=0 --force -f - --namespace=kubectl-539'
Mar  4 10:04:33.520: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  4 10:04:33.520: INFO: stdout: "pod \"pause\" force deleted\n"
Mar  4 10:04:33.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get rc,svc -l name=pause --no-headers --namespace=kubectl-539'
Mar  4 10:04:33.767: INFO: stderr: "No resources found in kubectl-539 namespace.\n"
Mar  4 10:04:33.767: INFO: stdout: ""
Mar  4 10:04:33.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods -l name=pause --namespace=kubectl-539 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  4 10:04:34.057: INFO: stderr: ""
Mar  4 10:04:34.057: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 10:04:34.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-539" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":278,"completed":265,"skipped":4369,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 10:04:34.080: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  4 10:04:34.266: INFO: Waiting up to 5m0s for pod "pod-7a95121c-ffd4-4a89-bc5e-8f8f02e005b5" in namespace "emptydir-1054" to be "success or failure"
Mar  4 10:04:34.273: INFO: Pod "pod-7a95121c-ffd4-4a89-bc5e-8f8f02e005b5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.529915ms
Mar  4 10:04:36.277: INFO: Pod "pod-7a95121c-ffd4-4a89-bc5e-8f8f02e005b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011619554s
Mar  4 10:04:38.281: INFO: Pod "pod-7a95121c-ffd4-4a89-bc5e-8f8f02e005b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014867033s
STEP: Saw pod success
Mar  4 10:04:38.281: INFO: Pod "pod-7a95121c-ffd4-4a89-bc5e-8f8f02e005b5" satisfied condition "success or failure"
Mar  4 10:04:38.284: INFO: Trying to get logs from node master3 pod pod-7a95121c-ffd4-4a89-bc5e-8f8f02e005b5 container test-container: <nil>
STEP: delete the pod
Mar  4 10:04:38.326: INFO: Waiting for pod pod-7a95121c-ffd4-4a89-bc5e-8f8f02e005b5 to disappear
Mar  4 10:04:38.329: INFO: Pod pod-7a95121c-ffd4-4a89-bc5e-8f8f02e005b5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 10:04:38.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1054" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":266,"skipped":4374,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 10:04:38.337: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  4 10:04:39.758: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  4 10:04:42.809: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 10:04:44.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5719" for this suite.
STEP: Destroying namespace "webhook-5719-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.187 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":278,"completed":267,"skipped":4377,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 10:04:45.524: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:330
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Mar  4 10:04:45.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 create -f - --namespace=kubectl-9417'
Mar  4 10:04:46.269: INFO: stderr: ""
Mar  4 10:04:46.269: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  4 10:04:46.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9417'
Mar  4 10:04:46.521: INFO: stderr: ""
Mar  4 10:04:46.522: INFO: stdout: "update-demo-nautilus-t9ss4 update-demo-nautilus-w27q8 "
Mar  4 10:04:46.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods update-demo-nautilus-t9ss4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9417'
Mar  4 10:04:46.773: INFO: stderr: ""
Mar  4 10:04:46.773: INFO: stdout: ""
Mar  4 10:04:46.773: INFO: update-demo-nautilus-t9ss4 is created but not running
Mar  4 10:04:51.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9417'
Mar  4 10:04:52.018: INFO: stderr: ""
Mar  4 10:04:52.018: INFO: stdout: "update-demo-nautilus-t9ss4 update-demo-nautilus-w27q8 "
Mar  4 10:04:52.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods update-demo-nautilus-t9ss4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9417'
Mar  4 10:04:52.114: INFO: stderr: ""
Mar  4 10:04:52.114: INFO: stdout: "true"
Mar  4 10:04:52.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods update-demo-nautilus-t9ss4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9417'
Mar  4 10:04:52.261: INFO: stderr: ""
Mar  4 10:04:52.261: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  4 10:04:52.261: INFO: validating pod update-demo-nautilus-t9ss4
Mar  4 10:04:52.268: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  4 10:04:52.268: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  4 10:04:52.268: INFO: update-demo-nautilus-t9ss4 is verified up and running
Mar  4 10:04:52.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods update-demo-nautilus-w27q8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9417'
Mar  4 10:04:52.379: INFO: stderr: ""
Mar  4 10:04:52.379: INFO: stdout: "true"
Mar  4 10:04:52.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods update-demo-nautilus-w27q8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9417'
Mar  4 10:04:52.517: INFO: stderr: ""
Mar  4 10:04:52.517: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  4 10:04:52.517: INFO: validating pod update-demo-nautilus-w27q8
Mar  4 10:04:52.524: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  4 10:04:52.524: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  4 10:04:52.524: INFO: update-demo-nautilus-w27q8 is verified up and running
STEP: scaling down the replication controller
Mar  4 10:04:52.527: INFO: scanned /root for discovery docs: <nil>
Mar  4 10:04:52.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-9417'
Mar  4 10:04:53.861: INFO: stderr: ""
Mar  4 10:04:53.861: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  4 10:04:53.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9417'
Mar  4 10:04:54.016: INFO: stderr: ""
Mar  4 10:04:54.016: INFO: stdout: "update-demo-nautilus-t9ss4 update-demo-nautilus-w27q8 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar  4 10:04:59.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9417'
Mar  4 10:04:59.251: INFO: stderr: ""
Mar  4 10:04:59.251: INFO: stdout: "update-demo-nautilus-w27q8 "
Mar  4 10:04:59.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods update-demo-nautilus-w27q8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9417'
Mar  4 10:04:59.347: INFO: stderr: ""
Mar  4 10:04:59.347: INFO: stdout: "true"
Mar  4 10:04:59.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods update-demo-nautilus-w27q8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9417'
Mar  4 10:04:59.512: INFO: stderr: ""
Mar  4 10:04:59.512: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  4 10:04:59.513: INFO: validating pod update-demo-nautilus-w27q8
Mar  4 10:04:59.518: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  4 10:04:59.518: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  4 10:04:59.518: INFO: update-demo-nautilus-w27q8 is verified up and running
STEP: scaling up the replication controller
Mar  4 10:04:59.520: INFO: scanned /root for discovery docs: <nil>
Mar  4 10:04:59.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-9417'
Mar  4 10:05:00.773: INFO: stderr: ""
Mar  4 10:05:00.773: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  4 10:05:00.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9417'
Mar  4 10:05:01.022: INFO: stderr: ""
Mar  4 10:05:01.022: INFO: stdout: "update-demo-nautilus-lhtqp update-demo-nautilus-w27q8 "
Mar  4 10:05:01.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods update-demo-nautilus-lhtqp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9417'
Mar  4 10:05:01.269: INFO: stderr: ""
Mar  4 10:05:01.269: INFO: stdout: ""
Mar  4 10:05:01.269: INFO: update-demo-nautilus-lhtqp is created but not running
Mar  4 10:05:06.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9417'
Mar  4 10:05:06.370: INFO: stderr: ""
Mar  4 10:05:06.370: INFO: stdout: "update-demo-nautilus-lhtqp update-demo-nautilus-w27q8 "
Mar  4 10:05:06.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods update-demo-nautilus-lhtqp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9417'
Mar  4 10:05:06.518: INFO: stderr: ""
Mar  4 10:05:06.518: INFO: stdout: "true"
Mar  4 10:05:06.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods update-demo-nautilus-lhtqp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9417'
Mar  4 10:05:06.614: INFO: stderr: ""
Mar  4 10:05:06.614: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  4 10:05:06.614: INFO: validating pod update-demo-nautilus-lhtqp
Mar  4 10:05:06.620: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  4 10:05:06.620: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  4 10:05:06.620: INFO: update-demo-nautilus-lhtqp is verified up and running
Mar  4 10:05:06.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods update-demo-nautilus-w27q8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9417'
Mar  4 10:05:06.765: INFO: stderr: ""
Mar  4 10:05:06.765: INFO: stdout: "true"
Mar  4 10:05:06.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods update-demo-nautilus-w27q8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9417'
Mar  4 10:05:06.858: INFO: stderr: ""
Mar  4 10:05:06.858: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  4 10:05:06.858: INFO: validating pod update-demo-nautilus-w27q8
Mar  4 10:05:06.863: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  4 10:05:06.863: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  4 10:05:06.863: INFO: update-demo-nautilus-w27q8 is verified up and running
STEP: using delete to clean up resources
Mar  4 10:05:06.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 delete --grace-period=0 --force -f - --namespace=kubectl-9417'
Mar  4 10:05:07.050: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  4 10:05:07.050: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  4 10:05:07.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9417'
Mar  4 10:05:07.286: INFO: stderr: "No resources found in kubectl-9417 namespace.\n"
Mar  4 10:05:07.286: INFO: stdout: ""
Mar  4 10:05:07.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195674220 get pods -l name=update-demo --namespace=kubectl-9417 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  4 10:05:07.572: INFO: stderr: ""
Mar  4 10:05:07.572: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 10:05:07.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9417" for this suite.

• [SLOW TEST:22.261 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:328
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":278,"completed":268,"skipped":4389,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 10:05:07.786: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 10:05:19.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9315" for this suite.

• [SLOW TEST:11.491 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":278,"completed":269,"skipped":4389,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 10:05:19.277: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  4 10:05:21.537: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 10:05:21.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5222" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":278,"completed":270,"skipped":4412,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 10:05:21.565: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Mar  4 10:05:21.804: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5474d4e4-95ee-4a2d-9e59-69e548730f74" in namespace "downward-api-9213" to be "success or failure"
Mar  4 10:05:21.821: INFO: Pod "downwardapi-volume-5474d4e4-95ee-4a2d-9e59-69e548730f74": Phase="Pending", Reason="", readiness=false. Elapsed: 17.112121ms
Mar  4 10:05:23.826: INFO: Pod "downwardapi-volume-5474d4e4-95ee-4a2d-9e59-69e548730f74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02145104s
STEP: Saw pod success
Mar  4 10:05:23.826: INFO: Pod "downwardapi-volume-5474d4e4-95ee-4a2d-9e59-69e548730f74" satisfied condition "success or failure"
Mar  4 10:05:23.828: INFO: Trying to get logs from node master3 pod downwardapi-volume-5474d4e4-95ee-4a2d-9e59-69e548730f74 container client-container: <nil>
STEP: delete the pod
Mar  4 10:05:24.028: INFO: Waiting for pod downwardapi-volume-5474d4e4-95ee-4a2d-9e59-69e548730f74 to disappear
Mar  4 10:05:24.039: INFO: Pod downwardapi-volume-5474d4e4-95ee-4a2d-9e59-69e548730f74 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 10:05:24.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9213" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":278,"completed":271,"skipped":4417,"failed":0}
SSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 10:05:24.055: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Mar  4 10:05:26.249: INFO: &Pod{ObjectMeta:{send-events-7b8ab9a3-3042-44df-a463-baf0a2755564  events-7727 /api/v1/namespaces/events-7727/pods/send-events-7b8ab9a3-3042-44df-a463-baf0a2755564 80317926-cfbc-47f1-9177-5f96d805858f 69523 0 2020-03-04 10:05:24 +0000 UTC <nil> <nil> map[name:foo time:102177842] map[cni.projectcalico.org/podIP:192.168.136.29/32 cni.projectcalico.org/podIPs:192.168.136.29/32] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-x5tlp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-x5tlp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-x5tlp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 10:05:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 10:05:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 10:05:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-04 10:05:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.4,PodIP:192.168.136.29,StartTime:2020-03-04 10:05:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-04 10:05:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://68cf266872244a7187807249f517ebe810cab50b0b2ce6fad0d76129fa5a2b8a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.136.29,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Mar  4 10:05:28.253: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Mar  4 10:05:30.264: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 10:05:30.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7727" for this suite.

• [SLOW TEST:6.256 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":278,"completed":272,"skipped":4421,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 10:05:30.311: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-7c790d40-f73d-4909-b935-abbe90ef2daa
STEP: Creating a pod to test consume secrets
Mar  4 10:05:30.797: INFO: Waiting up to 5m0s for pod "pod-secrets-07473e11-9181-4917-bdea-eaf326bdcb24" in namespace "secrets-8327" to be "success or failure"
Mar  4 10:05:30.801: INFO: Pod "pod-secrets-07473e11-9181-4917-bdea-eaf326bdcb24": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051678ms
Mar  4 10:05:32.814: INFO: Pod "pod-secrets-07473e11-9181-4917-bdea-eaf326bdcb24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017050857s
STEP: Saw pod success
Mar  4 10:05:32.814: INFO: Pod "pod-secrets-07473e11-9181-4917-bdea-eaf326bdcb24" satisfied condition "success or failure"
Mar  4 10:05:32.818: INFO: Trying to get logs from node master2 pod pod-secrets-07473e11-9181-4917-bdea-eaf326bdcb24 container secret-volume-test: <nil>
STEP: delete the pod
Mar  4 10:05:33.094: INFO: Waiting for pod pod-secrets-07473e11-9181-4917-bdea-eaf326bdcb24 to disappear
Mar  4 10:05:33.106: INFO: Pod pod-secrets-07473e11-9181-4917-bdea-eaf326bdcb24 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 10:05:33.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8327" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":278,"completed":273,"skipped":4425,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 10:05:33.118: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  4 10:05:35.372: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 10:05:35.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5476" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":278,"completed":274,"skipped":4444,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 10:05:35.551: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-d132079a-f078-402c-b68c-67df9c75c4fe
STEP: Creating a pod to test consume secrets
Mar  4 10:05:35.863: INFO: Waiting up to 5m0s for pod "pod-secrets-b29c9915-43ff-484c-b0e2-53ede9bfe296" in namespace "secrets-3325" to be "success or failure"
Mar  4 10:05:35.869: INFO: Pod "pod-secrets-b29c9915-43ff-484c-b0e2-53ede9bfe296": Phase="Pending", Reason="", readiness=false. Elapsed: 5.651075ms
Mar  4 10:05:38.032: INFO: Pod "pod-secrets-b29c9915-43ff-484c-b0e2-53ede9bfe296": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.16919729s
STEP: Saw pod success
Mar  4 10:05:38.032: INFO: Pod "pod-secrets-b29c9915-43ff-484c-b0e2-53ede9bfe296" satisfied condition "success or failure"
Mar  4 10:05:38.040: INFO: Trying to get logs from node master2 pod pod-secrets-b29c9915-43ff-484c-b0e2-53ede9bfe296 container secret-volume-test: <nil>
STEP: delete the pod
Mar  4 10:05:38.077: INFO: Waiting for pod pod-secrets-b29c9915-43ff-484c-b0e2-53ede9bfe296 to disappear
Mar  4 10:05:38.084: INFO: Pod pod-secrets-b29c9915-43ff-484c-b0e2-53ede9bfe296 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 10:05:38.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3325" for this suite.
STEP: Destroying namespace "secret-namespace-4330" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":278,"completed":275,"skipped":4487,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 10:05:38.109: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Mar  4 10:05:38.341: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 10:05:45.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7957" for this suite.

• [SLOW TEST:7.724 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":278,"completed":276,"skipped":4513,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 10:05:45.833: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override command
Mar  4 10:05:50.112: INFO: Waiting up to 5m0s for pod "client-containers-c327d43c-2b91-4a47-9ddd-1888a0679e3b" in namespace "containers-9567" to be "success or failure"
Mar  4 10:05:50.274: INFO: Pod "client-containers-c327d43c-2b91-4a47-9ddd-1888a0679e3b": Phase="Pending", Reason="", readiness=false. Elapsed: 161.331751ms
Mar  4 10:05:52.284: INFO: Pod "client-containers-c327d43c-2b91-4a47-9ddd-1888a0679e3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.172073661s
Mar  4 10:05:54.288: INFO: Pod "client-containers-c327d43c-2b91-4a47-9ddd-1888a0679e3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.175499785s
STEP: Saw pod success
Mar  4 10:05:54.288: INFO: Pod "client-containers-c327d43c-2b91-4a47-9ddd-1888a0679e3b" satisfied condition "success or failure"
Mar  4 10:05:54.291: INFO: Trying to get logs from node master3 pod client-containers-c327d43c-2b91-4a47-9ddd-1888a0679e3b container test-container: <nil>
STEP: delete the pod
Mar  4 10:05:54.317: INFO: Waiting for pod client-containers-c327d43c-2b91-4a47-9ddd-1888a0679e3b to disappear
Mar  4 10:05:54.321: INFO: Pod client-containers-c327d43c-2b91-4a47-9ddd-1888a0679e3b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 10:05:54.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9567" for this suite.

• [SLOW TEST:8.499 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":278,"completed":277,"skipped":4535,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  4 10:05:54.333: INFO: >>> kubeConfig: /tmp/kubeconfig-195674220
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Mar  4 10:05:54.553: INFO: Waiting up to 5m0s for pod "downward-api-db42610f-87c2-497f-a98f-76fe2d7bcdc6" in namespace "downward-api-7433" to be "success or failure"
Mar  4 10:05:54.570: INFO: Pod "downward-api-db42610f-87c2-497f-a98f-76fe2d7bcdc6": Phase="Pending", Reason="", readiness=false. Elapsed: 17.173792ms
Mar  4 10:05:56.577: INFO: Pod "downward-api-db42610f-87c2-497f-a98f-76fe2d7bcdc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023519327s
STEP: Saw pod success
Mar  4 10:05:56.577: INFO: Pod "downward-api-db42610f-87c2-497f-a98f-76fe2d7bcdc6" satisfied condition "success or failure"
Mar  4 10:05:56.581: INFO: Trying to get logs from node master2 pod downward-api-db42610f-87c2-497f-a98f-76fe2d7bcdc6 container dapi-container: <nil>
STEP: delete the pod
Mar  4 10:05:56.614: INFO: Waiting for pod downward-api-db42610f-87c2-497f-a98f-76fe2d7bcdc6 to disappear
Mar  4 10:05:56.763: INFO: Pod downward-api-db42610f-87c2-497f-a98f-76fe2d7bcdc6 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  4 10:05:56.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7433" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":278,"completed":278,"skipped":4556,"failed":0}
SSSSSSSSSMar  4 10:05:56.781: INFO: Running AfterSuite actions on all nodes
Mar  4 10:05:56.781: INFO: Running AfterSuite actions on node 1
Mar  4 10:05:56.781: INFO: Skipping dumping logs from cluster
{"msg":"Test Suite completed","total":278,"completed":278,"skipped":4565,"failed":0}

Ran 278 of 4843 Specs in 4052.807 seconds
SUCCESS! -- 278 Passed | 0 Failed | 0 Pending | 4565 Skipped
PASS

Ginkgo ran 1 suite in 1h7m35.506431491s
Test Suite Passed
