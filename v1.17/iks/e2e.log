I0217 16:15:17.519551      24 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-855842601
I0217 16:15:17.519574      24 test_context.go:419] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0217 16:15:17.519666      24 e2e.go:109] Starting e2e run "8eaa2618-b2be-4428-99ba-39b01a63fb56" on Ginkgo node 1
{"msg":"Test Suite starting","total":280,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1581956116 - Will randomize all specs
Will run 280 of 4843 specs

Feb 17 16:15:17.530: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 16:15:17.532: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0217 16:15:17.533083      24 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post http://localhost:8099/progress: dial tcp [::1]:8099: connect: connection refused
Feb 17 16:15:17.578: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 17 16:15:17.649: INFO: 22 / 22 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 17 16:15:17.649: INFO: expected 13 pod replicas in namespace 'kube-system', 13 are Running and Ready.
Feb 17 16:15:17.649: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 17 16:15:17.666: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Feb 17 16:15:17.666: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Feb 17 16:15:17.666: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'ibm-master-proxy' (0 seconds elapsed)
Feb 17 16:15:17.666: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Feb 17 16:15:17.666: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-driver-installer' (0 seconds elapsed)
Feb 17 16:15:17.666: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin' (0 seconds elapsed)
Feb 17 16:15:17.666: INFO: e2e test version: v1.17.3
Feb 17 16:15:17.669: INFO: kube-apiserver version: v1.17.3+IKS
Feb 17 16:15:17.669: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 16:15:17.680: INFO: Cluster IP family: ipv4
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:15:17.680: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename watch
Feb 17 16:15:17.760: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Feb 17 16:15:17.795: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1205
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb 17 16:15:17.992: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1205 /api/v1/namespaces/watch-1205/configmaps/e2e-watch-test-resource-version d520b6c2-b43b-4f81-8466-de44a1b1ae32 27012 0 2020-02-17 16:15:17 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 17 16:15:17.993: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1205 /api/v1/namespaces/watch-1205/configmaps/e2e-watch-test-resource-version d520b6c2-b43b-4f81-8466-de44a1b1ae32 27013 0 2020-02-17 16:15:17 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:15:17.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1205" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":280,"completed":1,"skipped":3,"failed":0}
SSSSSSSE0217 16:15:18.019609      24 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post http://localhost:8099/progress: dial tcp [::1]:8099: connect: connection refused
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:15:18.021: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-773
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 17 16:15:18.246: INFO: Waiting up to 5m0s for pod "pod-23da2a5a-e119-4eaa-bd5d-b7ea0a3f863d" in namespace "emptydir-773" to be "success or failure"
Feb 17 16:15:18.257: INFO: Pod "pod-23da2a5a-e119-4eaa-bd5d-b7ea0a3f863d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.515578ms
Feb 17 16:15:20.269: INFO: Pod "pod-23da2a5a-e119-4eaa-bd5d-b7ea0a3f863d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022933138s
Feb 17 16:15:22.280: INFO: Pod "pod-23da2a5a-e119-4eaa-bd5d-b7ea0a3f863d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034671502s
Feb 17 16:15:24.293: INFO: Pod "pod-23da2a5a-e119-4eaa-bd5d-b7ea0a3f863d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046876363s
STEP: Saw pod success
Feb 17 16:15:24.293: INFO: Pod "pod-23da2a5a-e119-4eaa-bd5d-b7ea0a3f863d" satisfied condition "success or failure"
Feb 17 16:15:24.304: INFO: Trying to get logs from node 10.195.53.9 pod pod-23da2a5a-e119-4eaa-bd5d-b7ea0a3f863d container test-container: <nil>
STEP: delete the pod
Feb 17 16:15:24.390: INFO: Waiting for pod pod-23da2a5a-e119-4eaa-bd5d-b7ea0a3f863d to disappear
Feb 17 16:15:24.402: INFO: Pod pod-23da2a5a-e119-4eaa-bd5d-b7ea0a3f863d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:15:24.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-773" for this suite.

• [SLOW TEST:6.408 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":2,"skipped":29,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:15:24.430: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3253
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 16:15:25.002: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 16:15:27.035: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717552924, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717552924, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717552925, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717552924, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 16:15:29.048: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717552924, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717552924, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717552925, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717552924, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 16:15:31.048: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717552924, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717552924, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717552925, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717552924, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 16:15:33.050: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717552924, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717552924, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717552925, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717552924, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 16:15:36.079: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:15:36.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3253" for this suite.
STEP: Destroying namespace "webhook-3253-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.892 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":280,"completed":3,"skipped":43,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:15:36.323: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8377
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 16:15:36.538: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ee6de1fb-4782-4262-90c7-127081ff32d8" in namespace "projected-8377" to be "success or failure"
Feb 17 16:15:36.551: INFO: Pod "downwardapi-volume-ee6de1fb-4782-4262-90c7-127081ff32d8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.912315ms
Feb 17 16:15:38.563: INFO: Pod "downwardapi-volume-ee6de1fb-4782-4262-90c7-127081ff32d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024842349s
Feb 17 16:15:40.576: INFO: Pod "downwardapi-volume-ee6de1fb-4782-4262-90c7-127081ff32d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037800636s
STEP: Saw pod success
Feb 17 16:15:40.576: INFO: Pod "downwardapi-volume-ee6de1fb-4782-4262-90c7-127081ff32d8" satisfied condition "success or failure"
Feb 17 16:15:40.588: INFO: Trying to get logs from node 10.195.53.9 pod downwardapi-volume-ee6de1fb-4782-4262-90c7-127081ff32d8 container client-container: <nil>
STEP: delete the pod
Feb 17 16:15:40.644: INFO: Waiting for pod downwardapi-volume-ee6de1fb-4782-4262-90c7-127081ff32d8 to disappear
Feb 17 16:15:40.654: INFO: Pod downwardapi-volume-ee6de1fb-4782-4262-90c7-127081ff32d8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:15:40.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8377" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":4,"skipped":82,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:15:40.680: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2459
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Feb 17 16:15:40.874: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 16:15:44.606: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:15:59.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2459" for this suite.

• [SLOW TEST:18.399 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":280,"completed":5,"skipped":83,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:15:59.080: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9002
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 16:15:59.347: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"63a976c0-53f3-4e4d-83f2-f74c159d0195", Controller:(*bool)(0xc00657640a), BlockOwnerDeletion:(*bool)(0xc00657640b)}}
Feb 17 16:15:59.368: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"856ad20d-d6ea-4489-913d-cbed95c12cd4", Controller:(*bool)(0xc00650bfba), BlockOwnerDeletion:(*bool)(0xc00650bfbb)}}
Feb 17 16:15:59.392: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"7e32f400-526d-464b-881e-764caea5de21", Controller:(*bool)(0xc006594586), BlockOwnerDeletion:(*bool)(0xc006594587)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:16:04.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9002" for this suite.

• [SLOW TEST:5.373 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":280,"completed":6,"skipped":95,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:16:04.454: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-8965
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 16:16:04.645: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:16:11.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8965" for this suite.

• [SLOW TEST:6.897 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:47
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":280,"completed":7,"skipped":119,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:16:11.351: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6232
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-secret-47zx
STEP: Creating a pod to test atomic-volume-subpath
Feb 17 16:16:11.595: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-47zx" in namespace "subpath-6232" to be "success or failure"
Feb 17 16:16:11.606: INFO: Pod "pod-subpath-test-secret-47zx": Phase="Pending", Reason="", readiness=false. Elapsed: 10.627534ms
Feb 17 16:16:13.618: INFO: Pod "pod-subpath-test-secret-47zx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022787811s
Feb 17 16:16:15.631: INFO: Pod "pod-subpath-test-secret-47zx": Phase="Running", Reason="", readiness=true. Elapsed: 4.035667911s
Feb 17 16:16:17.643: INFO: Pod "pod-subpath-test-secret-47zx": Phase="Running", Reason="", readiness=true. Elapsed: 6.047735879s
Feb 17 16:16:19.655: INFO: Pod "pod-subpath-test-secret-47zx": Phase="Running", Reason="", readiness=true. Elapsed: 8.059953956s
Feb 17 16:16:21.667: INFO: Pod "pod-subpath-test-secret-47zx": Phase="Running", Reason="", readiness=true. Elapsed: 10.071678849s
Feb 17 16:16:23.678: INFO: Pod "pod-subpath-test-secret-47zx": Phase="Running", Reason="", readiness=true. Elapsed: 12.083355249s
Feb 17 16:16:25.692: INFO: Pod "pod-subpath-test-secret-47zx": Phase="Running", Reason="", readiness=true. Elapsed: 14.097098226s
Feb 17 16:16:27.704: INFO: Pod "pod-subpath-test-secret-47zx": Phase="Running", Reason="", readiness=true. Elapsed: 16.10915384s
Feb 17 16:16:29.717: INFO: Pod "pod-subpath-test-secret-47zx": Phase="Running", Reason="", readiness=true. Elapsed: 18.121665306s
Feb 17 16:16:31.729: INFO: Pod "pod-subpath-test-secret-47zx": Phase="Running", Reason="", readiness=true. Elapsed: 20.133694797s
Feb 17 16:16:33.741: INFO: Pod "pod-subpath-test-secret-47zx": Phase="Running", Reason="", readiness=true. Elapsed: 22.145423152s
Feb 17 16:16:35.754: INFO: Pod "pod-subpath-test-secret-47zx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.159323077s
STEP: Saw pod success
Feb 17 16:16:35.755: INFO: Pod "pod-subpath-test-secret-47zx" satisfied condition "success or failure"
Feb 17 16:16:35.767: INFO: Trying to get logs from node 10.195.53.9 pod pod-subpath-test-secret-47zx container test-container-subpath-secret-47zx: <nil>
STEP: delete the pod
Feb 17 16:16:35.826: INFO: Waiting for pod pod-subpath-test-secret-47zx to disappear
Feb 17 16:16:35.836: INFO: Pod pod-subpath-test-secret-47zx no longer exists
STEP: Deleting pod pod-subpath-test-secret-47zx
Feb 17 16:16:35.836: INFO: Deleting pod "pod-subpath-test-secret-47zx" in namespace "subpath-6232"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:16:35.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6232" for this suite.

• [SLOW TEST:24.521 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":280,"completed":8,"skipped":127,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:16:35.873: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7302
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0217 16:16:46.153355      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 17 16:16:46.153: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:16:46.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7302" for this suite.

• [SLOW TEST:10.305 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":280,"completed":9,"skipped":148,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:16:46.178: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6375
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Feb 17 16:16:46.381: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Feb 17 16:17:00.486: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 16:17:04.174: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:17:18.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6375" for this suite.

• [SLOW TEST:32.490 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":280,"completed":10,"skipped":176,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:17:18.668: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9692
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 16:17:22.957: INFO: Waiting up to 5m0s for pod "client-envvars-bf676ab1-373a-4fb7-8dde-0870eb5b6da9" in namespace "pods-9692" to be "success or failure"
Feb 17 16:17:22.968: INFO: Pod "client-envvars-bf676ab1-373a-4fb7-8dde-0870eb5b6da9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.722044ms
Feb 17 16:17:24.980: INFO: Pod "client-envvars-bf676ab1-373a-4fb7-8dde-0870eb5b6da9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02302399s
Feb 17 16:17:26.991: INFO: Pod "client-envvars-bf676ab1-373a-4fb7-8dde-0870eb5b6da9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033935596s
Feb 17 16:17:29.004: INFO: Pod "client-envvars-bf676ab1-373a-4fb7-8dde-0870eb5b6da9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.046902951s
Feb 17 16:17:31.015: INFO: Pod "client-envvars-bf676ab1-373a-4fb7-8dde-0870eb5b6da9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.058567268s
STEP: Saw pod success
Feb 17 16:17:31.015: INFO: Pod "client-envvars-bf676ab1-373a-4fb7-8dde-0870eb5b6da9" satisfied condition "success or failure"
Feb 17 16:17:31.027: INFO: Trying to get logs from node 10.195.53.14 pod client-envvars-bf676ab1-373a-4fb7-8dde-0870eb5b6da9 container env3cont: <nil>
STEP: delete the pod
Feb 17 16:17:31.129: INFO: Waiting for pod client-envvars-bf676ab1-373a-4fb7-8dde-0870eb5b6da9 to disappear
Feb 17 16:17:31.140: INFO: Pod client-envvars-bf676ab1-373a-4fb7-8dde-0870eb5b6da9 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:17:31.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9692" for this suite.

• [SLOW TEST:12.501 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":280,"completed":11,"skipped":177,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:17:31.169: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9415
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 16:17:31.944: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 16:17:33.978: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553051, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553051, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553051, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553051, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 16:17:35.989: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553051, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553051, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553051, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553051, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 16:17:37.988: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553051, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553051, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553051, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553051, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 16:17:41.018: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
Feb 17 16:17:51.077: INFO: Waiting for webhook configuration to be ready...
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:17:51.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9415" for this suite.
STEP: Destroying namespace "webhook-9415-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:20.226 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":280,"completed":12,"skipped":185,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:17:51.396: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8980
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:18:02.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8980" for this suite.

• [SLOW TEST:11.345 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":280,"completed":13,"skipped":195,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:18:02.741: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9806
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 16:18:03.629: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553083, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553083, loc:(*time.Location)(0x7db7bc0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-5f65f8c764\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553083, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553083, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Feb 17 16:18:05.640: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553083, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553083, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553083, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553083, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 16:18:08.670: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 16:18:08.678: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Registering the custom resource webhook via the AdmissionRegistration API
Feb 17 16:18:19.255: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:18:20.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9806" for this suite.
STEP: Destroying namespace "webhook-9806-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:17.474 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":280,"completed":14,"skipped":196,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:18:20.216: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1413
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service multi-endpoint-test in namespace services-1413
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1413 to expose endpoints map[]
Feb 17 16:18:20.449: INFO: Get endpoints failed (6.473041ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Feb 17 16:18:21.457: INFO: successfully validated that service multi-endpoint-test in namespace services-1413 exposes endpoints map[] (1.013861595s elapsed)
STEP: Creating pod pod1 in namespace services-1413
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1413 to expose endpoints map[pod1:[100]]
Feb 17 16:18:23.543: INFO: successfully validated that service multi-endpoint-test in namespace services-1413 exposes endpoints map[pod1:[100]] (2.061860768s elapsed)
STEP: Creating pod pod2 in namespace services-1413
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1413 to expose endpoints map[pod1:[100] pod2:[101]]
Feb 17 16:18:26.675: INFO: successfully validated that service multi-endpoint-test in namespace services-1413 exposes endpoints map[pod1:[100] pod2:[101]] (3.118252578s elapsed)
STEP: Deleting pod pod1 in namespace services-1413
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1413 to expose endpoints map[pod2:[101]]
Feb 17 16:18:27.730: INFO: successfully validated that service multi-endpoint-test in namespace services-1413 exposes endpoints map[pod2:[101]] (1.035107664s elapsed)
STEP: Deleting pod pod2 in namespace services-1413
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1413 to expose endpoints map[]
Feb 17 16:18:27.756: INFO: successfully validated that service multi-endpoint-test in namespace services-1413 exposes endpoints map[] (7.031807ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:18:27.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1413" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:7.620 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":280,"completed":15,"skipped":232,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:18:27.837: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6933
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:18:30.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6933" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":16,"skipped":265,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:18:30.143: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-553
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:330
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the initial replication controller
Feb 17 16:18:30.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 create -f - --namespace=kubectl-553'
Feb 17 16:18:30.698: INFO: stderr: ""
Feb 17 16:18:30.698: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 17 16:18:30.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-553'
Feb 17 16:18:30.817: INFO: stderr: ""
Feb 17 16:18:30.817: INFO: stdout: "update-demo-nautilus-phxn8 update-demo-nautilus-sdn87 "
Feb 17 16:18:30.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods update-demo-nautilus-phxn8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-553'
Feb 17 16:18:30.915: INFO: stderr: ""
Feb 17 16:18:30.915: INFO: stdout: ""
Feb 17 16:18:30.915: INFO: update-demo-nautilus-phxn8 is created but not running
Feb 17 16:18:35.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-553'
Feb 17 16:18:36.037: INFO: stderr: ""
Feb 17 16:18:36.037: INFO: stdout: "update-demo-nautilus-phxn8 update-demo-nautilus-sdn87 "
Feb 17 16:18:36.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods update-demo-nautilus-phxn8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-553'
Feb 17 16:18:36.136: INFO: stderr: ""
Feb 17 16:18:36.136: INFO: stdout: "true"
Feb 17 16:18:36.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods update-demo-nautilus-phxn8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-553'
Feb 17 16:18:36.251: INFO: stderr: ""
Feb 17 16:18:36.251: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 17 16:18:36.251: INFO: validating pod update-demo-nautilus-phxn8
Feb 17 16:18:36.271: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 17 16:18:36.271: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 17 16:18:36.271: INFO: update-demo-nautilus-phxn8 is verified up and running
Feb 17 16:18:36.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods update-demo-nautilus-sdn87 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-553'
Feb 17 16:18:36.381: INFO: stderr: ""
Feb 17 16:18:36.381: INFO: stdout: ""
Feb 17 16:18:36.381: INFO: update-demo-nautilus-sdn87 is created but not running
Feb 17 16:18:41.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-553'
Feb 17 16:18:41.480: INFO: stderr: ""
Feb 17 16:18:41.480: INFO: stdout: "update-demo-nautilus-phxn8 update-demo-nautilus-sdn87 "
Feb 17 16:18:41.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods update-demo-nautilus-phxn8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-553'
Feb 17 16:18:41.580: INFO: stderr: ""
Feb 17 16:18:41.580: INFO: stdout: "true"
Feb 17 16:18:41.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods update-demo-nautilus-phxn8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-553'
Feb 17 16:18:41.690: INFO: stderr: ""
Feb 17 16:18:41.690: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 17 16:18:41.690: INFO: validating pod update-demo-nautilus-phxn8
Feb 17 16:18:41.706: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 17 16:18:41.706: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 17 16:18:41.706: INFO: update-demo-nautilus-phxn8 is verified up and running
Feb 17 16:18:41.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods update-demo-nautilus-sdn87 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-553'
Feb 17 16:18:41.797: INFO: stderr: ""
Feb 17 16:18:41.797: INFO: stdout: "true"
Feb 17 16:18:41.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods update-demo-nautilus-sdn87 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-553'
Feb 17 16:18:41.891: INFO: stderr: ""
Feb 17 16:18:41.891: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 17 16:18:41.891: INFO: validating pod update-demo-nautilus-sdn87
Feb 17 16:18:41.911: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 17 16:18:41.911: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 17 16:18:41.911: INFO: update-demo-nautilus-sdn87 is verified up and running
STEP: rolling-update to new replication controller
Feb 17 16:18:41.913: INFO: scanned /root for discovery docs: <nil>
Feb 17 16:18:41.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-553'
Feb 17 16:19:08.614: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 17 16:19:08.614: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 17 16:19:08.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-553'
Feb 17 16:19:08.725: INFO: stderr: ""
Feb 17 16:19:08.725: INFO: stdout: "update-demo-kitten-s99d7 update-demo-kitten-wq5rf "
Feb 17 16:19:08.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods update-demo-kitten-s99d7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-553'
Feb 17 16:19:08.831: INFO: stderr: ""
Feb 17 16:19:08.831: INFO: stdout: "true"
Feb 17 16:19:08.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods update-demo-kitten-s99d7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-553'
Feb 17 16:19:08.931: INFO: stderr: ""
Feb 17 16:19:08.931: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 17 16:19:08.931: INFO: validating pod update-demo-kitten-s99d7
Feb 17 16:19:08.952: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 17 16:19:08.952: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 17 16:19:08.952: INFO: update-demo-kitten-s99d7 is verified up and running
Feb 17 16:19:08.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods update-demo-kitten-wq5rf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-553'
Feb 17 16:19:09.057: INFO: stderr: ""
Feb 17 16:19:09.057: INFO: stdout: "true"
Feb 17 16:19:09.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods update-demo-kitten-wq5rf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-553'
Feb 17 16:19:09.163: INFO: stderr: ""
Feb 17 16:19:09.163: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 17 16:19:09.163: INFO: validating pod update-demo-kitten-wq5rf
Feb 17 16:19:09.181: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 17 16:19:09.182: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 17 16:19:09.182: INFO: update-demo-kitten-wq5rf is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:19:09.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-553" for this suite.

• [SLOW TEST:39.066 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:328
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should do a rolling update of a replication controller  [Conformance]","total":280,"completed":17,"skipped":312,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:19:09.210: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6223
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's args
Feb 17 16:19:09.423: INFO: Waiting up to 5m0s for pod "var-expansion-76008117-d1f2-4213-9d1e-e04aa48f26e1" in namespace "var-expansion-6223" to be "success or failure"
Feb 17 16:19:09.434: INFO: Pod "var-expansion-76008117-d1f2-4213-9d1e-e04aa48f26e1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.883852ms
Feb 17 16:19:11.446: INFO: Pod "var-expansion-76008117-d1f2-4213-9d1e-e04aa48f26e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022978165s
Feb 17 16:19:13.458: INFO: Pod "var-expansion-76008117-d1f2-4213-9d1e-e04aa48f26e1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034879593s
Feb 17 16:19:15.470: INFO: Pod "var-expansion-76008117-d1f2-4213-9d1e-e04aa48f26e1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.046913684s
Feb 17 16:19:17.482: INFO: Pod "var-expansion-76008117-d1f2-4213-9d1e-e04aa48f26e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.058620772s
STEP: Saw pod success
Feb 17 16:19:17.482: INFO: Pod "var-expansion-76008117-d1f2-4213-9d1e-e04aa48f26e1" satisfied condition "success or failure"
Feb 17 16:19:17.492: INFO: Trying to get logs from node 10.195.53.9 pod var-expansion-76008117-d1f2-4213-9d1e-e04aa48f26e1 container dapi-container: <nil>
STEP: delete the pod
Feb 17 16:19:17.575: INFO: Waiting for pod var-expansion-76008117-d1f2-4213-9d1e-e04aa48f26e1 to disappear
Feb 17 16:19:17.585: INFO: Pod var-expansion-76008117-d1f2-4213-9d1e-e04aa48f26e1 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:19:17.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6223" for this suite.

• [SLOW TEST:8.401 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":280,"completed":18,"skipped":333,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:19:17.612: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1309
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:19:28.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1309" for this suite.

• [SLOW TEST:11.306 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":280,"completed":19,"skipped":374,"failed":0}
SSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:19:28.918: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6585
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Feb 17 16:19:29.135: INFO: Waiting up to 5m0s for pod "downward-api-866fbe0b-97c6-4967-a031-4c0e841ad169" in namespace "downward-api-6585" to be "success or failure"
Feb 17 16:19:29.146: INFO: Pod "downward-api-866fbe0b-97c6-4967-a031-4c0e841ad169": Phase="Pending", Reason="", readiness=false. Elapsed: 10.660212ms
Feb 17 16:19:31.159: INFO: Pod "downward-api-866fbe0b-97c6-4967-a031-4c0e841ad169": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023676287s
Feb 17 16:19:33.170: INFO: Pod "downward-api-866fbe0b-97c6-4967-a031-4c0e841ad169": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035216978s
STEP: Saw pod success
Feb 17 16:19:33.170: INFO: Pod "downward-api-866fbe0b-97c6-4967-a031-4c0e841ad169" satisfied condition "success or failure"
Feb 17 16:19:33.181: INFO: Trying to get logs from node 10.195.53.9 pod downward-api-866fbe0b-97c6-4967-a031-4c0e841ad169 container dapi-container: <nil>
STEP: delete the pod
Feb 17 16:19:33.241: INFO: Waiting for pod downward-api-866fbe0b-97c6-4967-a031-4c0e841ad169 to disappear
Feb 17 16:19:33.251: INFO: Pod downward-api-866fbe0b-97c6-4967-a031-4c0e841ad169 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:19:33.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6585" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":280,"completed":20,"skipped":378,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:19:33.277: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-733
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 16:19:33.472: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 17 16:19:37.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 --namespace=crd-publish-openapi-733 create -f -'
Feb 17 16:19:37.579: INFO: stderr: ""
Feb 17 16:19:37.579: INFO: stdout: "e2e-test-crd-publish-openapi-8908-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 17 16:19:37.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 --namespace=crd-publish-openapi-733 delete e2e-test-crd-publish-openapi-8908-crds test-cr'
Feb 17 16:19:37.773: INFO: stderr: ""
Feb 17 16:19:37.773: INFO: stdout: "e2e-test-crd-publish-openapi-8908-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Feb 17 16:19:37.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 --namespace=crd-publish-openapi-733 apply -f -'
Feb 17 16:19:37.982: INFO: stderr: ""
Feb 17 16:19:37.982: INFO: stdout: "e2e-test-crd-publish-openapi-8908-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 17 16:19:37.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 --namespace=crd-publish-openapi-733 delete e2e-test-crd-publish-openapi-8908-crds test-cr'
Feb 17 16:19:38.126: INFO: stderr: ""
Feb 17 16:19:38.126: INFO: stdout: "e2e-test-crd-publish-openapi-8908-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Feb 17 16:19:38.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 explain e2e-test-crd-publish-openapi-8908-crds'
Feb 17 16:19:38.432: INFO: stderr: ""
Feb 17 16:19:38.432: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8908-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:19:42.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-733" for this suite.

• [SLOW TEST:8.857 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":280,"completed":21,"skipped":378,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:19:42.134: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7866
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 16:19:42.350: INFO: Waiting up to 5m0s for pod "downwardapi-volume-14f38b1d-a9a5-42c8-9fd8-9c8dba2bea0c" in namespace "downward-api-7866" to be "success or failure"
Feb 17 16:19:42.362: INFO: Pod "downwardapi-volume-14f38b1d-a9a5-42c8-9fd8-9c8dba2bea0c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.791019ms
Feb 17 16:19:44.374: INFO: Pod "downwardapi-volume-14f38b1d-a9a5-42c8-9fd8-9c8dba2bea0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023937453s
STEP: Saw pod success
Feb 17 16:19:44.375: INFO: Pod "downwardapi-volume-14f38b1d-a9a5-42c8-9fd8-9c8dba2bea0c" satisfied condition "success or failure"
Feb 17 16:19:44.387: INFO: Trying to get logs from node 10.195.53.9 pod downwardapi-volume-14f38b1d-a9a5-42c8-9fd8-9c8dba2bea0c container client-container: <nil>
STEP: delete the pod
Feb 17 16:19:44.444: INFO: Waiting for pod downwardapi-volume-14f38b1d-a9a5-42c8-9fd8-9c8dba2bea0c to disappear
Feb 17 16:19:44.460: INFO: Pod downwardapi-volume-14f38b1d-a9a5-42c8-9fd8-9c8dba2bea0c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:19:44.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7866" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":22,"skipped":385,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:19:44.488: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7729
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Feb 17 16:19:44.715: INFO: Pod name pod-release: Found 0 pods out of 1
Feb 17 16:19:49.760: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:19:49.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7729" for this suite.

• [SLOW TEST:5.336 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":280,"completed":23,"skipped":411,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:19:49.824: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6538
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 16:19:50.087: INFO: Create a RollingUpdate DaemonSet
Feb 17 16:19:50.098: INFO: Check that daemon pods launch on every node of the cluster
Feb 17 16:19:50.118: INFO: Number of nodes with available pods: 0
Feb 17 16:19:50.118: INFO: Node 10.195.53.14 is running more than one daemon pod
Feb 17 16:19:51.143: INFO: Number of nodes with available pods: 0
Feb 17 16:19:51.143: INFO: Node 10.195.53.14 is running more than one daemon pod
Feb 17 16:19:52.143: INFO: Number of nodes with available pods: 0
Feb 17 16:19:52.143: INFO: Node 10.195.53.14 is running more than one daemon pod
Feb 17 16:19:53.143: INFO: Number of nodes with available pods: 0
Feb 17 16:19:53.143: INFO: Node 10.195.53.14 is running more than one daemon pod
Feb 17 16:19:54.144: INFO: Number of nodes with available pods: 0
Feb 17 16:19:54.144: INFO: Node 10.195.53.14 is running more than one daemon pod
Feb 17 16:19:55.142: INFO: Number of nodes with available pods: 0
Feb 17 16:19:55.142: INFO: Node 10.195.53.14 is running more than one daemon pod
Feb 17 16:19:56.143: INFO: Number of nodes with available pods: 0
Feb 17 16:19:56.143: INFO: Node 10.195.53.14 is running more than one daemon pod
Feb 17 16:19:57.143: INFO: Number of nodes with available pods: 1
Feb 17 16:19:57.143: INFO: Node 10.195.53.14 is running more than one daemon pod
Feb 17 16:19:58.143: INFO: Number of nodes with available pods: 1
Feb 17 16:19:58.143: INFO: Node 10.195.53.14 is running more than one daemon pod
Feb 17 16:19:59.145: INFO: Number of nodes with available pods: 1
Feb 17 16:19:59.145: INFO: Node 10.195.53.14 is running more than one daemon pod
Feb 17 16:20:00.143: INFO: Number of nodes with available pods: 2
Feb 17 16:20:00.143: INFO: Node 10.195.53.14 is running more than one daemon pod
Feb 17 16:20:01.146: INFO: Number of nodes with available pods: 2
Feb 17 16:20:01.146: INFO: Node 10.195.53.14 is running more than one daemon pod
Feb 17 16:20:02.143: INFO: Number of nodes with available pods: 3
Feb 17 16:20:02.143: INFO: Number of running nodes: 3, number of available pods: 3
Feb 17 16:20:02.143: INFO: Update the DaemonSet to trigger a rollout
Feb 17 16:20:02.163: INFO: Updating DaemonSet daemon-set
Feb 17 16:20:12.209: INFO: Roll back the DaemonSet before rollout is complete
Feb 17 16:20:12.228: INFO: Updating DaemonSet daemon-set
Feb 17 16:20:12.228: INFO: Make sure DaemonSet rollback is complete
Feb 17 16:20:12.241: INFO: Wrong image for pod: daemon-set-xpwtl. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 17 16:20:12.241: INFO: Pod daemon-set-xpwtl is not available
Feb 17 16:20:13.262: INFO: Wrong image for pod: daemon-set-xpwtl. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 17 16:20:13.262: INFO: Pod daemon-set-xpwtl is not available
Feb 17 16:20:14.262: INFO: Wrong image for pod: daemon-set-xpwtl. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 17 16:20:14.262: INFO: Pod daemon-set-xpwtl is not available
Feb 17 16:20:15.264: INFO: Pod daemon-set-ck898 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6538, will wait for the garbage collector to delete the pods
Feb 17 16:20:15.379: INFO: Deleting DaemonSet.extensions daemon-set took: 24.175336ms
Feb 17 16:20:15.479: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.175418ms
Feb 17 16:21:51.391: INFO: Number of nodes with available pods: 0
Feb 17 16:21:51.391: INFO: Number of running nodes: 0, number of available pods: 0
Feb 17 16:21:51.401: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6538/daemonsets","resourceVersion":"29446"},"items":null}

Feb 17 16:21:51.412: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6538/pods","resourceVersion":"29446"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:21:51.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6538" for this suite.

• [SLOW TEST:121.653 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":280,"completed":24,"skipped":433,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:21:51.478: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-795
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-052d6846-f65f-4d10-a8e8-77d1a4013910
STEP: Creating a pod to test consume secrets
Feb 17 16:21:51.703: INFO: Waiting up to 5m0s for pod "pod-secrets-518e477d-a03f-4ebc-a0bc-abc4d4e25bd7" in namespace "secrets-795" to be "success or failure"
Feb 17 16:21:51.713: INFO: Pod "pod-secrets-518e477d-a03f-4ebc-a0bc-abc4d4e25bd7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.522026ms
Feb 17 16:21:53.728: INFO: Pod "pod-secrets-518e477d-a03f-4ebc-a0bc-abc4d4e25bd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02516342s
STEP: Saw pod success
Feb 17 16:21:53.728: INFO: Pod "pod-secrets-518e477d-a03f-4ebc-a0bc-abc4d4e25bd7" satisfied condition "success or failure"
Feb 17 16:21:53.739: INFO: Trying to get logs from node 10.195.53.9 pod pod-secrets-518e477d-a03f-4ebc-a0bc-abc4d4e25bd7 container secret-volume-test: <nil>
STEP: delete the pod
Feb 17 16:21:53.829: INFO: Waiting for pod pod-secrets-518e477d-a03f-4ebc-a0bc-abc4d4e25bd7 to disappear
Feb 17 16:21:53.839: INFO: Pod pod-secrets-518e477d-a03f-4ebc-a0bc-abc4d4e25bd7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:21:53.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-795" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":25,"skipped":455,"failed":0}
SSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:21:53.868: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7641
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-zxtpt in namespace proxy-7641
I0217 16:21:54.101736      24 runners.go:189] Created replication controller with name: proxy-service-zxtpt, namespace: proxy-7641, replica count: 1
I0217 16:21:55.154522      24 runners.go:189] proxy-service-zxtpt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0217 16:21:56.154702      24 runners.go:189] proxy-service-zxtpt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0217 16:21:57.154963      24 runners.go:189] proxy-service-zxtpt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0217 16:21:58.155222      24 runners.go:189] proxy-service-zxtpt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0217 16:21:59.155529      24 runners.go:189] proxy-service-zxtpt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0217 16:22:00.155837      24 runners.go:189] proxy-service-zxtpt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0217 16:22:01.156155      24 runners.go:189] proxy-service-zxtpt Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 17 16:22:01.163: INFO: setup took 7.102135858s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb 17 16:22:01.183: INFO: (0) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname2/proxy/: bar (200; 20.617123ms)
Feb 17 16:22:01.184: INFO: (0) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">... (200; 20.737898ms)
Feb 17 16:22:01.184: INFO: (0) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 20.980424ms)
Feb 17 16:22:01.184: INFO: (0) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 21.394219ms)
Feb 17 16:22:01.185: INFO: (0) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 21.978167ms)
Feb 17 16:22:01.185: INFO: (0) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 22.184156ms)
Feb 17 16:22:01.186: INFO: (0) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">test<... (200; 23.115082ms)
Feb 17 16:22:01.186: INFO: (0) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/rewriteme">test</a> (200; 23.438445ms)
Feb 17 16:22:01.187: INFO: (0) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname2/proxy/: bar (200; 24.312119ms)
Feb 17 16:22:01.189: INFO: (0) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname1/proxy/: foo (200; 25.499931ms)
Feb 17 16:22:01.189: INFO: (0) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname1/proxy/: foo (200; 25.853888ms)
Feb 17 16:22:01.189: INFO: (0) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/tlsrewritem... (200; 26.30579ms)
Feb 17 16:22:01.191: INFO: (0) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:460/proxy/: tls baz (200; 28.509086ms)
Feb 17 16:22:01.192: INFO: (0) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname1/proxy/: tls baz (200; 28.776841ms)
Feb 17 16:22:01.192: INFO: (0) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:462/proxy/: tls qux (200; 28.482262ms)
Feb 17 16:22:01.192: INFO: (0) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname2/proxy/: tls qux (200; 28.697698ms)
Feb 17 16:22:01.205: INFO: (1) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 13.338808ms)
Feb 17 16:22:01.209: INFO: (1) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/rewriteme">test</a> (200; 16.988469ms)
Feb 17 16:22:01.209: INFO: (1) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 17.053651ms)
Feb 17 16:22:01.209: INFO: (1) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">... (200; 17.099975ms)
Feb 17 16:22:01.209: INFO: (1) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 17.330774ms)
Feb 17 16:22:01.210: INFO: (1) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname1/proxy/: tls baz (200; 17.625402ms)
Feb 17 16:22:01.210: INFO: (1) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:460/proxy/: tls baz (200; 17.846559ms)
Feb 17 16:22:01.210: INFO: (1) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">test<... (200; 17.699738ms)
Feb 17 16:22:01.210: INFO: (1) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 17.761261ms)
Feb 17 16:22:01.210: INFO: (1) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:462/proxy/: tls qux (200; 18.281013ms)
Feb 17 16:22:01.210: INFO: (1) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/tlsrewritem... (200; 18.62271ms)
Feb 17 16:22:01.212: INFO: (1) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname2/proxy/: tls qux (200; 20.338047ms)
Feb 17 16:22:01.213: INFO: (1) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname2/proxy/: bar (200; 20.81822ms)
Feb 17 16:22:01.213: INFO: (1) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname1/proxy/: foo (200; 20.820163ms)
Feb 17 16:22:01.213: INFO: (1) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname1/proxy/: foo (200; 20.901448ms)
Feb 17 16:22:01.213: INFO: (1) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname2/proxy/: bar (200; 20.79123ms)
Feb 17 16:22:01.226: INFO: (2) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">test<... (200; 12.763763ms)
Feb 17 16:22:01.231: INFO: (2) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 17.384694ms)
Feb 17 16:22:01.231: INFO: (2) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/tlsrewritem... (200; 17.366546ms)
Feb 17 16:22:01.231: INFO: (2) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname2/proxy/: bar (200; 17.708691ms)
Feb 17 16:22:01.231: INFO: (2) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/rewriteme">test</a> (200; 17.747169ms)
Feb 17 16:22:01.231: INFO: (2) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 17.998441ms)
Feb 17 16:22:01.232: INFO: (2) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:460/proxy/: tls baz (200; 18.327471ms)
Feb 17 16:22:01.232: INFO: (2) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 18.265691ms)
Feb 17 16:22:01.232: INFO: (2) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:462/proxy/: tls qux (200; 18.584499ms)
Feb 17 16:22:01.232: INFO: (2) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">... (200; 18.448719ms)
Feb 17 16:22:01.233: INFO: (2) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 20.212385ms)
Feb 17 16:22:01.235: INFO: (2) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname2/proxy/: bar (200; 21.268912ms)
Feb 17 16:22:01.235: INFO: (2) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname2/proxy/: tls qux (200; 21.49331ms)
Feb 17 16:22:01.235: INFO: (2) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname1/proxy/: tls baz (200; 21.617103ms)
Feb 17 16:22:01.235: INFO: (2) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname1/proxy/: foo (200; 21.535333ms)
Feb 17 16:22:01.235: INFO: (2) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname1/proxy/: foo (200; 22.142563ms)
Feb 17 16:22:01.248: INFO: (3) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 12.914262ms)
Feb 17 16:22:01.253: INFO: (3) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:462/proxy/: tls qux (200; 18.161334ms)
Feb 17 16:22:01.253: INFO: (3) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">test<... (200; 18.04294ms)
Feb 17 16:22:01.254: INFO: (3) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname1/proxy/: tls baz (200; 18.086998ms)
Feb 17 16:22:01.254: INFO: (3) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 17.899503ms)
Feb 17 16:22:01.254: INFO: (3) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">... (200; 17.792275ms)
Feb 17 16:22:01.254: INFO: (3) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/rewriteme">test</a> (200; 18.329249ms)
Feb 17 16:22:01.254: INFO: (3) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 18.293091ms)
Feb 17 16:22:01.254: INFO: (3) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 18.291152ms)
Feb 17 16:22:01.254: INFO: (3) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/tlsrewritem... (200; 18.340094ms)
Feb 17 16:22:01.254: INFO: (3) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:460/proxy/: tls baz (200; 18.481852ms)
Feb 17 16:22:01.256: INFO: (3) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname1/proxy/: foo (200; 20.315926ms)
Feb 17 16:22:01.256: INFO: (3) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname2/proxy/: bar (200; 20.548392ms)
Feb 17 16:22:01.256: INFO: (3) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname2/proxy/: bar (200; 20.371899ms)
Feb 17 16:22:01.256: INFO: (3) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname1/proxy/: foo (200; 20.954159ms)
Feb 17 16:22:01.257: INFO: (3) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname2/proxy/: tls qux (200; 21.161285ms)
Feb 17 16:22:01.270: INFO: (4) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">test<... (200; 12.894863ms)
Feb 17 16:22:01.276: INFO: (4) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 18.316832ms)
Feb 17 16:22:01.276: INFO: (4) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 18.448883ms)
Feb 17 16:22:01.276: INFO: (4) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname2/proxy/: bar (200; 19.514584ms)
Feb 17 16:22:01.276: INFO: (4) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 19.35274ms)
Feb 17 16:22:01.276: INFO: (4) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 18.564221ms)
Feb 17 16:22:01.276: INFO: (4) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">... (200; 18.655145ms)
Feb 17 16:22:01.276: INFO: (4) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/tlsrewritem... (200; 19.100048ms)
Feb 17 16:22:01.276: INFO: (4) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:462/proxy/: tls qux (200; 19.023644ms)
Feb 17 16:22:01.276: INFO: (4) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:460/proxy/: tls baz (200; 18.95028ms)
Feb 17 16:22:01.277: INFO: (4) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/rewriteme">test</a> (200; 19.329767ms)
Feb 17 16:22:01.277: INFO: (4) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname1/proxy/: foo (200; 19.860751ms)
Feb 17 16:22:01.278: INFO: (4) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname1/proxy/: tls baz (200; 19.596661ms)
Feb 17 16:22:01.278: INFO: (4) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname1/proxy/: foo (200; 21.209936ms)
Feb 17 16:22:01.278: INFO: (4) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname2/proxy/: tls qux (200; 21.310664ms)
Feb 17 16:22:01.278: INFO: (4) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname2/proxy/: bar (200; 20.611158ms)
Feb 17 16:22:01.292: INFO: (5) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">test<... (200; 13.100623ms)
Feb 17 16:22:01.296: INFO: (5) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname2/proxy/: bar (200; 17.124768ms)
Feb 17 16:22:01.296: INFO: (5) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 17.055649ms)
Feb 17 16:22:01.296: INFO: (5) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/rewriteme">test</a> (200; 17.109147ms)
Feb 17 16:22:01.296: INFO: (5) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">... (200; 17.005854ms)
Feb 17 16:22:01.296: INFO: (5) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 17.133331ms)
Feb 17 16:22:01.296: INFO: (5) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/tlsrewritem... (200; 17.411693ms)
Feb 17 16:22:01.296: INFO: (5) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 17.523944ms)
Feb 17 16:22:01.296: INFO: (5) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:460/proxy/: tls baz (200; 17.407656ms)
Feb 17 16:22:01.296: INFO: (5) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:462/proxy/: tls qux (200; 17.422312ms)
Feb 17 16:22:01.298: INFO: (5) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 19.842079ms)
Feb 17 16:22:01.299: INFO: (5) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname1/proxy/: foo (200; 20.759709ms)
Feb 17 16:22:01.299: INFO: (5) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname2/proxy/: tls qux (200; 20.801727ms)
Feb 17 16:22:01.300: INFO: (5) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname1/proxy/: foo (200; 21.131283ms)
Feb 17 16:22:01.300: INFO: (5) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname1/proxy/: tls baz (200; 21.25468ms)
Feb 17 16:22:01.300: INFO: (5) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname2/proxy/: bar (200; 21.138043ms)
Feb 17 16:22:01.313: INFO: (6) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:460/proxy/: tls baz (200; 13.386611ms)
Feb 17 16:22:01.318: INFO: (6) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname2/proxy/: tls qux (200; 17.763909ms)
Feb 17 16:22:01.318: INFO: (6) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 17.52033ms)
Feb 17 16:22:01.318: INFO: (6) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 17.942713ms)
Feb 17 16:22:01.318: INFO: (6) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">test<... (200; 17.94344ms)
Feb 17 16:22:01.318: INFO: (6) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/tlsrewritem... (200; 18.036132ms)
Feb 17 16:22:01.318: INFO: (6) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:462/proxy/: tls qux (200; 18.16304ms)
Feb 17 16:22:01.318: INFO: (6) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 18.250573ms)
Feb 17 16:22:01.318: INFO: (6) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/rewriteme">test</a> (200; 18.374905ms)
Feb 17 16:22:01.318: INFO: (6) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">... (200; 18.388411ms)
Feb 17 16:22:01.318: INFO: (6) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 18.480238ms)
Feb 17 16:22:01.321: INFO: (6) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname2/proxy/: bar (200; 20.667195ms)
Feb 17 16:22:01.321: INFO: (6) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname1/proxy/: foo (200; 21.167345ms)
Feb 17 16:22:01.321: INFO: (6) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname1/proxy/: foo (200; 21.411271ms)
Feb 17 16:22:01.321: INFO: (6) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname1/proxy/: tls baz (200; 21.348437ms)
Feb 17 16:22:01.321: INFO: (6) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname2/proxy/: bar (200; 21.295273ms)
Feb 17 16:22:01.337: INFO: (7) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 14.824461ms)
Feb 17 16:22:01.352: INFO: (7) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 29.228237ms)
Feb 17 16:22:01.352: INFO: (7) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">... (200; 29.36568ms)
Feb 17 16:22:01.352: INFO: (7) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/rewriteme">test</a> (200; 29.769664ms)
Feb 17 16:22:01.352: INFO: (7) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 30.742859ms)
Feb 17 16:22:01.352: INFO: (7) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname1/proxy/: tls baz (200; 30.713229ms)
Feb 17 16:22:01.353: INFO: (7) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:462/proxy/: tls qux (200; 30.207731ms)
Feb 17 16:22:01.353: INFO: (7) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/tlsrewritem... (200; 30.707695ms)
Feb 17 16:22:01.353: INFO: (7) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname2/proxy/: tls qux (200; 30.929718ms)
Feb 17 16:22:01.353: INFO: (7) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">test<... (200; 31.37534ms)
Feb 17 16:22:01.353: INFO: (7) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname2/proxy/: bar (200; 30.728023ms)
Feb 17 16:22:01.353: INFO: (7) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:460/proxy/: tls baz (200; 30.814086ms)
Feb 17 16:22:01.353: INFO: (7) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 31.131118ms)
Feb 17 16:22:01.355: INFO: (7) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname1/proxy/: foo (200; 32.981054ms)
Feb 17 16:22:01.355: INFO: (7) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname2/proxy/: bar (200; 32.928306ms)
Feb 17 16:22:01.355: INFO: (7) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname1/proxy/: foo (200; 32.469068ms)
Feb 17 16:22:01.368: INFO: (8) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:462/proxy/: tls qux (200; 12.421051ms)
Feb 17 16:22:01.372: INFO: (8) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:460/proxy/: tls baz (200; 16.376758ms)
Feb 17 16:22:01.372: INFO: (8) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 16.283149ms)
Feb 17 16:22:01.372: INFO: (8) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 16.517918ms)
Feb 17 16:22:01.372: INFO: (8) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 16.933979ms)
Feb 17 16:22:01.372: INFO: (8) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 16.63895ms)
Feb 17 16:22:01.372: INFO: (8) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname1/proxy/: tls baz (200; 17.011397ms)
Feb 17 16:22:01.373: INFO: (8) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/tlsrewritem... (200; 17.072924ms)
Feb 17 16:22:01.373: INFO: (8) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">... (200; 17.034055ms)
Feb 17 16:22:01.373: INFO: (8) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">test<... (200; 16.907542ms)
Feb 17 16:22:01.373: INFO: (8) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/rewriteme">test</a> (200; 17.147649ms)
Feb 17 16:22:01.375: INFO: (8) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname1/proxy/: foo (200; 19.613228ms)
Feb 17 16:22:01.375: INFO: (8) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname2/proxy/: bar (200; 19.574039ms)
Feb 17 16:22:01.375: INFO: (8) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname1/proxy/: foo (200; 19.737528ms)
Feb 17 16:22:01.375: INFO: (8) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname2/proxy/: tls qux (200; 20.059697ms)
Feb 17 16:22:01.376: INFO: (8) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname2/proxy/: bar (200; 20.057519ms)
Feb 17 16:22:01.389: INFO: (9) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">... (200; 12.94775ms)
Feb 17 16:22:01.393: INFO: (9) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 17.153575ms)
Feb 17 16:22:01.394: INFO: (9) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 17.805708ms)
Feb 17 16:22:01.394: INFO: (9) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname1/proxy/: foo (200; 18.029322ms)
Feb 17 16:22:01.394: INFO: (9) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">test<... (200; 18.160302ms)
Feb 17 16:22:01.394: INFO: (9) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/rewriteme">test</a> (200; 18.190904ms)
Feb 17 16:22:01.394: INFO: (9) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/tlsrewritem... (200; 18.572616ms)
Feb 17 16:22:01.394: INFO: (9) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:462/proxy/: tls qux (200; 18.507137ms)
Feb 17 16:22:01.394: INFO: (9) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 18.566665ms)
Feb 17 16:22:01.394: INFO: (9) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:460/proxy/: tls baz (200; 18.338959ms)
Feb 17 16:22:01.396: INFO: (9) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 19.499712ms)
Feb 17 16:22:01.396: INFO: (9) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname2/proxy/: bar (200; 19.737225ms)
Feb 17 16:22:01.396: INFO: (9) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname2/proxy/: tls qux (200; 20.383471ms)
Feb 17 16:22:01.396: INFO: (9) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname2/proxy/: bar (200; 20.467043ms)
Feb 17 16:22:01.396: INFO: (9) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname1/proxy/: foo (200; 20.510351ms)
Feb 17 16:22:01.397: INFO: (9) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname1/proxy/: tls baz (200; 20.488409ms)
Feb 17 16:22:01.411: INFO: (10) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:462/proxy/: tls qux (200; 14.018954ms)
Feb 17 16:22:01.414: INFO: (10) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">... (200; 17.469635ms)
Feb 17 16:22:01.415: INFO: (10) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname2/proxy/: tls qux (200; 17.585694ms)
Feb 17 16:22:01.415: INFO: (10) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 18.624634ms)
Feb 17 16:22:01.415: INFO: (10) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:460/proxy/: tls baz (200; 18.558703ms)
Feb 17 16:22:01.415: INFO: (10) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/tlsrewritem... (200; 18.703918ms)
Feb 17 16:22:01.415: INFO: (10) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">test<... (200; 18.560029ms)
Feb 17 16:22:01.415: INFO: (10) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 18.450948ms)
Feb 17 16:22:01.415: INFO: (10) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 18.570861ms)
Feb 17 16:22:01.415: INFO: (10) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/rewriteme">test</a> (200; 18.453224ms)
Feb 17 16:22:01.416: INFO: (10) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 18.721519ms)
Feb 17 16:22:01.418: INFO: (10) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname2/proxy/: bar (200; 21.793708ms)
Feb 17 16:22:01.419: INFO: (10) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname1/proxy/: tls baz (200; 21.771689ms)
Feb 17 16:22:01.419: INFO: (10) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname2/proxy/: bar (200; 21.556943ms)
Feb 17 16:22:01.419: INFO: (10) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname1/proxy/: foo (200; 22.052115ms)
Feb 17 16:22:01.419: INFO: (10) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname1/proxy/: foo (200; 22.186247ms)
Feb 17 16:22:01.431: INFO: (11) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">test<... (200; 12.369683ms)
Feb 17 16:22:01.437: INFO: (11) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 16.954468ms)
Feb 17 16:22:01.437: INFO: (11) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/tlsrewritem... (200; 17.256051ms)
Feb 17 16:22:01.437: INFO: (11) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 17.60691ms)
Feb 17 16:22:01.438: INFO: (11) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:462/proxy/: tls qux (200; 18.171873ms)
Feb 17 16:22:01.438: INFO: (11) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">... (200; 18.015516ms)
Feb 17 16:22:01.438: INFO: (11) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 18.41539ms)
Feb 17 16:22:01.438: INFO: (11) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 17.96002ms)
Feb 17 16:22:01.438: INFO: (11) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:460/proxy/: tls baz (200; 17.988535ms)
Feb 17 16:22:01.438: INFO: (11) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/rewriteme">test</a> (200; 18.406139ms)
Feb 17 16:22:01.441: INFO: (11) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname2/proxy/: bar (200; 21.16781ms)
Feb 17 16:22:01.441: INFO: (11) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname1/proxy/: foo (200; 21.255396ms)
Feb 17 16:22:01.442: INFO: (11) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname1/proxy/: tls baz (200; 21.424434ms)
Feb 17 16:22:01.442: INFO: (11) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname1/proxy/: foo (200; 22.541514ms)
Feb 17 16:22:01.442: INFO: (11) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname2/proxy/: bar (200; 22.496297ms)
Feb 17 16:22:01.442: INFO: (11) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname2/proxy/: tls qux (200; 23.015403ms)
Feb 17 16:22:01.455: INFO: (12) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 13.255658ms)
Feb 17 16:22:01.460: INFO: (12) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:462/proxy/: tls qux (200; 17.501467ms)
Feb 17 16:22:01.460: INFO: (12) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 17.572828ms)
Feb 17 16:22:01.460: INFO: (12) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">test<... (200; 17.742347ms)
Feb 17 16:22:01.460: INFO: (12) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname2/proxy/: bar (200; 17.845105ms)
Feb 17 16:22:01.461: INFO: (12) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">... (200; 18.527509ms)
Feb 17 16:22:01.461: INFO: (12) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/rewriteme">test</a> (200; 18.522841ms)
Feb 17 16:22:01.461: INFO: (12) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:460/proxy/: tls baz (200; 18.400577ms)
Feb 17 16:22:01.461: INFO: (12) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 18.262571ms)
Feb 17 16:22:01.461: INFO: (12) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/tlsrewritem... (200; 18.674877ms)
Feb 17 16:22:01.463: INFO: (12) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 20.408995ms)
Feb 17 16:22:01.464: INFO: (12) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname2/proxy/: bar (200; 21.67873ms)
Feb 17 16:22:01.465: INFO: (12) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname1/proxy/: foo (200; 21.987121ms)
Feb 17 16:22:01.465: INFO: (12) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname1/proxy/: tls baz (200; 22.081577ms)
Feb 17 16:22:01.465: INFO: (12) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname1/proxy/: foo (200; 22.181068ms)
Feb 17 16:22:01.465: INFO: (12) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname2/proxy/: tls qux (200; 22.077655ms)
Feb 17 16:22:01.478: INFO: (13) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/rewriteme">test</a> (200; 12.675843ms)
Feb 17 16:22:01.483: INFO: (13) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 17.558262ms)
Feb 17 16:22:01.483: INFO: (13) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:460/proxy/: tls baz (200; 17.40092ms)
Feb 17 16:22:01.483: INFO: (13) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:462/proxy/: tls qux (200; 17.242007ms)
Feb 17 16:22:01.483: INFO: (13) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">test<... (200; 17.508593ms)
Feb 17 16:22:01.483: INFO: (13) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 17.390464ms)
Feb 17 16:22:01.483: INFO: (13) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname2/proxy/: tls qux (200; 17.51642ms)
Feb 17 16:22:01.483: INFO: (13) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">... (200; 17.202244ms)
Feb 17 16:22:01.483: INFO: (13) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 17.382356ms)
Feb 17 16:22:01.483: INFO: (13) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/tlsrewritem... (200; 17.604215ms)
Feb 17 16:22:01.483: INFO: (13) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 17.237301ms)
Feb 17 16:22:01.487: INFO: (13) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname1/proxy/: foo (200; 21.774667ms)
Feb 17 16:22:01.487: INFO: (13) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname1/proxy/: foo (200; 21.446052ms)
Feb 17 16:22:01.487: INFO: (13) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname1/proxy/: tls baz (200; 21.853787ms)
Feb 17 16:22:01.487: INFO: (13) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname2/proxy/: bar (200; 21.582265ms)
Feb 17 16:22:01.487: INFO: (13) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname2/proxy/: bar (200; 21.828935ms)
Feb 17 16:22:01.499: INFO: (14) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">test<... (200; 12.241879ms)
Feb 17 16:22:01.504: INFO: (14) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/tlsrewritem... (200; 16.36882ms)
Feb 17 16:22:01.504: INFO: (14) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 16.579118ms)
Feb 17 16:22:01.504: INFO: (14) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 16.572786ms)
Feb 17 16:22:01.504: INFO: (14) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 16.571593ms)
Feb 17 16:22:01.504: INFO: (14) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:460/proxy/: tls baz (200; 16.746234ms)
Feb 17 16:22:01.505: INFO: (14) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/rewriteme">test</a> (200; 17.152379ms)
Feb 17 16:22:01.505: INFO: (14) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 17.262927ms)
Feb 17 16:22:01.505: INFO: (14) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">... (200; 17.448412ms)
Feb 17 16:22:01.505: INFO: (14) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:462/proxy/: tls qux (200; 17.27135ms)
Feb 17 16:22:01.507: INFO: (14) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname2/proxy/: bar (200; 19.423288ms)
Feb 17 16:22:01.507: INFO: (14) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname2/proxy/: bar (200; 19.432327ms)
Feb 17 16:22:01.507: INFO: (14) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname2/proxy/: tls qux (200; 19.555213ms)
Feb 17 16:22:01.507: INFO: (14) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname1/proxy/: tls baz (200; 19.395491ms)
Feb 17 16:22:01.507: INFO: (14) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname1/proxy/: foo (200; 19.484112ms)
Feb 17 16:22:01.507: INFO: (14) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname1/proxy/: foo (200; 19.441056ms)
Feb 17 16:22:01.522: INFO: (15) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 14.465909ms)
Feb 17 16:22:01.522: INFO: (15) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/tlsrewritem... (200; 14.651711ms)
Feb 17 16:22:01.524: INFO: (15) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">test<... (200; 16.670084ms)
Feb 17 16:22:01.524: INFO: (15) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname1/proxy/: tls baz (200; 16.74153ms)
Feb 17 16:22:01.524: INFO: (15) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/rewriteme">test</a> (200; 16.978576ms)
Feb 17 16:22:01.524: INFO: (15) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 17.028827ms)
Feb 17 16:22:01.524: INFO: (15) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:462/proxy/: tls qux (200; 17.188154ms)
Feb 17 16:22:01.524: INFO: (15) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 17.419309ms)
Feb 17 16:22:01.524: INFO: (15) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:460/proxy/: tls baz (200; 17.461804ms)
Feb 17 16:22:01.525: INFO: (15) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 17.522992ms)
Feb 17 16:22:01.525: INFO: (15) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">... (200; 17.865507ms)
Feb 17 16:22:01.527: INFO: (15) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname2/proxy/: bar (200; 19.930613ms)
Feb 17 16:22:01.528: INFO: (15) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname1/proxy/: foo (200; 20.527397ms)
Feb 17 16:22:01.528: INFO: (15) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname2/proxy/: tls qux (200; 20.559573ms)
Feb 17 16:22:01.528: INFO: (15) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname2/proxy/: bar (200; 20.607253ms)
Feb 17 16:22:01.528: INFO: (15) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname1/proxy/: foo (200; 20.800813ms)
Feb 17 16:22:01.541: INFO: (16) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 12.87189ms)
Feb 17 16:22:01.546: INFO: (16) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname2/proxy/: tls qux (200; 17.629351ms)
Feb 17 16:22:01.546: INFO: (16) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">... (200; 17.689922ms)
Feb 17 16:22:01.546: INFO: (16) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:460/proxy/: tls baz (200; 17.834069ms)
Feb 17 16:22:01.546: INFO: (16) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">test<... (200; 17.479694ms)
Feb 17 16:22:01.546: INFO: (16) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/rewriteme">test</a> (200; 17.770538ms)
Feb 17 16:22:01.546: INFO: (16) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 18.006356ms)
Feb 17 16:22:01.546: INFO: (16) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 18.113145ms)
Feb 17 16:22:01.546: INFO: (16) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 18.118943ms)
Feb 17 16:22:01.546: INFO: (16) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/tlsrewritem... (200; 18.392486ms)
Feb 17 16:22:01.547: INFO: (16) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:462/proxy/: tls qux (200; 18.795793ms)
Feb 17 16:22:01.549: INFO: (16) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname2/proxy/: bar (200; 21.068142ms)
Feb 17 16:22:01.549: INFO: (16) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname1/proxy/: foo (200; 21.133678ms)
Feb 17 16:22:01.549: INFO: (16) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname1/proxy/: foo (200; 20.999966ms)
Feb 17 16:22:01.549: INFO: (16) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname2/proxy/: bar (200; 21.347485ms)
Feb 17 16:22:01.550: INFO: (16) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname1/proxy/: tls baz (200; 21.475022ms)
Feb 17 16:22:01.562: INFO: (17) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:462/proxy/: tls qux (200; 11.972601ms)
Feb 17 16:22:01.566: INFO: (17) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname1/proxy/: foo (200; 16.495763ms)
Feb 17 16:22:01.566: INFO: (17) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 16.192206ms)
Feb 17 16:22:01.567: INFO: (17) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/tlsrewritem... (200; 17.335078ms)
Feb 17 16:22:01.567: INFO: (17) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/rewriteme">test</a> (200; 17.333365ms)
Feb 17 16:22:01.567: INFO: (17) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 17.396055ms)
Feb 17 16:22:01.567: INFO: (17) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:460/proxy/: tls baz (200; 17.476186ms)
Feb 17 16:22:01.567: INFO: (17) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">... (200; 17.405522ms)
Feb 17 16:22:01.568: INFO: (17) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">test<... (200; 17.607069ms)
Feb 17 16:22:01.568: INFO: (17) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 17.563073ms)
Feb 17 16:22:01.569: INFO: (17) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 18.457455ms)
Feb 17 16:22:01.569: INFO: (17) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname1/proxy/: foo (200; 19.175312ms)
Feb 17 16:22:01.570: INFO: (17) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname2/proxy/: bar (200; 19.689502ms)
Feb 17 16:22:01.570: INFO: (17) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname1/proxy/: tls baz (200; 19.623402ms)
Feb 17 16:22:01.570: INFO: (17) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname2/proxy/: bar (200; 19.511486ms)
Feb 17 16:22:01.570: INFO: (17) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname2/proxy/: tls qux (200; 19.703494ms)
Feb 17 16:22:01.583: INFO: (18) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 12.951892ms)
Feb 17 16:22:01.587: INFO: (18) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 16.972377ms)
Feb 17 16:22:01.587: INFO: (18) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/rewriteme">test</a> (200; 16.986813ms)
Feb 17 16:22:01.587: INFO: (18) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">... (200; 17.103626ms)
Feb 17 16:22:01.587: INFO: (18) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 17.138731ms)
Feb 17 16:22:01.587: INFO: (18) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">test<... (200; 17.347083ms)
Feb 17 16:22:01.588: INFO: (18) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 17.464772ms)
Feb 17 16:22:01.588: INFO: (18) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:462/proxy/: tls qux (200; 17.594886ms)
Feb 17 16:22:01.588: INFO: (18) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:460/proxy/: tls baz (200; 17.532245ms)
Feb 17 16:22:01.588: INFO: (18) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/tlsrewritem... (200; 17.876513ms)
Feb 17 16:22:01.588: INFO: (18) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname1/proxy/: foo (200; 18.176046ms)
Feb 17 16:22:01.591: INFO: (18) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname1/proxy/: foo (200; 20.705852ms)
Feb 17 16:22:01.591: INFO: (18) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname2/proxy/: bar (200; 21.009859ms)
Feb 17 16:22:01.591: INFO: (18) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname2/proxy/: tls qux (200; 20.995877ms)
Feb 17 16:22:01.591: INFO: (18) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname2/proxy/: bar (200; 20.989892ms)
Feb 17 16:22:01.591: INFO: (18) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname1/proxy/: tls baz (200; 21.154266ms)
Feb 17 16:22:01.603: INFO: (19) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:460/proxy/: tls baz (200; 11.922452ms)
Feb 17 16:22:01.607: INFO: (19) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:443/proxy/tlsrewritem... (200; 15.773511ms)
Feb 17 16:22:01.607: INFO: (19) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">... (200; 15.723071ms)
Feb 17 16:22:01.608: INFO: (19) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname2/proxy/: bar (200; 16.899751ms)
Feb 17 16:22:01.608: INFO: (19) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 17.108137ms)
Feb 17 16:22:01.611: INFO: (19) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:1080/proxy/rewriteme">test<... (200; 19.930824ms)
Feb 17 16:22:01.611: INFO: (19) /api/v1/namespaces/proxy-7641/pods/http:proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 19.888479ms)
Feb 17 16:22:01.611: INFO: (19) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname1/proxy/: tls baz (200; 20.203365ms)
Feb 17 16:22:01.612: INFO: (19) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:162/proxy/: bar (200; 20.064743ms)
Feb 17 16:22:01.612: INFO: (19) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8:160/proxy/: foo (200; 20.286094ms)
Feb 17 16:22:01.612: INFO: (19) /api/v1/namespaces/proxy-7641/pods/https:proxy-service-zxtpt-9bzh8:462/proxy/: tls qux (200; 20.523562ms)
Feb 17 16:22:01.612: INFO: (19) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname1/proxy/: foo (200; 20.572028ms)
Feb 17 16:22:01.612: INFO: (19) /api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/: <a href="/api/v1/namespaces/proxy-7641/pods/proxy-service-zxtpt-9bzh8/proxy/rewriteme">test</a> (200; 20.66473ms)
Feb 17 16:22:01.613: INFO: (19) /api/v1/namespaces/proxy-7641/services/https:proxy-service-zxtpt:tlsportname2/proxy/: tls qux (200; 21.232311ms)
Feb 17 16:22:01.614: INFO: (19) /api/v1/namespaces/proxy-7641/services/proxy-service-zxtpt:portname1/proxy/: foo (200; 22.849436ms)
Feb 17 16:22:01.615: INFO: (19) /api/v1/namespaces/proxy-7641/services/http:proxy-service-zxtpt:portname2/proxy/: bar (200; 23.915587ms)
STEP: deleting ReplicationController proxy-service-zxtpt in namespace proxy-7641, will wait for the garbage collector to delete the pods
Feb 17 16:22:01.700: INFO: Deleting ReplicationController proxy-service-zxtpt took: 24.395498ms
Feb 17 16:22:01.800: INFO: Terminating ReplicationController proxy-service-zxtpt pods took: 100.220836ms
[AfterEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:22:03.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7641" for this suite.

• [SLOW TEST:9.760 seconds]
[sig-network] Proxy
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":280,"completed":26,"skipped":462,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:22:03.629: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6400
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: executing a command with run --rm and attach with stdin
Feb 17 16:22:03.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 --namespace=kubectl-6400 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Feb 17 16:22:05.775: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Feb 17 16:22:05.775: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:22:07.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6400" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run --rm job should create a job from an image, then delete the job  [Conformance]","total":280,"completed":27,"skipped":486,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:22:07.820: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1523
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-c0ec27c5-254e-4b52-a5b4-c6c8ed14bc03 in namespace container-probe-1523
Feb 17 16:22:10.061: INFO: Started pod busybox-c0ec27c5-254e-4b52-a5b4-c6c8ed14bc03 in namespace container-probe-1523
STEP: checking the pod's current state and verifying that restartCount is present
Feb 17 16:22:10.071: INFO: Initial restart count of pod busybox-c0ec27c5-254e-4b52-a5b4-c6c8ed14bc03 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:26:11.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1523" for this suite.

• [SLOW TEST:243.937 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":28,"skipped":496,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:26:11.759: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7456
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 17 16:26:11.973: INFO: Waiting up to 5m0s for pod "pod-26dd620c-bbfd-4f44-bb1c-7f06401d5805" in namespace "emptydir-7456" to be "success or failure"
Feb 17 16:26:11.984: INFO: Pod "pod-26dd620c-bbfd-4f44-bb1c-7f06401d5805": Phase="Pending", Reason="", readiness=false. Elapsed: 10.855216ms
Feb 17 16:26:13.995: INFO: Pod "pod-26dd620c-bbfd-4f44-bb1c-7f06401d5805": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022106736s
Feb 17 16:26:16.008: INFO: Pod "pod-26dd620c-bbfd-4f44-bb1c-7f06401d5805": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034813735s
Feb 17 16:26:18.020: INFO: Pod "pod-26dd620c-bbfd-4f44-bb1c-7f06401d5805": Phase="Running", Reason="", readiness=true. Elapsed: 6.047242695s
Feb 17 16:26:20.038: INFO: Pod "pod-26dd620c-bbfd-4f44-bb1c-7f06401d5805": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.064952423s
STEP: Saw pod success
Feb 17 16:26:20.038: INFO: Pod "pod-26dd620c-bbfd-4f44-bb1c-7f06401d5805" satisfied condition "success or failure"
Feb 17 16:26:20.050: INFO: Trying to get logs from node 10.195.53.9 pod pod-26dd620c-bbfd-4f44-bb1c-7f06401d5805 container test-container: <nil>
STEP: delete the pod
Feb 17 16:26:20.136: INFO: Waiting for pod pod-26dd620c-bbfd-4f44-bb1c-7f06401d5805 to disappear
Feb 17 16:26:20.146: INFO: Pod pod-26dd620c-bbfd-4f44-bb1c-7f06401d5805 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:26:20.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7456" for this suite.

• [SLOW TEST:8.413 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":29,"skipped":570,"failed":0}
SSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:26:20.172: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7776
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Feb 17 16:26:20.399: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Feb 17 16:26:27.512: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:26:27.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7776" for this suite.

• [SLOW TEST:7.376 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":280,"completed":30,"skipped":574,"failed":0}
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:26:27.548: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5973
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-5973/configmap-test-1c68690e-c3d9-4003-8d3d-554b27bb132c
STEP: Creating a pod to test consume configMaps
Feb 17 16:26:27.771: INFO: Waiting up to 5m0s for pod "pod-configmaps-68447d60-dc74-4bda-b2bd-0a5b7d77e667" in namespace "configmap-5973" to be "success or failure"
Feb 17 16:26:27.784: INFO: Pod "pod-configmaps-68447d60-dc74-4bda-b2bd-0a5b7d77e667": Phase="Pending", Reason="", readiness=false. Elapsed: 12.65733ms
Feb 17 16:26:29.795: INFO: Pod "pod-configmaps-68447d60-dc74-4bda-b2bd-0a5b7d77e667": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024388472s
STEP: Saw pod success
Feb 17 16:26:29.796: INFO: Pod "pod-configmaps-68447d60-dc74-4bda-b2bd-0a5b7d77e667" satisfied condition "success or failure"
Feb 17 16:26:29.807: INFO: Trying to get logs from node 10.195.53.9 pod pod-configmaps-68447d60-dc74-4bda-b2bd-0a5b7d77e667 container env-test: <nil>
STEP: delete the pod
Feb 17 16:26:29.867: INFO: Waiting for pod pod-configmaps-68447d60-dc74-4bda-b2bd-0a5b7d77e667 to disappear
Feb 17 16:26:29.877: INFO: Pod pod-configmaps-68447d60-dc74-4bda-b2bd-0a5b7d77e667 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:26:29.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5973" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":280,"completed":31,"skipped":574,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:26:29.904: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6934
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Feb 17 16:26:30.099: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:26:50.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6934" for this suite.

• [SLOW TEST:20.839 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":280,"completed":32,"skipped":575,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:26:50.744: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7102
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service nodeport-test with type=NodePort in namespace services-7102
STEP: creating replication controller nodeport-test in namespace services-7102
I0217 16:26:50.987959      24 runners.go:189] Created replication controller with name: nodeport-test, namespace: services-7102, replica count: 2
Feb 17 16:26:54.038: INFO: Creating new exec pod
I0217 16:26:54.038455      24 runners.go:189] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 17 16:26:59.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=services-7102 execpod5jbrp -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Feb 17 16:26:59.326: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 17 16:26:59.326: INFO: stdout: ""
Feb 17 16:26:59.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=services-7102 execpod5jbrp -- /bin/sh -x -c nc -zv -t -w 2 172.21.250.197 80'
Feb 17 16:26:59.574: INFO: stderr: "+ nc -zv -t -w 2 172.21.250.197 80\nConnection to 172.21.250.197 80 port [tcp/http] succeeded!\n"
Feb 17 16:26:59.574: INFO: stdout: ""
Feb 17 16:26:59.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=services-7102 execpod5jbrp -- /bin/sh -x -c nc -zv -t -w 2 10.195.53.47 32569'
Feb 17 16:26:59.847: INFO: stderr: "+ nc -zv -t -w 2 10.195.53.47 32569\nConnection to 10.195.53.47 32569 port [tcp/32569] succeeded!\n"
Feb 17 16:26:59.847: INFO: stdout: ""
Feb 17 16:26:59.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=services-7102 execpod5jbrp -- /bin/sh -x -c nc -zv -t -w 2 10.195.53.9 32569'
Feb 17 16:27:00.136: INFO: stderr: "+ nc -zv -t -w 2 10.195.53.9 32569\nConnection to 10.195.53.9 32569 port [tcp/32569] succeeded!\n"
Feb 17 16:27:00.136: INFO: stdout: ""
Feb 17 16:27:00.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=services-7102 execpod5jbrp -- /bin/sh -x -c nc -zv -t -w 2 135.90.88.62 32569'
Feb 17 16:27:00.385: INFO: stderr: "+ nc -zv -t -w 2 135.90.88.62 32569\nConnection to 135.90.88.62 32569 port [tcp/32569] succeeded!\n"
Feb 17 16:27:00.385: INFO: stdout: ""
Feb 17 16:27:00.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=services-7102 execpod5jbrp -- /bin/sh -x -c nc -zv -t -w 2 135.90.88.53 32569'
Feb 17 16:27:00.800: INFO: stderr: "+ nc -zv -t -w 2 135.90.88.53 32569\nConnection to 135.90.88.53 32569 port [tcp/32569] succeeded!\n"
Feb 17 16:27:00.800: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:27:00.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7102" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:10.084 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":280,"completed":33,"skipped":584,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:27:00.828: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3752
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-605b9d44-4158-4323-a657-bbe8dbeab4f4
STEP: Creating a pod to test consume configMaps
Feb 17 16:27:01.056: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a35cdeef-beb7-4b7e-8f5a-83f23da36d32" in namespace "projected-3752" to be "success or failure"
Feb 17 16:27:01.066: INFO: Pod "pod-projected-configmaps-a35cdeef-beb7-4b7e-8f5a-83f23da36d32": Phase="Pending", Reason="", readiness=false. Elapsed: 10.136337ms
Feb 17 16:27:03.078: INFO: Pod "pod-projected-configmaps-a35cdeef-beb7-4b7e-8f5a-83f23da36d32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022658006s
Feb 17 16:27:05.090: INFO: Pod "pod-projected-configmaps-a35cdeef-beb7-4b7e-8f5a-83f23da36d32": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034016422s
Feb 17 16:27:07.102: INFO: Pod "pod-projected-configmaps-a35cdeef-beb7-4b7e-8f5a-83f23da36d32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046192087s
STEP: Saw pod success
Feb 17 16:27:07.102: INFO: Pod "pod-projected-configmaps-a35cdeef-beb7-4b7e-8f5a-83f23da36d32" satisfied condition "success or failure"
Feb 17 16:27:07.113: INFO: Trying to get logs from node 10.195.53.14 pod pod-projected-configmaps-a35cdeef-beb7-4b7e-8f5a-83f23da36d32 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 16:27:07.207: INFO: Waiting for pod pod-projected-configmaps-a35cdeef-beb7-4b7e-8f5a-83f23da36d32 to disappear
Feb 17 16:27:07.218: INFO: Pod pod-projected-configmaps-a35cdeef-beb7-4b7e-8f5a-83f23da36d32 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:27:07.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3752" for this suite.

• [SLOW TEST:6.419 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":34,"skipped":594,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:27:07.247: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6042
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 16:27:07.903: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 16:27:10.967: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 16:27:10.975: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3284-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:27:12.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6042" for this suite.
STEP: Destroying namespace "webhook-6042-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.229 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":280,"completed":35,"skipped":599,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:27:12.477: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4142
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 17 16:27:12.697: INFO: Waiting up to 5m0s for pod "pod-eb10ae53-7047-4396-907c-9cbd3db91e47" in namespace "emptydir-4142" to be "success or failure"
Feb 17 16:27:12.708: INFO: Pod "pod-eb10ae53-7047-4396-907c-9cbd3db91e47": Phase="Pending", Reason="", readiness=false. Elapsed: 11.291299ms
Feb 17 16:27:14.719: INFO: Pod "pod-eb10ae53-7047-4396-907c-9cbd3db91e47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022664188s
STEP: Saw pod success
Feb 17 16:27:14.719: INFO: Pod "pod-eb10ae53-7047-4396-907c-9cbd3db91e47" satisfied condition "success or failure"
Feb 17 16:27:14.730: INFO: Trying to get logs from node 10.195.53.9 pod pod-eb10ae53-7047-4396-907c-9cbd3db91e47 container test-container: <nil>
STEP: delete the pod
Feb 17 16:27:14.794: INFO: Waiting for pod pod-eb10ae53-7047-4396-907c-9cbd3db91e47 to disappear
Feb 17 16:27:14.804: INFO: Pod pod-eb10ae53-7047-4396-907c-9cbd3db91e47 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:27:14.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4142" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":36,"skipped":622,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:27:14.840: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3338
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with configMap that has name projected-configmap-test-upd-a16be7d5-608c-4590-9662-bd13deda0142
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-a16be7d5-608c-4590-9662-bd13deda0142
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:28:32.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3338" for this suite.

• [SLOW TEST:77.241 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":37,"skipped":622,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:28:32.081: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-920
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
STEP: reading a file in the container
Feb 17 16:28:34.850: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-920 pod-service-account-6a23d754-ddcd-4f55-9fac-fccffd689795 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Feb 17 16:28:35.151: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-920 pod-service-account-6a23d754-ddcd-4f55-9fac-fccffd689795 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Feb 17 16:28:35.403: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-920 pod-service-account-6a23d754-ddcd-4f55-9fac-fccffd689795 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:28:35.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-920" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":280,"completed":38,"skipped":650,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:28:35.701: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7705
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 17 16:28:40.040: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 17 16:28:40.057: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 17 16:28:42.058: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 17 16:28:42.070: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 17 16:28:44.058: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 17 16:28:44.069: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 17 16:28:46.058: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 17 16:28:46.069: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 17 16:28:48.058: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 17 16:28:48.070: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:28:48.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7705" for this suite.

• [SLOW TEST:12.396 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":280,"completed":39,"skipped":681,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:28:48.098: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-5708
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Feb 17 16:28:48.293: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the sample API server.
Feb 17 16:28:49.264: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Feb 17 16:28:51.377: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 16:28:53.390: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 16:28:55.388: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 16:28:57.389: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 16:28:59.390: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 16:29:01.390: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 16:29:03.388: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 16:29:05.390: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 16:29:07.389: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 16:29:09.390: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553729, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 16:29:12.367: INFO: Waited 967.337639ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:29:12.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5708" for this suite.

• [SLOW TEST:24.927 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]","total":280,"completed":40,"skipped":686,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:29:13.027: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9215
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl label
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1382
STEP: creating the pod
Feb 17 16:29:13.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 create -f - --namespace=kubectl-9215'
Feb 17 16:29:13.591: INFO: stderr: ""
Feb 17 16:29:13.591: INFO: stdout: "pod/pause created\n"
Feb 17 16:29:13.591: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 17 16:29:13.591: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9215" to be "running and ready"
Feb 17 16:29:13.605: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 14.057298ms
Feb 17 16:29:15.618: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.026209829s
Feb 17 16:29:15.618: INFO: Pod "pause" satisfied condition "running and ready"
Feb 17 16:29:15.618: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: adding the label testing-label with value testing-label-value to a pod
Feb 17 16:29:15.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 label pods pause testing-label=testing-label-value --namespace=kubectl-9215'
Feb 17 16:29:15.735: INFO: stderr: ""
Feb 17 16:29:15.736: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb 17 16:29:15.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pod pause -L testing-label --namespace=kubectl-9215'
Feb 17 16:29:15.841: INFO: stderr: ""
Feb 17 16:29:15.841: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb 17 16:29:15.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 label pods pause testing-label- --namespace=kubectl-9215'
Feb 17 16:29:15.957: INFO: stderr: ""
Feb 17 16:29:15.958: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb 17 16:29:15.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pod pause -L testing-label --namespace=kubectl-9215'
Feb 17 16:29:16.059: INFO: stderr: ""
Feb 17 16:29:16.059: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
STEP: using delete to clean up resources
Feb 17 16:29:16.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 delete --grace-period=0 --force -f - --namespace=kubectl-9215'
Feb 17 16:29:16.196: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 17 16:29:16.196: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 17 16:29:16.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get rc,svc -l name=pause --no-headers --namespace=kubectl-9215'
Feb 17 16:29:16.304: INFO: stderr: "No resources found in kubectl-9215 namespace.\n"
Feb 17 16:29:16.304: INFO: stdout: ""
Feb 17 16:29:16.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods -l name=pause --namespace=kubectl-9215 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 17 16:29:16.400: INFO: stderr: ""
Feb 17 16:29:16.400: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:29:16.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9215" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":280,"completed":41,"skipped":711,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:29:16.429: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-4138
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-4138, will wait for the garbage collector to delete the pods
Feb 17 16:29:20.725: INFO: Deleting Job.batch foo took: 22.792233ms
Feb 17 16:29:20.826: INFO: Terminating Job.batch foo pods took: 100.2108ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:30:01.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4138" for this suite.

• [SLOW TEST:44.936 seconds]
[sig-apps] Job
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":280,"completed":42,"skipped":736,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:30:01.365: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9676
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 17 16:30:04.417: INFO: Successfully updated pod "pod-update-c203bbc0-6b4a-465a-8423-fcc8f5aa8708"
STEP: verifying the updated pod is in kubernetes
Feb 17 16:30:04.439: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:30:04.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9676" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":280,"completed":43,"skipped":753,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:30:04.466: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1730
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Feb 17 16:30:04.656: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:30:08.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1730" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":280,"completed":44,"skipped":769,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:30:08.878: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-6368
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Feb 17 16:30:11.630: INFO: Successfully updated pod "adopt-release-bwlc4"
STEP: Checking that the Job readopts the Pod
Feb 17 16:30:11.630: INFO: Waiting up to 15m0s for pod "adopt-release-bwlc4" in namespace "job-6368" to be "adopted"
Feb 17 16:30:11.641: INFO: Pod "adopt-release-bwlc4": Phase="Running", Reason="", readiness=true. Elapsed: 10.476474ms
Feb 17 16:30:13.653: INFO: Pod "adopt-release-bwlc4": Phase="Running", Reason="", readiness=true. Elapsed: 2.022336082s
Feb 17 16:30:13.653: INFO: Pod "adopt-release-bwlc4" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Feb 17 16:30:14.178: INFO: Successfully updated pod "adopt-release-bwlc4"
STEP: Checking that the Job releases the Pod
Feb 17 16:30:14.178: INFO: Waiting up to 15m0s for pod "adopt-release-bwlc4" in namespace "job-6368" to be "released"
Feb 17 16:30:14.192: INFO: Pod "adopt-release-bwlc4": Phase="Running", Reason="", readiness=true. Elapsed: 13.926779ms
Feb 17 16:30:14.192: INFO: Pod "adopt-release-bwlc4" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:30:14.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6368" for this suite.

• [SLOW TEST:5.345 seconds]
[sig-apps] Job
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":280,"completed":45,"skipped":777,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:30:14.223: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1760
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Feb 17 16:30:21.056: INFO: Successfully updated pod "labelsupdatea1a718ee-a01e-4974-b617-0e33a252213c"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:30:23.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1760" for this suite.

• [SLOW TEST:8.908 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":46,"skipped":785,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:30:23.131: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2816
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-1070c035-9398-4313-8710-d6527bf842cc
STEP: Creating a pod to test consume configMaps
Feb 17 16:30:23.357: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bfe71b7d-5c4e-429c-9efd-c1063ab69eef" in namespace "projected-2816" to be "success or failure"
Feb 17 16:30:23.369: INFO: Pod "pod-projected-configmaps-bfe71b7d-5c4e-429c-9efd-c1063ab69eef": Phase="Pending", Reason="", readiness=false. Elapsed: 12.162418ms
Feb 17 16:30:25.380: INFO: Pod "pod-projected-configmaps-bfe71b7d-5c4e-429c-9efd-c1063ab69eef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022786812s
Feb 17 16:30:27.391: INFO: Pod "pod-projected-configmaps-bfe71b7d-5c4e-429c-9efd-c1063ab69eef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034050307s
STEP: Saw pod success
Feb 17 16:30:27.391: INFO: Pod "pod-projected-configmaps-bfe71b7d-5c4e-429c-9efd-c1063ab69eef" satisfied condition "success or failure"
Feb 17 16:30:27.402: INFO: Trying to get logs from node 10.195.53.9 pod pod-projected-configmaps-bfe71b7d-5c4e-429c-9efd-c1063ab69eef container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 16:30:27.493: INFO: Waiting for pod pod-projected-configmaps-bfe71b7d-5c4e-429c-9efd-c1063ab69eef to disappear
Feb 17 16:30:27.503: INFO: Pod pod-projected-configmaps-bfe71b7d-5c4e-429c-9efd-c1063ab69eef no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:30:27.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2816" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":280,"completed":47,"skipped":813,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:30:27.531: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7362
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Feb 17 16:30:27.751: INFO: Waiting up to 5m0s for pod "downward-api-381c4817-8fe3-491a-ad95-b169675b7c5c" in namespace "downward-api-7362" to be "success or failure"
Feb 17 16:30:27.763: INFO: Pod "downward-api-381c4817-8fe3-491a-ad95-b169675b7c5c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.775269ms
Feb 17 16:30:29.777: INFO: Pod "downward-api-381c4817-8fe3-491a-ad95-b169675b7c5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025362934s
STEP: Saw pod success
Feb 17 16:30:29.777: INFO: Pod "downward-api-381c4817-8fe3-491a-ad95-b169675b7c5c" satisfied condition "success or failure"
Feb 17 16:30:29.787: INFO: Trying to get logs from node 10.195.53.9 pod downward-api-381c4817-8fe3-491a-ad95-b169675b7c5c container dapi-container: <nil>
STEP: delete the pod
Feb 17 16:30:29.847: INFO: Waiting for pod downward-api-381c4817-8fe3-491a-ad95-b169675b7c5c to disappear
Feb 17 16:30:29.857: INFO: Pod downward-api-381c4817-8fe3-491a-ad95-b169675b7c5c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:30:29.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7362" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":280,"completed":48,"skipped":840,"failed":0}
SS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:30:29.885: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-8561
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 16:30:30.075: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8561
I0217 16:30:30.099987      24 runners.go:189] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8561, replica count: 1
I0217 16:30:31.150381      24 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0217 16:30:32.150684      24 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 17 16:30:32.279: INFO: Created: latency-svc-4f64z
Feb 17 16:30:32.288: INFO: Got endpoints: latency-svc-4f64z [37.294274ms]
Feb 17 16:30:32.314: INFO: Created: latency-svc-44lw4
Feb 17 16:30:32.326: INFO: Got endpoints: latency-svc-44lw4 [37.266762ms]
Feb 17 16:30:32.334: INFO: Created: latency-svc-tzxrk
Feb 17 16:30:32.341: INFO: Got endpoints: latency-svc-tzxrk [52.972964ms]
Feb 17 16:30:32.352: INFO: Created: latency-svc-lj2k7
Feb 17 16:30:32.356: INFO: Got endpoints: latency-svc-lj2k7 [67.197146ms]
Feb 17 16:30:32.366: INFO: Created: latency-svc-ccsxn
Feb 17 16:30:32.374: INFO: Got endpoints: latency-svc-ccsxn [84.747882ms]
Feb 17 16:30:32.382: INFO: Created: latency-svc-9cbm7
Feb 17 16:30:32.388: INFO: Got endpoints: latency-svc-9cbm7 [98.141559ms]
Feb 17 16:30:32.399: INFO: Created: latency-svc-dl87x
Feb 17 16:30:32.407: INFO: Got endpoints: latency-svc-dl87x [117.396124ms]
Feb 17 16:30:32.416: INFO: Created: latency-svc-x7h75
Feb 17 16:30:32.425: INFO: Got endpoints: latency-svc-x7h75 [135.469226ms]
Feb 17 16:30:32.433: INFO: Created: latency-svc-bqzvh
Feb 17 16:30:32.440: INFO: Got endpoints: latency-svc-bqzvh [149.945787ms]
Feb 17 16:30:32.449: INFO: Created: latency-svc-rl4bl
Feb 17 16:30:32.455: INFO: Got endpoints: latency-svc-rl4bl [165.465191ms]
Feb 17 16:30:32.465: INFO: Created: latency-svc-clnz5
Feb 17 16:30:32.471: INFO: Got endpoints: latency-svc-clnz5 [181.725015ms]
Feb 17 16:30:32.481: INFO: Created: latency-svc-rwdrv
Feb 17 16:30:32.488: INFO: Got endpoints: latency-svc-rwdrv [198.063635ms]
Feb 17 16:30:32.497: INFO: Created: latency-svc-k6plq
Feb 17 16:30:32.505: INFO: Got endpoints: latency-svc-k6plq [214.847526ms]
Feb 17 16:30:32.514: INFO: Created: latency-svc-mwk2w
Feb 17 16:30:32.522: INFO: Got endpoints: latency-svc-mwk2w [232.516331ms]
Feb 17 16:30:32.530: INFO: Created: latency-svc-dpk87
Feb 17 16:30:32.537: INFO: Got endpoints: latency-svc-dpk87 [247.047475ms]
Feb 17 16:30:32.546: INFO: Created: latency-svc-xwg5z
Feb 17 16:30:32.553: INFO: Got endpoints: latency-svc-xwg5z [263.516774ms]
Feb 17 16:30:32.563: INFO: Created: latency-svc-f2nzx
Feb 17 16:30:32.571: INFO: Got endpoints: latency-svc-f2nzx [244.954363ms]
Feb 17 16:30:32.579: INFO: Created: latency-svc-vqc9l
Feb 17 16:30:32.586: INFO: Got endpoints: latency-svc-vqc9l [244.437369ms]
Feb 17 16:30:32.595: INFO: Created: latency-svc-jm4vl
Feb 17 16:30:32.601: INFO: Got endpoints: latency-svc-jm4vl [245.136811ms]
Feb 17 16:30:32.610: INFO: Created: latency-svc-n6phx
Feb 17 16:30:32.618: INFO: Got endpoints: latency-svc-n6phx [243.659858ms]
Feb 17 16:30:32.625: INFO: Created: latency-svc-zgjsj
Feb 17 16:30:32.632: INFO: Got endpoints: latency-svc-zgjsj [243.817008ms]
Feb 17 16:30:32.641: INFO: Created: latency-svc-njnrc
Feb 17 16:30:32.648: INFO: Got endpoints: latency-svc-njnrc [241.283288ms]
Feb 17 16:30:32.658: INFO: Created: latency-svc-86bmz
Feb 17 16:30:32.666: INFO: Got endpoints: latency-svc-86bmz [240.706767ms]
Feb 17 16:30:32.675: INFO: Created: latency-svc-jz876
Feb 17 16:30:32.682: INFO: Got endpoints: latency-svc-jz876 [242.125651ms]
Feb 17 16:30:32.690: INFO: Created: latency-svc-5hlsh
Feb 17 16:30:32.698: INFO: Got endpoints: latency-svc-5hlsh [242.649362ms]
Feb 17 16:30:32.706: INFO: Created: latency-svc-7c8d8
Feb 17 16:30:32.715: INFO: Got endpoints: latency-svc-7c8d8 [243.316558ms]
Feb 17 16:30:32.722: INFO: Created: latency-svc-qg8zg
Feb 17 16:30:32.730: INFO: Got endpoints: latency-svc-qg8zg [241.61119ms]
Feb 17 16:30:32.737: INFO: Created: latency-svc-r2x74
Feb 17 16:30:32.744: INFO: Got endpoints: latency-svc-r2x74 [238.904551ms]
Feb 17 16:30:32.753: INFO: Created: latency-svc-8f68z
Feb 17 16:30:32.761: INFO: Got endpoints: latency-svc-8f68z [239.351424ms]
Feb 17 16:30:32.770: INFO: Created: latency-svc-skjp9
Feb 17 16:30:32.778: INFO: Got endpoints: latency-svc-skjp9 [240.840718ms]
Feb 17 16:30:32.785: INFO: Created: latency-svc-b7xr6
Feb 17 16:30:32.792: INFO: Got endpoints: latency-svc-b7xr6 [238.667924ms]
Feb 17 16:30:32.802: INFO: Created: latency-svc-c9vft
Feb 17 16:30:32.809: INFO: Got endpoints: latency-svc-c9vft [238.406065ms]
Feb 17 16:30:32.817: INFO: Created: latency-svc-cxst6
Feb 17 16:30:32.824: INFO: Got endpoints: latency-svc-cxst6 [238.774225ms]
Feb 17 16:30:32.834: INFO: Created: latency-svc-jxmjf
Feb 17 16:30:32.841: INFO: Got endpoints: latency-svc-jxmjf [239.778399ms]
Feb 17 16:30:32.849: INFO: Created: latency-svc-66pmw
Feb 17 16:30:32.855: INFO: Got endpoints: latency-svc-66pmw [237.497449ms]
Feb 17 16:30:32.865: INFO: Created: latency-svc-qfkkb
Feb 17 16:30:32.871: INFO: Got endpoints: latency-svc-qfkkb [239.73108ms]
Feb 17 16:30:32.880: INFO: Created: latency-svc-dvrzt
Feb 17 16:30:32.886: INFO: Got endpoints: latency-svc-dvrzt [237.971799ms]
Feb 17 16:30:32.896: INFO: Created: latency-svc-5ctlc
Feb 17 16:30:32.902: INFO: Got endpoints: latency-svc-5ctlc [236.151635ms]
Feb 17 16:30:32.911: INFO: Created: latency-svc-9rbw7
Feb 17 16:30:32.918: INFO: Got endpoints: latency-svc-9rbw7 [235.885391ms]
Feb 17 16:30:32.925: INFO: Created: latency-svc-lc2kz
Feb 17 16:30:32.931: INFO: Got endpoints: latency-svc-lc2kz [232.961767ms]
Feb 17 16:30:32.940: INFO: Created: latency-svc-x9qgp
Feb 17 16:30:32.947: INFO: Got endpoints: latency-svc-x9qgp [232.376633ms]
Feb 17 16:30:32.958: INFO: Created: latency-svc-djtcr
Feb 17 16:30:32.964: INFO: Got endpoints: latency-svc-djtcr [234.279605ms]
Feb 17 16:30:32.974: INFO: Created: latency-svc-fk8bn
Feb 17 16:30:32.982: INFO: Got endpoints: latency-svc-fk8bn [238.582788ms]
Feb 17 16:30:32.992: INFO: Created: latency-svc-mchxb
Feb 17 16:30:32.999: INFO: Got endpoints: latency-svc-mchxb [237.348509ms]
Feb 17 16:30:33.008: INFO: Created: latency-svc-g2fcz
Feb 17 16:30:33.015: INFO: Got endpoints: latency-svc-g2fcz [237.102183ms]
Feb 17 16:30:33.024: INFO: Created: latency-svc-8mrg9
Feb 17 16:30:33.031: INFO: Got endpoints: latency-svc-8mrg9 [239.154759ms]
Feb 17 16:30:33.038: INFO: Created: latency-svc-f2n4c
Feb 17 16:30:33.046: INFO: Got endpoints: latency-svc-f2n4c [236.979773ms]
Feb 17 16:30:33.054: INFO: Created: latency-svc-mjbwt
Feb 17 16:30:33.063: INFO: Got endpoints: latency-svc-mjbwt [238.709639ms]
Feb 17 16:30:33.078: INFO: Created: latency-svc-wkk4j
Feb 17 16:30:33.085: INFO: Got endpoints: latency-svc-wkk4j [243.635769ms]
Feb 17 16:30:33.093: INFO: Created: latency-svc-qkkrd
Feb 17 16:30:33.099: INFO: Got endpoints: latency-svc-qkkrd [243.939568ms]
Feb 17 16:30:33.109: INFO: Created: latency-svc-m2jtx
Feb 17 16:30:33.115: INFO: Got endpoints: latency-svc-m2jtx [243.916244ms]
Feb 17 16:30:33.124: INFO: Created: latency-svc-v5c8t
Feb 17 16:30:33.132: INFO: Got endpoints: latency-svc-v5c8t [245.894858ms]
Feb 17 16:30:33.141: INFO: Created: latency-svc-ctr9h
Feb 17 16:30:33.148: INFO: Got endpoints: latency-svc-ctr9h [245.873628ms]
Feb 17 16:30:33.155: INFO: Created: latency-svc-whrcv
Feb 17 16:30:33.163: INFO: Got endpoints: latency-svc-whrcv [244.916158ms]
Feb 17 16:30:33.170: INFO: Created: latency-svc-ddhgr
Feb 17 16:30:33.178: INFO: Got endpoints: latency-svc-ddhgr [247.132975ms]
Feb 17 16:30:33.184: INFO: Created: latency-svc-hgbt8
Feb 17 16:30:33.193: INFO: Got endpoints: latency-svc-hgbt8 [245.80165ms]
Feb 17 16:30:33.201: INFO: Created: latency-svc-2trfp
Feb 17 16:30:33.209: INFO: Got endpoints: latency-svc-2trfp [244.441033ms]
Feb 17 16:30:33.217: INFO: Created: latency-svc-vvvkd
Feb 17 16:30:33.224: INFO: Got endpoints: latency-svc-vvvkd [242.2187ms]
Feb 17 16:30:33.232: INFO: Created: latency-svc-pfgqk
Feb 17 16:30:33.239: INFO: Got endpoints: latency-svc-pfgqk [239.96603ms]
Feb 17 16:30:33.248: INFO: Created: latency-svc-tq6hn
Feb 17 16:30:33.255: INFO: Got endpoints: latency-svc-tq6hn [240.231575ms]
Feb 17 16:30:33.278: INFO: Created: latency-svc-bdj8l
Feb 17 16:30:33.285: INFO: Got endpoints: latency-svc-bdj8l [253.062462ms]
Feb 17 16:30:33.296: INFO: Created: latency-svc-z4bw5
Feb 17 16:30:33.303: INFO: Got endpoints: latency-svc-z4bw5 [257.160363ms]
Feb 17 16:30:33.312: INFO: Created: latency-svc-slqx4
Feb 17 16:30:33.318: INFO: Got endpoints: latency-svc-slqx4 [255.343873ms]
Feb 17 16:30:33.328: INFO: Created: latency-svc-stxpw
Feb 17 16:30:33.335: INFO: Got endpoints: latency-svc-stxpw [250.523028ms]
Feb 17 16:30:33.344: INFO: Created: latency-svc-7756m
Feb 17 16:30:33.351: INFO: Got endpoints: latency-svc-7756m [252.004253ms]
Feb 17 16:30:33.361: INFO: Created: latency-svc-lct5f
Feb 17 16:30:33.368: INFO: Got endpoints: latency-svc-lct5f [253.068154ms]
Feb 17 16:30:33.376: INFO: Created: latency-svc-9t957
Feb 17 16:30:33.383: INFO: Got endpoints: latency-svc-9t957 [251.135672ms]
Feb 17 16:30:33.393: INFO: Created: latency-svc-zbxfn
Feb 17 16:30:33.401: INFO: Got endpoints: latency-svc-zbxfn [253.425597ms]
Feb 17 16:30:33.409: INFO: Created: latency-svc-5xsd7
Feb 17 16:30:33.416: INFO: Got endpoints: latency-svc-5xsd7 [32.776612ms]
Feb 17 16:30:33.425: INFO: Created: latency-svc-gtwlm
Feb 17 16:30:33.431: INFO: Got endpoints: latency-svc-gtwlm [268.63839ms]
Feb 17 16:30:33.440: INFO: Created: latency-svc-lfghm
Feb 17 16:30:33.458: INFO: Created: latency-svc-5kr25
Feb 17 16:30:33.458: INFO: Got endpoints: latency-svc-lfghm [279.718769ms]
Feb 17 16:30:33.466: INFO: Got endpoints: latency-svc-5kr25 [273.083343ms]
Feb 17 16:30:33.473: INFO: Created: latency-svc-dwp4b
Feb 17 16:30:33.480: INFO: Got endpoints: latency-svc-dwp4b [271.506122ms]
Feb 17 16:30:33.489: INFO: Created: latency-svc-s2fmc
Feb 17 16:30:33.498: INFO: Got endpoints: latency-svc-s2fmc [273.769248ms]
Feb 17 16:30:33.505: INFO: Created: latency-svc-89jnw
Feb 17 16:30:33.513: INFO: Got endpoints: latency-svc-89jnw [274.389573ms]
Feb 17 16:30:33.522: INFO: Created: latency-svc-brzgh
Feb 17 16:30:33.529: INFO: Got endpoints: latency-svc-brzgh [274.205377ms]
Feb 17 16:30:33.538: INFO: Created: latency-svc-txlrp
Feb 17 16:30:33.545: INFO: Got endpoints: latency-svc-txlrp [260.677017ms]
Feb 17 16:30:33.553: INFO: Created: latency-svc-9fr4c
Feb 17 16:30:33.559: INFO: Got endpoints: latency-svc-9fr4c [255.815229ms]
Feb 17 16:30:33.569: INFO: Created: latency-svc-d6k6v
Feb 17 16:30:33.576: INFO: Got endpoints: latency-svc-d6k6v [257.473934ms]
Feb 17 16:30:33.583: INFO: Created: latency-svc-pc24k
Feb 17 16:30:33.591: INFO: Got endpoints: latency-svc-pc24k [255.426958ms]
Feb 17 16:30:33.600: INFO: Created: latency-svc-c6w45
Feb 17 16:30:33.606: INFO: Got endpoints: latency-svc-c6w45 [254.629624ms]
Feb 17 16:30:33.614: INFO: Created: latency-svc-22gkp
Feb 17 16:30:33.622: INFO: Got endpoints: latency-svc-22gkp [254.006006ms]
Feb 17 16:30:33.630: INFO: Created: latency-svc-bvxc7
Feb 17 16:30:33.637: INFO: Got endpoints: latency-svc-bvxc7 [235.951941ms]
Feb 17 16:30:33.648: INFO: Created: latency-svc-l8dnj
Feb 17 16:30:33.656: INFO: Got endpoints: latency-svc-l8dnj [239.761861ms]
Feb 17 16:30:33.664: INFO: Created: latency-svc-l8vxb
Feb 17 16:30:33.671: INFO: Got endpoints: latency-svc-l8vxb [239.865019ms]
Feb 17 16:30:33.679: INFO: Created: latency-svc-lsq2w
Feb 17 16:30:33.687: INFO: Got endpoints: latency-svc-lsq2w [228.566487ms]
Feb 17 16:30:33.694: INFO: Created: latency-svc-7qhvb
Feb 17 16:30:33.700: INFO: Got endpoints: latency-svc-7qhvb [234.098246ms]
Feb 17 16:30:33.710: INFO: Created: latency-svc-n7jmh
Feb 17 16:30:33.718: INFO: Got endpoints: latency-svc-n7jmh [237.355564ms]
Feb 17 16:30:33.726: INFO: Created: latency-svc-vllkj
Feb 17 16:30:33.733: INFO: Got endpoints: latency-svc-vllkj [234.826393ms]
Feb 17 16:30:33.742: INFO: Created: latency-svc-ghttd
Feb 17 16:30:33.749: INFO: Got endpoints: latency-svc-ghttd [236.126625ms]
Feb 17 16:30:33.758: INFO: Created: latency-svc-c8b2c
Feb 17 16:30:33.764: INFO: Got endpoints: latency-svc-c8b2c [234.958936ms]
Feb 17 16:30:33.773: INFO: Created: latency-svc-fc8mx
Feb 17 16:30:33.780: INFO: Got endpoints: latency-svc-fc8mx [234.612005ms]
Feb 17 16:30:33.790: INFO: Created: latency-svc-mbpg7
Feb 17 16:30:33.797: INFO: Got endpoints: latency-svc-mbpg7 [237.881178ms]
Feb 17 16:30:33.806: INFO: Created: latency-svc-hrj99
Feb 17 16:30:33.813: INFO: Got endpoints: latency-svc-hrj99 [237.198015ms]
Feb 17 16:30:33.820: INFO: Created: latency-svc-rnn9p
Feb 17 16:30:33.827: INFO: Got endpoints: latency-svc-rnn9p [236.215645ms]
Feb 17 16:30:33.837: INFO: Created: latency-svc-vs6xc
Feb 17 16:30:33.843: INFO: Got endpoints: latency-svc-vs6xc [237.233712ms]
Feb 17 16:30:33.853: INFO: Created: latency-svc-qg7tf
Feb 17 16:30:33.861: INFO: Got endpoints: latency-svc-qg7tf [238.699965ms]
Feb 17 16:30:33.868: INFO: Created: latency-svc-g5c4g
Feb 17 16:30:33.875: INFO: Got endpoints: latency-svc-g5c4g [237.721787ms]
Feb 17 16:30:33.885: INFO: Created: latency-svc-dts87
Feb 17 16:30:33.893: INFO: Got endpoints: latency-svc-dts87 [237.410506ms]
Feb 17 16:30:33.901: INFO: Created: latency-svc-r78tp
Feb 17 16:30:33.907: INFO: Got endpoints: latency-svc-r78tp [235.978231ms]
Feb 17 16:30:33.917: INFO: Created: latency-svc-sbxld
Feb 17 16:30:33.924: INFO: Got endpoints: latency-svc-sbxld [237.591023ms]
Feb 17 16:30:33.938: INFO: Created: latency-svc-m9mtw
Feb 17 16:30:33.944: INFO: Got endpoints: latency-svc-m9mtw [243.774699ms]
Feb 17 16:30:33.953: INFO: Created: latency-svc-8968j
Feb 17 16:30:33.960: INFO: Got endpoints: latency-svc-8968j [242.334459ms]
Feb 17 16:30:33.969: INFO: Created: latency-svc-gx9tm
Feb 17 16:30:33.977: INFO: Got endpoints: latency-svc-gx9tm [243.908668ms]
Feb 17 16:30:33.985: INFO: Created: latency-svc-5kq8z
Feb 17 16:30:33.991: INFO: Got endpoints: latency-svc-5kq8z [241.7794ms]
Feb 17 16:30:34.001: INFO: Created: latency-svc-67btm
Feb 17 16:30:34.010: INFO: Got endpoints: latency-svc-67btm [245.175248ms]
Feb 17 16:30:34.017: INFO: Created: latency-svc-7mqln
Feb 17 16:30:34.030: INFO: Got endpoints: latency-svc-7mqln [250.392418ms]
Feb 17 16:30:34.034: INFO: Created: latency-svc-f5nvm
Feb 17 16:30:34.040: INFO: Got endpoints: latency-svc-f5nvm [242.975288ms]
Feb 17 16:30:34.051: INFO: Created: latency-svc-q756d
Feb 17 16:30:34.057: INFO: Got endpoints: latency-svc-q756d [243.978533ms]
Feb 17 16:30:34.071: INFO: Created: latency-svc-8lrhn
Feb 17 16:30:34.076: INFO: Got endpoints: latency-svc-8lrhn [248.587024ms]
Feb 17 16:30:34.084: INFO: Created: latency-svc-nscfs
Feb 17 16:30:34.091: INFO: Got endpoints: latency-svc-nscfs [247.606332ms]
Feb 17 16:30:34.101: INFO: Created: latency-svc-vz9bx
Feb 17 16:30:34.108: INFO: Got endpoints: latency-svc-vz9bx [246.376166ms]
Feb 17 16:30:34.116: INFO: Created: latency-svc-n96np
Feb 17 16:30:34.124: INFO: Got endpoints: latency-svc-n96np [248.945058ms]
Feb 17 16:30:34.132: INFO: Created: latency-svc-79bsv
Feb 17 16:30:34.139: INFO: Got endpoints: latency-svc-79bsv [245.366163ms]
Feb 17 16:30:34.147: INFO: Created: latency-svc-zc6tp
Feb 17 16:30:34.155: INFO: Got endpoints: latency-svc-zc6tp [247.200244ms]
Feb 17 16:30:34.162: INFO: Created: latency-svc-w7qg8
Feb 17 16:30:34.169: INFO: Got endpoints: latency-svc-w7qg8 [244.520811ms]
Feb 17 16:30:34.177: INFO: Created: latency-svc-gp224
Feb 17 16:30:34.184: INFO: Got endpoints: latency-svc-gp224 [240.077976ms]
Feb 17 16:30:34.193: INFO: Created: latency-svc-gr59l
Feb 17 16:30:34.200: INFO: Got endpoints: latency-svc-gr59l [240.274357ms]
Feb 17 16:30:34.210: INFO: Created: latency-svc-xld7n
Feb 17 16:30:34.221: INFO: Got endpoints: latency-svc-xld7n [243.545595ms]
Feb 17 16:30:34.226: INFO: Created: latency-svc-8xqc4
Feb 17 16:30:34.234: INFO: Got endpoints: latency-svc-8xqc4 [242.308745ms]
Feb 17 16:30:34.241: INFO: Created: latency-svc-mzqtx
Feb 17 16:30:34.249: INFO: Got endpoints: latency-svc-mzqtx [238.920436ms]
Feb 17 16:30:34.258: INFO: Created: latency-svc-qn8nx
Feb 17 16:30:34.264: INFO: Got endpoints: latency-svc-qn8nx [233.84271ms]
Feb 17 16:30:34.274: INFO: Created: latency-svc-d9kqt
Feb 17 16:30:34.287: INFO: Got endpoints: latency-svc-d9kqt [247.271416ms]
Feb 17 16:30:34.289: INFO: Created: latency-svc-mb5kz
Feb 17 16:30:34.297: INFO: Got endpoints: latency-svc-mb5kz [239.787732ms]
Feb 17 16:30:34.308: INFO: Created: latency-svc-bm82l
Feb 17 16:30:34.315: INFO: Got endpoints: latency-svc-bm82l [239.559672ms]
Feb 17 16:30:34.323: INFO: Created: latency-svc-77bgm
Feb 17 16:30:34.329: INFO: Got endpoints: latency-svc-77bgm [237.620335ms]
Feb 17 16:30:34.338: INFO: Created: latency-svc-wrpf7
Feb 17 16:30:34.344: INFO: Got endpoints: latency-svc-wrpf7 [236.353302ms]
Feb 17 16:30:34.353: INFO: Created: latency-svc-f6q7l
Feb 17 16:30:34.360: INFO: Got endpoints: latency-svc-f6q7l [235.515897ms]
Feb 17 16:30:34.368: INFO: Created: latency-svc-fjsxg
Feb 17 16:30:34.375: INFO: Got endpoints: latency-svc-fjsxg [236.138195ms]
Feb 17 16:30:34.384: INFO: Created: latency-svc-mt5xm
Feb 17 16:30:34.391: INFO: Got endpoints: latency-svc-mt5xm [236.331145ms]
Feb 17 16:30:34.400: INFO: Created: latency-svc-t28kb
Feb 17 16:30:34.408: INFO: Got endpoints: latency-svc-t28kb [238.838126ms]
Feb 17 16:30:34.416: INFO: Created: latency-svc-8s5lb
Feb 17 16:30:34.424: INFO: Got endpoints: latency-svc-8s5lb [240.022954ms]
Feb 17 16:30:34.431: INFO: Created: latency-svc-6vw2w
Feb 17 16:30:34.441: INFO: Got endpoints: latency-svc-6vw2w [241.006305ms]
Feb 17 16:30:34.447: INFO: Created: latency-svc-drjp6
Feb 17 16:30:34.453: INFO: Got endpoints: latency-svc-drjp6 [231.964258ms]
Feb 17 16:30:34.461: INFO: Created: latency-svc-xgbp5
Feb 17 16:30:34.467: INFO: Got endpoints: latency-svc-xgbp5 [233.745321ms]
Feb 17 16:30:34.477: INFO: Created: latency-svc-jn94x
Feb 17 16:30:34.483: INFO: Got endpoints: latency-svc-jn94x [233.889464ms]
Feb 17 16:30:34.493: INFO: Created: latency-svc-sdpk8
Feb 17 16:30:34.500: INFO: Got endpoints: latency-svc-sdpk8 [235.09769ms]
Feb 17 16:30:34.508: INFO: Created: latency-svc-zvh2c
Feb 17 16:30:34.514: INFO: Got endpoints: latency-svc-zvh2c [226.283005ms]
Feb 17 16:30:34.524: INFO: Created: latency-svc-vbnwb
Feb 17 16:30:34.530: INFO: Got endpoints: latency-svc-vbnwb [232.900813ms]
Feb 17 16:30:34.539: INFO: Created: latency-svc-pf2sz
Feb 17 16:30:34.545: INFO: Got endpoints: latency-svc-pf2sz [229.808832ms]
Feb 17 16:30:34.559: INFO: Created: latency-svc-k5x2w
Feb 17 16:30:34.566: INFO: Got endpoints: latency-svc-k5x2w [236.853905ms]
Feb 17 16:30:34.574: INFO: Created: latency-svc-2krm4
Feb 17 16:30:34.583: INFO: Got endpoints: latency-svc-2krm4 [238.609224ms]
Feb 17 16:30:34.589: INFO: Created: latency-svc-2wdpg
Feb 17 16:30:34.595: INFO: Got endpoints: latency-svc-2wdpg [234.70831ms]
Feb 17 16:30:34.605: INFO: Created: latency-svc-9sprz
Feb 17 16:30:34.612: INFO: Got endpoints: latency-svc-9sprz [237.283134ms]
Feb 17 16:30:34.619: INFO: Created: latency-svc-ssffg
Feb 17 16:30:34.629: INFO: Got endpoints: latency-svc-ssffg [237.506696ms]
Feb 17 16:30:34.636: INFO: Created: latency-svc-8vmnh
Feb 17 16:30:34.645: INFO: Got endpoints: latency-svc-8vmnh [237.170072ms]
Feb 17 16:30:34.653: INFO: Created: latency-svc-brbpn
Feb 17 16:30:34.661: INFO: Got endpoints: latency-svc-brbpn [237.257925ms]
Feb 17 16:30:34.669: INFO: Created: latency-svc-fbs8w
Feb 17 16:30:34.676: INFO: Got endpoints: latency-svc-fbs8w [234.914752ms]
Feb 17 16:30:34.685: INFO: Created: latency-svc-wdx2n
Feb 17 16:30:34.692: INFO: Got endpoints: latency-svc-wdx2n [238.67323ms]
Feb 17 16:30:34.708: INFO: Created: latency-svc-n9ttb
Feb 17 16:30:34.715: INFO: Got endpoints: latency-svc-n9ttb [247.48758ms]
Feb 17 16:30:34.725: INFO: Created: latency-svc-mpkmq
Feb 17 16:30:34.732: INFO: Got endpoints: latency-svc-mpkmq [249.233239ms]
Feb 17 16:30:34.743: INFO: Created: latency-svc-w9ts9
Feb 17 16:30:34.750: INFO: Got endpoints: latency-svc-w9ts9 [250.229365ms]
Feb 17 16:30:34.757: INFO: Created: latency-svc-2mw7n
Feb 17 16:30:34.764: INFO: Got endpoints: latency-svc-2mw7n [250.100382ms]
Feb 17 16:30:34.772: INFO: Created: latency-svc-4gfxp
Feb 17 16:30:34.779: INFO: Got endpoints: latency-svc-4gfxp [249.03357ms]
Feb 17 16:30:34.788: INFO: Created: latency-svc-2kvbx
Feb 17 16:30:34.795: INFO: Got endpoints: latency-svc-2kvbx [249.45033ms]
Feb 17 16:30:34.802: INFO: Created: latency-svc-tsjtk
Feb 17 16:30:34.808: INFO: Got endpoints: latency-svc-tsjtk [241.998603ms]
Feb 17 16:30:34.821: INFO: Created: latency-svc-c7fxb
Feb 17 16:30:34.831: INFO: Got endpoints: latency-svc-c7fxb [248.149189ms]
Feb 17 16:30:34.837: INFO: Created: latency-svc-fgzgx
Feb 17 16:30:34.844: INFO: Got endpoints: latency-svc-fgzgx [249.067315ms]
Feb 17 16:30:34.853: INFO: Created: latency-svc-95bz8
Feb 17 16:30:34.859: INFO: Got endpoints: latency-svc-95bz8 [246.769158ms]
Feb 17 16:30:34.868: INFO: Created: latency-svc-pwk94
Feb 17 16:30:34.877: INFO: Got endpoints: latency-svc-pwk94 [248.167003ms]
Feb 17 16:30:34.886: INFO: Created: latency-svc-6dc5q
Feb 17 16:30:34.892: INFO: Got endpoints: latency-svc-6dc5q [247.524178ms]
Feb 17 16:30:34.900: INFO: Created: latency-svc-bnctp
Feb 17 16:30:34.908: INFO: Got endpoints: latency-svc-bnctp [246.188727ms]
Feb 17 16:30:34.915: INFO: Created: latency-svc-xqm47
Feb 17 16:30:34.921: INFO: Got endpoints: latency-svc-xqm47 [245.040243ms]
Feb 17 16:30:34.930: INFO: Created: latency-svc-pqdg4
Feb 17 16:30:34.939: INFO: Got endpoints: latency-svc-pqdg4 [247.405442ms]
Feb 17 16:30:34.959: INFO: Created: latency-svc-wdl6m
Feb 17 16:30:34.960: INFO: Got endpoints: latency-svc-wdl6m [245.019693ms]
Feb 17 16:30:34.969: INFO: Created: latency-svc-tqmdm
Feb 17 16:30:34.976: INFO: Got endpoints: latency-svc-tqmdm [243.984936ms]
Feb 17 16:30:34.991: INFO: Created: latency-svc-7jjsc
Feb 17 16:30:34.998: INFO: Got endpoints: latency-svc-7jjsc [247.648213ms]
Feb 17 16:30:35.003: INFO: Created: latency-svc-fmjkb
Feb 17 16:30:35.009: INFO: Got endpoints: latency-svc-fmjkb [244.710003ms]
Feb 17 16:30:35.023: INFO: Created: latency-svc-4wqhz
Feb 17 16:30:35.030: INFO: Got endpoints: latency-svc-4wqhz [250.998148ms]
Feb 17 16:30:35.040: INFO: Created: latency-svc-xmcp8
Feb 17 16:30:35.046: INFO: Got endpoints: latency-svc-xmcp8 [251.684461ms]
Feb 17 16:30:35.057: INFO: Created: latency-svc-xp6v4
Feb 17 16:30:35.063: INFO: Got endpoints: latency-svc-xp6v4 [255.776174ms]
Feb 17 16:30:35.072: INFO: Created: latency-svc-6qs2x
Feb 17 16:30:35.079: INFO: Got endpoints: latency-svc-6qs2x [248.297698ms]
Feb 17 16:30:35.094: INFO: Created: latency-svc-6bn7j
Feb 17 16:30:35.102: INFO: Got endpoints: latency-svc-6bn7j [257.693507ms]
Feb 17 16:30:35.110: INFO: Created: latency-svc-9gngd
Feb 17 16:30:35.116: INFO: Got endpoints: latency-svc-9gngd [257.061284ms]
Feb 17 16:30:35.125: INFO: Created: latency-svc-s4hf4
Feb 17 16:30:35.133: INFO: Got endpoints: latency-svc-s4hf4 [256.0014ms]
Feb 17 16:30:35.141: INFO: Created: latency-svc-vnx87
Feb 17 16:30:35.148: INFO: Got endpoints: latency-svc-vnx87 [255.49249ms]
Feb 17 16:30:35.157: INFO: Created: latency-svc-xshwr
Feb 17 16:30:35.164: INFO: Got endpoints: latency-svc-xshwr [256.50555ms]
Feb 17 16:30:35.177: INFO: Created: latency-svc-jgpvr
Feb 17 16:30:35.183: INFO: Got endpoints: latency-svc-jgpvr [261.836575ms]
Feb 17 16:30:35.193: INFO: Created: latency-svc-dlljl
Feb 17 16:30:35.200: INFO: Got endpoints: latency-svc-dlljl [260.847269ms]
Feb 17 16:30:35.208: INFO: Created: latency-svc-x79xg
Feb 17 16:30:35.215: INFO: Got endpoints: latency-svc-x79xg [255.00146ms]
Feb 17 16:30:35.231: INFO: Created: latency-svc-89nbk
Feb 17 16:30:35.239: INFO: Got endpoints: latency-svc-89nbk [262.44715ms]
Feb 17 16:30:35.248: INFO: Created: latency-svc-stqc9
Feb 17 16:30:35.256: INFO: Got endpoints: latency-svc-stqc9 [258.701641ms]
Feb 17 16:30:35.267: INFO: Created: latency-svc-rd7bj
Feb 17 16:30:35.273: INFO: Got endpoints: latency-svc-rd7bj [264.656704ms]
Feb 17 16:30:35.283: INFO: Created: latency-svc-7tstx
Feb 17 16:30:35.304: INFO: Got endpoints: latency-svc-7tstx [273.552669ms]
Feb 17 16:30:35.309: INFO: Created: latency-svc-lp7gc
Feb 17 16:30:35.315: INFO: Got endpoints: latency-svc-lp7gc [268.927748ms]
Feb 17 16:30:35.324: INFO: Created: latency-svc-n869r
Feb 17 16:30:35.330: INFO: Got endpoints: latency-svc-n869r [266.825354ms]
Feb 17 16:30:35.338: INFO: Created: latency-svc-llqws
Feb 17 16:30:35.347: INFO: Got endpoints: latency-svc-llqws [268.316659ms]
Feb 17 16:30:35.353: INFO: Created: latency-svc-8kvm9
Feb 17 16:30:35.361: INFO: Got endpoints: latency-svc-8kvm9 [259.19067ms]
Feb 17 16:30:35.370: INFO: Created: latency-svc-8zzl6
Feb 17 16:30:35.376: INFO: Got endpoints: latency-svc-8zzl6 [259.210456ms]
Feb 17 16:30:35.387: INFO: Created: latency-svc-lvkh4
Feb 17 16:30:35.393: INFO: Got endpoints: latency-svc-lvkh4 [259.767945ms]
Feb 17 16:30:35.404: INFO: Created: latency-svc-2nfqw
Feb 17 16:30:35.410: INFO: Got endpoints: latency-svc-2nfqw [262.516852ms]
Feb 17 16:30:35.419: INFO: Created: latency-svc-rnrvh
Feb 17 16:30:35.426: INFO: Got endpoints: latency-svc-rnrvh [261.687061ms]
Feb 17 16:30:35.436: INFO: Created: latency-svc-plvjs
Feb 17 16:30:35.442: INFO: Got endpoints: latency-svc-plvjs [258.503043ms]
Feb 17 16:30:35.451: INFO: Created: latency-svc-t4k69
Feb 17 16:30:35.458: INFO: Got endpoints: latency-svc-t4k69 [258.06426ms]
Feb 17 16:30:35.470: INFO: Created: latency-svc-kp5nr
Feb 17 16:30:35.477: INFO: Got endpoints: latency-svc-kp5nr [261.582657ms]
Feb 17 16:30:35.486: INFO: Created: latency-svc-6gf4q
Feb 17 16:30:35.494: INFO: Got endpoints: latency-svc-6gf4q [255.491483ms]
Feb 17 16:30:35.502: INFO: Created: latency-svc-v8c2m
Feb 17 16:30:35.507: INFO: Got endpoints: latency-svc-v8c2m [250.930096ms]
Feb 17 16:30:35.517: INFO: Created: latency-svc-d6rpq
Feb 17 16:30:35.524: INFO: Got endpoints: latency-svc-d6rpq [250.909849ms]
Feb 17 16:30:35.534: INFO: Created: latency-svc-6jl5k
Feb 17 16:30:35.542: INFO: Got endpoints: latency-svc-6jl5k [238.177973ms]
Feb 17 16:30:35.550: INFO: Created: latency-svc-fv9qf
Feb 17 16:30:35.558: INFO: Got endpoints: latency-svc-fv9qf [242.341054ms]
Feb 17 16:30:35.564: INFO: Created: latency-svc-q7xh9
Feb 17 16:30:35.572: INFO: Got endpoints: latency-svc-q7xh9 [241.423082ms]
Feb 17 16:30:35.572: INFO: Latencies: [32.776612ms 37.266762ms 52.972964ms 67.197146ms 84.747882ms 98.141559ms 117.396124ms 135.469226ms 149.945787ms 165.465191ms 181.725015ms 198.063635ms 214.847526ms 226.283005ms 228.566487ms 229.808832ms 231.964258ms 232.376633ms 232.516331ms 232.900813ms 232.961767ms 233.745321ms 233.84271ms 233.889464ms 234.098246ms 234.279605ms 234.612005ms 234.70831ms 234.826393ms 234.914752ms 234.958936ms 235.09769ms 235.515897ms 235.885391ms 235.951941ms 235.978231ms 236.126625ms 236.138195ms 236.151635ms 236.215645ms 236.331145ms 236.353302ms 236.853905ms 236.979773ms 237.102183ms 237.170072ms 237.198015ms 237.233712ms 237.257925ms 237.283134ms 237.348509ms 237.355564ms 237.410506ms 237.497449ms 237.506696ms 237.591023ms 237.620335ms 237.721787ms 237.881178ms 237.971799ms 238.177973ms 238.406065ms 238.582788ms 238.609224ms 238.667924ms 238.67323ms 238.699965ms 238.709639ms 238.774225ms 238.838126ms 238.904551ms 238.920436ms 239.154759ms 239.351424ms 239.559672ms 239.73108ms 239.761861ms 239.778399ms 239.787732ms 239.865019ms 239.96603ms 240.022954ms 240.077976ms 240.231575ms 240.274357ms 240.706767ms 240.840718ms 241.006305ms 241.283288ms 241.423082ms 241.61119ms 241.7794ms 241.998603ms 242.125651ms 242.2187ms 242.308745ms 242.334459ms 242.341054ms 242.649362ms 242.975288ms 243.316558ms 243.545595ms 243.635769ms 243.659858ms 243.774699ms 243.817008ms 243.908668ms 243.916244ms 243.939568ms 243.978533ms 243.984936ms 244.437369ms 244.441033ms 244.520811ms 244.710003ms 244.916158ms 244.954363ms 245.019693ms 245.040243ms 245.136811ms 245.175248ms 245.366163ms 245.80165ms 245.873628ms 245.894858ms 246.188727ms 246.376166ms 246.769158ms 247.047475ms 247.132975ms 247.200244ms 247.271416ms 247.405442ms 247.48758ms 247.524178ms 247.606332ms 247.648213ms 248.149189ms 248.167003ms 248.297698ms 248.587024ms 248.945058ms 249.03357ms 249.067315ms 249.233239ms 249.45033ms 250.100382ms 250.229365ms 250.392418ms 250.523028ms 250.909849ms 250.930096ms 250.998148ms 251.135672ms 251.684461ms 252.004253ms 253.062462ms 253.068154ms 253.425597ms 254.006006ms 254.629624ms 255.00146ms 255.343873ms 255.426958ms 255.491483ms 255.49249ms 255.776174ms 255.815229ms 256.0014ms 256.50555ms 257.061284ms 257.160363ms 257.473934ms 257.693507ms 258.06426ms 258.503043ms 258.701641ms 259.19067ms 259.210456ms 259.767945ms 260.677017ms 260.847269ms 261.582657ms 261.687061ms 261.836575ms 262.44715ms 262.516852ms 263.516774ms 264.656704ms 266.825354ms 268.316659ms 268.63839ms 268.927748ms 271.506122ms 273.083343ms 273.552669ms 273.769248ms 274.205377ms 274.389573ms 279.718769ms]
Feb 17 16:30:35.572: INFO: 50 %ile: 243.316558ms
Feb 17 16:30:35.572: INFO: 90 %ile: 260.677017ms
Feb 17 16:30:35.572: INFO: 99 %ile: 274.389573ms
Feb 17 16:30:35.572: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:30:35.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8561" for this suite.

• [SLOW TEST:5.717 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":280,"completed":49,"skipped":842,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:30:35.603: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7473
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Feb 17 16:30:35.796: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 17 16:30:35.829: INFO: Waiting for terminating namespaces to be deleted...
Feb 17 16:30:35.838: INFO: 
Logging pods the kubelet thinks is on node 10.195.53.14 before test
Feb 17 16:30:35.897: INFO: ibm-cloud-provider-ip-135-90-78-211-7555ccd494-hhqb2 from ibm-system started at 2020-02-17 14:41:58 +0000 UTC (1 container statuses recorded)
Feb 17 16:30:35.897: INFO: 	Container ibm-cloud-provider-ip-135-90-78-211 ready: true, restart count 0
Feb 17 16:30:35.897: INFO: public-crbp5a3ais0mfj6cn3dl60-alb1-79849b9bd6-4z2xg from kube-system started at 2020-02-17 14:44:46 +0000 UTC (4 container statuses recorded)
Feb 17 16:30:35.897: INFO: 	Container ingress-auth-1 ready: true, restart count 1
Feb 17 16:30:35.897: INFO: 	Container ingress-auth-2 ready: true, restart count 1
Feb 17 16:30:35.897: INFO: 	Container ingress-auth-3 ready: true, restart count 1
Feb 17 16:30:35.897: INFO: 	Container nginx-ingress ready: true, restart count 0
Feb 17 16:30:35.897: INFO: sonobuoy-systemd-logs-daemon-set-bc697ed487d7421d-st8jd from sonobuoy started at 2020-02-17 16:14:42 +0000 UTC (2 container statuses recorded)
Feb 17 16:30:35.897: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 16:30:35.897: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 16:30:35.897: INFO: coredns-5b567488dd-kckdk from kube-system started at 2020-02-17 14:59:13 +0000 UTC (1 container statuses recorded)
Feb 17 16:30:35.897: INFO: 	Container coredns ready: true, restart count 0
Feb 17 16:30:35.897: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-02-17 14:39:39 +0000 UTC (1 container statuses recorded)
Feb 17 16:30:35.897: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Feb 17 16:30:35.897: INFO: adopt-release-ljs4w from job-6368 started at 2020-02-17 16:30:09 +0000 UTC (1 container statuses recorded)
Feb 17 16:30:35.897: INFO: 	Container c ready: true, restart count 0
Feb 17 16:30:35.897: INFO: adopt-release-bwlc4 from job-6368 started at 2020-02-17 16:30:09 +0000 UTC (1 container statuses recorded)
Feb 17 16:30:35.897: INFO: 	Container c ready: true, restart count 0
Feb 17 16:30:35.897: INFO: ibm-master-proxy-static-10.195.53.14 from kube-system started at 2020-02-17 14:36:26 +0000 UTC (2 container statuses recorded)
Feb 17 16:30:35.897: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 16:30:35.897: INFO: 	Container pause ready: true, restart count 0
Feb 17 16:30:35.897: INFO: ibm-keepalived-watcher-8pbd8 from kube-system started at 2020-02-17 14:36:27 +0000 UTC (1 container statuses recorded)
Feb 17 16:30:35.897: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 16:30:35.897: INFO: calico-node-94c2k from kube-system started at 2020-02-17 14:36:27 +0000 UTC (1 container statuses recorded)
Feb 17 16:30:35.897: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 16:30:35.897: INFO: 
Logging pods the kubelet thinks is on node 10.195.53.47 before test
Feb 17 16:30:35.941: INFO: ibm-master-proxy-static-10.195.53.47 from kube-system started at 2020-02-17 14:36:20 +0000 UTC (2 container statuses recorded)
Feb 17 16:30:35.941: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 16:30:35.941: INFO: 	Container pause ready: true, restart count 0
Feb 17 16:30:35.941: INFO: ibm-keepalived-watcher-bl4pr from kube-system started at 2020-02-17 14:36:22 +0000 UTC (1 container statuses recorded)
Feb 17 16:30:35.941: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 16:30:35.941: INFO: sonobuoy-systemd-logs-daemon-set-bc697ed487d7421d-5b85k from sonobuoy started at 2020-02-17 16:14:42 +0000 UTC (2 container statuses recorded)
Feb 17 16:30:35.941: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 16:30:35.941: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 16:30:35.941: INFO: public-crbp5a3ais0mfj6cn3dl60-alb1-79849b9bd6-8r8qk from kube-system started at 2020-02-17 14:44:46 +0000 UTC (4 container statuses recorded)
Feb 17 16:30:35.941: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Feb 17 16:30:35.941: INFO: 	Container ingress-auth-2 ready: true, restart count 1
Feb 17 16:30:35.941: INFO: 	Container ingress-auth-3 ready: true, restart count 1
Feb 17 16:30:35.941: INFO: 	Container nginx-ingress ready: true, restart count 0
Feb 17 16:30:35.941: INFO: calico-node-kmwlv from kube-system started at 2020-02-17 14:36:22 +0000 UTC (1 container statuses recorded)
Feb 17 16:30:35.941: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 16:30:35.941: INFO: addon-catalog-source-c5sks from ibm-system started at 2020-02-17 14:38:40 +0000 UTC (1 container statuses recorded)
Feb 17 16:30:35.941: INFO: 	Container configmap-registry-server ready: true, restart count 0
Feb 17 16:30:35.941: INFO: ibm-cloud-provider-ip-135-90-78-211-7555ccd494-fz6gg from ibm-system started at 2020-02-17 14:41:58 +0000 UTC (1 container statuses recorded)
Feb 17 16:30:35.941: INFO: 	Container ibm-cloud-provider-ip-135-90-78-211 ready: true, restart count 0
Feb 17 16:30:35.941: INFO: coredns-5b567488dd-qtx5q from kube-system started at 2020-02-17 14:59:13 +0000 UTC (1 container statuses recorded)
Feb 17 16:30:35.941: INFO: 	Container coredns ready: true, restart count 0
Feb 17 16:30:35.941: INFO: sonobuoy-e2e-job-c9def2901e004587 from sonobuoy started at 2020-02-17 16:14:42 +0000 UTC (2 container statuses recorded)
Feb 17 16:30:35.941: INFO: 	Container e2e ready: true, restart count 0
Feb 17 16:30:35.941: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 16:30:35.941: INFO: labelsupdatea1a718ee-a01e-4974-b617-0e33a252213c from projected-1760 started at 2020-02-17 16:30:14 +0000 UTC (1 container statuses recorded)
Feb 17 16:30:35.941: INFO: 	Container client-container ready: false, restart count 0
Feb 17 16:30:35.941: INFO: 
Logging pods the kubelet thinks is on node 10.195.53.9 before test
Feb 17 16:30:35.970: INFO: coredns-autoscaler-7dddb6f87c-mjx7q from kube-system started at 2020-02-17 14:36:35 +0000 UTC (1 container statuses recorded)
Feb 17 16:30:35.970: INFO: 	Container autoscaler ready: true, restart count 0
Feb 17 16:30:35.970: INFO: olm-operator-587889d75d-nsz4c from ibm-system started at 2020-02-17 14:36:35 +0000 UTC (1 container statuses recorded)
Feb 17 16:30:35.970: INFO: 	Container olm-operator ready: true, restart count 0
Feb 17 16:30:35.970: INFO: sonobuoy-systemd-logs-daemon-set-bc697ed487d7421d-ntb66 from sonobuoy started at 2020-02-17 16:14:42 +0000 UTC (2 container statuses recorded)
Feb 17 16:30:35.970: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 16:30:35.970: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 16:30:35.970: INFO: calico-node-r8w4v from kube-system started at 2020-02-17 14:36:12 +0000 UTC (1 container statuses recorded)
Feb 17 16:30:35.970: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 16:30:35.970: INFO: ibm-keepalived-watcher-7c6j2 from kube-system started at 2020-02-17 14:36:12 +0000 UTC (1 container statuses recorded)
Feb 17 16:30:35.970: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 16:30:35.970: INFO: kubernetes-dashboard-bbcc67fc-z4v4t from kube-system started at 2020-02-17 14:36:35 +0000 UTC (1 container statuses recorded)
Feb 17 16:30:35.970: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 17 16:30:35.970: INFO: dashboard-metrics-scraper-69468c6b44-m5jtg from kube-system started at 2020-02-17 14:36:35 +0000 UTC (1 container statuses recorded)
Feb 17 16:30:35.970: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 17 16:30:35.970: INFO: vpn-b5cd9dc8b-ql2rw from kube-system started at 2020-02-17 14:58:51 +0000 UTC (1 container statuses recorded)
Feb 17 16:30:35.970: INFO: 	Container vpn ready: true, restart count 0
Feb 17 16:30:35.970: INFO: svc-latency-rc-4hxjb from svc-latency-8561 started at 2020-02-17 16:30:30 +0000 UTC (1 container statuses recorded)
Feb 17 16:30:35.970: INFO: 	Container svc-latency-rc ready: true, restart count 0
Feb 17 16:30:35.970: INFO: ibm-storage-watcher-74f895486d-p8plw from kube-system started at 2020-02-17 14:36:35 +0000 UTC (1 container statuses recorded)
Feb 17 16:30:35.970: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Feb 17 16:30:35.970: INFO: sonobuoy from sonobuoy started at 2020-02-17 16:14:33 +0000 UTC (1 container statuses recorded)
Feb 17 16:30:35.970: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 17 16:30:35.970: INFO: ibm-file-plugin-6694f985b8-vmks4 from kube-system started at 2020-02-17 14:36:35 +0000 UTC (1 container statuses recorded)
Feb 17 16:30:35.970: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Feb 17 16:30:35.970: INFO: adopt-release-rfhxn from job-6368 started at 2020-02-17 16:30:14 +0000 UTC (1 container statuses recorded)
Feb 17 16:30:35.970: INFO: 	Container c ready: true, restart count 0
Feb 17 16:30:35.970: INFO: ibm-master-proxy-static-10.195.53.9 from kube-system started at 2020-02-17 14:36:11 +0000 UTC (2 container statuses recorded)
Feb 17 16:30:35.970: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 16:30:35.970: INFO: 	Container pause ready: true, restart count 0
Feb 17 16:30:35.970: INFO: calico-kube-controllers-866cf6f69c-42s2k from kube-system started at 2020-02-17 14:36:35 +0000 UTC (1 container statuses recorded)
Feb 17 16:30:35.970: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 17 16:30:35.970: INFO: catalog-operator-7d9cb6cf74-2rv6f from ibm-system started at 2020-02-17 14:36:35 +0000 UTC (1 container statuses recorded)
Feb 17 16:30:35.970: INFO: 	Container catalog-operator ready: true, restart count 0
Feb 17 16:30:35.970: INFO: metrics-server-647cf95c9b-v6wmx from kube-system started at 2020-02-17 14:37:43 +0000 UTC (2 container statuses recorded)
Feb 17 16:30:35.970: INFO: 	Container metrics-server ready: true, restart count 0
Feb 17 16:30:35.970: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Feb 17 16:30:35.970: INFO: coredns-5b567488dd-blzxj from kube-system started at 2020-02-17 14:59:13 +0000 UTC (1 container statuses recorded)
Feb 17 16:30:35.970: INFO: 	Container coredns ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-0cb25e74-f27c-490b-a7c5-bd40f0a7e7df 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-0cb25e74-f27c-490b-a7c5-bd40f0a7e7df off the node 10.195.53.47
STEP: verifying the node doesn't have the label kubernetes.io/e2e-0cb25e74-f27c-490b-a7c5-bd40f0a7e7df
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:30:48.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7473" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:12.659 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":280,"completed":50,"skipped":850,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:30:48.264: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6939
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-nnqm
STEP: Creating a pod to test atomic-volume-subpath
Feb 17 16:30:48.499: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-nnqm" in namespace "subpath-6939" to be "success or failure"
Feb 17 16:30:48.510: INFO: Pod "pod-subpath-test-configmap-nnqm": Phase="Pending", Reason="", readiness=false. Elapsed: 10.794195ms
Feb 17 16:30:50.524: INFO: Pod "pod-subpath-test-configmap-nnqm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025459051s
Feb 17 16:30:52.536: INFO: Pod "pod-subpath-test-configmap-nnqm": Phase="Running", Reason="", readiness=true. Elapsed: 4.036963367s
Feb 17 16:30:54.548: INFO: Pod "pod-subpath-test-configmap-nnqm": Phase="Running", Reason="", readiness=true. Elapsed: 6.049339983s
Feb 17 16:30:56.560: INFO: Pod "pod-subpath-test-configmap-nnqm": Phase="Running", Reason="", readiness=true. Elapsed: 8.060860962s
Feb 17 16:30:58.571: INFO: Pod "pod-subpath-test-configmap-nnqm": Phase="Running", Reason="", readiness=true. Elapsed: 10.072267734s
Feb 17 16:31:00.583: INFO: Pod "pod-subpath-test-configmap-nnqm": Phase="Running", Reason="", readiness=true. Elapsed: 12.084083349s
Feb 17 16:31:02.595: INFO: Pod "pod-subpath-test-configmap-nnqm": Phase="Running", Reason="", readiness=true. Elapsed: 14.096105959s
Feb 17 16:31:04.607: INFO: Pod "pod-subpath-test-configmap-nnqm": Phase="Running", Reason="", readiness=true. Elapsed: 16.108278149s
Feb 17 16:31:06.624: INFO: Pod "pod-subpath-test-configmap-nnqm": Phase="Running", Reason="", readiness=true. Elapsed: 18.124976738s
Feb 17 16:31:08.636: INFO: Pod "pod-subpath-test-configmap-nnqm": Phase="Running", Reason="", readiness=true. Elapsed: 20.13691874s
Feb 17 16:31:10.650: INFO: Pod "pod-subpath-test-configmap-nnqm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.151288707s
STEP: Saw pod success
Feb 17 16:31:10.650: INFO: Pod "pod-subpath-test-configmap-nnqm" satisfied condition "success or failure"
Feb 17 16:31:10.661: INFO: Trying to get logs from node 10.195.53.9 pod pod-subpath-test-configmap-nnqm container test-container-subpath-configmap-nnqm: <nil>
STEP: delete the pod
Feb 17 16:31:10.717: INFO: Waiting for pod pod-subpath-test-configmap-nnqm to disappear
Feb 17 16:31:10.727: INFO: Pod pod-subpath-test-configmap-nnqm no longer exists
STEP: Deleting pod pod-subpath-test-configmap-nnqm
Feb 17 16:31:10.727: INFO: Deleting pod "pod-subpath-test-configmap-nnqm" in namespace "subpath-6939"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:31:10.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6939" for this suite.

• [SLOW TEST:22.524 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":280,"completed":51,"skipped":890,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:31:10.789: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4177
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4177.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4177.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 17 16:31:31.176: INFO: DNS probes using dns-4177/dns-test-98528c9c-f7f2-4c20-84fd-3a53899983a8 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:31:31.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4177" for this suite.

• [SLOW TEST:20.452 seconds]
[sig-network] DNS
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":280,"completed":52,"skipped":892,"failed":0}
SSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:31:31.241: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5204
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:178
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:31:31.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5204" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":280,"completed":53,"skipped":897,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:31:31.498: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5776
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 16:31:31.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 create -f - --namespace=kubectl-5776'
Feb 17 16:31:32.067: INFO: stderr: ""
Feb 17 16:31:32.067: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Feb 17 16:31:32.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 create -f - --namespace=kubectl-5776'
Feb 17 16:31:32.398: INFO: stderr: ""
Feb 17 16:31:32.398: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Feb 17 16:31:33.409: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 17 16:31:33.409: INFO: Found 0 / 1
Feb 17 16:31:34.410: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 17 16:31:34.410: INFO: Found 1 / 1
Feb 17 16:31:34.410: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 17 16:31:34.421: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 17 16:31:34.421: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 17 16:31:34.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 describe pod agnhost-master-2p5p5 --namespace=kubectl-5776'
Feb 17 16:31:34.558: INFO: stderr: ""
Feb 17 16:31:34.558: INFO: stdout: "Name:         agnhost-master-2p5p5\nNamespace:    kubectl-5776\nPriority:     0\nNode:         10.195.53.14/10.195.53.14\nStart Time:   Mon, 17 Feb 2020 16:31:32 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           172.30.89.196\nIPs:\n  IP:           172.30.89.196\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   containerd://905c4bd2db86d9765e2f485c312223ebfaf38785b2379cd72f33ec9eff15dd72\n    Image:          gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Image ID:       gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 17 Feb 2020 16:31:33 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-7qx54 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-7qx54:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-7qx54\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 600s\n                 node.kubernetes.io/unreachable:NoExecute for 600s\nEvents:\n  Type    Reason     Age        From                   Message\n  ----    ------     ----       ----                   -------\n  Normal  Scheduled  <unknown>  default-scheduler      Successfully assigned kubectl-5776/agnhost-master-2p5p5 to 10.195.53.14\n  Normal  Pulled     1s         kubelet, 10.195.53.14  Container image \"gcr.io/kubernetes-e2e-test-images/agnhost:2.8\" already present on machine\n  Normal  Created    1s         kubelet, 10.195.53.14  Created container agnhost-master\n  Normal  Started    1s         kubelet, 10.195.53.14  Started container agnhost-master\n"
Feb 17 16:31:34.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 describe rc agnhost-master --namespace=kubectl-5776'
Feb 17 16:31:34.699: INFO: stderr: ""
Feb 17 16:31:34.699: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-5776\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-master-2p5p5\n"
Feb 17 16:31:34.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 describe service agnhost-master --namespace=kubectl-5776'
Feb 17 16:31:34.834: INFO: stderr: ""
Feb 17 16:31:34.834: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-5776\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                172.21.207.116\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.89.196:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 17 16:31:34.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 describe node 10.195.53.14'
Feb 17 16:31:35.067: INFO: stderr: ""
Feb 17 16:31:35.067: INFO: stdout: "Name:               10.195.53.14\nRoles:              <none>\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=au-syd\n                    failure-domain.beta.kubernetes.io/zone=syd05\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=135.90.88.54\n                    ibm-cloud.kubernetes.io/ha-worker=true\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.195.53.14\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=UBUNTU_18_64\n                    ibm-cloud.kubernetes.io/region=au-syd\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-bp5a3ais0mfj6cn3dl60-kubee2epvga-default-00000369\n                    ibm-cloud.kubernetes.io/worker-pool-id=bp5a3ais0mfj6cn3dl60-4a04889\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=1.17.2_1515\n                    ibm-cloud.kubernetes.io/zone=syd05\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.195.53.14\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    privateVLAN=2723074\n                    publicVLAN=2723072\n                    topology.kubernetes.io/region=au-syd\n                    topology.kubernetes.io/zone=syd05\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 17 Feb 2020 14:36:27 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.195.53.14\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 17 Feb 2020 16:31:26 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Mon, 17 Feb 2020 16:29:33 +0000   Mon, 17 Feb 2020 14:36:27 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Mon, 17 Feb 2020 16:29:33 +0000   Mon, 17 Feb 2020 14:36:27 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Mon, 17 Feb 2020 16:29:33 +0000   Mon, 17 Feb 2020 14:36:27 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Mon, 17 Feb 2020 16:29:33 +0000   Mon, 17 Feb 2020 14:36:37 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.195.53.14\n  ExternalIP:  135.90.88.54\n  Hostname:    10.195.53.14\nCapacity:\n  cpu:                4\n  ephemeral-storage:  102685624Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16419700Ki\n  pods:               110\nAllocatable:\n  cpu:                3910m\n  ephemeral-storage:  99892574949\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             13627252Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 8731e0da83f942fa980b90aaede3044b\n  System UUID:                7FF15A55-E2FA-C889-CAEC-1A2929D72A49\n  Boot ID:                    36e545cd-b53b-4cda-8691-b07d33e939aa\n  Kernel Version:             4.15.0-76-generic\n  OS Image:                   Ubuntu 18.04.4 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.3.2\n  Kubelet Version:            v1.17.2+IKS\n  Kube-Proxy Version:         v1.17.2+IKS\nProviderID:                   ibm://fee034388aa6435883a1f720010ab3a2///bp5a3ais0mfj6cn3dl60/kube-bp5a3ais0mfj6cn3dl60-kubee2epvga-default-00000369\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  default                     test-k8s-e2e-pvg-master-verification                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         111m\n  ibm-system                  ibm-cloud-provider-ip-135-90-78-211-7555ccd494-hhqb2       5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         109m\n  kube-system                 calico-node-94c2k                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         115m\n  kube-system                 coredns-5b567488dd-kckdk                                   100m (2%)     0 (0%)      70Mi (0%)        400Mi (3%)     92m\n  kube-system                 ibm-keepalived-watcher-8pbd8                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         115m\n  kube-system                 ibm-master-proxy-static-10.195.53.14                       25m (0%)      300m (7%)   32M (0%)         512M (3%)      115m\n  kube-system                 public-crbp5a3ais0mfj6cn3dl60-alb1-79849b9bd6-4z2xg        10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         106m\n  kubectl-5776                agnhost-master-2p5p5                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-bc697ed487d7421d-st8jd    0 (0%)        0 (0%)      0 (0%)           0 (0%)         16m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests       Limits\n  --------           --------       ------\n  cpu                395m (10%)     300m (7%)\n  memory             307730Ki (2%)  909600Ki (6%)\n  ephemeral-storage  0 (0%)         0 (0%)\nEvents:              <none>\n"
Feb 17 16:31:35.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 describe namespace kubectl-5776'
Feb 17 16:31:35.204: INFO: stderr: ""
Feb 17 16:31:35.204: INFO: stdout: "Name:         kubectl-5776\nLabels:       e2e-framework=kubectl\n              e2e-run=8eaa2618-b2be-4428-99ba-39b01a63fb56\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:31:35.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5776" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":280,"completed":54,"skipped":918,"failed":0}

------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:31:35.233: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-6841
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Feb 17 16:31:36.164: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 16:31:39.225: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 16:31:39.233: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 16:32:09.878: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-657-crd failed: Post https://e2e-test-crd-conversion-webhook.crd-webhook-6841.svc:9443/crdconvert?timeout=30s: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:32:10.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6841" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

• [SLOW TEST:35.506 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":280,"completed":55,"skipped":918,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:32:10.739: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1955
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0217 16:32:21.131905      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 17 16:32:21.132: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:32:21.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1955" for this suite.

• [SLOW TEST:10.417 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":280,"completed":56,"skipped":930,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:32:21.156: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9076
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-34c910cd-7d50-4e55-b1a9-8947d4bd5291
STEP: Creating a pod to test consume configMaps
Feb 17 16:32:21.382: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-183129ba-7907-4d76-b775-d29581d9e071" in namespace "projected-9076" to be "success or failure"
Feb 17 16:32:21.392: INFO: Pod "pod-projected-configmaps-183129ba-7907-4d76-b775-d29581d9e071": Phase="Pending", Reason="", readiness=false. Elapsed: 10.374692ms
Feb 17 16:32:23.403: INFO: Pod "pod-projected-configmaps-183129ba-7907-4d76-b775-d29581d9e071": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021329374s
STEP: Saw pod success
Feb 17 16:32:23.403: INFO: Pod "pod-projected-configmaps-183129ba-7907-4d76-b775-d29581d9e071" satisfied condition "success or failure"
Feb 17 16:32:23.414: INFO: Trying to get logs from node 10.195.53.9 pod pod-projected-configmaps-183129ba-7907-4d76-b775-d29581d9e071 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 16:32:23.481: INFO: Waiting for pod pod-projected-configmaps-183129ba-7907-4d76-b775-d29581d9e071 to disappear
Feb 17 16:32:23.492: INFO: Pod pod-projected-configmaps-183129ba-7907-4d76-b775-d29581d9e071 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:32:23.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9076" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":57,"skipped":935,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:32:23.521: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8080
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Feb 17 16:32:27.788: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8080 PodName:pod-sharedvolume-fd90c939-8aca-4844-96a7-79358b33b179 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 16:32:27.788: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 16:32:27.938: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:32:27.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8080" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":280,"completed":58,"skipped":944,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:32:27.968: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8055
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 16:32:28.158: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 17 16:32:31.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 --namespace=crd-publish-openapi-8055 create -f -'
Feb 17 16:32:31.835: INFO: stderr: ""
Feb 17 16:32:31.835: INFO: stdout: "e2e-test-crd-publish-openapi-2020-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 17 16:32:31.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 --namespace=crd-publish-openapi-8055 delete e2e-test-crd-publish-openapi-2020-crds test-cr'
Feb 17 16:32:31.947: INFO: stderr: ""
Feb 17 16:32:31.947: INFO: stdout: "e2e-test-crd-publish-openapi-2020-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Feb 17 16:32:31.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 --namespace=crd-publish-openapi-8055 apply -f -'
Feb 17 16:32:32.161: INFO: stderr: ""
Feb 17 16:32:32.161: INFO: stdout: "e2e-test-crd-publish-openapi-2020-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 17 16:32:32.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 --namespace=crd-publish-openapi-8055 delete e2e-test-crd-publish-openapi-2020-crds test-cr'
Feb 17 16:32:32.296: INFO: stderr: ""
Feb 17 16:32:32.296: INFO: stdout: "e2e-test-crd-publish-openapi-2020-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Feb 17 16:32:32.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 explain e2e-test-crd-publish-openapi-2020-crds'
Feb 17 16:32:32.577: INFO: stderr: ""
Feb 17 16:32:32.577: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2020-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:32:36.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8055" for this suite.

• [SLOW TEST:8.334 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":280,"completed":59,"skipped":953,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:32:36.303: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-5158
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Feb 17 16:32:37.018: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Feb 17 16:32:39.051: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553957, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553957, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553957, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553957, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 16:32:42.091: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 16:32:42.100: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:32:43.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5158" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

• [SLOW TEST:7.339 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":280,"completed":60,"skipped":976,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:32:43.643: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1922
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-47dfb310-11d6-4aa0-87e3-a8d364274581
STEP: Creating a pod to test consume configMaps
Feb 17 16:32:43.867: INFO: Waiting up to 5m0s for pod "pod-configmaps-304e821e-6b7e-4201-9b7a-7b97677c1d36" in namespace "configmap-1922" to be "success or failure"
Feb 17 16:32:43.878: INFO: Pod "pod-configmaps-304e821e-6b7e-4201-9b7a-7b97677c1d36": Phase="Pending", Reason="", readiness=false. Elapsed: 11.562748ms
Feb 17 16:32:45.890: INFO: Pod "pod-configmaps-304e821e-6b7e-4201-9b7a-7b97677c1d36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022866135s
STEP: Saw pod success
Feb 17 16:32:45.890: INFO: Pod "pod-configmaps-304e821e-6b7e-4201-9b7a-7b97677c1d36" satisfied condition "success or failure"
Feb 17 16:32:45.901: INFO: Trying to get logs from node 10.195.53.9 pod pod-configmaps-304e821e-6b7e-4201-9b7a-7b97677c1d36 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 16:32:45.959: INFO: Waiting for pod pod-configmaps-304e821e-6b7e-4201-9b7a-7b97677c1d36 to disappear
Feb 17 16:32:45.987: INFO: Pod pod-configmaps-304e821e-6b7e-4201-9b7a-7b97677c1d36 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:32:45.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1922" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":61,"skipped":978,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:32:46.021: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3718
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on node default medium
Feb 17 16:32:46.232: INFO: Waiting up to 5m0s for pod "pod-6e33d3b3-9b1a-4936-97d9-658e7f706726" in namespace "emptydir-3718" to be "success or failure"
Feb 17 16:32:46.242: INFO: Pod "pod-6e33d3b3-9b1a-4936-97d9-658e7f706726": Phase="Pending", Reason="", readiness=false. Elapsed: 10.541777ms
Feb 17 16:32:48.254: INFO: Pod "pod-6e33d3b3-9b1a-4936-97d9-658e7f706726": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022485693s
STEP: Saw pod success
Feb 17 16:32:48.254: INFO: Pod "pod-6e33d3b3-9b1a-4936-97d9-658e7f706726" satisfied condition "success or failure"
Feb 17 16:32:48.264: INFO: Trying to get logs from node 10.195.53.9 pod pod-6e33d3b3-9b1a-4936-97d9-658e7f706726 container test-container: <nil>
STEP: delete the pod
Feb 17 16:32:48.322: INFO: Waiting for pod pod-6e33d3b3-9b1a-4936-97d9-658e7f706726 to disappear
Feb 17 16:32:48.332: INFO: Pod pod-6e33d3b3-9b1a-4936-97d9-658e7f706726 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:32:48.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3718" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":62,"skipped":982,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:32:48.368: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7576
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-projected-all-test-volume-d8cd5685-578f-4241-b105-97a0330f03fb
STEP: Creating secret with name secret-projected-all-test-volume-069a13f5-536d-4548-9ba8-1a64e6d0cf45
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb 17 16:32:48.603: INFO: Waiting up to 5m0s for pod "projected-volume-8412a628-3df9-4a6d-9071-9272967e2350" in namespace "projected-7576" to be "success or failure"
Feb 17 16:32:48.612: INFO: Pod "projected-volume-8412a628-3df9-4a6d-9071-9272967e2350": Phase="Pending", Reason="", readiness=false. Elapsed: 9.361884ms
Feb 17 16:32:50.628: INFO: Pod "projected-volume-8412a628-3df9-4a6d-9071-9272967e2350": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025344547s
STEP: Saw pod success
Feb 17 16:32:50.629: INFO: Pod "projected-volume-8412a628-3df9-4a6d-9071-9272967e2350" satisfied condition "success or failure"
Feb 17 16:32:50.640: INFO: Trying to get logs from node 10.195.53.9 pod projected-volume-8412a628-3df9-4a6d-9071-9272967e2350 container projected-all-volume-test: <nil>
STEP: delete the pod
Feb 17 16:32:50.703: INFO: Waiting for pod projected-volume-8412a628-3df9-4a6d-9071-9272967e2350 to disappear
Feb 17 16:32:50.713: INFO: Pod projected-volume-8412a628-3df9-4a6d-9071-9272967e2350 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:32:50.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7576" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":280,"completed":63,"skipped":1004,"failed":0}

------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:32:50.739: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2422
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2422.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2422.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2422.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2422.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 17 16:32:53.038: INFO: DNS probes using dns-test-b819f5e8-992f-40ae-9bb7-b3c9289d69de succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2422.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2422.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2422.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2422.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 17 16:32:55.166: INFO: File wheezy_udp@dns-test-service-3.dns-2422.svc.cluster.local from pod  dns-2422/dns-test-6ac02199-d81e-41ef-b781-3e9e9cc439c2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 17 16:32:55.183: INFO: File jessie_udp@dns-test-service-3.dns-2422.svc.cluster.local from pod  dns-2422/dns-test-6ac02199-d81e-41ef-b781-3e9e9cc439c2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 17 16:32:55.183: INFO: Lookups using dns-2422/dns-test-6ac02199-d81e-41ef-b781-3e9e9cc439c2 failed for: [wheezy_udp@dns-test-service-3.dns-2422.svc.cluster.local jessie_udp@dns-test-service-3.dns-2422.svc.cluster.local]

Feb 17 16:33:00.198: INFO: File wheezy_udp@dns-test-service-3.dns-2422.svc.cluster.local from pod  dns-2422/dns-test-6ac02199-d81e-41ef-b781-3e9e9cc439c2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 17 16:33:00.212: INFO: Lookups using dns-2422/dns-test-6ac02199-d81e-41ef-b781-3e9e9cc439c2 failed for: [wheezy_udp@dns-test-service-3.dns-2422.svc.cluster.local]

Feb 17 16:33:05.199: INFO: File wheezy_udp@dns-test-service-3.dns-2422.svc.cluster.local from pod  dns-2422/dns-test-6ac02199-d81e-41ef-b781-3e9e9cc439c2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 17 16:33:05.213: INFO: File jessie_udp@dns-test-service-3.dns-2422.svc.cluster.local from pod  dns-2422/dns-test-6ac02199-d81e-41ef-b781-3e9e9cc439c2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 17 16:33:05.214: INFO: Lookups using dns-2422/dns-test-6ac02199-d81e-41ef-b781-3e9e9cc439c2 failed for: [wheezy_udp@dns-test-service-3.dns-2422.svc.cluster.local jessie_udp@dns-test-service-3.dns-2422.svc.cluster.local]

Feb 17 16:33:10.199: INFO: File wheezy_udp@dns-test-service-3.dns-2422.svc.cluster.local from pod  dns-2422/dns-test-6ac02199-d81e-41ef-b781-3e9e9cc439c2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 17 16:33:10.214: INFO: Lookups using dns-2422/dns-test-6ac02199-d81e-41ef-b781-3e9e9cc439c2 failed for: [wheezy_udp@dns-test-service-3.dns-2422.svc.cluster.local]

Feb 17 16:33:15.212: INFO: File jessie_udp@dns-test-service-3.dns-2422.svc.cluster.local from pod  dns-2422/dns-test-6ac02199-d81e-41ef-b781-3e9e9cc439c2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 17 16:33:15.213: INFO: Lookups using dns-2422/dns-test-6ac02199-d81e-41ef-b781-3e9e9cc439c2 failed for: [jessie_udp@dns-test-service-3.dns-2422.svc.cluster.local]

Feb 17 16:33:20.221: INFO: File jessie_udp@dns-test-service-3.dns-2422.svc.cluster.local from pod  dns-2422/dns-test-6ac02199-d81e-41ef-b781-3e9e9cc439c2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 17 16:33:20.221: INFO: Lookups using dns-2422/dns-test-6ac02199-d81e-41ef-b781-3e9e9cc439c2 failed for: [jessie_udp@dns-test-service-3.dns-2422.svc.cluster.local]

Feb 17 16:33:25.213: INFO: DNS probes using dns-test-6ac02199-d81e-41ef-b781-3e9e9cc439c2 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2422.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2422.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2422.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2422.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 17 16:33:27.387: INFO: DNS probes using dns-test-aa01cc55-3551-4092-8082-a4476022832b succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:33:27.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2422" for this suite.

• [SLOW TEST:36.761 seconds]
[sig-network] DNS
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":280,"completed":64,"skipped":1004,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:33:27.501: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-940
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Feb 17 16:33:30.309: INFO: Successfully updated pod "annotationupdate2b92bc77-92e6-4208-9d72-6abcc3864e6a"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:33:34.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-940" for this suite.

• [SLOW TEST:6.903 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":65,"skipped":1034,"failed":0}
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:33:34.404: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7700
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-7700
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7700
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7700
Feb 17 16:33:34.635: INFO: Found 0 stateful pods, waiting for 1
Feb 17 16:33:44.648: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb 17 16:33:44.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-7700 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 16:33:44.897: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 16:33:44.897: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 16:33:44.897: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 17 16:33:44.908: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 17 16:33:54.922: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 17 16:33:54.922: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 16:33:54.960: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998457s
Feb 17 16:33:55.972: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.989061925s
Feb 17 16:33:56.984: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.976916088s
Feb 17 16:33:58.014: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.965151379s
Feb 17 16:33:59.025: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.934668651s
Feb 17 16:34:00.037: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.923374071s
Feb 17 16:34:01.049: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.9118036s
Feb 17 16:34:02.060: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.89961561s
Feb 17 16:34:03.072: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.888726552s
Feb 17 16:34:04.084: INFO: Verifying statefulset ss doesn't scale past 1 for another 876.528038ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7700
Feb 17 16:34:05.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-7700 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 16:34:05.350: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 17 16:34:05.350: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 17 16:34:05.350: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 17 16:34:05.362: INFO: Found 1 stateful pods, waiting for 3
Feb 17 16:34:15.375: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 16:34:15.375: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 16:34:15.375: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb 17 16:34:15.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-7700 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 16:34:15.653: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 16:34:15.653: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 16:34:15.653: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 17 16:34:15.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-7700 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 16:34:15.891: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 16:34:15.891: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 16:34:15.891: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 17 16:34:15.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-7700 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 16:34:16.166: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 16:34:16.166: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 16:34:16.166: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 17 16:34:16.166: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 16:34:16.175: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Feb 17 16:34:26.197: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 17 16:34:26.197: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 17 16:34:26.198: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 17 16:34:26.227: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998469s
Feb 17 16:34:27.239: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.98922435s
Feb 17 16:34:28.252: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.977109369s
Feb 17 16:34:29.263: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.964075627s
Feb 17 16:34:30.276: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.952579977s
Feb 17 16:34:31.289: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.939711872s
Feb 17 16:34:32.302: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.927364424s
Feb 17 16:34:33.315: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.914096835s
Feb 17 16:34:34.327: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.901505314s
Feb 17 16:34:35.341: INFO: Verifying statefulset ss doesn't scale past 3 for another 889.355593ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7700
Feb 17 16:34:36.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-7700 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 16:34:36.615: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 17 16:34:36.615: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 17 16:34:36.615: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 17 16:34:36.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-7700 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 16:34:36.996: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 17 16:34:36.996: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 17 16:34:36.996: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 17 16:34:36.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-7700 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 16:34:37.236: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 17 16:34:37.236: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 17 16:34:37.236: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 17 16:34:37.236: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Feb 17 16:35:07.281: INFO: Deleting all statefulset in ns statefulset-7700
Feb 17 16:35:07.289: INFO: Scaling statefulset ss to 0
Feb 17 16:35:07.318: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 16:35:07.326: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:35:07.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7700" for this suite.

• [SLOW TEST:92.981 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":280,"completed":66,"skipped":1034,"failed":0}
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:35:07.385: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-645
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-a0493f40-c33d-4559-98ac-0946796c1042
STEP: Creating a pod to test consume secrets
Feb 17 16:35:07.612: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3c7755a5-0e11-41ee-9c59-463fe8b3d7ea" in namespace "projected-645" to be "success or failure"
Feb 17 16:35:07.624: INFO: Pod "pod-projected-secrets-3c7755a5-0e11-41ee-9c59-463fe8b3d7ea": Phase="Pending", Reason="", readiness=false. Elapsed: 10.981918ms
Feb 17 16:35:09.635: INFO: Pod "pod-projected-secrets-3c7755a5-0e11-41ee-9c59-463fe8b3d7ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022365594s
STEP: Saw pod success
Feb 17 16:35:09.635: INFO: Pod "pod-projected-secrets-3c7755a5-0e11-41ee-9c59-463fe8b3d7ea" satisfied condition "success or failure"
Feb 17 16:35:09.646: INFO: Trying to get logs from node 10.195.53.9 pod pod-projected-secrets-3c7755a5-0e11-41ee-9c59-463fe8b3d7ea container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 17 16:35:09.738: INFO: Waiting for pod pod-projected-secrets-3c7755a5-0e11-41ee-9c59-463fe8b3d7ea to disappear
Feb 17 16:35:09.747: INFO: Pod pod-projected-secrets-3c7755a5-0e11-41ee-9c59-463fe8b3d7ea no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:35:09.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-645" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":67,"skipped":1034,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:35:09.776: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9750
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 16:35:10.935: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 16:35:13.995: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Feb 17 16:35:16.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 attach --namespace=webhook-9750 to-be-attached-pod -i -c=container1'
Feb 17 16:35:16.228: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:35:16.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9750" for this suite.
STEP: Destroying namespace "webhook-9750-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.624 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":280,"completed":68,"skipped":1040,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:35:16.401: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4599
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 17 16:35:16.611: INFO: Waiting up to 5m0s for pod "pod-48c8940d-b2d4-49bd-901e-1cb9b42de701" in namespace "emptydir-4599" to be "success or failure"
Feb 17 16:35:16.622: INFO: Pod "pod-48c8940d-b2d4-49bd-901e-1cb9b42de701": Phase="Pending", Reason="", readiness=false. Elapsed: 11.402909ms
Feb 17 16:35:18.637: INFO: Pod "pod-48c8940d-b2d4-49bd-901e-1cb9b42de701": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026195299s
STEP: Saw pod success
Feb 17 16:35:18.637: INFO: Pod "pod-48c8940d-b2d4-49bd-901e-1cb9b42de701" satisfied condition "success or failure"
Feb 17 16:35:18.648: INFO: Trying to get logs from node 10.195.53.9 pod pod-48c8940d-b2d4-49bd-901e-1cb9b42de701 container test-container: <nil>
STEP: delete the pod
Feb 17 16:35:18.707: INFO: Waiting for pod pod-48c8940d-b2d4-49bd-901e-1cb9b42de701 to disappear
Feb 17 16:35:18.717: INFO: Pod pod-48c8940d-b2d4-49bd-901e-1cb9b42de701 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:35:18.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4599" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":69,"skipped":1093,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:35:18.745: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5125
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5125
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-5125
I0217 16:35:19.005514      24 runners.go:189] Created replication controller with name: externalname-service, namespace: services-5125, replica count: 2
Feb 17 16:35:22.056: INFO: Creating new exec pod
I0217 16:35:22.056205      24 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 17 16:35:27.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=services-5125 execpod9tfdm -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Feb 17 16:35:27.345: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 17 16:35:27.345: INFO: stdout: ""
Feb 17 16:35:27.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=services-5125 execpod9tfdm -- /bin/sh -x -c nc -zv -t -w 2 172.21.56.194 80'
Feb 17 16:35:27.600: INFO: stderr: "+ nc -zv -t -w 2 172.21.56.194 80\nConnection to 172.21.56.194 80 port [tcp/http] succeeded!\n"
Feb 17 16:35:27.600: INFO: stdout: ""
Feb 17 16:35:27.600: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:35:27.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5125" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:8.939 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":280,"completed":70,"skipped":1100,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:35:27.684: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8945
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 17 16:35:27.900: INFO: Waiting up to 5m0s for pod "pod-41a79a29-4e1b-4cea-bb76-dfdaadc24218" in namespace "emptydir-8945" to be "success or failure"
Feb 17 16:35:27.910: INFO: Pod "pod-41a79a29-4e1b-4cea-bb76-dfdaadc24218": Phase="Pending", Reason="", readiness=false. Elapsed: 10.479ms
Feb 17 16:35:29.924: INFO: Pod "pod-41a79a29-4e1b-4cea-bb76-dfdaadc24218": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023532126s
Feb 17 16:35:31.936: INFO: Pod "pod-41a79a29-4e1b-4cea-bb76-dfdaadc24218": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035706281s
Feb 17 16:35:33.948: INFO: Pod "pod-41a79a29-4e1b-4cea-bb76-dfdaadc24218": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.047724606s
STEP: Saw pod success
Feb 17 16:35:33.948: INFO: Pod "pod-41a79a29-4e1b-4cea-bb76-dfdaadc24218" satisfied condition "success or failure"
Feb 17 16:35:33.958: INFO: Trying to get logs from node 10.195.53.14 pod pod-41a79a29-4e1b-4cea-bb76-dfdaadc24218 container test-container: <nil>
STEP: delete the pod
Feb 17 16:35:34.054: INFO: Waiting for pod pod-41a79a29-4e1b-4cea-bb76-dfdaadc24218 to disappear
Feb 17 16:35:34.064: INFO: Pod pod-41a79a29-4e1b-4cea-bb76-dfdaadc24218 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:35:34.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8945" for this suite.

• [SLOW TEST:6.405 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":71,"skipped":1105,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:35:34.090: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-9793
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:35:34.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-9793" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":280,"completed":72,"skipped":1123,"failed":0}
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:35:34.442: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-574
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Feb 17 16:35:34.639: INFO: PodSpec: initContainers in spec.initContainers
Feb 17 16:36:19.358: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-48813162-65ef-48dd-b625-09f64ff30c82", GenerateName:"", Namespace:"init-container-574", SelfLink:"/api/v1/namespaces/init-container-574/pods/pod-init-48813162-65ef-48dd-b625-09f64ff30c82", UID:"e0cd3237-3a91-47a6-bb7b-288290304449", ResourceVersion:"36019", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63717554134, loc:(*time.Location)(0x7db7bc0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"639572928"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-x9jtp", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc003594a40), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-x9jtp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-x9jtp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-x9jtp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0049eea58), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.195.53.9", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc004887ec0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0049eeae0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0049eeb00)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0049eeb08), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0049eeb0c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717554134, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717554134, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717554134, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717554134, loc:(*time.Location)(0x7db7bc0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.195.53.9", PodIP:"172.30.20.120", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.20.120"}}, StartTime:(*v1.Time)(0xc0030514c0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0022e2af0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0022e2b60)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://226e1af8e8c8129215f8aead013ab87d202a8968ac5d3292b671c7328ffc6291", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003051500), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0030514e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc0049eeb8f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:36:19.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-574" for this suite.

• [SLOW TEST:44.945 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":280,"completed":73,"skipped":1129,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:36:19.387: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7494
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb 17 16:36:19.633: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7494 /api/v1/namespaces/watch-7494/configmaps/e2e-watch-test-label-changed 7d88c316-363f-4190-9d06-be0e3606571a 36028 0 2020-02-17 16:36:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 17 16:36:19.633: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7494 /api/v1/namespaces/watch-7494/configmaps/e2e-watch-test-label-changed 7d88c316-363f-4190-9d06-be0e3606571a 36029 0 2020-02-17 16:36:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 17 16:36:19.633: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7494 /api/v1/namespaces/watch-7494/configmaps/e2e-watch-test-label-changed 7d88c316-363f-4190-9d06-be0e3606571a 36030 0 2020-02-17 16:36:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb 17 16:36:29.713: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7494 /api/v1/namespaces/watch-7494/configmaps/e2e-watch-test-label-changed 7d88c316-363f-4190-9d06-be0e3606571a 36073 0 2020-02-17 16:36:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 17 16:36:29.713: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7494 /api/v1/namespaces/watch-7494/configmaps/e2e-watch-test-label-changed 7d88c316-363f-4190-9d06-be0e3606571a 36074 0 2020-02-17 16:36:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Feb 17 16:36:29.713: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7494 /api/v1/namespaces/watch-7494/configmaps/e2e-watch-test-label-changed 7d88c316-363f-4190-9d06-be0e3606571a 36076 0 2020-02-17 16:36:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:36:29.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7494" for this suite.

• [SLOW TEST:10.352 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":280,"completed":74,"skipped":1139,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:36:29.740: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7048
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:36:54.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7048" for this suite.

• [SLOW TEST:24.859 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":280,"completed":75,"skipped":1153,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:36:54.599: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6365
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-80b1eccb-cbdc-4f50-90fc-abca9aaff31b
STEP: Creating a pod to test consume secrets
Feb 17 16:36:54.831: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-80872d66-3d23-4b57-aeea-44458a16a93b" in namespace "projected-6365" to be "success or failure"
Feb 17 16:36:54.848: INFO: Pod "pod-projected-secrets-80872d66-3d23-4b57-aeea-44458a16a93b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.887257ms
Feb 17 16:36:57.016: INFO: Pod "pod-projected-secrets-80872d66-3d23-4b57-aeea-44458a16a93b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.185004401s
STEP: Saw pod success
Feb 17 16:36:57.016: INFO: Pod "pod-projected-secrets-80872d66-3d23-4b57-aeea-44458a16a93b" satisfied condition "success or failure"
Feb 17 16:36:57.027: INFO: Trying to get logs from node 10.195.53.9 pod pod-projected-secrets-80872d66-3d23-4b57-aeea-44458a16a93b container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 17 16:36:57.119: INFO: Waiting for pod pod-projected-secrets-80872d66-3d23-4b57-aeea-44458a16a93b to disappear
Feb 17 16:36:57.130: INFO: Pod pod-projected-secrets-80872d66-3d23-4b57-aeea-44458a16a93b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:36:57.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6365" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":76,"skipped":1156,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:36:57.160: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7539
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Feb 17 16:37:27.455: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0217 16:37:27.455948      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:37:27.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7539" for this suite.

• [SLOW TEST:30.319 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":280,"completed":77,"skipped":1184,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:37:27.479: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5513
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl logs
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1464
STEP: creating an pod
Feb 17 16:37:27.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.8 --namespace=kubectl-5513 -- logs-generator --log-lines-total 100 --run-duration 20s'
Feb 17 16:37:27.792: INFO: stderr: ""
Feb 17 16:37:27.792: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Waiting for log generator to start.
Feb 17 16:37:27.792: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Feb 17 16:37:27.792: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5513" to be "running and ready, or succeeded"
Feb 17 16:37:27.803: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 10.339131ms
Feb 17 16:37:29.816: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.024021419s
Feb 17 16:37:29.816: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Feb 17 16:37:29.816: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Feb 17 16:37:29.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 logs logs-generator logs-generator --namespace=kubectl-5513'
Feb 17 16:37:30.007: INFO: stderr: ""
Feb 17 16:37:30.007: INFO: stdout: "I0217 16:37:28.955519       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/gjc 305\nI0217 16:37:29.155663       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/bjq2 398\nI0217 16:37:29.355744       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/g7z 341\nI0217 16:37:29.555689       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/bhgf 493\nI0217 16:37:29.755659       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/5sl 470\nI0217 16:37:29.955678       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/6wqx 207\n"
STEP: limiting log lines
Feb 17 16:37:30.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 logs logs-generator logs-generator --namespace=kubectl-5513 --tail=1'
Feb 17 16:37:30.134: INFO: stderr: ""
Feb 17 16:37:30.134: INFO: stdout: "I0217 16:37:29.955678       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/6wqx 207\n"
Feb 17 16:37:30.134: INFO: got output "I0217 16:37:29.955678       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/6wqx 207\n"
STEP: limiting log bytes
Feb 17 16:37:30.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 logs logs-generator logs-generator --namespace=kubectl-5513 --limit-bytes=1'
Feb 17 16:37:30.315: INFO: stderr: ""
Feb 17 16:37:30.315: INFO: stdout: "I"
Feb 17 16:37:30.315: INFO: got output "I"
STEP: exposing timestamps
Feb 17 16:37:30.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 logs logs-generator logs-generator --namespace=kubectl-5513 --tail=1 --timestamps'
Feb 17 16:37:30.444: INFO: stderr: ""
Feb 17 16:37:30.444: INFO: stdout: "2020-02-17T16:37:30.356807084Z I0217 16:37:30.356668       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/n2lf 522\n"
Feb 17 16:37:30.444: INFO: got output "2020-02-17T16:37:30.356807084Z I0217 16:37:30.356668       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/n2lf 522\n"
STEP: restricting to a time range
Feb 17 16:37:32.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 logs logs-generator logs-generator --namespace=kubectl-5513 --since=1s'
Feb 17 16:37:33.074: INFO: stderr: ""
Feb 17 16:37:33.074: INFO: stdout: "I0217 16:37:32.155666       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/l8w 413\nI0217 16:37:32.355685       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/6g6 370\nI0217 16:37:32.555667       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/w8kg 568\nI0217 16:37:32.755693       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/z747 283\nI0217 16:37:32.955683       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/9vn 286\n"
Feb 17 16:37:33.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 logs logs-generator logs-generator --namespace=kubectl-5513 --since=24h'
Feb 17 16:37:33.216: INFO: stderr: ""
Feb 17 16:37:33.216: INFO: stdout: "I0217 16:37:28.955519       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/gjc 305\nI0217 16:37:29.155663       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/bjq2 398\nI0217 16:37:29.355744       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/g7z 341\nI0217 16:37:29.555689       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/bhgf 493\nI0217 16:37:29.755659       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/5sl 470\nI0217 16:37:29.955678       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/6wqx 207\nI0217 16:37:30.155657       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/bpm 574\nI0217 16:37:30.356668       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/n2lf 522\nI0217 16:37:30.555784       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/p26 511\nI0217 16:37:30.755677       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/b75j 343\nI0217 16:37:30.955648       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/msd 525\nI0217 16:37:31.155662       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/s95 243\nI0217 16:37:31.355689       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/9sl8 369\nI0217 16:37:31.555679       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/79xm 201\nI0217 16:37:31.755658       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/xm9 421\nI0217 16:37:31.955677       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/87p 238\nI0217 16:37:32.155666       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/l8w 413\nI0217 16:37:32.355685       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/6g6 370\nI0217 16:37:32.555667       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/w8kg 568\nI0217 16:37:32.755693       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/z747 283\nI0217 16:37:32.955683       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/9vn 286\nI0217 16:37:33.155692       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/tzg 240\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1470
Feb 17 16:37:33.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 delete pod logs-generator --namespace=kubectl-5513'
Feb 17 16:37:46.404: INFO: stderr: ""
Feb 17 16:37:46.404: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:37:46.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5513" for this suite.

• [SLOW TEST:18.952 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":280,"completed":78,"skipped":1189,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:37:46.432: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3211
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-53f1accd-8ec1-4fc2-b519-6ae32bd3a463
STEP: Creating secret with name s-test-opt-upd-f6b671bd-a4b5-4621-b2c2-ba77a509479a
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-53f1accd-8ec1-4fc2-b519-6ae32bd3a463
STEP: Updating secret s-test-opt-upd-f6b671bd-a4b5-4621-b2c2-ba77a509479a
STEP: Creating secret with name s-test-opt-create-f0637b8c-464b-4402-bcb8-a56cb7acb551
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:37:50.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3211" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":79,"skipped":1224,"failed":0}

------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:37:50.922: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1458
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1458.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1458.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1458.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1458.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1458.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1458.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 17 16:38:09.297: INFO: DNS probes using dns-1458/dns-test-efee37c5-cca0-497f-a55f-275ab8e94881 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:38:09.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1458" for this suite.

• [SLOW TEST:18.525 seconds]
[sig-network] DNS
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":280,"completed":80,"skipped":1224,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:38:09.447: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9738
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1692
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 17 16:38:09.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-9738'
Feb 17 16:38:09.855: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 17 16:38:09.855: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Feb 17 16:38:09.868: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Feb 17 16:38:09.870: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Feb 17 16:38:09.879: INFO: scanned /root for discovery docs: <nil>
Feb 17 16:38:09.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-9738'
Feb 17 16:38:25.781: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 17 16:38:25.781: INFO: stdout: "Created e2e-test-httpd-rc-4a05778cee25c40b218af96def68def2\nScaling up e2e-test-httpd-rc-4a05778cee25c40b218af96def68def2 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-4a05778cee25c40b218af96def68def2 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-4a05778cee25c40b218af96def68def2 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Feb 17 16:38:25.781: INFO: stdout: "Created e2e-test-httpd-rc-4a05778cee25c40b218af96def68def2\nScaling up e2e-test-httpd-rc-4a05778cee25c40b218af96def68def2 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-4a05778cee25c40b218af96def68def2 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-4a05778cee25c40b218af96def68def2 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Feb 17 16:38:25.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-9738'
Feb 17 16:38:25.883: INFO: stderr: ""
Feb 17 16:38:25.883: INFO: stdout: "e2e-test-httpd-rc-4a05778cee25c40b218af96def68def2-7mxd7 "
Feb 17 16:38:25.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods e2e-test-httpd-rc-4a05778cee25c40b218af96def68def2-7mxd7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9738'
Feb 17 16:38:25.981: INFO: stderr: ""
Feb 17 16:38:25.981: INFO: stdout: "true"
Feb 17 16:38:25.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods e2e-test-httpd-rc-4a05778cee25c40b218af96def68def2-7mxd7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9738'
Feb 17 16:38:26.087: INFO: stderr: ""
Feb 17 16:38:26.087: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Feb 17 16:38:26.088: INFO: e2e-test-httpd-rc-4a05778cee25c40b218af96def68def2-7mxd7 is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1698
Feb 17 16:38:26.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 delete rc e2e-test-httpd-rc --namespace=kubectl-9738'
Feb 17 16:38:26.191: INFO: stderr: ""
Feb 17 16:38:26.191: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:38:26.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9738" for this suite.

• [SLOW TEST:16.777 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl rolling-update should support rolling-update to same image  [Conformance]","total":280,"completed":81,"skipped":1246,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:38:26.224: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4342
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-57062570-0dbc-4d28-bcc4-4015687eb154
STEP: Creating a pod to test consume secrets
Feb 17 16:38:26.443: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-87a19ea3-4f25-446d-9132-76d00bf72b4f" in namespace "projected-4342" to be "success or failure"
Feb 17 16:38:26.453: INFO: Pod "pod-projected-secrets-87a19ea3-4f25-446d-9132-76d00bf72b4f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.052909ms
Feb 17 16:38:28.468: INFO: Pod "pod-projected-secrets-87a19ea3-4f25-446d-9132-76d00bf72b4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024287421s
Feb 17 16:38:30.480: INFO: Pod "pod-projected-secrets-87a19ea3-4f25-446d-9132-76d00bf72b4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036358425s
STEP: Saw pod success
Feb 17 16:38:30.480: INFO: Pod "pod-projected-secrets-87a19ea3-4f25-446d-9132-76d00bf72b4f" satisfied condition "success or failure"
Feb 17 16:38:30.491: INFO: Trying to get logs from node 10.195.53.9 pod pod-projected-secrets-87a19ea3-4f25-446d-9132-76d00bf72b4f container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 17 16:38:30.549: INFO: Waiting for pod pod-projected-secrets-87a19ea3-4f25-446d-9132-76d00bf72b4f to disappear
Feb 17 16:38:30.559: INFO: Pod pod-projected-secrets-87a19ea3-4f25-446d-9132-76d00bf72b4f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:38:30.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4342" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":82,"skipped":1291,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:38:30.586: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6420
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-b8243038-3abd-4dfd-af15-ee4dcba4a9b7
STEP: Creating a pod to test consume secrets
Feb 17 16:38:30.811: INFO: Waiting up to 5m0s for pod "pod-secrets-94fb43b6-f22c-4baa-9954-12aea93d743e" in namespace "secrets-6420" to be "success or failure"
Feb 17 16:38:30.823: INFO: Pod "pod-secrets-94fb43b6-f22c-4baa-9954-12aea93d743e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.852761ms
Feb 17 16:38:32.835: INFO: Pod "pod-secrets-94fb43b6-f22c-4baa-9954-12aea93d743e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023805014s
STEP: Saw pod success
Feb 17 16:38:32.835: INFO: Pod "pod-secrets-94fb43b6-f22c-4baa-9954-12aea93d743e" satisfied condition "success or failure"
Feb 17 16:38:32.846: INFO: Trying to get logs from node 10.195.53.9 pod pod-secrets-94fb43b6-f22c-4baa-9954-12aea93d743e container secret-volume-test: <nil>
STEP: delete the pod
Feb 17 16:38:32.905: INFO: Waiting for pod pod-secrets-94fb43b6-f22c-4baa-9954-12aea93d743e to disappear
Feb 17 16:38:32.917: INFO: Pod pod-secrets-94fb43b6-f22c-4baa-9954-12aea93d743e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:38:32.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6420" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":83,"skipped":1300,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:38:32.944: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7231
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:38:37.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7231" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":280,"completed":84,"skipped":1313,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:38:37.542: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4280
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-f3f8ab76-b137-4569-899a-8b3aaa7e9a47
STEP: Creating a pod to test consume secrets
Feb 17 16:38:37.771: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-708942a5-d708-417f-83d4-966d2a18ff17" in namespace "projected-4280" to be "success or failure"
Feb 17 16:38:37.782: INFO: Pod "pod-projected-secrets-708942a5-d708-417f-83d4-966d2a18ff17": Phase="Pending", Reason="", readiness=false. Elapsed: 11.432103ms
Feb 17 16:38:39.794: INFO: Pod "pod-projected-secrets-708942a5-d708-417f-83d4-966d2a18ff17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02328878s
STEP: Saw pod success
Feb 17 16:38:39.794: INFO: Pod "pod-projected-secrets-708942a5-d708-417f-83d4-966d2a18ff17" satisfied condition "success or failure"
Feb 17 16:38:39.806: INFO: Trying to get logs from node 10.195.53.9 pod pod-projected-secrets-708942a5-d708-417f-83d4-966d2a18ff17 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 17 16:38:39.865: INFO: Waiting for pod pod-projected-secrets-708942a5-d708-417f-83d4-966d2a18ff17 to disappear
Feb 17 16:38:39.876: INFO: Pod pod-projected-secrets-708942a5-d708-417f-83d4-966d2a18ff17 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:38:39.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4280" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":85,"skipped":1341,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:38:39.903: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5175
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-86c2852a-3272-47a6-a713-b2ff748f8717
STEP: Creating configMap with name cm-test-opt-upd-3993bfaf-988e-4cb8-9da2-cfa9f220e40a
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-86c2852a-3272-47a6-a713-b2ff748f8717
STEP: Updating configmap cm-test-opt-upd-3993bfaf-988e-4cb8-9da2-cfa9f220e40a
STEP: Creating configMap with name cm-test-opt-create-1d787f15-5ccc-46c5-b569-57f8a4b1a5db
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:39:57.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5175" for this suite.

• [SLOW TEST:77.430 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":86,"skipped":1362,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:39:57.334: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3114
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 16:39:57.542: INFO: Creating deployment "webserver-deployment"
Feb 17 16:39:57.554: INFO: Waiting for observed generation 1
Feb 17 16:39:59.578: INFO: Waiting for all required pods to come up
Feb 17 16:39:59.593: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb 17 16:40:01.621: INFO: Waiting for deployment "webserver-deployment" to complete
Feb 17 16:40:01.643: INFO: Updating deployment "webserver-deployment" with a non-existent image
Feb 17 16:40:01.664: INFO: Updating deployment webserver-deployment
Feb 17 16:40:01.664: INFO: Waiting for observed generation 2
Feb 17 16:40:03.687: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 17 16:40:03.698: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 17 16:40:03.709: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 17 16:40:03.740: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 17 16:40:03.740: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 17 16:40:03.751: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 17 16:40:03.772: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Feb 17 16:40:03.772: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Feb 17 16:40:03.795: INFO: Updating deployment webserver-deployment
Feb 17 16:40:03.795: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Feb 17 16:40:03.815: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 17 16:40:05.840: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Feb 17 16:40:05.862: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-3114 /apis/apps/v1/namespaces/deployment-3114/deployments/webserver-deployment 2a2d31a0-4097-49d5-803f-0b38dde3033d 37895 3 2020-02-17 16:39:57 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002c95718 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:10,UnavailableReplicas:23,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-02-17 16:40:03 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-02-17 16:40:05 +0000 UTC,LastTransitionTime:2020-02-17 16:39:57 +0000 UTC,},},ReadyReplicas:10,CollisionCount:nil,},}

Feb 17 16:40:05.874: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-3114 /apis/apps/v1/namespaces/deployment-3114/replicasets/webserver-deployment-c7997dcc8 37e2f228-aea7-4115-a1be-4210d96fdcc7 37727 3 2020-02-17 16:40:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 2a2d31a0-4097-49d5-803f-0b38dde3033d 0xc006457727 0xc006457728}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006457798 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 17 16:40:05.874: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Feb 17 16:40:05.875: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-3114 /apis/apps/v1/namespaces/deployment-3114/replicasets/webserver-deployment-595b5b9587 c7b492e9-81ed-449c-a8f5-a25bef4dd779 37897 3 2020-02-17 16:39:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 2a2d31a0-4097-49d5-803f-0b38dde3033d 0xc006457667 0xc006457668}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0064576c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:11,AvailableReplicas:11,Conditions:[]ReplicaSetCondition{},},}
Feb 17 16:40:05.894: INFO: Pod "webserver-deployment-595b5b9587-226jp" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-226jp webserver-deployment-595b5b9587- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-595b5b9587-226jp 866fe8d3-b0cc-4b07-89eb-67da7b56c004 37837 0 2020-02-17 16:40:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c7b492e9-81ed-449c-a8f5-a25bef4dd779 0xc006457c97 0xc006457c98}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.47,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.47,PodIP:172.30.117.217,StartTime:2020-02-17 16:40:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 16:40:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://7b7eaf314abe153dad6ee9a8d3fa59751aa42449de937cdbf63a3a1de93cdb70,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.117.217,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.894: INFO: Pod "webserver-deployment-595b5b9587-55nsw" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-55nsw webserver-deployment-595b5b9587- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-595b5b9587-55nsw 32fea4b7-0650-477f-a04f-dce1fffe05c5 37756 0 2020-02-17 16:40:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c7b492e9-81ed-449c-a8f5-a25bef4dd779 0xc006457e17 0xc006457e18}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.9,PodIP:,StartTime:2020-02-17 16:40:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.895: INFO: Pod "webserver-deployment-595b5b9587-5hn9n" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-5hn9n webserver-deployment-595b5b9587- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-595b5b9587-5hn9n 0b3f7f26-5bbf-4bf8-9044-28f45991bab9 37746 0 2020-02-17 16:40:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c7b492e9-81ed-449c-a8f5-a25bef4dd779 0xc006457f77 0xc006457f78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.14,PodIP:,StartTime:2020-02-17 16:40:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.895: INFO: Pod "webserver-deployment-595b5b9587-86g4x" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-86g4x webserver-deployment-595b5b9587- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-595b5b9587-86g4x 42e45b11-a4a1-46be-8c58-dc8c6d8c9f85 37518 0 2020-02-17 16:39:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c7b492e9-81ed-449c-a8f5-a25bef4dd779 0xc0030e00d7 0xc0030e00d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.47,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.47,PodIP:172.30.117.214,StartTime:2020-02-17 16:39:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 16:39:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://3f3198ec11a6e57bf15ea895cb9fce09994465e74f953165c08256ebcc6e60e9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.117.214,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.895: INFO: Pod "webserver-deployment-595b5b9587-9lnv9" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-9lnv9 webserver-deployment-595b5b9587- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-595b5b9587-9lnv9 8fb0c13c-9ee1-49c4-99bf-72e0af064249 37502 0 2020-02-17 16:39:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c7b492e9-81ed-449c-a8f5-a25bef4dd779 0xc0030e0267 0xc0030e0268}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.14,PodIP:172.30.89.211,StartTime:2020-02-17 16:39:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 16:39:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://3ff067e38c89da2a2953494036fc2143b34e9049cb017b007b4cc8bb599fd1fb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.89.211,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.895: INFO: Pod "webserver-deployment-595b5b9587-drbbb" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-drbbb webserver-deployment-595b5b9587- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-595b5b9587-drbbb 54bccb29-6a3a-4982-8e2f-9e012c390335 37522 0 2020-02-17 16:39:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c7b492e9-81ed-449c-a8f5-a25bef4dd779 0xc0030e0407 0xc0030e0408}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.47,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.47,PodIP:172.30.117.213,StartTime:2020-02-17 16:39:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 16:39:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://a840fb181e64c672d2d7893d452dd1be518e9688401cb14cf6d5be59f4800bd4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.117.213,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.896: INFO: Pod "webserver-deployment-595b5b9587-f62c9" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-f62c9 webserver-deployment-595b5b9587- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-595b5b9587-f62c9 a4146942-2cc6-44b3-8409-faa87b5595ee 37745 0 2020-02-17 16:40:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c7b492e9-81ed-449c-a8f5-a25bef4dd779 0xc0030e0597 0xc0030e0598}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.47,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.47,PodIP:,StartTime:2020-02-17 16:40:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.896: INFO: Pod "webserver-deployment-595b5b9587-g6cvl" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-g6cvl webserver-deployment-595b5b9587- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-595b5b9587-g6cvl 4161bb45-2cc1-4b6b-a1e1-dc9808f607e7 37769 0 2020-02-17 16:40:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c7b492e9-81ed-449c-a8f5-a25bef4dd779 0xc0030e0707 0xc0030e0708}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.14,PodIP:,StartTime:2020-02-17 16:40:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.896: INFO: Pod "webserver-deployment-595b5b9587-g7k6c" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-g7k6c webserver-deployment-595b5b9587- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-595b5b9587-g7k6c 72551a03-f3f9-4b3c-9fc4-3029ae210d80 37515 0 2020-02-17 16:39:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c7b492e9-81ed-449c-a8f5-a25bef4dd779 0xc0030e0867 0xc0030e0868}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.47,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.47,PodIP:172.30.117.212,StartTime:2020-02-17 16:39:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 16:39:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://50acfc4888cfb529226eb4ea007b1188ef77268a1f6a3e244ff746a8d6930cd4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.117.212,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.896: INFO: Pod "webserver-deployment-595b5b9587-gvxxh" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-gvxxh webserver-deployment-595b5b9587- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-595b5b9587-gvxxh 2d946464-c688-489a-beb1-94d41b69ab3f 37749 0 2020-02-17 16:40:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c7b492e9-81ed-449c-a8f5-a25bef4dd779 0xc0030e09e7 0xc0030e09e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.47,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.47,PodIP:,StartTime:2020-02-17 16:40:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.896: INFO: Pod "webserver-deployment-595b5b9587-gzpgs" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-gzpgs webserver-deployment-595b5b9587- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-595b5b9587-gzpgs a4c13c47-9823-4ad2-85a0-2209334c39ba 37532 0 2020-02-17 16:39:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c7b492e9-81ed-449c-a8f5-a25bef4dd779 0xc0030e0b47 0xc0030e0b48}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.9,PodIP:172.30.20.67,StartTime:2020-02-17 16:39:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 16:39:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://7c8b8218d30dd711595595aab640abfced333460ee8d92314ec8b5916d181a70,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.20.67,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.897: INFO: Pod "webserver-deployment-595b5b9587-hfmms" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-hfmms webserver-deployment-595b5b9587- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-595b5b9587-hfmms 1fe04826-1713-48f6-b390-3170874e42b9 37535 0 2020-02-17 16:39:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c7b492e9-81ed-449c-a8f5-a25bef4dd779 0xc0030e0cc7 0xc0030e0cc8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.9,PodIP:172.30.20.79,StartTime:2020-02-17 16:39:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 16:39:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://20dcb92d8e5f76e4e3930def8f5cd21a238fdda5568ac6f925549bdfeb0bd5ec,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.20.79,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.897: INFO: Pod "webserver-deployment-595b5b9587-hvvht" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-hvvht webserver-deployment-595b5b9587- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-595b5b9587-hvvht 921b4a49-6cdc-49fd-ae53-fa6ca6f709bb 37892 0 2020-02-17 16:40:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c7b492e9-81ed-449c-a8f5-a25bef4dd779 0xc0030e0e47 0xc0030e0e48}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.9,PodIP:172.30.20.84,StartTime:2020-02-17 16:40:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 16:40:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://6fea63e6603de3ba3a26053021981803fb3fb575f3eb2c66bcbc4558ff3cf9f9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.20.84,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.897: INFO: Pod "webserver-deployment-595b5b9587-jkx4z" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-jkx4z webserver-deployment-595b5b9587- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-595b5b9587-jkx4z 41d8d4ca-f053-4f2e-9178-9c55249501ec 37538 0 2020-02-17 16:39:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c7b492e9-81ed-449c-a8f5-a25bef4dd779 0xc0030e0fc7 0xc0030e0fc8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.9,PodIP:172.30.20.78,StartTime:2020-02-17 16:39:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 16:39:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://c9b8729706793bf27e9412455ff85c30bd8aa49fa59ea3e8d8f1cb1f303b24d0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.20.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.897: INFO: Pod "webserver-deployment-595b5b9587-jq5jd" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-jq5jd webserver-deployment-595b5b9587- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-595b5b9587-jq5jd 1654885f-a9c5-4d23-9969-2f411c42225f 37703 0 2020-02-17 16:40:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c7b492e9-81ed-449c-a8f5-a25bef4dd779 0xc0030e1147 0xc0030e1148}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.14,PodIP:,StartTime:2020-02-17 16:40:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.898: INFO: Pod "webserver-deployment-595b5b9587-llck6" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-llck6 webserver-deployment-595b5b9587- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-595b5b9587-llck6 47f334cf-1dba-40e8-a427-0a5dc70ae0b6 37767 0 2020-02-17 16:40:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c7b492e9-81ed-449c-a8f5-a25bef4dd779 0xc0030e12a7 0xc0030e12a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.9,PodIP:,StartTime:2020-02-17 16:40:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.898: INFO: Pod "webserver-deployment-595b5b9587-lsq94" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-lsq94 webserver-deployment-595b5b9587- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-595b5b9587-lsq94 b8374644-d376-4144-b1fc-bcb839b64878 37547 0 2020-02-17 16:39:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c7b492e9-81ed-449c-a8f5-a25bef4dd779 0xc0030e1407 0xc0030e1408}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:39:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.14,PodIP:172.30.89.213,StartTime:2020-02-17 16:39:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 16:39:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://c9db1622f74360fb159132fe84a9f74ec2121216ed089d3621b1341f2c82207e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.89.213,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.898: INFO: Pod "webserver-deployment-595b5b9587-p5kf8" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-p5kf8 webserver-deployment-595b5b9587- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-595b5b9587-p5kf8 279fba9e-ccb8-4531-a2a8-20f39e2af2ad 37896 0 2020-02-17 16:40:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c7b492e9-81ed-449c-a8f5-a25bef4dd779 0xc0030e1597 0xc0030e1598}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.9,PodIP:172.30.20.87,StartTime:2020-02-17 16:40:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 16:40:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://170eef168e46936d75981ae3d4c33416dfec6a6755a25e9b3644977261db0576,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.20.87,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.900: INFO: Pod "webserver-deployment-595b5b9587-pc5w8" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-pc5w8 webserver-deployment-595b5b9587- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-595b5b9587-pc5w8 5f966384-7464-4fc5-b026-6d5ed52f1fea 37755 0 2020-02-17 16:40:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c7b492e9-81ed-449c-a8f5-a25bef4dd779 0xc0030e1717 0xc0030e1718}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.14,PodIP:,StartTime:2020-02-17 16:40:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.901: INFO: Pod "webserver-deployment-595b5b9587-tbwbn" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-tbwbn webserver-deployment-595b5b9587- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-595b5b9587-tbwbn d1ea9b7f-0cd0-4bfc-9c3a-1657aca4020e 37741 0 2020-02-17 16:40:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c7b492e9-81ed-449c-a8f5-a25bef4dd779 0xc0030e1877 0xc0030e1878}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.14,PodIP:,StartTime:2020-02-17 16:40:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.901: INFO: Pod "webserver-deployment-c7997dcc8-4lfw7" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-4lfw7 webserver-deployment-c7997dcc8- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-c7997dcc8-4lfw7 69b76fac-ef97-4443-9029-1049895983ec 37743 0 2020-02-17 16:40:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 37e2f228-aea7-4115-a1be-4210d96fdcc7 0xc0030e19d7 0xc0030e19d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.9,PodIP:,StartTime:2020-02-17 16:40:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.901: INFO: Pod "webserver-deployment-c7997dcc8-52m9d" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-52m9d webserver-deployment-c7997dcc8- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-c7997dcc8-52m9d 9c05c633-3614-4af6-bc88-5f7ff651896e 37762 0 2020-02-17 16:40:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 37e2f228-aea7-4115-a1be-4210d96fdcc7 0xc0030e1b50 0xc0030e1b51}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.9,PodIP:,StartTime:2020-02-17 16:40:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.901: INFO: Pod "webserver-deployment-c7997dcc8-8z4ml" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-8z4ml webserver-deployment-c7997dcc8- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-c7997dcc8-8z4ml c9be5d04-6c09-4e64-b79e-09c80d0bdecb 37602 0 2020-02-17 16:40:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 37e2f228-aea7-4115-a1be-4210d96fdcc7 0xc0030e1cc0 0xc0030e1cc1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.14,PodIP:,StartTime:2020-02-17 16:40:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.902: INFO: Pod "webserver-deployment-c7997dcc8-bgk2n" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-bgk2n webserver-deployment-c7997dcc8- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-c7997dcc8-bgk2n 13366ffb-1303-4354-befb-54a211a74a24 37758 0 2020-02-17 16:40:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 37e2f228-aea7-4115-a1be-4210d96fdcc7 0xc0030e1e37 0xc0030e1e38}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.47,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.47,PodIP:,StartTime:2020-02-17 16:40:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.902: INFO: Pod "webserver-deployment-c7997dcc8-cllsp" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-cllsp webserver-deployment-c7997dcc8- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-c7997dcc8-cllsp 214171a9-4678-4ba4-96b5-014978ea7d7c 37754 0 2020-02-17 16:40:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 37e2f228-aea7-4115-a1be-4210d96fdcc7 0xc0030e1fb7 0xc0030e1fb8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.47,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.47,PodIP:,StartTime:2020-02-17 16:40:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.902: INFO: Pod "webserver-deployment-c7997dcc8-cz9h6" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-cz9h6 webserver-deployment-c7997dcc8- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-c7997dcc8-cz9h6 bda14180-13dd-4773-88b0-9a28d5b11aaa 37724 0 2020-02-17 16:40:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 37e2f228-aea7-4115-a1be-4210d96fdcc7 0xc004bc0137 0xc004bc0138}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.9,PodIP:,StartTime:2020-02-17 16:40:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.902: INFO: Pod "webserver-deployment-c7997dcc8-d5kvr" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-d5kvr webserver-deployment-c7997dcc8- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-c7997dcc8-d5kvr ffe4ce55-8e13-4d6d-9efd-6311f213395f 37798 0 2020-02-17 16:40:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 37e2f228-aea7-4115-a1be-4210d96fdcc7 0xc004bc02c0 0xc004bc02c1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.9,PodIP:172.30.20.82,StartTime:2020-02-17 16:40:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.20.82,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.902: INFO: Pod "webserver-deployment-c7997dcc8-jn546" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-jn546 webserver-deployment-c7997dcc8- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-c7997dcc8-jn546 c1c689b6-d2e3-4d78-9134-8dfa6fc07dd9 37830 0 2020-02-17 16:40:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 37e2f228-aea7-4115-a1be-4210d96fdcc7 0xc004bc0460 0xc004bc0461}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.47,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.47,PodIP:172.30.117.215,StartTime:2020-02-17 16:40:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.117.215,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.903: INFO: Pod "webserver-deployment-c7997dcc8-jwdth" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-jwdth webserver-deployment-c7997dcc8- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-c7997dcc8-jwdth b4a91414-c81d-489f-af4d-7e6b047a8c77 37604 0 2020-02-17 16:40:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 37e2f228-aea7-4115-a1be-4210d96fdcc7 0xc004bc0607 0xc004bc0608}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.47,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.47,PodIP:,StartTime:2020-02-17 16:40:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.903: INFO: Pod "webserver-deployment-c7997dcc8-k4nvx" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-k4nvx webserver-deployment-c7997dcc8- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-c7997dcc8-k4nvx 49ecfb0e-de2d-41a7-9fb9-4962a86c4ab2 37813 0 2020-02-17 16:40:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 37e2f228-aea7-4115-a1be-4210d96fdcc7 0xc004bc08b7 0xc004bc08b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.14,PodIP:172.30.89.215,StartTime:2020-02-17 16:40:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.89.215,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.903: INFO: Pod "webserver-deployment-c7997dcc8-p6jqr" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-p6jqr webserver-deployment-c7997dcc8- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-c7997dcc8-p6jqr 73e4e1f0-21f0-4aed-840f-5b3eb4848223 37760 0 2020-02-17 16:40:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 37e2f228-aea7-4115-a1be-4210d96fdcc7 0xc004bc0af7 0xc004bc0af8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.14,PodIP:,StartTime:2020-02-17 16:40:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.903: INFO: Pod "webserver-deployment-c7997dcc8-rn7bx" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-rn7bx webserver-deployment-c7997dcc8- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-c7997dcc8-rn7bx e9118440-ea4e-4ce1-ad12-b2fde8ad8894 37765 0 2020-02-17 16:40:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 37e2f228-aea7-4115-a1be-4210d96fdcc7 0xc004bc0dd7 0xc004bc0dd8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.14,PodIP:,StartTime:2020-02-17 16:40:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 16:40:05.904: INFO: Pod "webserver-deployment-c7997dcc8-tnccl" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-tnccl webserver-deployment-c7997dcc8- deployment-3114 /api/v1/namespaces/deployment-3114/pods/webserver-deployment-c7997dcc8-tnccl 5608d76c-f1e6-4d52-896c-79e204529591 37751 0 2020-02-17 16:40:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 37e2f228-aea7-4115-a1be-4210d96fdcc7 0xc004bc0fa7 0xc004bc0fa8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwzqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwzqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwzqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.14,PodIP:,StartTime:2020-02-17 16:40:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:40:05.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3114" for this suite.

• [SLOW TEST:8.596 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":280,"completed":87,"skipped":1380,"failed":0}
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:40:05.931: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6305
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 17 16:40:08.191: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:40:08.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6305" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":88,"skipped":1380,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:40:08.266: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7514
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb 17 16:40:08.480: INFO: Waiting up to 5m0s for pod "pod-90664c94-1c6f-4c8f-adbe-e8693a3f68b5" in namespace "emptydir-7514" to be "success or failure"
Feb 17 16:40:08.493: INFO: Pod "pod-90664c94-1c6f-4c8f-adbe-e8693a3f68b5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.139702ms
Feb 17 16:40:10.504: INFO: Pod "pod-90664c94-1c6f-4c8f-adbe-e8693a3f68b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0241757s
STEP: Saw pod success
Feb 17 16:40:10.504: INFO: Pod "pod-90664c94-1c6f-4c8f-adbe-e8693a3f68b5" satisfied condition "success or failure"
Feb 17 16:40:10.515: INFO: Trying to get logs from node 10.195.53.47 pod pod-90664c94-1c6f-4c8f-adbe-e8693a3f68b5 container test-container: <nil>
STEP: delete the pod
Feb 17 16:40:10.614: INFO: Waiting for pod pod-90664c94-1c6f-4c8f-adbe-e8693a3f68b5 to disappear
Feb 17 16:40:10.623: INFO: Pod pod-90664c94-1c6f-4c8f-adbe-e8693a3f68b5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:40:10.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7514" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":89,"skipped":1393,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:40:10.659: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9681
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating pod
Feb 17 16:40:12.912: INFO: Pod pod-hostip-910568ca-3842-4475-8bee-1640ab3c56bc has hostIP: 10.195.53.47
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:40:12.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9681" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":280,"completed":90,"skipped":1412,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:40:12.945: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5160
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Feb 17 16:40:15.771: INFO: Successfully updated pod "annotationupdate1c10f164-21f6-4646-bca7-71d60bc87ff0"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:40:17.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5160" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":91,"skipped":1426,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:40:17.841: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5280
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 16:40:18.044: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 17 16:40:18.071: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 17 16:40:23.084: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 17 16:40:23.084: INFO: Creating deployment "test-rolling-update-deployment"
Feb 17 16:40:23.096: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 17 16:40:23.119: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
Feb 17 16:40:25.143: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 17 16:40:25.153: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Feb 17 16:40:25.185: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5280 /apis/apps/v1/namespaces/deployment-5280/deployments/test-rolling-update-deployment 76d55f16-f1ff-4c87-b51f-e9e7b05ab265 38512 1 2020-02-17 16:40:23 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00512bfa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-02-17 16:40:23 +0000 UTC,LastTransitionTime:2020-02-17 16:40:23 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67cf4f6444" has successfully progressed.,LastUpdateTime:2020-02-17 16:40:25 +0000 UTC,LastTransitionTime:2020-02-17 16:40:23 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 17 16:40:25.195: INFO: New ReplicaSet "test-rolling-update-deployment-67cf4f6444" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67cf4f6444  deployment-5280 /apis/apps/v1/namespaces/deployment-5280/replicasets/test-rolling-update-deployment-67cf4f6444 d6c1a63f-02e7-4881-9a56-075b8425947d 38501 1 2020-02-17 16:40:23 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 76d55f16-f1ff-4c87-b51f-e9e7b05ab265 0xc002d14977 0xc002d14978}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67cf4f6444,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002d149f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 17 16:40:25.195: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 17 16:40:25.196: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5280 /apis/apps/v1/namespaces/deployment-5280/replicasets/test-rolling-update-controller ac0f57b4-3269-42d6-aeab-7009dbfad2d8 38510 2 2020-02-17 16:40:18 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 76d55f16-f1ff-4c87-b51f-e9e7b05ab265 0xc002d148a7 0xc002d148a8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002d14908 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 17 16:40:25.207: INFO: Pod "test-rolling-update-deployment-67cf4f6444-fqkvc" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67cf4f6444-fqkvc test-rolling-update-deployment-67cf4f6444- deployment-5280 /api/v1/namespaces/deployment-5280/pods/test-rolling-update-deployment-67cf4f6444-fqkvc ff248fa7-f5d7-4d27-b5b4-f47c16e06885 38500 0 2020-02-17 16:40:23 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-67cf4f6444 d6c1a63f-02e7-4881-9a56-075b8425947d 0xc002d15467 0xc002d15468}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jm4jb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jm4jb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jm4jb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 16:40:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.14,PodIP:172.30.89.226,StartTime:2020-02-17 16:40:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 16:40:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:containerd://d31eb51ad710515f90402e1a91dbbf105ac424e2ed911ac7c810a4a8700c6e1f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.89.226,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:40:25.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5280" for this suite.

• [SLOW TEST:7.398 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":92,"skipped":1439,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:40:25.239: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1586
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:330
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Feb 17 16:40:25.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 create -f - --namespace=kubectl-1586'
Feb 17 16:40:25.710: INFO: stderr: ""
Feb 17 16:40:25.710: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 17 16:40:25.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1586'
Feb 17 16:40:25.814: INFO: stderr: ""
Feb 17 16:40:25.814: INFO: stdout: "update-demo-nautilus-kdqbm update-demo-nautilus-kpl6c "
Feb 17 16:40:25.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods update-demo-nautilus-kdqbm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1586'
Feb 17 16:40:25.917: INFO: stderr: ""
Feb 17 16:40:25.917: INFO: stdout: ""
Feb 17 16:40:25.917: INFO: update-demo-nautilus-kdqbm is created but not running
Feb 17 16:40:30.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1586'
Feb 17 16:40:31.026: INFO: stderr: ""
Feb 17 16:40:31.026: INFO: stdout: "update-demo-nautilus-kdqbm update-demo-nautilus-kpl6c "
Feb 17 16:40:31.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods update-demo-nautilus-kdqbm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1586'
Feb 17 16:40:31.137: INFO: stderr: ""
Feb 17 16:40:31.137: INFO: stdout: "true"
Feb 17 16:40:31.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods update-demo-nautilus-kdqbm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1586'
Feb 17 16:40:31.241: INFO: stderr: ""
Feb 17 16:40:31.241: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 17 16:40:31.241: INFO: validating pod update-demo-nautilus-kdqbm
Feb 17 16:40:31.262: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 17 16:40:31.262: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 17 16:40:31.262: INFO: update-demo-nautilus-kdqbm is verified up and running
Feb 17 16:40:31.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods update-demo-nautilus-kpl6c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1586'
Feb 17 16:40:31.366: INFO: stderr: ""
Feb 17 16:40:31.366: INFO: stdout: "true"
Feb 17 16:40:31.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods update-demo-nautilus-kpl6c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1586'
Feb 17 16:40:31.467: INFO: stderr: ""
Feb 17 16:40:31.467: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 17 16:40:31.467: INFO: validating pod update-demo-nautilus-kpl6c
Feb 17 16:40:31.486: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 17 16:40:31.486: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 17 16:40:31.486: INFO: update-demo-nautilus-kpl6c is verified up and running
STEP: using delete to clean up resources
Feb 17 16:40:31.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 delete --grace-period=0 --force -f - --namespace=kubectl-1586'
Feb 17 16:40:31.597: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 17 16:40:31.597: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 17 16:40:31.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1586'
Feb 17 16:40:31.717: INFO: stderr: "No resources found in kubectl-1586 namespace.\n"
Feb 17 16:40:31.717: INFO: stdout: ""
Feb 17 16:40:31.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods -l name=update-demo --namespace=kubectl-1586 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 17 16:40:31.834: INFO: stderr: ""
Feb 17 16:40:31.834: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:40:31.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1586" for this suite.

• [SLOW TEST:6.626 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:328
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":280,"completed":93,"skipped":1449,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:40:31.866: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9818
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:330
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Feb 17 16:40:32.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 create -f - --namespace=kubectl-9818'
Feb 17 16:40:32.349: INFO: stderr: ""
Feb 17 16:40:32.349: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 17 16:40:32.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9818'
Feb 17 16:40:32.499: INFO: stderr: ""
Feb 17 16:40:32.499: INFO: stdout: "update-demo-nautilus-dqqsf update-demo-nautilus-r7z5v "
Feb 17 16:40:32.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods update-demo-nautilus-dqqsf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9818'
Feb 17 16:40:32.599: INFO: stderr: ""
Feb 17 16:40:32.599: INFO: stdout: ""
Feb 17 16:40:32.599: INFO: update-demo-nautilus-dqqsf is created but not running
Feb 17 16:40:37.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9818'
Feb 17 16:40:37.714: INFO: stderr: ""
Feb 17 16:40:37.714: INFO: stdout: "update-demo-nautilus-dqqsf update-demo-nautilus-r7z5v "
Feb 17 16:40:37.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods update-demo-nautilus-dqqsf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9818'
Feb 17 16:40:37.817: INFO: stderr: ""
Feb 17 16:40:37.817: INFO: stdout: "true"
Feb 17 16:40:37.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods update-demo-nautilus-dqqsf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9818'
Feb 17 16:40:37.907: INFO: stderr: ""
Feb 17 16:40:37.907: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 17 16:40:37.907: INFO: validating pod update-demo-nautilus-dqqsf
Feb 17 16:40:37.930: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 17 16:40:37.930: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 17 16:40:37.930: INFO: update-demo-nautilus-dqqsf is verified up and running
Feb 17 16:40:37.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods update-demo-nautilus-r7z5v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9818'
Feb 17 16:40:38.024: INFO: stderr: ""
Feb 17 16:40:38.024: INFO: stdout: "true"
Feb 17 16:40:38.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods update-demo-nautilus-r7z5v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9818'
Feb 17 16:40:38.135: INFO: stderr: ""
Feb 17 16:40:38.135: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 17 16:40:38.135: INFO: validating pod update-demo-nautilus-r7z5v
Feb 17 16:40:38.153: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 17 16:40:38.153: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 17 16:40:38.153: INFO: update-demo-nautilus-r7z5v is verified up and running
STEP: scaling down the replication controller
Feb 17 16:40:38.155: INFO: scanned /root for discovery docs: <nil>
Feb 17 16:40:38.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-9818'
Feb 17 16:40:39.279: INFO: stderr: ""
Feb 17 16:40:39.279: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 17 16:40:39.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9818'
Feb 17 16:40:39.380: INFO: stderr: ""
Feb 17 16:40:39.380: INFO: stdout: "update-demo-nautilus-dqqsf update-demo-nautilus-r7z5v "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 17 16:40:44.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9818'
Feb 17 16:40:44.488: INFO: stderr: ""
Feb 17 16:40:44.489: INFO: stdout: "update-demo-nautilus-dqqsf update-demo-nautilus-r7z5v "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 17 16:40:49.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9818'
Feb 17 16:40:49.588: INFO: stderr: ""
Feb 17 16:40:49.588: INFO: stdout: "update-demo-nautilus-r7z5v "
Feb 17 16:40:49.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods update-demo-nautilus-r7z5v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9818'
Feb 17 16:40:49.690: INFO: stderr: ""
Feb 17 16:40:49.690: INFO: stdout: "true"
Feb 17 16:40:49.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods update-demo-nautilus-r7z5v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9818'
Feb 17 16:40:49.789: INFO: stderr: ""
Feb 17 16:40:49.789: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 17 16:40:49.789: INFO: validating pod update-demo-nautilus-r7z5v
Feb 17 16:40:49.804: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 17 16:40:49.804: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 17 16:40:49.804: INFO: update-demo-nautilus-r7z5v is verified up and running
STEP: scaling up the replication controller
Feb 17 16:40:49.806: INFO: scanned /root for discovery docs: <nil>
Feb 17 16:40:49.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-9818'
Feb 17 16:40:50.970: INFO: stderr: ""
Feb 17 16:40:50.970: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 17 16:40:50.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9818'
Feb 17 16:40:51.068: INFO: stderr: ""
Feb 17 16:40:51.068: INFO: stdout: "update-demo-nautilus-lngrl update-demo-nautilus-r7z5v "
Feb 17 16:40:51.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods update-demo-nautilus-lngrl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9818'
Feb 17 16:40:51.184: INFO: stderr: ""
Feb 17 16:40:51.184: INFO: stdout: "true"
Feb 17 16:40:51.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods update-demo-nautilus-lngrl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9818'
Feb 17 16:40:51.291: INFO: stderr: ""
Feb 17 16:40:51.291: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 17 16:40:51.291: INFO: validating pod update-demo-nautilus-lngrl
Feb 17 16:40:51.311: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 17 16:40:51.311: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 17 16:40:51.311: INFO: update-demo-nautilus-lngrl is verified up and running
Feb 17 16:40:51.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods update-demo-nautilus-r7z5v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9818'
Feb 17 16:40:51.413: INFO: stderr: ""
Feb 17 16:40:51.413: INFO: stdout: "true"
Feb 17 16:40:51.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods update-demo-nautilus-r7z5v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9818'
Feb 17 16:40:51.516: INFO: stderr: ""
Feb 17 16:40:51.516: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 17 16:40:51.516: INFO: validating pod update-demo-nautilus-r7z5v
Feb 17 16:40:51.531: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 17 16:40:51.531: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 17 16:40:51.531: INFO: update-demo-nautilus-r7z5v is verified up and running
STEP: using delete to clean up resources
Feb 17 16:40:51.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 delete --grace-period=0 --force -f - --namespace=kubectl-9818'
Feb 17 16:40:51.636: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 17 16:40:51.636: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 17 16:40:51.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9818'
Feb 17 16:40:51.748: INFO: stderr: "No resources found in kubectl-9818 namespace.\n"
Feb 17 16:40:51.748: INFO: stdout: ""
Feb 17 16:40:51.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pods -l name=update-demo --namespace=kubectl-9818 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 17 16:40:51.844: INFO: stderr: ""
Feb 17 16:40:51.844: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:40:51.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9818" for this suite.

• [SLOW TEST:20.005 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:328
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":280,"completed":94,"skipped":1468,"failed":0}
S
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:40:51.872: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8777
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating secret secrets-8777/secret-test-1d588362-7dd1-4943-9a9c-f12a991a5a51
STEP: Creating a pod to test consume secrets
Feb 17 16:40:52.098: INFO: Waiting up to 5m0s for pod "pod-configmaps-f4e5d750-f388-4bed-b3ef-a6795d31b954" in namespace "secrets-8777" to be "success or failure"
Feb 17 16:40:52.109: INFO: Pod "pod-configmaps-f4e5d750-f388-4bed-b3ef-a6795d31b954": Phase="Pending", Reason="", readiness=false. Elapsed: 10.304876ms
Feb 17 16:40:54.120: INFO: Pod "pod-configmaps-f4e5d750-f388-4bed-b3ef-a6795d31b954": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021810652s
STEP: Saw pod success
Feb 17 16:40:54.120: INFO: Pod "pod-configmaps-f4e5d750-f388-4bed-b3ef-a6795d31b954" satisfied condition "success or failure"
Feb 17 16:40:54.131: INFO: Trying to get logs from node 10.195.53.9 pod pod-configmaps-f4e5d750-f388-4bed-b3ef-a6795d31b954 container env-test: <nil>
STEP: delete the pod
Feb 17 16:40:54.190: INFO: Waiting for pod pod-configmaps-f4e5d750-f388-4bed-b3ef-a6795d31b954 to disappear
Feb 17 16:40:54.201: INFO: Pod pod-configmaps-f4e5d750-f388-4bed-b3ef-a6795d31b954 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:40:54.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8777" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":95,"skipped":1469,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:40:54.228: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9131
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-8ee51a4c-ec45-4ea4-91f4-493640e2e9ac
STEP: Creating a pod to test consume secrets
Feb 17 16:40:54.450: INFO: Waiting up to 5m0s for pod "pod-secrets-e44367e6-f729-4497-981d-bf8b195d282d" in namespace "secrets-9131" to be "success or failure"
Feb 17 16:40:54.460: INFO: Pod "pod-secrets-e44367e6-f729-4497-981d-bf8b195d282d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.260991ms
Feb 17 16:40:56.476: INFO: Pod "pod-secrets-e44367e6-f729-4497-981d-bf8b195d282d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025367915s
STEP: Saw pod success
Feb 17 16:40:56.476: INFO: Pod "pod-secrets-e44367e6-f729-4497-981d-bf8b195d282d" satisfied condition "success or failure"
Feb 17 16:40:56.487: INFO: Trying to get logs from node 10.195.53.9 pod pod-secrets-e44367e6-f729-4497-981d-bf8b195d282d container secret-env-test: <nil>
STEP: delete the pod
Feb 17 16:40:56.545: INFO: Waiting for pod pod-secrets-e44367e6-f729-4497-981d-bf8b195d282d to disappear
Feb 17 16:40:56.556: INFO: Pod pod-secrets-e44367e6-f729-4497-981d-bf8b195d282d no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:40:56.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9131" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":280,"completed":96,"skipped":1478,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:40:56.583: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2271
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-57d330c2-07ce-49ce-824f-1ba3ba64ca1d
STEP: Creating a pod to test consume secrets
Feb 17 16:40:56.803: INFO: Waiting up to 5m0s for pod "pod-secrets-b89ef5b4-bfd8-4ae1-a232-abf072ae7b37" in namespace "secrets-2271" to be "success or failure"
Feb 17 16:40:56.813: INFO: Pod "pod-secrets-b89ef5b4-bfd8-4ae1-a232-abf072ae7b37": Phase="Pending", Reason="", readiness=false. Elapsed: 10.160248ms
Feb 17 16:40:58.830: INFO: Pod "pod-secrets-b89ef5b4-bfd8-4ae1-a232-abf072ae7b37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026471837s
STEP: Saw pod success
Feb 17 16:40:58.830: INFO: Pod "pod-secrets-b89ef5b4-bfd8-4ae1-a232-abf072ae7b37" satisfied condition "success or failure"
Feb 17 16:40:58.841: INFO: Trying to get logs from node 10.195.53.14 pod pod-secrets-b89ef5b4-bfd8-4ae1-a232-abf072ae7b37 container secret-volume-test: <nil>
STEP: delete the pod
Feb 17 16:40:58.931: INFO: Waiting for pod pod-secrets-b89ef5b4-bfd8-4ae1-a232-abf072ae7b37 to disappear
Feb 17 16:40:58.942: INFO: Pod pod-secrets-b89ef5b4-bfd8-4ae1-a232-abf072ae7b37 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:40:58.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2271" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":97,"skipped":1489,"failed":0}
SS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:40:58.968: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-1350
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 16:40:59.179: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-4a487b3f-ce71-4b7f-b76f-987453b7274c" in namespace "security-context-test-1350" to be "success or failure"
Feb 17 16:40:59.189: INFO: Pod "busybox-readonly-false-4a487b3f-ce71-4b7f-b76f-987453b7274c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.624714ms
Feb 17 16:41:01.204: INFO: Pod "busybox-readonly-false-4a487b3f-ce71-4b7f-b76f-987453b7274c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024825588s
Feb 17 16:41:01.204: INFO: Pod "busybox-readonly-false-4a487b3f-ce71-4b7f-b76f-987453b7274c" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:41:01.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1350" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":280,"completed":98,"skipped":1491,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:41:01.231: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-7389
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:163
Feb 17 16:41:01.430: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 17 16:42:01.474: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 16:42:01.484: INFO: Starting informer...
STEP: Starting pod...
Feb 17 16:42:01.715: INFO: Pod is running on 10.195.53.9. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Feb 17 16:42:01.820: INFO: Pod wasn't evicted. Proceeding
Feb 17 16:42:01.820: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Feb 17 16:43:16.847: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:43:16.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-7389" for this suite.

• [SLOW TEST:135.647 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":280,"completed":99,"skipped":1510,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:43:16.878: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5653
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-4ff65379-0c32-4e18-b3af-1b4f3b12f40d in namespace container-probe-5653
Feb 17 16:43:19.116: INFO: Started pod liveness-4ff65379-0c32-4e18-b3af-1b4f3b12f40d in namespace container-probe-5653
STEP: checking the pod's current state and verifying that restartCount is present
Feb 17 16:43:19.128: INFO: Initial restart count of pod liveness-4ff65379-0c32-4e18-b3af-1b4f3b12f40d is 0
Feb 17 16:43:33.229: INFO: Restart count of pod container-probe-5653/liveness-4ff65379-0c32-4e18-b3af-1b4f3b12f40d is now 1 (14.101281853s elapsed)
Feb 17 16:43:53.348: INFO: Restart count of pod container-probe-5653/liveness-4ff65379-0c32-4e18-b3af-1b4f3b12f40d is now 2 (34.22022998s elapsed)
Feb 17 16:44:13.473: INFO: Restart count of pod container-probe-5653/liveness-4ff65379-0c32-4e18-b3af-1b4f3b12f40d is now 3 (54.344883119s elapsed)
Feb 17 16:44:31.580: INFO: Restart count of pod container-probe-5653/liveness-4ff65379-0c32-4e18-b3af-1b4f3b12f40d is now 4 (1m12.452159527s elapsed)
Feb 17 16:45:46.063: INFO: Restart count of pod container-probe-5653/liveness-4ff65379-0c32-4e18-b3af-1b4f3b12f40d is now 5 (2m26.93539064s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:45:46.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5653" for this suite.

• [SLOW TEST:149.436 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":280,"completed":100,"skipped":1539,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:45:46.315: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4350
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 16:45:47.012: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 16:45:50.071: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 16:45:50.078: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7106-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:45:51.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4350" for this suite.
STEP: Destroying namespace "webhook-4350-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.137 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":280,"completed":101,"skipped":1559,"failed":0}
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:45:51.452: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5317
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-5317
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Feb 17 16:45:51.682: INFO: Found 0 stateful pods, waiting for 3
Feb 17 16:46:01.696: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 16:46:01.696: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 16:46:01.696: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 16:46:01.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-5317 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 16:46:02.098: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 16:46:02.098: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 16:46:02.098: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Feb 17 16:46:12.169: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb 17 16:46:22.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-5317 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 16:46:22.481: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 17 16:46:22.481: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 17 16:46:22.481: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 17 16:46:32.643: INFO: Waiting for StatefulSet statefulset-5317/ss2 to complete update
Feb 17 16:46:32.643: INFO: Waiting for Pod statefulset-5317/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 17 16:46:32.643: INFO: Waiting for Pod statefulset-5317/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 17 16:46:42.666: INFO: Waiting for StatefulSet statefulset-5317/ss2 to complete update
Feb 17 16:46:42.666: INFO: Waiting for Pod statefulset-5317/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 17 16:46:52.667: INFO: Waiting for StatefulSet statefulset-5317/ss2 to complete update
Feb 17 16:46:52.667: INFO: Waiting for Pod statefulset-5317/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 17 16:47:02.665: INFO: Waiting for StatefulSet statefulset-5317/ss2 to complete update
STEP: Rolling back to a previous revision
Feb 17 16:47:12.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-5317 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 16:47:12.959: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 16:47:12.959: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 16:47:12.959: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 17 16:47:23.030: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb 17 16:47:33.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-5317 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 16:47:33.319: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 17 16:47:33.319: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 17 16:47:33.319: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 17 16:47:43.382: INFO: Waiting for StatefulSet statefulset-5317/ss2 to complete update
Feb 17 16:47:43.382: INFO: Waiting for Pod statefulset-5317/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 17 16:47:43.382: INFO: Waiting for Pod statefulset-5317/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 17 16:47:43.382: INFO: Waiting for Pod statefulset-5317/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 17 16:47:53.403: INFO: Waiting for StatefulSet statefulset-5317/ss2 to complete update
Feb 17 16:47:53.403: INFO: Waiting for Pod statefulset-5317/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 17 16:47:53.404: INFO: Waiting for Pod statefulset-5317/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 17 16:48:03.403: INFO: Waiting for StatefulSet statefulset-5317/ss2 to complete update
Feb 17 16:48:03.403: INFO: Waiting for Pod statefulset-5317/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 17 16:48:13.403: INFO: Waiting for StatefulSet statefulset-5317/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Feb 17 16:48:23.404: INFO: Deleting all statefulset in ns statefulset-5317
Feb 17 16:48:23.413: INFO: Scaling statefulset ss2 to 0
Feb 17 16:48:33.459: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 16:48:33.468: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:48:33.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5317" for this suite.

• [SLOW TEST:162.076 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":280,"completed":102,"skipped":1559,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:48:33.529: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5938
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Feb 17 16:48:33.733: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 17 16:48:33.763: INFO: Waiting for terminating namespaces to be deleted...
Feb 17 16:48:33.771: INFO: 
Logging pods the kubelet thinks is on node 10.195.53.14 before test
Feb 17 16:48:34.002: INFO: calico-node-94c2k from kube-system started at 2020-02-17 14:36:27 +0000 UTC (1 container statuses recorded)
Feb 17 16:48:34.002: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 16:48:34.002: INFO: calico-kube-controllers-866cf6f69c-fm97n from kube-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 16:48:34.002: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 17 16:48:34.002: INFO: kubernetes-dashboard-bbcc67fc-x8mnz from kube-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 16:48:34.002: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 17 16:48:34.002: INFO: ibm-cloud-provider-ip-135-90-78-211-7555ccd494-hhqb2 from ibm-system started at 2020-02-17 14:41:58 +0000 UTC (1 container statuses recorded)
Feb 17 16:48:34.002: INFO: 	Container ibm-cloud-provider-ip-135-90-78-211 ready: true, restart count 0
Feb 17 16:48:34.002: INFO: ibm-file-plugin-6694f985b8-s5cjp from kube-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 16:48:34.002: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Feb 17 16:48:34.002: INFO: public-crbp5a3ais0mfj6cn3dl60-alb1-79849b9bd6-4z2xg from kube-system started at 2020-02-17 14:44:46 +0000 UTC (4 container statuses recorded)
Feb 17 16:48:34.002: INFO: 	Container ingress-auth-1 ready: true, restart count 1
Feb 17 16:48:34.002: INFO: 	Container ingress-auth-2 ready: true, restart count 1
Feb 17 16:48:34.002: INFO: 	Container ingress-auth-3 ready: true, restart count 1
Feb 17 16:48:34.002: INFO: 	Container nginx-ingress ready: true, restart count 0
Feb 17 16:48:34.002: INFO: sonobuoy-systemd-logs-daemon-set-bc697ed487d7421d-st8jd from sonobuoy started at 2020-02-17 16:14:42 +0000 UTC (2 container statuses recorded)
Feb 17 16:48:34.002: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 16:48:34.002: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 16:48:34.002: INFO: ibm-storage-watcher-74f895486d-k5zxb from kube-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 16:48:34.002: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Feb 17 16:48:34.002: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-02-17 14:39:39 +0000 UTC (1 container statuses recorded)
Feb 17 16:48:34.002: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Feb 17 16:48:34.002: INFO: catalog-operator-7d9cb6cf74-qr9nb from ibm-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 16:48:34.002: INFO: 	Container catalog-operator ready: true, restart count 0
Feb 17 16:48:34.002: INFO: coredns-autoscaler-7dddb6f87c-g6nrf from kube-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 16:48:34.003: INFO: 	Container autoscaler ready: true, restart count 0
Feb 17 16:48:34.003: INFO: ibm-master-proxy-static-10.195.53.14 from kube-system started at 2020-02-17 14:36:26 +0000 UTC (2 container statuses recorded)
Feb 17 16:48:34.003: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 16:48:34.003: INFO: 	Container pause ready: true, restart count 0
Feb 17 16:48:34.003: INFO: ibm-keepalived-watcher-8pbd8 from kube-system started at 2020-02-17 14:36:27 +0000 UTC (1 container statuses recorded)
Feb 17 16:48:34.003: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 16:48:34.003: INFO: coredns-5b567488dd-kckdk from kube-system started at 2020-02-17 14:59:13 +0000 UTC (1 container statuses recorded)
Feb 17 16:48:34.003: INFO: 	Container coredns ready: true, restart count 0
Feb 17 16:48:34.003: INFO: 
Logging pods the kubelet thinks is on node 10.195.53.47 before test
Feb 17 16:48:34.091: INFO: ibm-master-proxy-static-10.195.53.47 from kube-system started at 2020-02-17 14:36:20 +0000 UTC (2 container statuses recorded)
Feb 17 16:48:34.091: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 16:48:34.091: INFO: 	Container pause ready: true, restart count 0
Feb 17 16:48:34.091: INFO: ibm-keepalived-watcher-bl4pr from kube-system started at 2020-02-17 14:36:22 +0000 UTC (1 container statuses recorded)
Feb 17 16:48:34.091: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 16:48:34.091: INFO: calico-node-kmwlv from kube-system started at 2020-02-17 14:36:22 +0000 UTC (1 container statuses recorded)
Feb 17 16:48:34.091: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 16:48:34.091: INFO: addon-catalog-source-c5sks from ibm-system started at 2020-02-17 14:38:40 +0000 UTC (1 container statuses recorded)
Feb 17 16:48:34.091: INFO: 	Container configmap-registry-server ready: true, restart count 0
Feb 17 16:48:34.091: INFO: ibm-cloud-provider-ip-135-90-78-211-7555ccd494-fz6gg from ibm-system started at 2020-02-17 14:41:58 +0000 UTC (1 container statuses recorded)
Feb 17 16:48:34.091: INFO: 	Container ibm-cloud-provider-ip-135-90-78-211 ready: true, restart count 0
Feb 17 16:48:34.091: INFO: sonobuoy-e2e-job-c9def2901e004587 from sonobuoy started at 2020-02-17 16:14:42 +0000 UTC (2 container statuses recorded)
Feb 17 16:48:34.091: INFO: 	Container e2e ready: true, restart count 0
Feb 17 16:48:34.091: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 16:48:34.091: INFO: olm-operator-587889d75d-29ltv from ibm-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 16:48:34.091: INFO: 	Container olm-operator ready: true, restart count 0
Feb 17 16:48:34.091: INFO: dashboard-metrics-scraper-69468c6b44-gqtkk from kube-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 16:48:34.091: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 17 16:48:34.091: INFO: sonobuoy-systemd-logs-daemon-set-bc697ed487d7421d-5b85k from sonobuoy started at 2020-02-17 16:14:42 +0000 UTC (2 container statuses recorded)
Feb 17 16:48:34.091: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 16:48:34.091: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 16:48:34.091: INFO: public-crbp5a3ais0mfj6cn3dl60-alb1-79849b9bd6-8r8qk from kube-system started at 2020-02-17 14:44:46 +0000 UTC (4 container statuses recorded)
Feb 17 16:48:34.091: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Feb 17 16:48:34.091: INFO: 	Container ingress-auth-2 ready: true, restart count 1
Feb 17 16:48:34.091: INFO: 	Container ingress-auth-3 ready: true, restart count 1
Feb 17 16:48:34.091: INFO: 	Container nginx-ingress ready: true, restart count 0
Feb 17 16:48:34.091: INFO: coredns-5b567488dd-qtx5q from kube-system started at 2020-02-17 14:59:13 +0000 UTC (1 container statuses recorded)
Feb 17 16:48:34.091: INFO: 	Container coredns ready: true, restart count 0
Feb 17 16:48:34.091: INFO: 
Logging pods the kubelet thinks is on node 10.195.53.9 before test
Feb 17 16:48:34.154: INFO: sonobuoy-systemd-logs-daemon-set-bc697ed487d7421d-ntb66 from sonobuoy started at 2020-02-17 16:14:42 +0000 UTC (2 container statuses recorded)
Feb 17 16:48:34.154: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 16:48:34.154: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 16:48:34.154: INFO: vpn-b5cd9dc8b-wk5ms from kube-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 16:48:34.154: INFO: 	Container vpn ready: true, restart count 0
Feb 17 16:48:34.154: INFO: sonobuoy from sonobuoy started at 2020-02-17 16:14:33 +0000 UTC (1 container statuses recorded)
Feb 17 16:48:34.154: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 17 16:48:34.154: INFO: ibm-master-proxy-static-10.195.53.9 from kube-system started at 2020-02-17 14:36:11 +0000 UTC (2 container statuses recorded)
Feb 17 16:48:34.154: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 16:48:34.154: INFO: 	Container pause ready: true, restart count 0
Feb 17 16:48:34.154: INFO: calico-node-r8w4v from kube-system started at 2020-02-17 14:36:12 +0000 UTC (1 container statuses recorded)
Feb 17 16:48:34.154: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 16:48:34.154: INFO: ibm-keepalived-watcher-7c6j2 from kube-system started at 2020-02-17 14:36:12 +0000 UTC (1 container statuses recorded)
Feb 17 16:48:34.155: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 16:48:34.155: INFO: metrics-server-647cf95c9b-nkqhm from kube-system started at 2020-02-17 16:42:01 +0000 UTC (2 container statuses recorded)
Feb 17 16:48:34.155: INFO: 	Container metrics-server ready: true, restart count 0
Feb 17 16:48:34.155: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Feb 17 16:48:34.155: INFO: coredns-5b567488dd-nhtdm from kube-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 16:48:34.155: INFO: 	Container coredns ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-f28370a5-db62-429d-995e-18f1ed5a3592 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-f28370a5-db62-429d-995e-18f1ed5a3592 off the node 10.195.53.9
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f28370a5-db62-429d-995e-18f1ed5a3592
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:53:40.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5938" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:306.895 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":280,"completed":103,"skipped":1566,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:53:40.425: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8240
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:53:42.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8240" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":104,"skipped":1582,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:53:42.753: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8153
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 16:53:42.972: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6d3bcf27-f3de-4e5b-a3fe-c18b85fb8e92" in namespace "projected-8153" to be "success or failure"
Feb 17 16:53:42.989: INFO: Pod "downwardapi-volume-6d3bcf27-f3de-4e5b-a3fe-c18b85fb8e92": Phase="Pending", Reason="", readiness=false. Elapsed: 17.279251ms
Feb 17 16:53:45.001: INFO: Pod "downwardapi-volume-6d3bcf27-f3de-4e5b-a3fe-c18b85fb8e92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029098603s
STEP: Saw pod success
Feb 17 16:53:45.001: INFO: Pod "downwardapi-volume-6d3bcf27-f3de-4e5b-a3fe-c18b85fb8e92" satisfied condition "success or failure"
Feb 17 16:53:45.013: INFO: Trying to get logs from node 10.195.53.9 pod downwardapi-volume-6d3bcf27-f3de-4e5b-a3fe-c18b85fb8e92 container client-container: <nil>
STEP: delete the pod
Feb 17 16:53:45.070: INFO: Waiting for pod downwardapi-volume-6d3bcf27-f3de-4e5b-a3fe-c18b85fb8e92 to disappear
Feb 17 16:53:45.082: INFO: Pod downwardapi-volume-6d3bcf27-f3de-4e5b-a3fe-c18b85fb8e92 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:53:45.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8153" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":105,"skipped":1593,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:53:45.112: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5825
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 16:53:45.794: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 16:53:48.857: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:53:49.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5825" for this suite.
STEP: Destroying namespace "webhook-5825-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":280,"completed":106,"skipped":1609,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:53:49.308: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5056
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-5056
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-5056
STEP: Creating statefulset with conflicting port in namespace statefulset-5056
STEP: Waiting until pod test-pod will start running in namespace statefulset-5056
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5056
Feb 17 16:53:53.586: INFO: Observed stateful pod in namespace: statefulset-5056, name: ss-0, uid: 91fac40f-eb80-49ed-a80a-a4b764d0f8ef, status phase: Pending. Waiting for statefulset controller to delete.
Feb 17 16:53:53.612: INFO: Observed stateful pod in namespace: statefulset-5056, name: ss-0, uid: 91fac40f-eb80-49ed-a80a-a4b764d0f8ef, status phase: Failed. Waiting for statefulset controller to delete.
Feb 17 16:53:53.625: INFO: Observed stateful pod in namespace: statefulset-5056, name: ss-0, uid: 91fac40f-eb80-49ed-a80a-a4b764d0f8ef, status phase: Failed. Waiting for statefulset controller to delete.
Feb 17 16:53:53.638: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5056
STEP: Removing pod with conflicting port in namespace statefulset-5056
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5056 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Feb 17 16:53:57.706: INFO: Deleting all statefulset in ns statefulset-5056
Feb 17 16:53:57.715: INFO: Scaling statefulset ss to 0
Feb 17 16:54:07.760: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 16:54:07.770: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:54:07.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5056" for this suite.

• [SLOW TEST:18.522 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":280,"completed":107,"skipped":1619,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:54:07.831: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-854
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 17 16:54:08.112: INFO: Number of nodes with available pods: 0
Feb 17 16:54:08.112: INFO: Node 10.195.53.14 is running more than one daemon pod
Feb 17 16:54:09.137: INFO: Number of nodes with available pods: 0
Feb 17 16:54:09.137: INFO: Node 10.195.53.14 is running more than one daemon pod
Feb 17 16:54:10.140: INFO: Number of nodes with available pods: 3
Feb 17 16:54:10.140: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb 17 16:54:10.198: INFO: Number of nodes with available pods: 2
Feb 17 16:54:10.198: INFO: Node 10.195.53.14 is running more than one daemon pod
Feb 17 16:54:11.223: INFO: Number of nodes with available pods: 2
Feb 17 16:54:11.223: INFO: Node 10.195.53.14 is running more than one daemon pod
Feb 17 16:54:12.224: INFO: Number of nodes with available pods: 3
Feb 17 16:54:12.225: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-854, will wait for the garbage collector to delete the pods
Feb 17 16:54:12.328: INFO: Deleting DaemonSet.extensions daemon-set took: 21.185561ms
Feb 17 16:54:12.428: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.306102ms
Feb 17 16:54:26.439: INFO: Number of nodes with available pods: 0
Feb 17 16:54:26.439: INFO: Number of running nodes: 0, number of available pods: 0
Feb 17 16:54:26.448: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-854/daemonsets","resourceVersion":"42741"},"items":null}

Feb 17 16:54:26.459: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-854/pods","resourceVersion":"42741"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:54:26.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-854" for this suite.

• [SLOW TEST:18.693 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":280,"completed":108,"skipped":1622,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:54:26.524: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4588
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:54:43.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4588" for this suite.

• [SLOW TEST:16.619 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":280,"completed":109,"skipped":1627,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:54:43.144: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4709
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 16:54:43.344: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Feb 17 16:54:47.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 --namespace=crd-publish-openapi-4709 create -f -'
Feb 17 16:54:47.511: INFO: stderr: ""
Feb 17 16:54:47.511: INFO: stdout: "e2e-test-crd-publish-openapi-1844-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 17 16:54:47.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 --namespace=crd-publish-openapi-4709 delete e2e-test-crd-publish-openapi-1844-crds test-foo'
Feb 17 16:54:47.633: INFO: stderr: ""
Feb 17 16:54:47.633: INFO: stdout: "e2e-test-crd-publish-openapi-1844-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Feb 17 16:54:47.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 --namespace=crd-publish-openapi-4709 apply -f -'
Feb 17 16:54:48.020: INFO: stderr: ""
Feb 17 16:54:48.020: INFO: stdout: "e2e-test-crd-publish-openapi-1844-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 17 16:54:48.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 --namespace=crd-publish-openapi-4709 delete e2e-test-crd-publish-openapi-1844-crds test-foo'
Feb 17 16:54:48.141: INFO: stderr: ""
Feb 17 16:54:48.141: INFO: stdout: "e2e-test-crd-publish-openapi-1844-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Feb 17 16:54:48.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 --namespace=crd-publish-openapi-4709 create -f -'
Feb 17 16:54:48.552: INFO: rc: 1
Feb 17 16:54:48.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 --namespace=crd-publish-openapi-4709 apply -f -'
Feb 17 16:54:48.843: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Feb 17 16:54:48.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 --namespace=crd-publish-openapi-4709 create -f -'
Feb 17 16:54:49.108: INFO: rc: 1
Feb 17 16:54:49.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 --namespace=crd-publish-openapi-4709 apply -f -'
Feb 17 16:54:49.272: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Feb 17 16:54:49.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 explain e2e-test-crd-publish-openapi-1844-crds'
Feb 17 16:54:49.576: INFO: stderr: ""
Feb 17 16:54:49.576: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1844-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Feb 17 16:54:49.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 explain e2e-test-crd-publish-openapi-1844-crds.metadata'
Feb 17 16:54:49.867: INFO: stderr: ""
Feb 17 16:54:49.867: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1844-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Feb 17 16:54:49.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 explain e2e-test-crd-publish-openapi-1844-crds.spec'
Feb 17 16:54:50.281: INFO: stderr: ""
Feb 17 16:54:50.281: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1844-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Feb 17 16:54:50.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 explain e2e-test-crd-publish-openapi-1844-crds.spec.bars'
Feb 17 16:54:50.446: INFO: stderr: ""
Feb 17 16:54:50.446: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1844-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Feb 17 16:54:50.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 explain e2e-test-crd-publish-openapi-1844-crds.spec.bars2'
Feb 17 16:54:50.734: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:54:54.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4709" for this suite.

• [SLOW TEST:11.321 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":280,"completed":110,"skipped":1666,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:54:54.465: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5226
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 16:54:54.683: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e49570d-d26a-48b3-ad8d-bcf73500dc2d" in namespace "projected-5226" to be "success or failure"
Feb 17 16:54:54.695: INFO: Pod "downwardapi-volume-1e49570d-d26a-48b3-ad8d-bcf73500dc2d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.691845ms
Feb 17 16:54:56.706: INFO: Pod "downwardapi-volume-1e49570d-d26a-48b3-ad8d-bcf73500dc2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022782875s
STEP: Saw pod success
Feb 17 16:54:56.706: INFO: Pod "downwardapi-volume-1e49570d-d26a-48b3-ad8d-bcf73500dc2d" satisfied condition "success or failure"
Feb 17 16:54:56.717: INFO: Trying to get logs from node 10.195.53.9 pod downwardapi-volume-1e49570d-d26a-48b3-ad8d-bcf73500dc2d container client-container: <nil>
STEP: delete the pod
Feb 17 16:54:56.771: INFO: Waiting for pod downwardapi-volume-1e49570d-d26a-48b3-ad8d-bcf73500dc2d to disappear
Feb 17 16:54:56.781: INFO: Pod downwardapi-volume-1e49570d-d26a-48b3-ad8d-bcf73500dc2d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:54:56.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5226" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":111,"skipped":1678,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:54:56.809: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-2510
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7429
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2616
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:55:03.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2510" for this suite.
STEP: Destroying namespace "nsdeletetest-7429" for this suite.
Feb 17 16:55:03.478: INFO: Namespace nsdeletetest-7429 was already deleted
STEP: Destroying namespace "nsdeletetest-2616" for this suite.

• [SLOW TEST:6.682 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":280,"completed":112,"skipped":1697,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:55:03.493: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-248
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Feb 17 16:55:06.786: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:55:06.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-248" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":280,"completed":113,"skipped":1728,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:55:06.859: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-629
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Feb 17 16:55:07.055: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 16:55:10.762: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:55:25.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-629" for this suite.

• [SLOW TEST:18.352 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":280,"completed":114,"skipped":1737,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:55:25.211: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5508
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name secret-emptykey-test-a6880e6f-116d-48bb-bc35-8aa128d98c75
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:55:25.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5508" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":280,"completed":115,"skipped":1756,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:55:25.444: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-3728
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 16:55:25.656: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-7c58d77f-81b2-468d-987a-760bebe8bb5d" in namespace "security-context-test-3728" to be "success or failure"
Feb 17 16:55:25.667: INFO: Pod "alpine-nnp-false-7c58d77f-81b2-468d-987a-760bebe8bb5d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.974542ms
Feb 17 16:55:27.678: INFO: Pod "alpine-nnp-false-7c58d77f-81b2-468d-987a-760bebe8bb5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022645839s
Feb 17 16:55:29.692: INFO: Pod "alpine-nnp-false-7c58d77f-81b2-468d-987a-760bebe8bb5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036350036s
Feb 17 16:55:31.703: INFO: Pod "alpine-nnp-false-7c58d77f-81b2-468d-987a-760bebe8bb5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.047466285s
Feb 17 16:55:31.703: INFO: Pod "alpine-nnp-false-7c58d77f-81b2-468d-987a-760bebe8bb5d" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:55:31.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3728" for this suite.

• [SLOW TEST:6.306 seconds]
[k8s.io] Security Context
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:289
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":116,"skipped":1774,"failed":0}
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:55:31.751: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-8818
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 16:55:31.978: INFO: (0) /api/v1/nodes/10.195.53.9/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 18.886944ms)
Feb 17 16:55:31.991: INFO: (1) /api/v1/nodes/10.195.53.9/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.971091ms)
Feb 17 16:55:32.004: INFO: (2) /api/v1/nodes/10.195.53.9/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.777723ms)
Feb 17 16:55:32.017: INFO: (3) /api/v1/nodes/10.195.53.9/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.746344ms)
Feb 17 16:55:32.029: INFO: (4) /api/v1/nodes/10.195.53.9/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.715004ms)
Feb 17 16:55:32.042: INFO: (5) /api/v1/nodes/10.195.53.9/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.398581ms)
Feb 17 16:55:32.055: INFO: (6) /api/v1/nodes/10.195.53.9/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 13.060537ms)
Feb 17 16:55:32.068: INFO: (7) /api/v1/nodes/10.195.53.9/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.986213ms)
Feb 17 16:55:32.081: INFO: (8) /api/v1/nodes/10.195.53.9/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 13.016236ms)
Feb 17 16:55:32.093: INFO: (9) /api/v1/nodes/10.195.53.9/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.288707ms)
Feb 17 16:55:32.106: INFO: (10) /api/v1/nodes/10.195.53.9/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.378433ms)
Feb 17 16:55:32.119: INFO: (11) /api/v1/nodes/10.195.53.9/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.909576ms)
Feb 17 16:55:32.132: INFO: (12) /api/v1/nodes/10.195.53.9/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.895988ms)
Feb 17 16:55:32.145: INFO: (13) /api/v1/nodes/10.195.53.9/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 13.138997ms)
Feb 17 16:55:32.158: INFO: (14) /api/v1/nodes/10.195.53.9/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.855801ms)
Feb 17 16:55:32.170: INFO: (15) /api/v1/nodes/10.195.53.9/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.694205ms)
Feb 17 16:55:32.183: INFO: (16) /api/v1/nodes/10.195.53.9/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.917118ms)
Feb 17 16:55:32.196: INFO: (17) /api/v1/nodes/10.195.53.9/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.78489ms)
Feb 17 16:55:32.209: INFO: (18) /api/v1/nodes/10.195.53.9/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.721604ms)
Feb 17 16:55:32.222: INFO: (19) /api/v1/nodes/10.195.53.9/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.496487ms)
[AfterEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:55:32.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8818" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":280,"completed":117,"skipped":1776,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:55:32.246: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5721
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb 17 16:55:32.469: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5721 /api/v1/namespaces/watch-5721/configmaps/e2e-watch-test-configmap-a da4593e3-b6bf-4246-9a9e-f2cb558faa89 43292 0 2020-02-17 16:55:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 17 16:55:32.469: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5721 /api/v1/namespaces/watch-5721/configmaps/e2e-watch-test-configmap-a da4593e3-b6bf-4246-9a9e-f2cb558faa89 43292 0 2020-02-17 16:55:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb 17 16:55:42.491: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5721 /api/v1/namespaces/watch-5721/configmaps/e2e-watch-test-configmap-a da4593e3-b6bf-4246-9a9e-f2cb558faa89 43341 0 2020-02-17 16:55:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 17 16:55:42.491: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5721 /api/v1/namespaces/watch-5721/configmaps/e2e-watch-test-configmap-a da4593e3-b6bf-4246-9a9e-f2cb558faa89 43341 0 2020-02-17 16:55:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb 17 16:55:52.512: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5721 /api/v1/namespaces/watch-5721/configmaps/e2e-watch-test-configmap-a da4593e3-b6bf-4246-9a9e-f2cb558faa89 43367 0 2020-02-17 16:55:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 17 16:55:52.512: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5721 /api/v1/namespaces/watch-5721/configmaps/e2e-watch-test-configmap-a da4593e3-b6bf-4246-9a9e-f2cb558faa89 43367 0 2020-02-17 16:55:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb 17 16:56:02.536: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5721 /api/v1/namespaces/watch-5721/configmaps/e2e-watch-test-configmap-a da4593e3-b6bf-4246-9a9e-f2cb558faa89 43398 0 2020-02-17 16:55:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 17 16:56:02.536: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5721 /api/v1/namespaces/watch-5721/configmaps/e2e-watch-test-configmap-a da4593e3-b6bf-4246-9a9e-f2cb558faa89 43398 0 2020-02-17 16:55:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb 17 16:56:12.554: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5721 /api/v1/namespaces/watch-5721/configmaps/e2e-watch-test-configmap-b cbacefea-7206-47f2-b225-4e4fb1a3aa2d 43424 0 2020-02-17 16:56:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 17 16:56:12.554: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5721 /api/v1/namespaces/watch-5721/configmaps/e2e-watch-test-configmap-b cbacefea-7206-47f2-b225-4e4fb1a3aa2d 43424 0 2020-02-17 16:56:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb 17 16:56:22.577: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5721 /api/v1/namespaces/watch-5721/configmaps/e2e-watch-test-configmap-b cbacefea-7206-47f2-b225-4e4fb1a3aa2d 43450 0 2020-02-17 16:56:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 17 16:56:22.577: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5721 /api/v1/namespaces/watch-5721/configmaps/e2e-watch-test-configmap-b cbacefea-7206-47f2-b225-4e4fb1a3aa2d 43450 0 2020-02-17 16:56:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:56:32.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5721" for this suite.

• [SLOW TEST:60.362 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":280,"completed":118,"skipped":1780,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:56:32.609: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7899
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-fc424169-2ad9-486a-8b92-7183454e652d in namespace container-probe-7899
Feb 17 16:56:34.853: INFO: Started pod liveness-fc424169-2ad9-486a-8b92-7183454e652d in namespace container-probe-7899
STEP: checking the pod's current state and verifying that restartCount is present
Feb 17 16:56:34.869: INFO: Initial restart count of pod liveness-fc424169-2ad9-486a-8b92-7183454e652d is 0
Feb 17 16:56:57.011: INFO: Restart count of pod container-probe-7899/liveness-fc424169-2ad9-486a-8b92-7183454e652d is now 1 (22.141626071s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:56:57.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7899" for this suite.

• [SLOW TEST:24.467 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":119,"skipped":1800,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:56:57.076: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8764
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 16:56:57.329: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb 17 16:56:57.360: INFO: Number of nodes with available pods: 0
Feb 17 16:56:57.361: INFO: Node 10.195.53.14 is running more than one daemon pod
Feb 17 16:56:58.389: INFO: Number of nodes with available pods: 0
Feb 17 16:56:58.389: INFO: Node 10.195.53.14 is running more than one daemon pod
Feb 17 16:56:59.384: INFO: Number of nodes with available pods: 3
Feb 17 16:56:59.384: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb 17 16:56:59.446: INFO: Wrong image for pod: daemon-set-6gvlp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:56:59.446: INFO: Wrong image for pod: daemon-set-7rfq7. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:56:59.446: INFO: Wrong image for pod: daemon-set-p2xzz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:00.468: INFO: Wrong image for pod: daemon-set-6gvlp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:00.468: INFO: Wrong image for pod: daemon-set-7rfq7. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:00.468: INFO: Wrong image for pod: daemon-set-p2xzz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:01.467: INFO: Wrong image for pod: daemon-set-6gvlp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:01.467: INFO: Wrong image for pod: daemon-set-7rfq7. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:01.467: INFO: Wrong image for pod: daemon-set-p2xzz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:02.468: INFO: Wrong image for pod: daemon-set-6gvlp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:02.468: INFO: Wrong image for pod: daemon-set-7rfq7. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:02.468: INFO: Pod daemon-set-7rfq7 is not available
Feb 17 16:57:02.468: INFO: Wrong image for pod: daemon-set-p2xzz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:03.468: INFO: Wrong image for pod: daemon-set-6gvlp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:03.469: INFO: Wrong image for pod: daemon-set-p2xzz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:03.469: INFO: Pod daemon-set-sfdbf is not available
Feb 17 16:57:04.467: INFO: Wrong image for pod: daemon-set-6gvlp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:04.467: INFO: Wrong image for pod: daemon-set-p2xzz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:04.467: INFO: Pod daemon-set-sfdbf is not available
Feb 17 16:57:05.467: INFO: Wrong image for pod: daemon-set-6gvlp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:05.467: INFO: Wrong image for pod: daemon-set-p2xzz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:06.469: INFO: Wrong image for pod: daemon-set-6gvlp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:06.469: INFO: Wrong image for pod: daemon-set-p2xzz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:06.469: INFO: Pod daemon-set-p2xzz is not available
Feb 17 16:57:07.466: INFO: Wrong image for pod: daemon-set-6gvlp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:07.466: INFO: Pod daemon-set-nrbbg is not available
Feb 17 16:57:08.468: INFO: Wrong image for pod: daemon-set-6gvlp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:08.468: INFO: Pod daemon-set-nrbbg is not available
Feb 17 16:57:09.468: INFO: Wrong image for pod: daemon-set-6gvlp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:09.468: INFO: Pod daemon-set-nrbbg is not available
Feb 17 16:57:10.468: INFO: Wrong image for pod: daemon-set-6gvlp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:10.468: INFO: Pod daemon-set-nrbbg is not available
Feb 17 16:57:11.467: INFO: Wrong image for pod: daemon-set-6gvlp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:11.467: INFO: Pod daemon-set-nrbbg is not available
Feb 17 16:57:12.467: INFO: Wrong image for pod: daemon-set-6gvlp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:12.467: INFO: Pod daemon-set-nrbbg is not available
Feb 17 16:57:13.472: INFO: Wrong image for pod: daemon-set-6gvlp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:14.466: INFO: Wrong image for pod: daemon-set-6gvlp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:14.466: INFO: Pod daemon-set-6gvlp is not available
Feb 17 16:57:15.467: INFO: Wrong image for pod: daemon-set-6gvlp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:15.467: INFO: Pod daemon-set-6gvlp is not available
Feb 17 16:57:16.468: INFO: Wrong image for pod: daemon-set-6gvlp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:16.468: INFO: Pod daemon-set-6gvlp is not available
Feb 17 16:57:17.469: INFO: Wrong image for pod: daemon-set-6gvlp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:17.469: INFO: Pod daemon-set-6gvlp is not available
Feb 17 16:57:18.468: INFO: Wrong image for pod: daemon-set-6gvlp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:18.468: INFO: Pod daemon-set-6gvlp is not available
Feb 17 16:57:19.468: INFO: Wrong image for pod: daemon-set-6gvlp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:19.468: INFO: Pod daemon-set-6gvlp is not available
Feb 17 16:57:20.468: INFO: Wrong image for pod: daemon-set-6gvlp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 16:57:20.468: INFO: Pod daemon-set-6gvlp is not available
Feb 17 16:57:21.467: INFO: Pod daemon-set-2b4h7 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Feb 17 16:57:21.501: INFO: Number of nodes with available pods: 2
Feb 17 16:57:21.501: INFO: Node 10.195.53.9 is running more than one daemon pod
Feb 17 16:57:22.526: INFO: Number of nodes with available pods: 2
Feb 17 16:57:22.526: INFO: Node 10.195.53.9 is running more than one daemon pod
Feb 17 16:57:23.527: INFO: Number of nodes with available pods: 3
Feb 17 16:57:23.527: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8764, will wait for the garbage collector to delete the pods
Feb 17 16:57:23.645: INFO: Deleting DaemonSet.extensions daemon-set took: 18.527277ms
Feb 17 16:57:23.745: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.358866ms
Feb 17 16:57:36.456: INFO: Number of nodes with available pods: 0
Feb 17 16:57:36.456: INFO: Number of running nodes: 0, number of available pods: 0
Feb 17 16:57:36.465: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8764/daemonsets","resourceVersion":"43862"},"items":null}

Feb 17 16:57:36.476: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8764/pods","resourceVersion":"43862"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:57:36.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8764" for this suite.

• [SLOW TEST:39.462 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":280,"completed":120,"skipped":1809,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:57:36.538: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9967
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-bc0b3283-8b46-4a9e-b6d2-87e608bd2620
STEP: Creating a pod to test consume configMaps
Feb 17 16:57:36.763: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-411ec876-c896-426f-9be8-92fa04523c76" in namespace "projected-9967" to be "success or failure"
Feb 17 16:57:36.774: INFO: Pod "pod-projected-configmaps-411ec876-c896-426f-9be8-92fa04523c76": Phase="Pending", Reason="", readiness=false. Elapsed: 10.761101ms
Feb 17 16:57:38.786: INFO: Pod "pod-projected-configmaps-411ec876-c896-426f-9be8-92fa04523c76": Phase="Running", Reason="", readiness=true. Elapsed: 2.022314587s
Feb 17 16:57:40.797: INFO: Pod "pod-projected-configmaps-411ec876-c896-426f-9be8-92fa04523c76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033737426s
STEP: Saw pod success
Feb 17 16:57:40.797: INFO: Pod "pod-projected-configmaps-411ec876-c896-426f-9be8-92fa04523c76" satisfied condition "success or failure"
Feb 17 16:57:40.808: INFO: Trying to get logs from node 10.195.53.9 pod pod-projected-configmaps-411ec876-c896-426f-9be8-92fa04523c76 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 16:57:40.894: INFO: Waiting for pod pod-projected-configmaps-411ec876-c896-426f-9be8-92fa04523c76 to disappear
Feb 17 16:57:40.904: INFO: Pod pod-projected-configmaps-411ec876-c896-426f-9be8-92fa04523c76 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:57:40.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9967" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":121,"skipped":1816,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:57:40.932: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2584
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 16:57:41.131: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:57:41.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2584" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":280,"completed":122,"skipped":1817,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:57:41.736: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5512
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Feb 17 16:57:48.013: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W0217 16:57:48.013090      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 17 16:57:48.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5512" for this suite.

• [SLOW TEST:6.302 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":280,"completed":123,"skipped":1860,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:57:48.039: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9454
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run default
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1596
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 17 16:57:48.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-9454'
Feb 17 16:57:48.356: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 17 16:57:48.356: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1602
Feb 17 16:57:50.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 delete deployment e2e-test-httpd-deployment --namespace=kubectl-9454'
Feb 17 16:57:50.521: INFO: stderr: ""
Feb 17 16:57:50.521: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:57:50.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9454" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run default should create an rc or deployment from an image  [Conformance]","total":280,"completed":124,"skipped":1879,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:57:50.550: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6267
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's command
Feb 17 16:57:50.840: INFO: Waiting up to 5m0s for pod "var-expansion-3390c0db-21c5-42f7-b1de-288a2f64d3db" in namespace "var-expansion-6267" to be "success or failure"
Feb 17 16:57:50.850: INFO: Pod "var-expansion-3390c0db-21c5-42f7-b1de-288a2f64d3db": Phase="Pending", Reason="", readiness=false. Elapsed: 10.397301ms
Feb 17 16:57:52.862: INFO: Pod "var-expansion-3390c0db-21c5-42f7-b1de-288a2f64d3db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022137098s
Feb 17 16:57:54.874: INFO: Pod "var-expansion-3390c0db-21c5-42f7-b1de-288a2f64d3db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034623201s
STEP: Saw pod success
Feb 17 16:57:54.874: INFO: Pod "var-expansion-3390c0db-21c5-42f7-b1de-288a2f64d3db" satisfied condition "success or failure"
Feb 17 16:57:54.885: INFO: Trying to get logs from node 10.195.53.9 pod var-expansion-3390c0db-21c5-42f7-b1de-288a2f64d3db container dapi-container: <nil>
STEP: delete the pod
Feb 17 16:57:54.941: INFO: Waiting for pod var-expansion-3390c0db-21c5-42f7-b1de-288a2f64d3db to disappear
Feb 17 16:57:54.952: INFO: Pod var-expansion-3390c0db-21c5-42f7-b1de-288a2f64d3db no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:57:54.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6267" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":280,"completed":125,"skipped":1927,"failed":0}
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:57:54.982: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8414
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override all
Feb 17 16:57:55.203: INFO: Waiting up to 5m0s for pod "client-containers-62958069-b52f-4046-9437-2be27ea2d623" in namespace "containers-8414" to be "success or failure"
Feb 17 16:57:55.214: INFO: Pod "client-containers-62958069-b52f-4046-9437-2be27ea2d623": Phase="Pending", Reason="", readiness=false. Elapsed: 11.29273ms
Feb 17 16:57:57.227: INFO: Pod "client-containers-62958069-b52f-4046-9437-2be27ea2d623": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024359845s
STEP: Saw pod success
Feb 17 16:57:57.227: INFO: Pod "client-containers-62958069-b52f-4046-9437-2be27ea2d623" satisfied condition "success or failure"
Feb 17 16:57:57.239: INFO: Trying to get logs from node 10.195.53.9 pod client-containers-62958069-b52f-4046-9437-2be27ea2d623 container test-container: <nil>
STEP: delete the pod
Feb 17 16:57:57.295: INFO: Waiting for pod client-containers-62958069-b52f-4046-9437-2be27ea2d623 to disappear
Feb 17 16:57:57.307: INFO: Pod client-containers-62958069-b52f-4046-9437-2be27ea2d623 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:57:57.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8414" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":280,"completed":126,"skipped":1928,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:57:57.334: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1484
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-483a0fb9-5c9f-425e-9d76-b317939cee29
STEP: Creating a pod to test consume secrets
Feb 17 16:57:57.556: INFO: Waiting up to 5m0s for pod "pod-secrets-d3e2d961-48f0-4daa-a749-a0210456b3e0" in namespace "secrets-1484" to be "success or failure"
Feb 17 16:57:57.567: INFO: Pod "pod-secrets-d3e2d961-48f0-4daa-a749-a0210456b3e0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.173622ms
Feb 17 16:57:59.579: INFO: Pod "pod-secrets-d3e2d961-48f0-4daa-a749-a0210456b3e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022887832s
STEP: Saw pod success
Feb 17 16:57:59.579: INFO: Pod "pod-secrets-d3e2d961-48f0-4daa-a749-a0210456b3e0" satisfied condition "success or failure"
Feb 17 16:57:59.590: INFO: Trying to get logs from node 10.195.53.9 pod pod-secrets-d3e2d961-48f0-4daa-a749-a0210456b3e0 container secret-volume-test: <nil>
STEP: delete the pod
Feb 17 16:57:59.651: INFO: Waiting for pod pod-secrets-d3e2d961-48f0-4daa-a749-a0210456b3e0 to disappear
Feb 17 16:57:59.662: INFO: Pod pod-secrets-d3e2d961-48f0-4daa-a749-a0210456b3e0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:57:59.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1484" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":127,"skipped":1935,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:57:59.691: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1803
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-1803
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 17 16:57:59.892: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 17 16:58:22.213: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.89.247 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1803 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 16:58:22.213: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 16:58:23.369: INFO: Found all expected endpoints: [netserver-0]
Feb 17 16:58:23.382: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.117.237 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1803 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 16:58:23.382: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 16:58:24.537: INFO: Found all expected endpoints: [netserver-1]
Feb 17 16:58:24.548: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.20.76 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1803 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 16:58:24.548: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 16:58:25.687: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:58:25.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1803" for this suite.

• [SLOW TEST:26.024 seconds]
[sig-network] Networking
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":128,"skipped":1943,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:58:25.716: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-4180
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 16:58:25.927: INFO: Waiting up to 5m0s for pod "busybox-user-65534-2118e31b-d53a-4ab0-9050-ba21464bda35" in namespace "security-context-test-4180" to be "success or failure"
Feb 17 16:58:25.939: INFO: Pod "busybox-user-65534-2118e31b-d53a-4ab0-9050-ba21464bda35": Phase="Pending", Reason="", readiness=false. Elapsed: 12.223453ms
Feb 17 16:58:27.952: INFO: Pod "busybox-user-65534-2118e31b-d53a-4ab0-9050-ba21464bda35": Phase="Running", Reason="", readiness=true. Elapsed: 2.024935913s
Feb 17 16:58:29.964: INFO: Pod "busybox-user-65534-2118e31b-d53a-4ab0-9050-ba21464bda35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037076347s
Feb 17 16:58:29.964: INFO: Pod "busybox-user-65534-2118e31b-d53a-4ab0-9050-ba21464bda35" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:58:29.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4180" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":129,"skipped":1955,"failed":0}
SSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:58:29.995: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1378
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 16:58:30.197: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:58:32.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1378" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":280,"completed":130,"skipped":1959,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:58:32.379: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2900
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-j49z
STEP: Creating a pod to test atomic-volume-subpath
Feb 17 16:58:32.612: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-j49z" in namespace "subpath-2900" to be "success or failure"
Feb 17 16:58:32.623: INFO: Pod "pod-subpath-test-configmap-j49z": Phase="Pending", Reason="", readiness=false. Elapsed: 10.550062ms
Feb 17 16:58:34.635: INFO: Pod "pod-subpath-test-configmap-j49z": Phase="Running", Reason="", readiness=true. Elapsed: 2.022792726s
Feb 17 16:58:36.647: INFO: Pod "pod-subpath-test-configmap-j49z": Phase="Running", Reason="", readiness=true. Elapsed: 4.034848134s
Feb 17 16:58:38.659: INFO: Pod "pod-subpath-test-configmap-j49z": Phase="Running", Reason="", readiness=true. Elapsed: 6.046916042s
Feb 17 16:58:40.672: INFO: Pod "pod-subpath-test-configmap-j49z": Phase="Running", Reason="", readiness=true. Elapsed: 8.059191289s
Feb 17 16:58:42.683: INFO: Pod "pod-subpath-test-configmap-j49z": Phase="Running", Reason="", readiness=true. Elapsed: 10.070639579s
Feb 17 16:58:44.698: INFO: Pod "pod-subpath-test-configmap-j49z": Phase="Running", Reason="", readiness=true. Elapsed: 12.085043878s
Feb 17 16:58:46.710: INFO: Pod "pod-subpath-test-configmap-j49z": Phase="Running", Reason="", readiness=true. Elapsed: 14.097185312s
Feb 17 16:58:48.721: INFO: Pod "pod-subpath-test-configmap-j49z": Phase="Running", Reason="", readiness=true. Elapsed: 16.10871919s
Feb 17 16:58:50.734: INFO: Pod "pod-subpath-test-configmap-j49z": Phase="Running", Reason="", readiness=true. Elapsed: 18.121411261s
Feb 17 16:58:52.745: INFO: Pod "pod-subpath-test-configmap-j49z": Phase="Running", Reason="", readiness=true. Elapsed: 20.132917462s
Feb 17 16:58:54.757: INFO: Pod "pod-subpath-test-configmap-j49z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.144406759s
STEP: Saw pod success
Feb 17 16:58:54.757: INFO: Pod "pod-subpath-test-configmap-j49z" satisfied condition "success or failure"
Feb 17 16:58:54.768: INFO: Trying to get logs from node 10.195.53.9 pod pod-subpath-test-configmap-j49z container test-container-subpath-configmap-j49z: <nil>
STEP: delete the pod
Feb 17 16:58:54.839: INFO: Waiting for pod pod-subpath-test-configmap-j49z to disappear
Feb 17 16:58:54.849: INFO: Pod pod-subpath-test-configmap-j49z no longer exists
STEP: Deleting pod pod-subpath-test-configmap-j49z
Feb 17 16:58:54.849: INFO: Deleting pod "pod-subpath-test-configmap-j49z" in namespace "subpath-2900"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:58:54.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2900" for this suite.

• [SLOW TEST:22.509 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":280,"completed":131,"skipped":1977,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:58:54.889: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-134
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Feb 17 16:58:55.084: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:59:14.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-134" for this suite.

• [SLOW TEST:19.523 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":280,"completed":132,"skipped":2001,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:59:14.413: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2665
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 16:59:15.196: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 16:59:17.229: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717555555, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717555555, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717555555, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717555555, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 16:59:20.270: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:59:20.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2665" for this suite.
STEP: Destroying namespace "webhook-2665-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.154 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":280,"completed":133,"skipped":2034,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:59:20.567: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3570
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 17 16:59:20.779: INFO: Waiting up to 5m0s for pod "pod-24c30d73-c7a9-4c92-acf6-985a3aa5ba0b" in namespace "emptydir-3570" to be "success or failure"
Feb 17 16:59:20.789: INFO: Pod "pod-24c30d73-c7a9-4c92-acf6-985a3aa5ba0b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.058901ms
Feb 17 16:59:22.801: INFO: Pod "pod-24c30d73-c7a9-4c92-acf6-985a3aa5ba0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022363578s
STEP: Saw pod success
Feb 17 16:59:22.801: INFO: Pod "pod-24c30d73-c7a9-4c92-acf6-985a3aa5ba0b" satisfied condition "success or failure"
Feb 17 16:59:22.812: INFO: Trying to get logs from node 10.195.53.9 pod pod-24c30d73-c7a9-4c92-acf6-985a3aa5ba0b container test-container: <nil>
STEP: delete the pod
Feb 17 16:59:22.875: INFO: Waiting for pod pod-24c30d73-c7a9-4c92-acf6-985a3aa5ba0b to disappear
Feb 17 16:59:22.884: INFO: Pod pod-24c30d73-c7a9-4c92-acf6-985a3aa5ba0b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:59:22.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3570" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":134,"skipped":2034,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:59:22.914: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9809
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:46
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Feb 17 16:59:27.201: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-855842601 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Feb 17 16:59:32.332: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:59:32.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9809" for this suite.

• [SLOW TEST:9.457 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should be submitted and removed [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period should be submitted and removed [Conformance]","total":280,"completed":135,"skipped":2046,"failed":0}
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:59:32.373: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8640
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:59:36.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8640" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":280,"completed":136,"skipped":2046,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:59:36.657: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9720
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 17 16:59:36.879: INFO: Waiting up to 5m0s for pod "pod-ce72797d-269f-431e-a8b2-d57ef543dae6" in namespace "emptydir-9720" to be "success or failure"
Feb 17 16:59:36.891: INFO: Pod "pod-ce72797d-269f-431e-a8b2-d57ef543dae6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.499669ms
Feb 17 16:59:38.903: INFO: Pod "pod-ce72797d-269f-431e-a8b2-d57ef543dae6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023383121s
STEP: Saw pod success
Feb 17 16:59:38.903: INFO: Pod "pod-ce72797d-269f-431e-a8b2-d57ef543dae6" satisfied condition "success or failure"
Feb 17 16:59:38.914: INFO: Trying to get logs from node 10.195.53.9 pod pod-ce72797d-269f-431e-a8b2-d57ef543dae6 container test-container: <nil>
STEP: delete the pod
Feb 17 16:59:38.972: INFO: Waiting for pod pod-ce72797d-269f-431e-a8b2-d57ef543dae6 to disappear
Feb 17 16:59:38.983: INFO: Pod pod-ce72797d-269f-431e-a8b2-d57ef543dae6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:59:38.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9720" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":137,"skipped":2078,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:59:39.014: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8862
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 16:59:39.210: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 17 16:59:42.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 --namespace=crd-publish-openapi-8862 create -f -'
Feb 17 16:59:43.364: INFO: stderr: ""
Feb 17 16:59:43.364: INFO: stdout: "e2e-test-crd-publish-openapi-8729-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 17 16:59:43.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 --namespace=crd-publish-openapi-8862 delete e2e-test-crd-publish-openapi-8729-crds test-cr'
Feb 17 16:59:43.487: INFO: stderr: ""
Feb 17 16:59:43.487: INFO: stdout: "e2e-test-crd-publish-openapi-8729-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Feb 17 16:59:43.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 --namespace=crd-publish-openapi-8862 apply -f -'
Feb 17 16:59:43.876: INFO: stderr: ""
Feb 17 16:59:43.876: INFO: stdout: "e2e-test-crd-publish-openapi-8729-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 17 16:59:43.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 --namespace=crd-publish-openapi-8862 delete e2e-test-crd-publish-openapi-8729-crds test-cr'
Feb 17 16:59:44.002: INFO: stderr: ""
Feb 17 16:59:44.002: INFO: stdout: "e2e-test-crd-publish-openapi-8729-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Feb 17 16:59:44.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 explain e2e-test-crd-publish-openapi-8729-crds'
Feb 17 16:59:44.196: INFO: stderr: ""
Feb 17 16:59:44.196: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8729-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:59:47.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8862" for this suite.

• [SLOW TEST:8.905 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":280,"completed":138,"skipped":2100,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:59:47.919: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9117
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-03bcc4a7-0c1d-4f2b-a39f-8951b4272a1e
STEP: Creating a pod to test consume secrets
Feb 17 16:59:48.153: INFO: Waiting up to 5m0s for pod "pod-secrets-b8dca089-5c17-4ca8-9207-49501200a60c" in namespace "secrets-9117" to be "success or failure"
Feb 17 16:59:48.168: INFO: Pod "pod-secrets-b8dca089-5c17-4ca8-9207-49501200a60c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.587514ms
Feb 17 16:59:50.180: INFO: Pod "pod-secrets-b8dca089-5c17-4ca8-9207-49501200a60c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026989405s
Feb 17 16:59:52.191: INFO: Pod "pod-secrets-b8dca089-5c17-4ca8-9207-49501200a60c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037994194s
STEP: Saw pod success
Feb 17 16:59:52.191: INFO: Pod "pod-secrets-b8dca089-5c17-4ca8-9207-49501200a60c" satisfied condition "success or failure"
Feb 17 16:59:52.201: INFO: Trying to get logs from node 10.195.53.9 pod pod-secrets-b8dca089-5c17-4ca8-9207-49501200a60c container secret-volume-test: <nil>
STEP: delete the pod
Feb 17 16:59:52.264: INFO: Waiting for pod pod-secrets-b8dca089-5c17-4ca8-9207-49501200a60c to disappear
Feb 17 16:59:52.275: INFO: Pod pod-secrets-b8dca089-5c17-4ca8-9207-49501200a60c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:59:52.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9117" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":139,"skipped":2107,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:59:52.304: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7184
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Feb 17 16:59:52.498: INFO: namespace kubectl-7184
Feb 17 16:59:52.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 create -f - --namespace=kubectl-7184'
Feb 17 16:59:52.784: INFO: stderr: ""
Feb 17 16:59:52.784: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Feb 17 16:59:53.797: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 17 16:59:53.797: INFO: Found 0 / 1
Feb 17 16:59:54.874: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 17 16:59:54.874: INFO: Found 1 / 1
Feb 17 16:59:54.874: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 17 16:59:54.884: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 17 16:59:54.884: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 17 16:59:54.884: INFO: wait on agnhost-master startup in kubectl-7184 
Feb 17 16:59:54.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 logs agnhost-master-d9bnc agnhost-master --namespace=kubectl-7184'
Feb 17 16:59:55.012: INFO: stderr: ""
Feb 17 16:59:55.012: INFO: stdout: "Paused\n"
STEP: exposing RC
Feb 17 16:59:55.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-7184'
Feb 17 16:59:55.157: INFO: stderr: ""
Feb 17 16:59:55.157: INFO: stdout: "service/rm2 exposed\n"
Feb 17 16:59:55.165: INFO: Service rm2 in namespace kubectl-7184 found.
STEP: exposing service
Feb 17 16:59:57.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-7184'
Feb 17 16:59:57.317: INFO: stderr: ""
Feb 17 16:59:57.317: INFO: stdout: "service/rm3 exposed\n"
Feb 17 16:59:57.326: INFO: Service rm3 in namespace kubectl-7184 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:59:59.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7184" for this suite.

• [SLOW TEST:7.068 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1295
    should create services for rc  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":280,"completed":140,"skipped":2125,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:59:59.372: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5124
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 16:59:59.576: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Feb 17 17:00:01.658: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:00:01.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5124" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":280,"completed":141,"skipped":2134,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:00:01.697: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8343
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 17 17:00:03.957: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:00:04.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8343" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":142,"skipped":2172,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:00:04.039: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2592
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 17:00:04.232: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:00:05.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2592" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":280,"completed":143,"skipped":2188,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:00:05.626: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7186
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Feb 17 17:00:05.848: INFO: Waiting up to 5m0s for pod "downward-api-03fbd352-c51a-4796-8d73-3429c964c31a" in namespace "downward-api-7186" to be "success or failure"
Feb 17 17:00:05.860: INFO: Pod "downward-api-03fbd352-c51a-4796-8d73-3429c964c31a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.775465ms
Feb 17 17:00:07.890: INFO: Pod "downward-api-03fbd352-c51a-4796-8d73-3429c964c31a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.041810607s
STEP: Saw pod success
Feb 17 17:00:07.890: INFO: Pod "downward-api-03fbd352-c51a-4796-8d73-3429c964c31a" satisfied condition "success or failure"
Feb 17 17:00:07.900: INFO: Trying to get logs from node 10.195.53.9 pod downward-api-03fbd352-c51a-4796-8d73-3429c964c31a container dapi-container: <nil>
STEP: delete the pod
Feb 17 17:00:07.976: INFO: Waiting for pod downward-api-03fbd352-c51a-4796-8d73-3429c964c31a to disappear
Feb 17 17:00:07.986: INFO: Pod downward-api-03fbd352-c51a-4796-8d73-3429c964c31a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:00:07.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7186" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":280,"completed":144,"skipped":2199,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:00:08.014: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7394
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 17:00:08.236: INFO: Waiting up to 5m0s for pod "downwardapi-volume-539ec41e-0cc3-46dd-89f4-71df1b3efae6" in namespace "downward-api-7394" to be "success or failure"
Feb 17 17:00:08.247: INFO: Pod "downwardapi-volume-539ec41e-0cc3-46dd-89f4-71df1b3efae6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.577842ms
Feb 17 17:00:10.258: INFO: Pod "downwardapi-volume-539ec41e-0cc3-46dd-89f4-71df1b3efae6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021748442s
Feb 17 17:00:12.270: INFO: Pod "downwardapi-volume-539ec41e-0cc3-46dd-89f4-71df1b3efae6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033792218s
STEP: Saw pod success
Feb 17 17:00:12.270: INFO: Pod "downwardapi-volume-539ec41e-0cc3-46dd-89f4-71df1b3efae6" satisfied condition "success or failure"
Feb 17 17:00:12.281: INFO: Trying to get logs from node 10.195.53.9 pod downwardapi-volume-539ec41e-0cc3-46dd-89f4-71df1b3efae6 container client-container: <nil>
STEP: delete the pod
Feb 17 17:00:12.339: INFO: Waiting for pod downwardapi-volume-539ec41e-0cc3-46dd-89f4-71df1b3efae6 to disappear
Feb 17 17:00:12.351: INFO: Pod downwardapi-volume-539ec41e-0cc3-46dd-89f4-71df1b3efae6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:00:12.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7394" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":280,"completed":145,"skipped":2302,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:00:12.379: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-766
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:00:18.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-766" for this suite.

• [SLOW TEST:6.251 seconds]
[sig-apps] Job
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":280,"completed":146,"skipped":2329,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:00:18.631: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1445
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 17 17:00:18.848: INFO: Waiting up to 5m0s for pod "pod-25b03cde-8c89-44a2-937c-c74b9c313ad0" in namespace "emptydir-1445" to be "success or failure"
Feb 17 17:00:18.858: INFO: Pod "pod-25b03cde-8c89-44a2-937c-c74b9c313ad0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.368243ms
Feb 17 17:00:20.870: INFO: Pod "pod-25b03cde-8c89-44a2-937c-c74b9c313ad0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021956649s
STEP: Saw pod success
Feb 17 17:00:20.870: INFO: Pod "pod-25b03cde-8c89-44a2-937c-c74b9c313ad0" satisfied condition "success or failure"
Feb 17 17:00:20.880: INFO: Trying to get logs from node 10.195.53.9 pod pod-25b03cde-8c89-44a2-937c-c74b9c313ad0 container test-container: <nil>
STEP: delete the pod
Feb 17 17:00:20.945: INFO: Waiting for pod pod-25b03cde-8c89-44a2-937c-c74b9c313ad0 to disappear
Feb 17 17:00:20.955: INFO: Pod pod-25b03cde-8c89-44a2-937c-c74b9c313ad0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:00:20.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1445" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":147,"skipped":2352,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:00:20.984: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6402
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6402
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-6402
I0217 17:00:21.247714      24 runners.go:189] Created replication controller with name: externalname-service, namespace: services-6402, replica count: 2
I0217 17:00:24.298199      24 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 17 17:00:24.298: INFO: Creating new exec pod
Feb 17 17:00:29.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=services-6402 execpod575p8 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Feb 17 17:00:29.601: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 17 17:00:29.601: INFO: stdout: ""
Feb 17 17:00:29.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=services-6402 execpod575p8 -- /bin/sh -x -c nc -zv -t -w 2 172.21.210.116 80'
Feb 17 17:00:29.856: INFO: stderr: "+ nc -zv -t -w 2 172.21.210.116 80\nConnection to 172.21.210.116 80 port [tcp/http] succeeded!\n"
Feb 17 17:00:29.856: INFO: stdout: ""
Feb 17 17:00:29.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=services-6402 execpod575p8 -- /bin/sh -x -c nc -zv -t -w 2 10.195.53.14 32282'
Feb 17 17:00:30.100: INFO: stderr: "+ nc -zv -t -w 2 10.195.53.14 32282\nConnection to 10.195.53.14 32282 port [tcp/32282] succeeded!\n"
Feb 17 17:00:30.100: INFO: stdout: ""
Feb 17 17:00:30.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=services-6402 execpod575p8 -- /bin/sh -x -c nc -zv -t -w 2 10.195.53.47 32282'
Feb 17 17:00:30.353: INFO: stderr: "+ nc -zv -t -w 2 10.195.53.47 32282\nConnection to 10.195.53.47 32282 port [tcp/32282] succeeded!\n"
Feb 17 17:00:30.353: INFO: stdout: ""
Feb 17 17:00:30.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=services-6402 execpod575p8 -- /bin/sh -x -c nc -zv -t -w 2 135.90.88.54 32282'
Feb 17 17:00:30.597: INFO: stderr: "+ nc -zv -t -w 2 135.90.88.54 32282\nConnection to 135.90.88.54 32282 port [tcp/32282] succeeded!\n"
Feb 17 17:00:30.597: INFO: stdout: ""
Feb 17 17:00:30.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=services-6402 execpod575p8 -- /bin/sh -x -c nc -zv -t -w 2 135.90.88.62 32282'
Feb 17 17:00:30.847: INFO: stderr: "+ nc -zv -t -w 2 135.90.88.62 32282\nConnection to 135.90.88.62 32282 port [tcp/32282] succeeded!\n"
Feb 17 17:00:30.847: INFO: stdout: ""
Feb 17 17:00:30.847: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:00:30.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6402" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:9.944 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":280,"completed":148,"skipped":2373,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:00:30.928: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6137
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 17:00:31.144: INFO: Waiting up to 5m0s for pod "downwardapi-volume-913b6a73-a1e8-423a-99d9-7b9c5b7e812c" in namespace "downward-api-6137" to be "success or failure"
Feb 17 17:00:31.156: INFO: Pod "downwardapi-volume-913b6a73-a1e8-423a-99d9-7b9c5b7e812c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.934674ms
Feb 17 17:00:33.168: INFO: Pod "downwardapi-volume-913b6a73-a1e8-423a-99d9-7b9c5b7e812c": Phase="Running", Reason="", readiness=true. Elapsed: 2.023668308s
Feb 17 17:00:35.182: INFO: Pod "downwardapi-volume-913b6a73-a1e8-423a-99d9-7b9c5b7e812c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038200943s
STEP: Saw pod success
Feb 17 17:00:35.182: INFO: Pod "downwardapi-volume-913b6a73-a1e8-423a-99d9-7b9c5b7e812c" satisfied condition "success or failure"
Feb 17 17:00:35.193: INFO: Trying to get logs from node 10.195.53.9 pod downwardapi-volume-913b6a73-a1e8-423a-99d9-7b9c5b7e812c container client-container: <nil>
STEP: delete the pod
Feb 17 17:00:35.252: INFO: Waiting for pod downwardapi-volume-913b6a73-a1e8-423a-99d9-7b9c5b7e812c to disappear
Feb 17 17:00:35.262: INFO: Pod downwardapi-volume-913b6a73-a1e8-423a-99d9-7b9c5b7e812c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:00:35.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6137" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":149,"skipped":2396,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:00:35.289: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1937
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 17:00:35.482: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:00:37.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1937" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":280,"completed":150,"skipped":2407,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:00:37.687: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7474
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 17:00:38.518: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 17:00:40.558: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717555638, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717555638, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717555638, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717555638, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 17:00:43.599: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:00:43.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7474" for this suite.
STEP: Destroying namespace "webhook-7474-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.161 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":280,"completed":151,"skipped":2418,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:00:43.849: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2028
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 17:00:44.425: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 17:00:46.458: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717555644, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717555644, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717555644, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717555644, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 17:00:49.559: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
Feb 17 17:00:59.612: INFO: Waiting for webhook configuration to be ready...
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:00:59.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2028" for this suite.
STEP: Destroying namespace "webhook-2028-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.153 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":280,"completed":152,"skipped":2429,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:01:00.002: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8743
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Feb 17 17:01:00.219: INFO: Created pod &Pod{ObjectMeta:{dns-8743  dns-8743 /api/v1/namespaces/dns-8743/pods/dns-8743 2b561e46-ea5e-4762-918a-fa09d1d77337 46312 0 2020-02-17 17:01:00 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jgvrz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jgvrz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jgvrz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
STEP: Verifying customized DNS suffix list is configured on pod...
Feb 17 17:01:02.242: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8743 PodName:dns-8743 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 17:01:02.242: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Verifying customized DNS server is configured on pod...
Feb 17 17:01:04.634: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8743 PodName:dns-8743 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 17:01:04.634: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 17:01:05.127: INFO: Deleting pod dns-8743...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:01:05.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8743" for this suite.

• [SLOW TEST:5.194 seconds]
[sig-network] DNS
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":280,"completed":153,"skipped":2438,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:01:05.197: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4060
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override arguments
Feb 17 17:01:05.409: INFO: Waiting up to 5m0s for pod "client-containers-b9d1042b-cfc3-463c-b314-dcd3d043a88d" in namespace "containers-4060" to be "success or failure"
Feb 17 17:01:05.423: INFO: Pod "client-containers-b9d1042b-cfc3-463c-b314-dcd3d043a88d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.735118ms
Feb 17 17:01:07.435: INFO: Pod "client-containers-b9d1042b-cfc3-463c-b314-dcd3d043a88d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025701818s
Feb 17 17:01:09.448: INFO: Pod "client-containers-b9d1042b-cfc3-463c-b314-dcd3d043a88d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03875891s
STEP: Saw pod success
Feb 17 17:01:09.448: INFO: Pod "client-containers-b9d1042b-cfc3-463c-b314-dcd3d043a88d" satisfied condition "success or failure"
Feb 17 17:01:09.461: INFO: Trying to get logs from node 10.195.53.9 pod client-containers-b9d1042b-cfc3-463c-b314-dcd3d043a88d container test-container: <nil>
STEP: delete the pod
Feb 17 17:01:09.520: INFO: Waiting for pod client-containers-b9d1042b-cfc3-463c-b314-dcd3d043a88d to disappear
Feb 17 17:01:09.531: INFO: Pod client-containers-b9d1042b-cfc3-463c-b314-dcd3d043a88d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:01:09.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4060" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":280,"completed":154,"skipped":2451,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:01:09.561: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6918
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-6918
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 17 17:01:09.759: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 17 17:01:33.997: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.89.249:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6918 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 17:01:33.997: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 17:01:34.144: INFO: Found all expected endpoints: [netserver-0]
Feb 17 17:01:34.156: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.117.239:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6918 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 17:01:34.156: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 17:01:34.311: INFO: Found all expected endpoints: [netserver-1]
Feb 17 17:01:34.322: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.20.101:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6918 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 17:01:34.322: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 17:01:34.574: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:01:34.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6918" for this suite.

• [SLOW TEST:25.042 seconds]
[sig-network] Networking
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":155,"skipped":2466,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:01:34.604: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1122
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-605eca6c-387f-48ff-be80-57d1be796353
STEP: Creating a pod to test consume secrets
Feb 17 17:01:34.843: INFO: Waiting up to 5m0s for pod "pod-secrets-82abed38-488b-46be-a1cc-9ea4d41b0aef" in namespace "secrets-1122" to be "success or failure"
Feb 17 17:01:34.855: INFO: Pod "pod-secrets-82abed38-488b-46be-a1cc-9ea4d41b0aef": Phase="Pending", Reason="", readiness=false. Elapsed: 12.375794ms
Feb 17 17:01:36.866: INFO: Pod "pod-secrets-82abed38-488b-46be-a1cc-9ea4d41b0aef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023342946s
STEP: Saw pod success
Feb 17 17:01:36.866: INFO: Pod "pod-secrets-82abed38-488b-46be-a1cc-9ea4d41b0aef" satisfied condition "success or failure"
Feb 17 17:01:36.877: INFO: Trying to get logs from node 10.195.53.9 pod pod-secrets-82abed38-488b-46be-a1cc-9ea4d41b0aef container secret-volume-test: <nil>
STEP: delete the pod
Feb 17 17:01:36.936: INFO: Waiting for pod pod-secrets-82abed38-488b-46be-a1cc-9ea4d41b0aef to disappear
Feb 17 17:01:36.946: INFO: Pod pod-secrets-82abed38-488b-46be-a1cc-9ea4d41b0aef no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:01:36.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1122" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":156,"skipped":2487,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:01:36.972: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-616
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1632
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 17 17:01:37.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-616'
Feb 17 17:01:37.281: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 17 17:01:37.281: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Feb 17 17:01:37.304: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-r2q2p]
Feb 17 17:01:37.304: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-r2q2p" in namespace "kubectl-616" to be "running and ready"
Feb 17 17:01:37.315: INFO: Pod "e2e-test-httpd-rc-r2q2p": Phase="Pending", Reason="", readiness=false. Elapsed: 10.699041ms
Feb 17 17:01:39.326: INFO: Pod "e2e-test-httpd-rc-r2q2p": Phase="Running", Reason="", readiness=true. Elapsed: 2.021921484s
Feb 17 17:01:39.326: INFO: Pod "e2e-test-httpd-rc-r2q2p" satisfied condition "running and ready"
Feb 17 17:01:39.326: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-r2q2p]
Feb 17 17:01:39.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 logs rc/e2e-test-httpd-rc --namespace=kubectl-616'
Feb 17 17:01:39.476: INFO: stderr: ""
Feb 17 17:01:39.476: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.30.20.103. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.30.20.103. Set the 'ServerName' directive globally to suppress this message\n[Mon Feb 17 17:01:38.545792 2020] [mpm_event:notice] [pid 1:tid 140586361416552] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Mon Feb 17 17:01:38.545841 2020] [core:notice] [pid 1:tid 140586361416552] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1637
Feb 17 17:01:39.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 delete rc e2e-test-httpd-rc --namespace=kubectl-616'
Feb 17 17:01:39.608: INFO: stderr: ""
Feb 17 17:01:39.608: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:01:39.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-616" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run rc should create an rc from an image  [Conformance]","total":280,"completed":157,"skipped":2487,"failed":0}
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:01:39.638: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1540
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Feb 17 17:01:39.843: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:01:48.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1540" for this suite.

• [SLOW TEST:9.012 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":280,"completed":158,"skipped":2490,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:01:48.650: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9051
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 17:01:48.865: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7c5d8722-eea7-461f-becb-7fbe223cc57d" in namespace "downward-api-9051" to be "success or failure"
Feb 17 17:01:48.875: INFO: Pod "downwardapi-volume-7c5d8722-eea7-461f-becb-7fbe223cc57d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.040598ms
Feb 17 17:01:50.886: INFO: Pod "downwardapi-volume-7c5d8722-eea7-461f-becb-7fbe223cc57d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021764333s
STEP: Saw pod success
Feb 17 17:01:50.886: INFO: Pod "downwardapi-volume-7c5d8722-eea7-461f-becb-7fbe223cc57d" satisfied condition "success or failure"
Feb 17 17:01:50.897: INFO: Trying to get logs from node 10.195.53.9 pod downwardapi-volume-7c5d8722-eea7-461f-becb-7fbe223cc57d container client-container: <nil>
STEP: delete the pod
Feb 17 17:01:50.954: INFO: Waiting for pod downwardapi-volume-7c5d8722-eea7-461f-becb-7fbe223cc57d to disappear
Feb 17 17:01:50.964: INFO: Pod downwardapi-volume-7c5d8722-eea7-461f-becb-7fbe223cc57d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:01:50.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9051" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":159,"skipped":2511,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:01:50.991: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3063
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 17:01:51.210: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ebe90fe0-a09d-4c6f-8ccb-d2a594d67ef6" in namespace "projected-3063" to be "success or failure"
Feb 17 17:01:51.224: INFO: Pod "downwardapi-volume-ebe90fe0-a09d-4c6f-8ccb-d2a594d67ef6": Phase="Pending", Reason="", readiness=false. Elapsed: 13.361377ms
Feb 17 17:01:53.235: INFO: Pod "downwardapi-volume-ebe90fe0-a09d-4c6f-8ccb-d2a594d67ef6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024328194s
STEP: Saw pod success
Feb 17 17:01:53.235: INFO: Pod "downwardapi-volume-ebe90fe0-a09d-4c6f-8ccb-d2a594d67ef6" satisfied condition "success or failure"
Feb 17 17:01:53.246: INFO: Trying to get logs from node 10.195.53.9 pod downwardapi-volume-ebe90fe0-a09d-4c6f-8ccb-d2a594d67ef6 container client-container: <nil>
STEP: delete the pod
Feb 17 17:01:53.302: INFO: Waiting for pod downwardapi-volume-ebe90fe0-a09d-4c6f-8ccb-d2a594d67ef6 to disappear
Feb 17 17:01:53.311: INFO: Pod downwardapi-volume-ebe90fe0-a09d-4c6f-8ccb-d2a594d67ef6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:01:53.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3063" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":160,"skipped":2527,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:01:53.344: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8863
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-8863
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Feb 17 17:01:53.569: INFO: Found 0 stateful pods, waiting for 3
Feb 17 17:02:03.583: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 17:02:03.583: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 17:02:03.583: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Feb 17 17:02:03.641: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb 17 17:02:13.712: INFO: Updating stateful set ss2
Feb 17 17:02:13.737: INFO: Waiting for Pod statefulset-8863/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 17 17:02:23.758: INFO: Waiting for Pod statefulset-8863/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Feb 17 17:02:33.842: INFO: Found 2 stateful pods, waiting for 3
Feb 17 17:02:43.854: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 17:02:43.854: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 17:02:43.854: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb 17 17:02:43.909: INFO: Updating stateful set ss2
Feb 17 17:02:43.931: INFO: Waiting for Pod statefulset-8863/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 17 17:02:53.981: INFO: Updating stateful set ss2
Feb 17 17:02:54.001: INFO: Waiting for StatefulSet statefulset-8863/ss2 to complete update
Feb 17 17:02:54.001: INFO: Waiting for Pod statefulset-8863/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Feb 17 17:03:04.023: INFO: Deleting all statefulset in ns statefulset-8863
Feb 17 17:03:04.031: INFO: Scaling statefulset ss2 to 0
Feb 17 17:03:34.074: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 17:03:34.083: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:03:34.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8863" for this suite.

• [SLOW TEST:100.802 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":280,"completed":161,"skipped":2540,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:03:34.146: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2026
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-projected-rxxz
STEP: Creating a pod to test atomic-volume-subpath
Feb 17 17:03:34.511: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-rxxz" in namespace "subpath-2026" to be "success or failure"
Feb 17 17:03:34.522: INFO: Pod "pod-subpath-test-projected-rxxz": Phase="Pending", Reason="", readiness=false. Elapsed: 11.101187ms
Feb 17 17:03:36.534: INFO: Pod "pod-subpath-test-projected-rxxz": Phase="Running", Reason="", readiness=true. Elapsed: 2.02266488s
Feb 17 17:03:38.546: INFO: Pod "pod-subpath-test-projected-rxxz": Phase="Running", Reason="", readiness=true. Elapsed: 4.034999973s
Feb 17 17:03:40.558: INFO: Pod "pod-subpath-test-projected-rxxz": Phase="Running", Reason="", readiness=true. Elapsed: 6.047364892s
Feb 17 17:03:42.571: INFO: Pod "pod-subpath-test-projected-rxxz": Phase="Running", Reason="", readiness=true. Elapsed: 8.060273454s
Feb 17 17:03:44.585: INFO: Pod "pod-subpath-test-projected-rxxz": Phase="Running", Reason="", readiness=true. Elapsed: 10.073897459s
Feb 17 17:03:46.596: INFO: Pod "pod-subpath-test-projected-rxxz": Phase="Running", Reason="", readiness=true. Elapsed: 12.085469008s
Feb 17 17:03:48.608: INFO: Pod "pod-subpath-test-projected-rxxz": Phase="Running", Reason="", readiness=true. Elapsed: 14.097191789s
Feb 17 17:03:50.620: INFO: Pod "pod-subpath-test-projected-rxxz": Phase="Running", Reason="", readiness=true. Elapsed: 16.108924794s
Feb 17 17:03:52.632: INFO: Pod "pod-subpath-test-projected-rxxz": Phase="Running", Reason="", readiness=true. Elapsed: 18.120916433s
Feb 17 17:03:54.644: INFO: Pod "pod-subpath-test-projected-rxxz": Phase="Running", Reason="", readiness=true. Elapsed: 20.133033702s
Feb 17 17:03:56.765: INFO: Pod "pod-subpath-test-projected-rxxz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.254083142s
STEP: Saw pod success
Feb 17 17:03:56.765: INFO: Pod "pod-subpath-test-projected-rxxz" satisfied condition "success or failure"
Feb 17 17:03:56.777: INFO: Trying to get logs from node 10.195.53.9 pod pod-subpath-test-projected-rxxz container test-container-subpath-projected-rxxz: <nil>
STEP: delete the pod
Feb 17 17:03:56.874: INFO: Waiting for pod pod-subpath-test-projected-rxxz to disappear
Feb 17 17:03:56.884: INFO: Pod pod-subpath-test-projected-rxxz no longer exists
STEP: Deleting pod pod-subpath-test-projected-rxxz
Feb 17 17:03:56.884: INFO: Deleting pod "pod-subpath-test-projected-rxxz" in namespace "subpath-2026"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:03:56.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2026" for this suite.

• [SLOW TEST:22.778 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":280,"completed":162,"skipped":2554,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:03:56.924: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6903
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 17:03:57.159: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb 17 17:04:02.173: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 17 17:04:02.173: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Feb 17 17:04:04.261: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6903 /apis/apps/v1/namespaces/deployment-6903/deployments/test-cleanup-deployment 6b7bff3e-c358-4b27-be25-728b4e790804 47623 1 2020-02-17 17:04:02 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0044570d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-02-17 17:04:02 +0000 UTC,LastTransitionTime:2020-02-17 17:04:02 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-55ffc6b7b6" has successfully progressed.,LastUpdateTime:2020-02-17 17:04:03 +0000 UTC,LastTransitionTime:2020-02-17 17:04:02 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 17 17:04:04.271: INFO: New ReplicaSet "test-cleanup-deployment-55ffc6b7b6" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-55ffc6b7b6  deployment-6903 /apis/apps/v1/namespaces/deployment-6903/replicasets/test-cleanup-deployment-55ffc6b7b6 c4190522-5c95-42ba-bfc3-cced10992422 47613 1 2020-02-17 17:04:02 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 6b7bff3e-c358-4b27-be25-728b4e790804 0xc0044574c7 0xc0044574c8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55ffc6b7b6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004457538 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 17 17:04:04.282: INFO: Pod "test-cleanup-deployment-55ffc6b7b6-b8zfn" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-55ffc6b7b6-b8zfn test-cleanup-deployment-55ffc6b7b6- deployment-6903 /api/v1/namespaces/deployment-6903/pods/test-cleanup-deployment-55ffc6b7b6-b8zfn 41afdb53-eed6-4864-8a25-a7ccf241e979 47612 0 2020-02-17 17:04:02 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-deployment-55ffc6b7b6 c4190522-5c95-42ba-bfc3-cced10992422 0xc0044578e7 0xc0044578e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-9nr2r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-9nr2r,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-9nr2r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:04:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:04:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:04:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:04:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.9,PodIP:172.30.20.113,StartTime:2020-02-17 17:04:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 17:04:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:containerd://c66b0bc2871a869602cf8d29735d124200ea3ee2d950b0d7a74773730fdc28f1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.20.113,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:04:04.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6903" for this suite.

• [SLOW TEST:7.386 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":280,"completed":163,"skipped":2575,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:04:04.313: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7843
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-b38aa545-1340-49ea-9b85-49324a73c7a4
STEP: Creating a pod to test consume configMaps
Feb 17 17:04:04.534: INFO: Waiting up to 5m0s for pod "pod-configmaps-81d56551-916f-45d4-a403-984745ccb3b8" in namespace "configmap-7843" to be "success or failure"
Feb 17 17:04:04.547: INFO: Pod "pod-configmaps-81d56551-916f-45d4-a403-984745ccb3b8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.681577ms
Feb 17 17:04:06.560: INFO: Pod "pod-configmaps-81d56551-916f-45d4-a403-984745ccb3b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02545648s
STEP: Saw pod success
Feb 17 17:04:06.560: INFO: Pod "pod-configmaps-81d56551-916f-45d4-a403-984745ccb3b8" satisfied condition "success or failure"
Feb 17 17:04:06.571: INFO: Trying to get logs from node 10.195.53.9 pod pod-configmaps-81d56551-916f-45d4-a403-984745ccb3b8 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 17:04:06.630: INFO: Waiting for pod pod-configmaps-81d56551-916f-45d4-a403-984745ccb3b8 to disappear
Feb 17 17:04:06.640: INFO: Pod pod-configmaps-81d56551-916f-45d4-a403-984745ccb3b8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:04:06.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7843" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":164,"skipped":2583,"failed":0}

------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:04:06.672: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3381
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service endpoint-test2 in namespace services-3381
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3381 to expose endpoints map[]
Feb 17 17:04:06.908: INFO: Get endpoints failed (6.640543ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Feb 17 17:04:07.940: INFO: successfully validated that service endpoint-test2 in namespace services-3381 exposes endpoints map[] (1.038841402s elapsed)
STEP: Creating pod pod1 in namespace services-3381
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3381 to expose endpoints map[pod1:[80]]
Feb 17 17:04:10.015: INFO: successfully validated that service endpoint-test2 in namespace services-3381 exposes endpoints map[pod1:[80]] (2.053623413s elapsed)
STEP: Creating pod pod2 in namespace services-3381
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3381 to expose endpoints map[pod1:[80] pod2:[80]]
Feb 17 17:04:12.171: INFO: successfully validated that service endpoint-test2 in namespace services-3381 exposes endpoints map[pod1:[80] pod2:[80]] (2.137809354s elapsed)
STEP: Deleting pod pod1 in namespace services-3381
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3381 to expose endpoints map[pod2:[80]]
Feb 17 17:04:12.211: INFO: successfully validated that service endpoint-test2 in namespace services-3381 exposes endpoints map[pod2:[80]] (18.369579ms elapsed)
STEP: Deleting pod pod2 in namespace services-3381
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3381 to expose endpoints map[]
Feb 17 17:04:12.237: INFO: successfully validated that service endpoint-test2 in namespace services-3381 exposes endpoints map[] (6.115632ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:04:12.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3381" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:5.642 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":280,"completed":165,"skipped":2583,"failed":0}
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:04:12.315: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-969
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-97824628-be6b-49d0-9a50-26d2671fb442
STEP: Creating a pod to test consume configMaps
Feb 17 17:04:12.540: INFO: Waiting up to 5m0s for pod "pod-configmaps-f8bac6c0-3f85-4679-a644-354775ecb307" in namespace "configmap-969" to be "success or failure"
Feb 17 17:04:12.550: INFO: Pod "pod-configmaps-f8bac6c0-3f85-4679-a644-354775ecb307": Phase="Pending", Reason="", readiness=false. Elapsed: 10.0749ms
Feb 17 17:04:14.562: INFO: Pod "pod-configmaps-f8bac6c0-3f85-4679-a644-354775ecb307": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021942212s
STEP: Saw pod success
Feb 17 17:04:14.562: INFO: Pod "pod-configmaps-f8bac6c0-3f85-4679-a644-354775ecb307" satisfied condition "success or failure"
Feb 17 17:04:14.573: INFO: Trying to get logs from node 10.195.53.9 pod pod-configmaps-f8bac6c0-3f85-4679-a644-354775ecb307 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 17:04:14.633: INFO: Waiting for pod pod-configmaps-f8bac6c0-3f85-4679-a644-354775ecb307 to disappear
Feb 17 17:04:14.644: INFO: Pod pod-configmaps-f8bac6c0-3f85-4679-a644-354775ecb307 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:04:14.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-969" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":166,"skipped":2583,"failed":0}
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:04:14.674: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-3511
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9644
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7313
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:04:45.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3511" for this suite.
STEP: Destroying namespace "nsdeletetest-9644" for this suite.
Feb 17 17:04:45.367: INFO: Namespace nsdeletetest-9644 was already deleted
STEP: Destroying namespace "nsdeletetest-7313" for this suite.

• [SLOW TEST:30.708 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":280,"completed":167,"skipped":2586,"failed":0}
S
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:04:45.381: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-385
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:344
Feb 17 17:04:45.579: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 17 17:05:45.618: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 17:05:45.627: INFO: Starting informer...
STEP: Starting pods...
Feb 17 17:05:45.874: INFO: Pod1 is running on 10.195.53.9. Tainting Node
Feb 17 17:05:48.135: INFO: Pod2 is running on 10.195.53.9. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Feb 17 17:06:01.297: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Feb 17 17:06:21.288: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:06:21.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-385" for this suite.

• [SLOW TEST:95.957 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":280,"completed":168,"skipped":2587,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:06:21.338: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7396
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 17 17:06:25.739: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 17 17:06:25.750: INFO: Pod pod-with-poststart-http-hook still exists
Feb 17 17:06:27.750: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 17 17:06:27.762: INFO: Pod pod-with-poststart-http-hook still exists
Feb 17 17:06:29.750: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 17 17:06:29.763: INFO: Pod pod-with-poststart-http-hook still exists
Feb 17 17:06:31.750: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 17 17:06:31.763: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:06:31.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7396" for this suite.

• [SLOW TEST:10.452 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":280,"completed":169,"skipped":2603,"failed":0}
SSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:06:31.791: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5538
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap that has name configmap-test-emptyKey-a4d9aee9-6165-4fe1-8a49-587e86997ab2
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:06:31.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5538" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":280,"completed":170,"skipped":2607,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:06:32.030: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-493
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:06:48.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-493" for this suite.

• [SLOW TEST:16.419 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":280,"completed":171,"skipped":2613,"failed":0}
SS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:06:48.448: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1825
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 17 17:06:50.710: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:06:50.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1825" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":280,"completed":172,"skipped":2615,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:06:50.798: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4633
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 17:06:51.025: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e257eca3-5d71-489a-81a4-da91ab956ded" in namespace "projected-4633" to be "success or failure"
Feb 17 17:06:51.037: INFO: Pod "downwardapi-volume-e257eca3-5d71-489a-81a4-da91ab956ded": Phase="Pending", Reason="", readiness=false. Elapsed: 11.070281ms
Feb 17 17:06:53.049: INFO: Pod "downwardapi-volume-e257eca3-5d71-489a-81a4-da91ab956ded": Phase="Running", Reason="", readiness=true. Elapsed: 2.023707028s
Feb 17 17:06:55.061: INFO: Pod "downwardapi-volume-e257eca3-5d71-489a-81a4-da91ab956ded": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035677109s
STEP: Saw pod success
Feb 17 17:06:55.061: INFO: Pod "downwardapi-volume-e257eca3-5d71-489a-81a4-da91ab956ded" satisfied condition "success or failure"
Feb 17 17:06:55.072: INFO: Trying to get logs from node 10.195.53.9 pod downwardapi-volume-e257eca3-5d71-489a-81a4-da91ab956ded container client-container: <nil>
STEP: delete the pod
Feb 17 17:06:55.134: INFO: Waiting for pod downwardapi-volume-e257eca3-5d71-489a-81a4-da91ab956ded to disappear
Feb 17 17:06:55.148: INFO: Pod downwardapi-volume-e257eca3-5d71-489a-81a4-da91ab956ded no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:06:55.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4633" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":173,"skipped":2637,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:06:55.174: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7779
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-17fe559e-7042-4388-b1b3-e9f224e9d775
STEP: Creating a pod to test consume configMaps
Feb 17 17:06:55.400: INFO: Waiting up to 5m0s for pod "pod-configmaps-a148c185-0891-440d-8bc0-534e982772be" in namespace "configmap-7779" to be "success or failure"
Feb 17 17:06:55.412: INFO: Pod "pod-configmaps-a148c185-0891-440d-8bc0-534e982772be": Phase="Pending", Reason="", readiness=false. Elapsed: 11.577244ms
Feb 17 17:06:57.423: INFO: Pod "pod-configmaps-a148c185-0891-440d-8bc0-534e982772be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022854256s
STEP: Saw pod success
Feb 17 17:06:57.423: INFO: Pod "pod-configmaps-a148c185-0891-440d-8bc0-534e982772be" satisfied condition "success or failure"
Feb 17 17:06:57.433: INFO: Trying to get logs from node 10.195.53.9 pod pod-configmaps-a148c185-0891-440d-8bc0-534e982772be container configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 17:06:57.491: INFO: Waiting for pod pod-configmaps-a148c185-0891-440d-8bc0-534e982772be to disappear
Feb 17 17:06:57.503: INFO: Pod pod-configmaps-a148c185-0891-440d-8bc0-534e982772be no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:06:57.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7779" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":174,"skipped":2639,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:06:57.530: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2357
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2357 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2357;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2357 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2357;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2357.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2357.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2357.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2357.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2357.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2357.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2357.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2357.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2357.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2357.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2357.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2357.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2357.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 120.79.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.79.120_udp@PTR;check="$$(dig +tcp +noall +answer +search 120.79.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.79.120_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2357 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2357;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2357 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2357;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2357.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2357.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2357.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2357.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2357.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2357.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2357.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2357.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2357.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2357.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2357.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2357.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2357.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 120.79.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.79.120_udp@PTR;check="$$(dig +tcp +noall +answer +search 120.79.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.79.120_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 17 17:07:01.841: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:01.856: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:01.870: INFO: Unable to read wheezy_udp@dns-test-service.dns-2357 from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:01.885: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2357 from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:01.900: INFO: Unable to read wheezy_udp@dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:01.915: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:01.931: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:01.945: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:02.059: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:02.074: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:02.088: INFO: Unable to read jessie_udp@dns-test-service.dns-2357 from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:02.103: INFO: Unable to read jessie_tcp@dns-test-service.dns-2357 from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:02.117: INFO: Unable to read jessie_udp@dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:02.145: INFO: Unable to read jessie_tcp@dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:02.165: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:02.180: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:02.285: INFO: Lookups using dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2357 wheezy_tcp@dns-test-service.dns-2357 wheezy_udp@dns-test-service.dns-2357.svc wheezy_tcp@dns-test-service.dns-2357.svc wheezy_udp@_http._tcp.dns-test-service.dns-2357.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2357.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2357 jessie_tcp@dns-test-service.dns-2357 jessie_udp@dns-test-service.dns-2357.svc jessie_tcp@dns-test-service.dns-2357.svc jessie_udp@_http._tcp.dns-test-service.dns-2357.svc jessie_tcp@_http._tcp.dns-test-service.dns-2357.svc]

Feb 17 17:07:07.304: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:07.318: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:07.331: INFO: Unable to read wheezy_udp@dns-test-service.dns-2357 from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:07.346: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2357 from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:07.361: INFO: Unable to read wheezy_udp@dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:07.375: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:07.391: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:07.406: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:07.524: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:07.545: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:07.558: INFO: Unable to read jessie_udp@dns-test-service.dns-2357 from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:07.574: INFO: Unable to read jessie_tcp@dns-test-service.dns-2357 from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:07.589: INFO: Unable to read jessie_udp@dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:07.604: INFO: Unable to read jessie_tcp@dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:07.619: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:07.633: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:07.737: INFO: Lookups using dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2357 wheezy_tcp@dns-test-service.dns-2357 wheezy_udp@dns-test-service.dns-2357.svc wheezy_tcp@dns-test-service.dns-2357.svc wheezy_udp@_http._tcp.dns-test-service.dns-2357.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2357.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2357 jessie_tcp@dns-test-service.dns-2357 jessie_udp@dns-test-service.dns-2357.svc jessie_tcp@dns-test-service.dns-2357.svc jessie_udp@_http._tcp.dns-test-service.dns-2357.svc jessie_tcp@_http._tcp.dns-test-service.dns-2357.svc]

Feb 17 17:07:12.303: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:12.316: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:12.332: INFO: Unable to read wheezy_udp@dns-test-service.dns-2357 from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:12.347: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2357 from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:12.361: INFO: Unable to read wheezy_udp@dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:12.376: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:12.395: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:12.410: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:12.523: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:12.537: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:12.553: INFO: Unable to read jessie_udp@dns-test-service.dns-2357 from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:12.567: INFO: Unable to read jessie_tcp@dns-test-service.dns-2357 from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:12.581: INFO: Unable to read jessie_udp@dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:12.595: INFO: Unable to read jessie_tcp@dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:12.610: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:12.625: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:12.717: INFO: Lookups using dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2357 wheezy_tcp@dns-test-service.dns-2357 wheezy_udp@dns-test-service.dns-2357.svc wheezy_tcp@dns-test-service.dns-2357.svc wheezy_udp@_http._tcp.dns-test-service.dns-2357.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2357.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2357 jessie_tcp@dns-test-service.dns-2357 jessie_udp@dns-test-service.dns-2357.svc jessie_tcp@dns-test-service.dns-2357.svc jessie_udp@_http._tcp.dns-test-service.dns-2357.svc jessie_tcp@_http._tcp.dns-test-service.dns-2357.svc]

Feb 17 17:07:17.305: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:17.323: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:17.337: INFO: Unable to read wheezy_udp@dns-test-service.dns-2357 from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:17.352: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2357 from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:17.367: INFO: Unable to read wheezy_udp@dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:17.388: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:17.402: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:17.424: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:17.535: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:17.549: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:17.563: INFO: Unable to read jessie_udp@dns-test-service.dns-2357 from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:17.578: INFO: Unable to read jessie_tcp@dns-test-service.dns-2357 from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:17.615: INFO: Unable to read jessie_udp@dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:17.629: INFO: Unable to read jessie_tcp@dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:17.643: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:17.659: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:17.758: INFO: Lookups using dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2357 wheezy_tcp@dns-test-service.dns-2357 wheezy_udp@dns-test-service.dns-2357.svc wheezy_tcp@dns-test-service.dns-2357.svc wheezy_udp@_http._tcp.dns-test-service.dns-2357.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2357.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2357 jessie_tcp@dns-test-service.dns-2357 jessie_udp@dns-test-service.dns-2357.svc jessie_tcp@dns-test-service.dns-2357.svc jessie_udp@_http._tcp.dns-test-service.dns-2357.svc jessie_tcp@_http._tcp.dns-test-service.dns-2357.svc]

Feb 17 17:07:22.300: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:22.319: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:22.334: INFO: Unable to read wheezy_udp@dns-test-service.dns-2357 from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:22.349: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2357 from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:22.364: INFO: Unable to read wheezy_udp@dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:22.381: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:22.397: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:22.413: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:22.525: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:22.540: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:22.555: INFO: Unable to read jessie_udp@dns-test-service.dns-2357 from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:22.570: INFO: Unable to read jessie_tcp@dns-test-service.dns-2357 from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:22.594: INFO: Unable to read jessie_udp@dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:22.609: INFO: Unable to read jessie_tcp@dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:22.623: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:22.637: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:22.727: INFO: Lookups using dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2357 wheezy_tcp@dns-test-service.dns-2357 wheezy_udp@dns-test-service.dns-2357.svc wheezy_tcp@dns-test-service.dns-2357.svc wheezy_udp@_http._tcp.dns-test-service.dns-2357.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2357.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2357 jessie_tcp@dns-test-service.dns-2357 jessie_udp@dns-test-service.dns-2357.svc jessie_tcp@dns-test-service.dns-2357.svc jessie_udp@_http._tcp.dns-test-service.dns-2357.svc jessie_tcp@_http._tcp.dns-test-service.dns-2357.svc]

Feb 17 17:07:27.300: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:27.324: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:27.345: INFO: Unable to read wheezy_udp@dns-test-service.dns-2357 from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:27.359: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2357 from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:27.392: INFO: Unable to read wheezy_udp@dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:27.407: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:27.434: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:27.449: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:27.570: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:27.585: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:27.599: INFO: Unable to read jessie_udp@dns-test-service.dns-2357 from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:27.614: INFO: Unable to read jessie_tcp@dns-test-service.dns-2357 from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:27.628: INFO: Unable to read jessie_udp@dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:27.655: INFO: Unable to read jessie_tcp@dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:27.668: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:27.687: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2357.svc from pod dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775: the server could not find the requested resource (get pods dns-test-896ea026-878f-493c-8344-3d07656b1775)
Feb 17 17:07:27.777: INFO: Lookups using dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2357 wheezy_tcp@dns-test-service.dns-2357 wheezy_udp@dns-test-service.dns-2357.svc wheezy_tcp@dns-test-service.dns-2357.svc wheezy_udp@_http._tcp.dns-test-service.dns-2357.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2357.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2357 jessie_tcp@dns-test-service.dns-2357 jessie_udp@dns-test-service.dns-2357.svc jessie_tcp@dns-test-service.dns-2357.svc jessie_udp@_http._tcp.dns-test-service.dns-2357.svc jessie_tcp@_http._tcp.dns-test-service.dns-2357.svc]

Feb 17 17:07:32.729: INFO: DNS probes using dns-2357/dns-test-896ea026-878f-493c-8344-3d07656b1775 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:07:32.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2357" for this suite.

• [SLOW TEST:35.342 seconds]
[sig-network] DNS
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":280,"completed":175,"skipped":2651,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:07:32.874: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-481
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-dcb8b795-d816-4260-8d6d-0066d93f1be7 in namespace container-probe-481
Feb 17 17:07:35.123: INFO: Started pod busybox-dcb8b795-d816-4260-8d6d-0066d93f1be7 in namespace container-probe-481
STEP: checking the pod's current state and verifying that restartCount is present
Feb 17 17:07:35.135: INFO: Initial restart count of pod busybox-dcb8b795-d816-4260-8d6d-0066d93f1be7 is 0
Feb 17 17:08:29.540: INFO: Restart count of pod container-probe-481/busybox-dcb8b795-d816-4260-8d6d-0066d93f1be7 is now 1 (54.405058208s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:08:29.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-481" for this suite.

• [SLOW TEST:56.729 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":176,"skipped":2678,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:08:29.603: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8518
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-31690f5c-7ac7-4d88-b86f-e56afb55afe0
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-31690f5c-7ac7-4d88-b86f-e56afb55afe0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:08:33.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8518" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":177,"skipped":2685,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:08:34.008: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6553
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-55d07b92-1ccd-426e-9a31-464c54ba1e5f
STEP: Creating a pod to test consume configMaps
Feb 17 17:08:34.234: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-684803a3-0630-4b01-bc62-542bb56bfe90" in namespace "projected-6553" to be "success or failure"
Feb 17 17:08:34.247: INFO: Pod "pod-projected-configmaps-684803a3-0630-4b01-bc62-542bb56bfe90": Phase="Pending", Reason="", readiness=false. Elapsed: 13.581114ms
Feb 17 17:08:36.261: INFO: Pod "pod-projected-configmaps-684803a3-0630-4b01-bc62-542bb56bfe90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027481604s
Feb 17 17:08:38.272: INFO: Pod "pod-projected-configmaps-684803a3-0630-4b01-bc62-542bb56bfe90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038517753s
STEP: Saw pod success
Feb 17 17:08:38.272: INFO: Pod "pod-projected-configmaps-684803a3-0630-4b01-bc62-542bb56bfe90" satisfied condition "success or failure"
Feb 17 17:08:38.287: INFO: Trying to get logs from node 10.195.53.9 pod pod-projected-configmaps-684803a3-0630-4b01-bc62-542bb56bfe90 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 17:08:38.344: INFO: Waiting for pod pod-projected-configmaps-684803a3-0630-4b01-bc62-542bb56bfe90 to disappear
Feb 17 17:08:38.354: INFO: Pod pod-projected-configmaps-684803a3-0630-4b01-bc62-542bb56bfe90 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:08:38.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6553" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":178,"skipped":2726,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:08:38.386: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6389
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 17 17:08:38.599: INFO: Waiting up to 5m0s for pod "pod-a4804b1b-89d7-426d-b5f7-357c9a0fff87" in namespace "emptydir-6389" to be "success or failure"
Feb 17 17:08:38.611: INFO: Pod "pod-a4804b1b-89d7-426d-b5f7-357c9a0fff87": Phase="Pending", Reason="", readiness=false. Elapsed: 11.717674ms
Feb 17 17:08:40.624: INFO: Pod "pod-a4804b1b-89d7-426d-b5f7-357c9a0fff87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024716778s
STEP: Saw pod success
Feb 17 17:08:40.624: INFO: Pod "pod-a4804b1b-89d7-426d-b5f7-357c9a0fff87" satisfied condition "success or failure"
Feb 17 17:08:40.635: INFO: Trying to get logs from node 10.195.53.9 pod pod-a4804b1b-89d7-426d-b5f7-357c9a0fff87 container test-container: <nil>
STEP: delete the pod
Feb 17 17:08:40.691: INFO: Waiting for pod pod-a4804b1b-89d7-426d-b5f7-357c9a0fff87 to disappear
Feb 17 17:08:40.701: INFO: Pod pod-a4804b1b-89d7-426d-b5f7-357c9a0fff87 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:08:40.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6389" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":179,"skipped":2731,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:08:40.730: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4855
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 17 17:08:40.946: INFO: Waiting up to 5m0s for pod "pod-24e2f91f-94e0-40cd-a6eb-fac4aef80396" in namespace "emptydir-4855" to be "success or failure"
Feb 17 17:08:40.956: INFO: Pod "pod-24e2f91f-94e0-40cd-a6eb-fac4aef80396": Phase="Pending", Reason="", readiness=false. Elapsed: 10.223351ms
Feb 17 17:08:42.969: INFO: Pod "pod-24e2f91f-94e0-40cd-a6eb-fac4aef80396": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022770272s
STEP: Saw pod success
Feb 17 17:08:42.969: INFO: Pod "pod-24e2f91f-94e0-40cd-a6eb-fac4aef80396" satisfied condition "success or failure"
Feb 17 17:08:42.980: INFO: Trying to get logs from node 10.195.53.9 pod pod-24e2f91f-94e0-40cd-a6eb-fac4aef80396 container test-container: <nil>
STEP: delete the pod
Feb 17 17:08:43.037: INFO: Waiting for pod pod-24e2f91f-94e0-40cd-a6eb-fac4aef80396 to disappear
Feb 17 17:08:43.047: INFO: Pod pod-24e2f91f-94e0-40cd-a6eb-fac4aef80396 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:08:43.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4855" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":180,"skipped":2769,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:08:43.076: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9280
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-afb0d232-f0c2-46a2-9d8d-907dbc1eda06
STEP: Creating a pod to test consume configMaps
Feb 17 17:08:43.297: INFO: Waiting up to 5m0s for pod "pod-configmaps-d1db2cb0-bf47-4853-b98c-9d5965f590f7" in namespace "configmap-9280" to be "success or failure"
Feb 17 17:08:43.308: INFO: Pod "pod-configmaps-d1db2cb0-bf47-4853-b98c-9d5965f590f7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.851926ms
Feb 17 17:08:45.322: INFO: Pod "pod-configmaps-d1db2cb0-bf47-4853-b98c-9d5965f590f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024806381s
Feb 17 17:08:47.333: INFO: Pod "pod-configmaps-d1db2cb0-bf47-4853-b98c-9d5965f590f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036293506s
STEP: Saw pod success
Feb 17 17:08:47.333: INFO: Pod "pod-configmaps-d1db2cb0-bf47-4853-b98c-9d5965f590f7" satisfied condition "success or failure"
Feb 17 17:08:47.343: INFO: Trying to get logs from node 10.195.53.9 pod pod-configmaps-d1db2cb0-bf47-4853-b98c-9d5965f590f7 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 17:08:47.401: INFO: Waiting for pod pod-configmaps-d1db2cb0-bf47-4853-b98c-9d5965f590f7 to disappear
Feb 17 17:08:47.412: INFO: Pod pod-configmaps-d1db2cb0-bf47-4853-b98c-9d5965f590f7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:08:47.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9280" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":181,"skipped":2797,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:08:47.439: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1665
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1665.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1665.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1665.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1665.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1665.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1665.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1665.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1665.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1665.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1665.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1665.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1665.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1665.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 85.2.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.2.85_udp@PTR;check="$$(dig +tcp +noall +answer +search 85.2.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.2.85_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1665.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1665.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1665.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1665.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1665.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1665.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1665.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1665.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1665.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1665.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1665.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1665.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1665.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 85.2.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.2.85_udp@PTR;check="$$(dig +tcp +noall +answer +search 85.2.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.2.85_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 17 17:08:49.740: INFO: Unable to read wheezy_udp@dns-test-service.dns-1665.svc.cluster.local from pod dns-1665/dns-test-88526013-3548-4f85-8700-455a558e7c54: the server could not find the requested resource (get pods dns-test-88526013-3548-4f85-8700-455a558e7c54)
Feb 17 17:08:49.755: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1665.svc.cluster.local from pod dns-1665/dns-test-88526013-3548-4f85-8700-455a558e7c54: the server could not find the requested resource (get pods dns-test-88526013-3548-4f85-8700-455a558e7c54)
Feb 17 17:08:49.776: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1665.svc.cluster.local from pod dns-1665/dns-test-88526013-3548-4f85-8700-455a558e7c54: the server could not find the requested resource (get pods dns-test-88526013-3548-4f85-8700-455a558e7c54)
Feb 17 17:08:49.791: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1665.svc.cluster.local from pod dns-1665/dns-test-88526013-3548-4f85-8700-455a558e7c54: the server could not find the requested resource (get pods dns-test-88526013-3548-4f85-8700-455a558e7c54)
Feb 17 17:08:49.891: INFO: Unable to read jessie_udp@dns-test-service.dns-1665.svc.cluster.local from pod dns-1665/dns-test-88526013-3548-4f85-8700-455a558e7c54: the server could not find the requested resource (get pods dns-test-88526013-3548-4f85-8700-455a558e7c54)
Feb 17 17:08:49.905: INFO: Unable to read jessie_tcp@dns-test-service.dns-1665.svc.cluster.local from pod dns-1665/dns-test-88526013-3548-4f85-8700-455a558e7c54: the server could not find the requested resource (get pods dns-test-88526013-3548-4f85-8700-455a558e7c54)
Feb 17 17:08:49.920: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1665.svc.cluster.local from pod dns-1665/dns-test-88526013-3548-4f85-8700-455a558e7c54: the server could not find the requested resource (get pods dns-test-88526013-3548-4f85-8700-455a558e7c54)
Feb 17 17:08:49.935: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1665.svc.cluster.local from pod dns-1665/dns-test-88526013-3548-4f85-8700-455a558e7c54: the server could not find the requested resource (get pods dns-test-88526013-3548-4f85-8700-455a558e7c54)
Feb 17 17:08:50.037: INFO: Lookups using dns-1665/dns-test-88526013-3548-4f85-8700-455a558e7c54 failed for: [wheezy_udp@dns-test-service.dns-1665.svc.cluster.local wheezy_tcp@dns-test-service.dns-1665.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1665.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1665.svc.cluster.local jessie_udp@dns-test-service.dns-1665.svc.cluster.local jessie_tcp@dns-test-service.dns-1665.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1665.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1665.svc.cluster.local]

Feb 17 17:08:55.237: INFO: Unable to read jessie_tcp@dns-test-service.dns-1665.svc.cluster.local from pod dns-1665/dns-test-88526013-3548-4f85-8700-455a558e7c54: the server could not find the requested resource (get pods dns-test-88526013-3548-4f85-8700-455a558e7c54)
Feb 17 17:08:55.350: INFO: Lookups using dns-1665/dns-test-88526013-3548-4f85-8700-455a558e7c54 failed for: [jessie_tcp@dns-test-service.dns-1665.svc.cluster.local]

Feb 17 17:09:00.321: INFO: DNS probes using dns-1665/dns-test-88526013-3548-4f85-8700-455a558e7c54 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:09:00.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1665" for this suite.

• [SLOW TEST:13.017 seconds]
[sig-network] DNS
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":280,"completed":182,"skipped":2816,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:09:00.459: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-4197
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test hostPath mode
Feb 17 17:09:00.773: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-4197" to be "success or failure"
Feb 17 17:09:00.786: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 12.315012ms
Feb 17 17:09:02.800: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026038528s
STEP: Saw pod success
Feb 17 17:09:02.800: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Feb 17 17:09:02.811: INFO: Trying to get logs from node 10.195.53.9 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Feb 17 17:09:02.875: INFO: Waiting for pod pod-host-path-test to disappear
Feb 17 17:09:02.885: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:09:02.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-4197" for this suite.
•{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":183,"skipped":2826,"failed":0}

------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:09:02.912: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7895
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:09:03.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7895" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":280,"completed":184,"skipped":2826,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:09:03.140: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9767
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 17:09:03.355: INFO: Waiting up to 5m0s for pod "downwardapi-volume-11796865-4b9e-4d1e-ab05-748a20a60edd" in namespace "downward-api-9767" to be "success or failure"
Feb 17 17:09:03.367: INFO: Pod "downwardapi-volume-11796865-4b9e-4d1e-ab05-748a20a60edd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.59225ms
Feb 17 17:09:05.380: INFO: Pod "downwardapi-volume-11796865-4b9e-4d1e-ab05-748a20a60edd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024691981s
Feb 17 17:09:07.392: INFO: Pod "downwardapi-volume-11796865-4b9e-4d1e-ab05-748a20a60edd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037115031s
STEP: Saw pod success
Feb 17 17:09:07.392: INFO: Pod "downwardapi-volume-11796865-4b9e-4d1e-ab05-748a20a60edd" satisfied condition "success or failure"
Feb 17 17:09:07.404: INFO: Trying to get logs from node 10.195.53.9 pod downwardapi-volume-11796865-4b9e-4d1e-ab05-748a20a60edd container client-container: <nil>
STEP: delete the pod
Feb 17 17:09:07.461: INFO: Waiting for pod downwardapi-volume-11796865-4b9e-4d1e-ab05-748a20a60edd to disappear
Feb 17 17:09:07.471: INFO: Pod downwardapi-volume-11796865-4b9e-4d1e-ab05-748a20a60edd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:09:07.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9767" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":185,"skipped":2886,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:09:07.505: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2947
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Feb 17 17:09:07.707: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 17 17:09:07.736: INFO: Waiting for terminating namespaces to be deleted...
Feb 17 17:09:07.745: INFO: 
Logging pods the kubelet thinks is on node 10.195.53.14 before test
Feb 17 17:09:07.806: INFO: coredns-5b567488dd-kckdk from kube-system started at 2020-02-17 14:59:13 +0000 UTC (1 container statuses recorded)
Feb 17 17:09:07.806: INFO: 	Container coredns ready: true, restart count 0
Feb 17 17:09:07.806: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-02-17 14:39:39 +0000 UTC (1 container statuses recorded)
Feb 17 17:09:07.806: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Feb 17 17:09:07.806: INFO: catalog-operator-7d9cb6cf74-qr9nb from ibm-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 17:09:07.806: INFO: 	Container catalog-operator ready: true, restart count 0
Feb 17 17:09:07.806: INFO: coredns-autoscaler-7dddb6f87c-g6nrf from kube-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 17:09:07.806: INFO: 	Container autoscaler ready: true, restart count 0
Feb 17 17:09:07.806: INFO: metrics-server-647cf95c9b-5j4cj from kube-system started at 2020-02-17 17:05:48 +0000 UTC (2 container statuses recorded)
Feb 17 17:09:07.806: INFO: 	Container metrics-server ready: true, restart count 0
Feb 17 17:09:07.806: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Feb 17 17:09:07.806: INFO: ibm-master-proxy-static-10.195.53.14 from kube-system started at 2020-02-17 14:36:26 +0000 UTC (2 container statuses recorded)
Feb 17 17:09:07.806: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 17:09:07.806: INFO: 	Container pause ready: true, restart count 0
Feb 17 17:09:07.806: INFO: ibm-keepalived-watcher-8pbd8 from kube-system started at 2020-02-17 14:36:27 +0000 UTC (1 container statuses recorded)
Feb 17 17:09:07.806: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 17:09:07.806: INFO: kubernetes-dashboard-bbcc67fc-x8mnz from kube-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 17:09:07.806: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 17 17:09:07.807: INFO: calico-node-94c2k from kube-system started at 2020-02-17 14:36:27 +0000 UTC (1 container statuses recorded)
Feb 17 17:09:07.807: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 17:09:07.807: INFO: calico-kube-controllers-866cf6f69c-fm97n from kube-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 17:09:07.807: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 17 17:09:07.807: INFO: vpn-b5cd9dc8b-fp9qk from kube-system started at 2020-02-17 17:05:48 +0000 UTC (1 container statuses recorded)
Feb 17 17:09:07.807: INFO: 	Container vpn ready: true, restart count 0
Feb 17 17:09:07.807: INFO: ibm-cloud-provider-ip-135-90-78-211-7555ccd494-hhqb2 from ibm-system started at 2020-02-17 14:41:58 +0000 UTC (1 container statuses recorded)
Feb 17 17:09:07.807: INFO: 	Container ibm-cloud-provider-ip-135-90-78-211 ready: true, restart count 0
Feb 17 17:09:07.807: INFO: ibm-file-plugin-6694f985b8-s5cjp from kube-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 17:09:07.807: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Feb 17 17:09:07.807: INFO: ibm-storage-watcher-74f895486d-k5zxb from kube-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 17:09:07.807: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Feb 17 17:09:07.807: INFO: public-crbp5a3ais0mfj6cn3dl60-alb1-79849b9bd6-4z2xg from kube-system started at 2020-02-17 14:44:46 +0000 UTC (4 container statuses recorded)
Feb 17 17:09:07.807: INFO: 	Container ingress-auth-1 ready: true, restart count 1
Feb 17 17:09:07.807: INFO: 	Container ingress-auth-2 ready: true, restart count 1
Feb 17 17:09:07.807: INFO: 	Container ingress-auth-3 ready: true, restart count 1
Feb 17 17:09:07.807: INFO: 	Container nginx-ingress ready: true, restart count 0
Feb 17 17:09:07.807: INFO: sonobuoy-systemd-logs-daemon-set-bc697ed487d7421d-st8jd from sonobuoy started at 2020-02-17 16:14:42 +0000 UTC (2 container statuses recorded)
Feb 17 17:09:07.807: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 17:09:07.807: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 17:09:07.807: INFO: 
Logging pods the kubelet thinks is on node 10.195.53.47 before test
Feb 17 17:09:07.865: INFO: sonobuoy-systemd-logs-daemon-set-bc697ed487d7421d-5b85k from sonobuoy started at 2020-02-17 16:14:42 +0000 UTC (2 container statuses recorded)
Feb 17 17:09:07.865: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 17:09:07.865: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 17:09:07.865: INFO: public-crbp5a3ais0mfj6cn3dl60-alb1-79849b9bd6-8r8qk from kube-system started at 2020-02-17 14:44:46 +0000 UTC (4 container statuses recorded)
Feb 17 17:09:07.865: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Feb 17 17:09:07.865: INFO: 	Container ingress-auth-2 ready: true, restart count 1
Feb 17 17:09:07.865: INFO: 	Container ingress-auth-3 ready: true, restart count 1
Feb 17 17:09:07.865: INFO: 	Container nginx-ingress ready: true, restart count 0
Feb 17 17:09:07.865: INFO: coredns-5b567488dd-n2wt7 from kube-system started at 2020-02-17 17:05:48 +0000 UTC (1 container statuses recorded)
Feb 17 17:09:07.865: INFO: 	Container coredns ready: true, restart count 0
Feb 17 17:09:07.865: INFO: coredns-5b567488dd-qtx5q from kube-system started at 2020-02-17 14:59:13 +0000 UTC (1 container statuses recorded)
Feb 17 17:09:07.865: INFO: 	Container coredns ready: true, restart count 0
Feb 17 17:09:07.865: INFO: ibm-master-proxy-static-10.195.53.47 from kube-system started at 2020-02-17 14:36:20 +0000 UTC (2 container statuses recorded)
Feb 17 17:09:07.865: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 17:09:07.865: INFO: 	Container pause ready: true, restart count 0
Feb 17 17:09:07.865: INFO: ibm-keepalived-watcher-bl4pr from kube-system started at 2020-02-17 14:36:22 +0000 UTC (1 container statuses recorded)
Feb 17 17:09:07.865: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 17:09:07.865: INFO: olm-operator-587889d75d-29ltv from ibm-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 17:09:07.865: INFO: 	Container olm-operator ready: true, restart count 0
Feb 17 17:09:07.865: INFO: dashboard-metrics-scraper-69468c6b44-gqtkk from kube-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 17:09:07.865: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 17 17:09:07.865: INFO: calico-node-kmwlv from kube-system started at 2020-02-17 14:36:22 +0000 UTC (1 container statuses recorded)
Feb 17 17:09:07.865: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 17:09:07.865: INFO: addon-catalog-source-c5sks from ibm-system started at 2020-02-17 14:38:40 +0000 UTC (1 container statuses recorded)
Feb 17 17:09:07.865: INFO: 	Container configmap-registry-server ready: true, restart count 0
Feb 17 17:09:07.865: INFO: ibm-cloud-provider-ip-135-90-78-211-7555ccd494-fz6gg from ibm-system started at 2020-02-17 14:41:58 +0000 UTC (1 container statuses recorded)
Feb 17 17:09:07.865: INFO: 	Container ibm-cloud-provider-ip-135-90-78-211 ready: true, restart count 0
Feb 17 17:09:07.865: INFO: sonobuoy-e2e-job-c9def2901e004587 from sonobuoy started at 2020-02-17 16:14:42 +0000 UTC (2 container statuses recorded)
Feb 17 17:09:07.865: INFO: 	Container e2e ready: true, restart count 0
Feb 17 17:09:07.865: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 17:09:07.865: INFO: 
Logging pods the kubelet thinks is on node 10.195.53.9 before test
Feb 17 17:09:07.885: INFO: calico-node-r8w4v from kube-system started at 2020-02-17 14:36:12 +0000 UTC (1 container statuses recorded)
Feb 17 17:09:07.885: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 17:09:07.885: INFO: ibm-keepalived-watcher-7c6j2 from kube-system started at 2020-02-17 14:36:12 +0000 UTC (1 container statuses recorded)
Feb 17 17:09:07.885: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 17:09:07.885: INFO: sonobuoy-systemd-logs-daemon-set-bc697ed487d7421d-ntb66 from sonobuoy started at 2020-02-17 16:14:42 +0000 UTC (2 container statuses recorded)
Feb 17 17:09:07.885: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 17:09:07.885: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 17:09:07.885: INFO: sonobuoy from sonobuoy started at 2020-02-17 16:14:33 +0000 UTC (1 container statuses recorded)
Feb 17 17:09:07.885: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 17 17:09:07.885: INFO: ibm-master-proxy-static-10.195.53.9 from kube-system started at 2020-02-17 14:36:11 +0000 UTC (2 container statuses recorded)
Feb 17 17:09:07.885: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 17:09:07.885: INFO: 	Container pause ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: verifying the node has the label node 10.195.53.14
STEP: verifying the node has the label node 10.195.53.47
STEP: verifying the node has the label node 10.195.53.9
Feb 17 17:09:07.995: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.195.53.14
Feb 17 17:09:07.995: INFO: Pod addon-catalog-source-c5sks requesting resource cpu=10m on Node 10.195.53.47
Feb 17 17:09:07.995: INFO: Pod catalog-operator-7d9cb6cf74-qr9nb requesting resource cpu=10m on Node 10.195.53.14
Feb 17 17:09:07.995: INFO: Pod ibm-cloud-provider-ip-135-90-78-211-7555ccd494-fz6gg requesting resource cpu=5m on Node 10.195.53.47
Feb 17 17:09:07.995: INFO: Pod ibm-cloud-provider-ip-135-90-78-211-7555ccd494-hhqb2 requesting resource cpu=5m on Node 10.195.53.14
Feb 17 17:09:07.995: INFO: Pod olm-operator-587889d75d-29ltv requesting resource cpu=10m on Node 10.195.53.47
Feb 17 17:09:07.995: INFO: Pod calico-kube-controllers-866cf6f69c-fm97n requesting resource cpu=10m on Node 10.195.53.14
Feb 17 17:09:07.995: INFO: Pod calico-node-94c2k requesting resource cpu=250m on Node 10.195.53.14
Feb 17 17:09:07.995: INFO: Pod calico-node-kmwlv requesting resource cpu=250m on Node 10.195.53.47
Feb 17 17:09:07.995: INFO: Pod calico-node-r8w4v requesting resource cpu=250m on Node 10.195.53.9
Feb 17 17:09:07.995: INFO: Pod coredns-5b567488dd-kckdk requesting resource cpu=100m on Node 10.195.53.14
Feb 17 17:09:07.995: INFO: Pod coredns-5b567488dd-n2wt7 requesting resource cpu=100m on Node 10.195.53.47
Feb 17 17:09:07.995: INFO: Pod coredns-5b567488dd-qtx5q requesting resource cpu=100m on Node 10.195.53.47
Feb 17 17:09:07.995: INFO: Pod coredns-autoscaler-7dddb6f87c-g6nrf requesting resource cpu=20m on Node 10.195.53.14
Feb 17 17:09:07.995: INFO: Pod dashboard-metrics-scraper-69468c6b44-gqtkk requesting resource cpu=1m on Node 10.195.53.47
Feb 17 17:09:07.995: INFO: Pod ibm-file-plugin-6694f985b8-s5cjp requesting resource cpu=50m on Node 10.195.53.14
Feb 17 17:09:07.995: INFO: Pod ibm-keepalived-watcher-7c6j2 requesting resource cpu=5m on Node 10.195.53.9
Feb 17 17:09:07.995: INFO: Pod ibm-keepalived-watcher-8pbd8 requesting resource cpu=5m on Node 10.195.53.14
Feb 17 17:09:07.995: INFO: Pod ibm-keepalived-watcher-bl4pr requesting resource cpu=5m on Node 10.195.53.47
Feb 17 17:09:07.995: INFO: Pod ibm-master-proxy-static-10.195.53.14 requesting resource cpu=25m on Node 10.195.53.14
Feb 17 17:09:07.995: INFO: Pod ibm-master-proxy-static-10.195.53.47 requesting resource cpu=25m on Node 10.195.53.47
Feb 17 17:09:07.995: INFO: Pod ibm-master-proxy-static-10.195.53.9 requesting resource cpu=25m on Node 10.195.53.9
Feb 17 17:09:07.995: INFO: Pod ibm-storage-watcher-74f895486d-k5zxb requesting resource cpu=50m on Node 10.195.53.14
Feb 17 17:09:07.995: INFO: Pod kubernetes-dashboard-bbcc67fc-x8mnz requesting resource cpu=50m on Node 10.195.53.14
Feb 17 17:09:07.995: INFO: Pod metrics-server-647cf95c9b-5j4cj requesting resource cpu=113m on Node 10.195.53.14
Feb 17 17:09:07.995: INFO: Pod public-crbp5a3ais0mfj6cn3dl60-alb1-79849b9bd6-4z2xg requesting resource cpu=10m on Node 10.195.53.14
Feb 17 17:09:07.995: INFO: Pod public-crbp5a3ais0mfj6cn3dl60-alb1-79849b9bd6-8r8qk requesting resource cpu=10m on Node 10.195.53.47
Feb 17 17:09:07.995: INFO: Pod vpn-b5cd9dc8b-fp9qk requesting resource cpu=5m on Node 10.195.53.14
Feb 17 17:09:07.995: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.195.53.9
Feb 17 17:09:07.995: INFO: Pod sonobuoy-e2e-job-c9def2901e004587 requesting resource cpu=0m on Node 10.195.53.47
Feb 17 17:09:07.995: INFO: Pod sonobuoy-systemd-logs-daemon-set-bc697ed487d7421d-5b85k requesting resource cpu=0m on Node 10.195.53.47
Feb 17 17:09:07.995: INFO: Pod sonobuoy-systemd-logs-daemon-set-bc697ed487d7421d-ntb66 requesting resource cpu=0m on Node 10.195.53.9
Feb 17 17:09:07.995: INFO: Pod sonobuoy-systemd-logs-daemon-set-bc697ed487d7421d-st8jd requesting resource cpu=0m on Node 10.195.53.14
STEP: Starting Pods to consume most of the cluster CPU.
Feb 17 17:09:07.995: INFO: Creating a pod which consumes cpu=2244m on Node 10.195.53.14
Feb 17 17:09:08.013: INFO: Creating a pod which consumes cpu=2375m on Node 10.195.53.47
Feb 17 17:09:08.033: INFO: Creating a pod which consumes cpu=2541m on Node 10.195.53.9
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-09665001-6f96-4764-b912-9ab332a6b5c5.15f43fa53c10881a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2947/filler-pod-09665001-6f96-4764-b912-9ab332a6b5c5 to 10.195.53.14]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-09665001-6f96-4764-b912-9ab332a6b5c5.15f43fa5782d84f2], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-09665001-6f96-4764-b912-9ab332a6b5c5.15f43fa57c13ca75], Reason = [Created], Message = [Created container filler-pod-09665001-6f96-4764-b912-9ab332a6b5c5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-09665001-6f96-4764-b912-9ab332a6b5c5.15f43fa58509902b], Reason = [Started], Message = [Started container filler-pod-09665001-6f96-4764-b912-9ab332a6b5c5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3af8b988-bb2f-458a-8ccd-740b164253d3.15f43fa53e252296], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2947/filler-pod-3af8b988-bb2f-458a-8ccd-740b164253d3 to 10.195.53.9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3af8b988-bb2f-458a-8ccd-740b164253d3.15f43fa578a5e12a], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3af8b988-bb2f-458a-8ccd-740b164253d3.15f43fa57bbff1c5], Reason = [Created], Message = [Created container filler-pod-3af8b988-bb2f-458a-8ccd-740b164253d3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3af8b988-bb2f-458a-8ccd-740b164253d3.15f43fa5852e3789], Reason = [Started], Message = [Started container filler-pod-3af8b988-bb2f-458a-8ccd-740b164253d3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-89b80786-83c4-4c4f-9c14-c68d2d7d545b.15f43fa53d408cee], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2947/filler-pod-89b80786-83c4-4c4f-9c14-c68d2d7d545b to 10.195.53.47]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-89b80786-83c4-4c4f-9c14-c68d2d7d545b.15f43fa578d99583], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-89b80786-83c4-4c4f-9c14-c68d2d7d545b.15f43fa57b5d18de], Reason = [Created], Message = [Created container filler-pod-89b80786-83c4-4c4f-9c14-c68d2d7d545b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-89b80786-83c4-4c4f-9c14-c68d2d7d545b.15f43fa585023358], Reason = [Started], Message = [Started container filler-pod-89b80786-83c4-4c4f-9c14-c68d2d7d545b]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15f43fa5b8b81f1f], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.195.53.14
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.195.53.47
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.195.53.9
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:09:11.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2947" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":280,"completed":186,"skipped":2920,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:09:11.249: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9482
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0217 17:09:51.541740      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 17 17:09:51.541: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:09:51.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9482" for this suite.

• [SLOW TEST:40.318 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":280,"completed":187,"skipped":2982,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:09:51.567: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6640
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 17:09:51.767: INFO: Creating deployment "test-recreate-deployment"
Feb 17 17:09:51.779: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 17 17:09:51.802: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Feb 17 17:09:53.825: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 17 17:09:53.835: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 17 17:09:53.857: INFO: Updating deployment test-recreate-deployment
Feb 17 17:09:53.857: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Feb 17 17:09:53.942: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-6640 /apis/apps/v1/namespaces/deployment-6640/deployments/test-recreate-deployment 4776a165-37b4-4768-9936-496b9dab1006 50007 2 2020-02-17 17:09:51 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004b82998 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-02-17 17:09:53 +0000 UTC,LastTransitionTime:2020-02-17 17:09:53 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-02-17 17:09:53 +0000 UTC,LastTransitionTime:2020-02-17 17:09:51 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Feb 17 17:09:53.952: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-6640 /apis/apps/v1/namespaces/deployment-6640/replicasets/test-recreate-deployment-5f94c574ff 1ef25a0c-3304-4b94-aae0-b13d8909bdd0 50006 1 2020-02-17 17:09:53 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 4776a165-37b4-4768-9936-496b9dab1006 0xc004b82d87 0xc004b82d88}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004b82de8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 17 17:09:53.953: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 17 17:09:53.953: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-799c574856  deployment-6640 /apis/apps/v1/namespaces/deployment-6640/replicasets/test-recreate-deployment-799c574856 7a6a890b-4d88-4817-8734-d705cb130882 49997 2 2020-02-17 17:09:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 4776a165-37b4-4768-9936-496b9dab1006 0xc004b82e57 0xc004b82e58}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 799c574856,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004b82ec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 17 17:09:53.964: INFO: Pod "test-recreate-deployment-5f94c574ff-wx22r" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-wx22r test-recreate-deployment-5f94c574ff- deployment-6640 /api/v1/namespaces/deployment-6640/pods/test-recreate-deployment-5f94c574ff-wx22r 6e133c7f-f155-463e-95b2-7a7faac10332 50009 0 2020-02-17 17:09:53 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 1ef25a0c-3304-4b94-aae0-b13d8909bdd0 0xc004b83367 0xc004b83368}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mhvgz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mhvgz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mhvgz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:09:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:09:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:09:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:09:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.9,PodIP:,StartTime:2020-02-17 17:09:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:09:53.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6640" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":188,"skipped":3005,"failed":0}
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:09:53.996: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-5581
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 17 17:09:58.319: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 17 17:09:58.332: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 17 17:10:00.332: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 17 17:10:00.344: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 17 17:10:02.332: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 17 17:10:02.345: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 17 17:10:04.332: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 17 17:10:04.344: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 17 17:10:06.332: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 17 17:10:06.345: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 17 17:10:08.332: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 17 17:10:08.348: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 17 17:10:10.332: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 17 17:10:10.344: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 17 17:10:12.332: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 17 17:10:12.344: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:10:12.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5581" for this suite.

• [SLOW TEST:18.400 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":280,"completed":189,"skipped":3007,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:10:12.396: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-66
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-550d974c-31a0-424a-a4ed-997428c08029
STEP: Creating a pod to test consume secrets
Feb 17 17:10:12.707: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cbfed25c-d68e-4afb-922d-942d5f090644" in namespace "projected-66" to be "success or failure"
Feb 17 17:10:12.719: INFO: Pod "pod-projected-secrets-cbfed25c-d68e-4afb-922d-942d5f090644": Phase="Pending", Reason="", readiness=false. Elapsed: 12.324012ms
Feb 17 17:10:14.730: INFO: Pod "pod-projected-secrets-cbfed25c-d68e-4afb-922d-942d5f090644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023540233s
STEP: Saw pod success
Feb 17 17:10:14.731: INFO: Pod "pod-projected-secrets-cbfed25c-d68e-4afb-922d-942d5f090644" satisfied condition "success or failure"
Feb 17 17:10:14.741: INFO: Trying to get logs from node 10.195.53.9 pod pod-projected-secrets-cbfed25c-d68e-4afb-922d-942d5f090644 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 17 17:10:14.798: INFO: Waiting for pod pod-projected-secrets-cbfed25c-d68e-4afb-922d-942d5f090644 to disappear
Feb 17 17:10:14.807: INFO: Pod pod-projected-secrets-cbfed25c-d68e-4afb-922d-942d5f090644 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:10:14.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-66" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":190,"skipped":3046,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:10:14.836: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8485
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1733
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 17 17:10:15.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-8485'
Feb 17 17:10:15.231: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 17 17:10:15.231: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1738
Feb 17 17:10:17.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 delete deployment e2e-test-httpd-deployment --namespace=kubectl-8485'
Feb 17 17:10:17.382: INFO: stderr: ""
Feb 17 17:10:17.382: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:10:17.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8485" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run deployment should create a deployment from an image  [Conformance]","total":280,"completed":191,"skipped":3088,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:10:17.415: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4909
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 17:10:18.150: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 17:10:20.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556218, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556218, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556218, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556218, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 17:10:23.241: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:10:23.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4909" for this suite.
STEP: Destroying namespace "webhook-4909-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.328 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":280,"completed":192,"skipped":3150,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:10:23.745: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7154
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting the proxy server
Feb 17 17:10:23.941: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-855842601 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:10:24.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7154" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":280,"completed":193,"skipped":3151,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:10:24.050: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-542
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod test-webserver-5386ce98-642e-4e4c-b767-dd21bbbd55fa in namespace container-probe-542
Feb 17 17:10:26.286: INFO: Started pod test-webserver-5386ce98-642e-4e4c-b767-dd21bbbd55fa in namespace container-probe-542
STEP: checking the pod's current state and verifying that restartCount is present
Feb 17 17:10:26.296: INFO: Initial restart count of pod test-webserver-5386ce98-642e-4e4c-b767-dd21bbbd55fa is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:14:27.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-542" for this suite.

• [SLOW TEST:243.910 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":194,"skipped":3159,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:14:27.961: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7054
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Feb 17 17:14:28.175: INFO: Waiting up to 5m0s for pod "downward-api-790e3288-f2ce-4a50-a989-c9704d1a672d" in namespace "downward-api-7054" to be "success or failure"
Feb 17 17:14:28.187: INFO: Pod "downward-api-790e3288-f2ce-4a50-a989-c9704d1a672d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.010632ms
Feb 17 17:14:30.198: INFO: Pod "downward-api-790e3288-f2ce-4a50-a989-c9704d1a672d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022201181s
STEP: Saw pod success
Feb 17 17:14:30.198: INFO: Pod "downward-api-790e3288-f2ce-4a50-a989-c9704d1a672d" satisfied condition "success or failure"
Feb 17 17:14:30.209: INFO: Trying to get logs from node 10.195.53.9 pod downward-api-790e3288-f2ce-4a50-a989-c9704d1a672d container dapi-container: <nil>
STEP: delete the pod
Feb 17 17:14:30.301: INFO: Waiting for pod downward-api-790e3288-f2ce-4a50-a989-c9704d1a672d to disappear
Feb 17 17:14:30.315: INFO: Pod downward-api-790e3288-f2ce-4a50-a989-c9704d1a672d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:14:30.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7054" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":280,"completed":195,"skipped":3160,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:14:30.342: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6065
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-47e43cbf-d622-48e8-b8d9-845e56d732d5
STEP: Creating a pod to test consume configMaps
Feb 17 17:14:30.566: INFO: Waiting up to 5m0s for pod "pod-configmaps-036940b4-8d37-48cd-ae80-0c60460676e2" in namespace "configmap-6065" to be "success or failure"
Feb 17 17:14:30.581: INFO: Pod "pod-configmaps-036940b4-8d37-48cd-ae80-0c60460676e2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.603605ms
Feb 17 17:14:32.593: INFO: Pod "pod-configmaps-036940b4-8d37-48cd-ae80-0c60460676e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026040947s
STEP: Saw pod success
Feb 17 17:14:32.593: INFO: Pod "pod-configmaps-036940b4-8d37-48cd-ae80-0c60460676e2" satisfied condition "success or failure"
Feb 17 17:14:32.603: INFO: Trying to get logs from node 10.195.53.9 pod pod-configmaps-036940b4-8d37-48cd-ae80-0c60460676e2 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 17:14:32.658: INFO: Waiting for pod pod-configmaps-036940b4-8d37-48cd-ae80-0c60460676e2 to disappear
Feb 17 17:14:32.668: INFO: Pod pod-configmaps-036940b4-8d37-48cd-ae80-0c60460676e2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:14:32.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6065" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":196,"skipped":3162,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:14:32.698: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3092
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1861
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 17 17:14:32.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-3092'
Feb 17 17:14:32.998: INFO: stderr: ""
Feb 17 17:14:32.998: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1866
Feb 17 17:14:33.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 delete pods e2e-test-httpd-pod --namespace=kubectl-3092'
Feb 17 17:14:41.291: INFO: stderr: ""
Feb 17 17:14:41.291: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:14:41.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3092" for this suite.

• [SLOW TEST:8.624 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1857
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":280,"completed":197,"skipped":3187,"failed":0}
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:14:41.322: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9868
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 17 17:14:45.635: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 17 17:14:45.648: INFO: Pod pod-with-prestop-http-hook still exists
Feb 17 17:14:47.648: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 17 17:14:47.660: INFO: Pod pod-with-prestop-http-hook still exists
Feb 17 17:14:49.648: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 17 17:14:49.660: INFO: Pod pod-with-prestop-http-hook still exists
Feb 17 17:14:51.648: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 17 17:14:51.661: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:14:51.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9868" for this suite.

• [SLOW TEST:10.386 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":280,"completed":198,"skipped":3192,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:14:51.708: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2627
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 17:14:51.918: INFO: Waiting up to 5m0s for pod "downwardapi-volume-208603cd-7ce5-4173-8ea4-ea871f8e19f3" in namespace "projected-2627" to be "success or failure"
Feb 17 17:14:51.929: INFO: Pod "downwardapi-volume-208603cd-7ce5-4173-8ea4-ea871f8e19f3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.320289ms
Feb 17 17:14:53.951: INFO: Pod "downwardapi-volume-208603cd-7ce5-4173-8ea4-ea871f8e19f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032493629s
STEP: Saw pod success
Feb 17 17:14:53.951: INFO: Pod "downwardapi-volume-208603cd-7ce5-4173-8ea4-ea871f8e19f3" satisfied condition "success or failure"
Feb 17 17:14:53.962: INFO: Trying to get logs from node 10.195.53.9 pod downwardapi-volume-208603cd-7ce5-4173-8ea4-ea871f8e19f3 container client-container: <nil>
STEP: delete the pod
Feb 17 17:14:54.019: INFO: Waiting for pod downwardapi-volume-208603cd-7ce5-4173-8ea4-ea871f8e19f3 to disappear
Feb 17 17:14:54.029: INFO: Pod downwardapi-volume-208603cd-7ce5-4173-8ea4-ea871f8e19f3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:14:54.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2627" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":199,"skipped":3193,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:14:54.058: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3058
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override command
Feb 17 17:14:54.270: INFO: Waiting up to 5m0s for pod "client-containers-648405c0-56ef-4b5a-b61d-60c9265f8a25" in namespace "containers-3058" to be "success or failure"
Feb 17 17:14:54.281: INFO: Pod "client-containers-648405c0-56ef-4b5a-b61d-60c9265f8a25": Phase="Pending", Reason="", readiness=false. Elapsed: 11.052208ms
Feb 17 17:14:56.294: INFO: Pod "client-containers-648405c0-56ef-4b5a-b61d-60c9265f8a25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023530306s
STEP: Saw pod success
Feb 17 17:14:56.294: INFO: Pod "client-containers-648405c0-56ef-4b5a-b61d-60c9265f8a25" satisfied condition "success or failure"
Feb 17 17:14:56.305: INFO: Trying to get logs from node 10.195.53.9 pod client-containers-648405c0-56ef-4b5a-b61d-60c9265f8a25 container test-container: <nil>
STEP: delete the pod
Feb 17 17:14:56.364: INFO: Waiting for pod client-containers-648405c0-56ef-4b5a-b61d-60c9265f8a25 to disappear
Feb 17 17:14:56.375: INFO: Pod client-containers-648405c0-56ef-4b5a-b61d-60c9265f8a25 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:14:56.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3058" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":280,"completed":200,"skipped":3219,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:14:56.404: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1937
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1937.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-1937.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1937.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1937.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1937.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-1937.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1937.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-1937.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1937.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1937.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-1937.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1937.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-1937.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1937.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-1937.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1937.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-1937.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1937.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 17 17:14:58.680: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1937.svc.cluster.local from pod dns-1937/dns-test-28953e90-ddf3-43cb-994e-dc14474d9a82: the server could not find the requested resource (get pods dns-test-28953e90-ddf3-43cb-994e-dc14474d9a82)
Feb 17 17:14:58.707: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1937.svc.cluster.local from pod dns-1937/dns-test-28953e90-ddf3-43cb-994e-dc14474d9a82: the server could not find the requested resource (get pods dns-test-28953e90-ddf3-43cb-994e-dc14474d9a82)
Feb 17 17:14:58.722: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1937.svc.cluster.local from pod dns-1937/dns-test-28953e90-ddf3-43cb-994e-dc14474d9a82: the server could not find the requested resource (get pods dns-test-28953e90-ddf3-43cb-994e-dc14474d9a82)
Feb 17 17:14:58.739: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1937.svc.cluster.local from pod dns-1937/dns-test-28953e90-ddf3-43cb-994e-dc14474d9a82: the server could not find the requested resource (get pods dns-test-28953e90-ddf3-43cb-994e-dc14474d9a82)
Feb 17 17:14:58.784: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1937.svc.cluster.local from pod dns-1937/dns-test-28953e90-ddf3-43cb-994e-dc14474d9a82: the server could not find the requested resource (get pods dns-test-28953e90-ddf3-43cb-994e-dc14474d9a82)
Feb 17 17:14:58.798: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1937.svc.cluster.local from pod dns-1937/dns-test-28953e90-ddf3-43cb-994e-dc14474d9a82: the server could not find the requested resource (get pods dns-test-28953e90-ddf3-43cb-994e-dc14474d9a82)
Feb 17 17:14:58.811: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1937.svc.cluster.local from pod dns-1937/dns-test-28953e90-ddf3-43cb-994e-dc14474d9a82: the server could not find the requested resource (get pods dns-test-28953e90-ddf3-43cb-994e-dc14474d9a82)
Feb 17 17:14:58.826: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1937.svc.cluster.local from pod dns-1937/dns-test-28953e90-ddf3-43cb-994e-dc14474d9a82: the server could not find the requested resource (get pods dns-test-28953e90-ddf3-43cb-994e-dc14474d9a82)
Feb 17 17:14:58.861: INFO: Lookups using dns-1937/dns-test-28953e90-ddf3-43cb-994e-dc14474d9a82 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1937.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1937.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1937.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1937.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1937.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1937.svc.cluster.local jessie_udp@dns-test-service-2.dns-1937.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1937.svc.cluster.local]

Feb 17 17:15:03.920: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1937.svc.cluster.local from pod dns-1937/dns-test-28953e90-ddf3-43cb-994e-dc14474d9a82: the server could not find the requested resource (get pods dns-test-28953e90-ddf3-43cb-994e-dc14474d9a82)
Feb 17 17:15:04.048: INFO: Lookups using dns-1937/dns-test-28953e90-ddf3-43cb-994e-dc14474d9a82 failed for: [wheezy_tcp@dns-test-service-2.dns-1937.svc.cluster.local]

Feb 17 17:15:09.039: INFO: DNS probes using dns-1937/dns-test-28953e90-ddf3-43cb-994e-dc14474d9a82 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:15:09.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1937" for this suite.

• [SLOW TEST:12.761 seconds]
[sig-network] DNS
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":280,"completed":201,"skipped":3255,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:15:09.164: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3882
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 17 17:15:09.387: INFO: Waiting up to 5m0s for pod "pod-671c64dc-c084-4159-9f63-2bd7f9dbfaef" in namespace "emptydir-3882" to be "success or failure"
Feb 17 17:15:09.397: INFO: Pod "pod-671c64dc-c084-4159-9f63-2bd7f9dbfaef": Phase="Pending", Reason="", readiness=false. Elapsed: 10.120808ms
Feb 17 17:15:11.408: INFO: Pod "pod-671c64dc-c084-4159-9f63-2bd7f9dbfaef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021153588s
STEP: Saw pod success
Feb 17 17:15:11.408: INFO: Pod "pod-671c64dc-c084-4159-9f63-2bd7f9dbfaef" satisfied condition "success or failure"
Feb 17 17:15:11.418: INFO: Trying to get logs from node 10.195.53.9 pod pod-671c64dc-c084-4159-9f63-2bd7f9dbfaef container test-container: <nil>
STEP: delete the pod
Feb 17 17:15:11.472: INFO: Waiting for pod pod-671c64dc-c084-4159-9f63-2bd7f9dbfaef to disappear
Feb 17 17:15:11.482: INFO: Pod pod-671c64dc-c084-4159-9f63-2bd7f9dbfaef no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:15:11.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3882" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":202,"skipped":3256,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:15:11.510: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3691
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:15:14.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3691" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":280,"completed":203,"skipped":3257,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:15:14.831: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9036
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 17:15:15.588: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 17:15:17.622: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556515, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556515, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556515, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556515, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 17:15:20.662: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 17:15:20.670: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5377-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:15:21.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9036" for this suite.
STEP: Destroying namespace "webhook-9036-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.184 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":280,"completed":204,"skipped":3257,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:15:22.015: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8782
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 17:15:22.796: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 17:15:25.856: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the crd webhook via the AdmissionRegistration API
Feb 17 17:15:35.909: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource definition that should be denied by the webhook
Feb 17 17:15:36.036: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:15:36.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8782" for this suite.
STEP: Destroying namespace "webhook-8782-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:14.196 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":280,"completed":205,"skipped":3275,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:15:36.212: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-688
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:15:36.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-688" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":280,"completed":206,"skipped":3290,"failed":0}
SSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:15:36.471: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-2515
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb 17 17:15:40.766: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2515 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 17:15:40.766: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 17:15:40.918: INFO: Exec stderr: ""
Feb 17 17:15:40.918: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2515 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 17:15:40.918: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 17:15:41.067: INFO: Exec stderr: ""
Feb 17 17:15:41.067: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2515 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 17:15:41.067: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 17:15:41.212: INFO: Exec stderr: ""
Feb 17 17:15:41.212: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2515 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 17:15:41.212: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 17:15:41.366: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb 17 17:15:41.366: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2515 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 17:15:41.366: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 17:15:41.509: INFO: Exec stderr: ""
Feb 17 17:15:41.509: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2515 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 17:15:41.509: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 17:15:41.645: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb 17 17:15:41.645: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2515 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 17:15:41.645: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 17:15:41.784: INFO: Exec stderr: ""
Feb 17 17:15:41.784: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2515 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 17:15:41.784: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 17:15:41.936: INFO: Exec stderr: ""
Feb 17 17:15:41.936: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2515 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 17:15:41.936: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 17:15:42.098: INFO: Exec stderr: ""
Feb 17 17:15:42.098: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2515 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 17:15:42.098: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 17:15:42.252: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:15:42.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2515" for this suite.

• [SLOW TEST:5.812 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":207,"skipped":3295,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:15:42.284: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2392
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 17:15:42.499: INFO: Waiting up to 5m0s for pod "downwardapi-volume-192bbe6a-7415-4a46-84a9-6e1e1cc0be72" in namespace "downward-api-2392" to be "success or failure"
Feb 17 17:15:42.510: INFO: Pod "downwardapi-volume-192bbe6a-7415-4a46-84a9-6e1e1cc0be72": Phase="Pending", Reason="", readiness=false. Elapsed: 10.520283ms
Feb 17 17:15:44.521: INFO: Pod "downwardapi-volume-192bbe6a-7415-4a46-84a9-6e1e1cc0be72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021437273s
STEP: Saw pod success
Feb 17 17:15:44.521: INFO: Pod "downwardapi-volume-192bbe6a-7415-4a46-84a9-6e1e1cc0be72" satisfied condition "success or failure"
Feb 17 17:15:44.531: INFO: Trying to get logs from node 10.195.53.9 pod downwardapi-volume-192bbe6a-7415-4a46-84a9-6e1e1cc0be72 container client-container: <nil>
STEP: delete the pod
Feb 17 17:15:44.589: INFO: Waiting for pod downwardapi-volume-192bbe6a-7415-4a46-84a9-6e1e1cc0be72 to disappear
Feb 17 17:15:44.599: INFO: Pod downwardapi-volume-192bbe6a-7415-4a46-84a9-6e1e1cc0be72 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:15:44.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2392" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":208,"skipped":3322,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:15:44.628: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5039
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-d5b251ea-c776-4042-9c5f-461bd409e7aa
STEP: Creating a pod to test consume configMaps
Feb 17 17:15:44.854: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6c93cd96-8bde-42e6-986e-5f9d99b76d34" in namespace "projected-5039" to be "success or failure"
Feb 17 17:15:44.864: INFO: Pod "pod-projected-configmaps-6c93cd96-8bde-42e6-986e-5f9d99b76d34": Phase="Pending", Reason="", readiness=false. Elapsed: 10.091168ms
Feb 17 17:15:46.877: INFO: Pod "pod-projected-configmaps-6c93cd96-8bde-42e6-986e-5f9d99b76d34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022757966s
STEP: Saw pod success
Feb 17 17:15:46.877: INFO: Pod "pod-projected-configmaps-6c93cd96-8bde-42e6-986e-5f9d99b76d34" satisfied condition "success or failure"
Feb 17 17:15:46.888: INFO: Trying to get logs from node 10.195.53.9 pod pod-projected-configmaps-6c93cd96-8bde-42e6-986e-5f9d99b76d34 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 17:15:46.944: INFO: Waiting for pod pod-projected-configmaps-6c93cd96-8bde-42e6-986e-5f9d99b76d34 to disappear
Feb 17 17:15:46.955: INFO: Pod pod-projected-configmaps-6c93cd96-8bde-42e6-986e-5f9d99b76d34 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:15:46.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5039" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":209,"skipped":3330,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:15:46.988: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4299
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4299.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4299.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4299.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4299.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4299.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4299.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 17 17:15:51.362: INFO: DNS probes using dns-4299/dns-test-2d86526e-deeb-4419-9f58-0b0e4bed7dd5 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:15:51.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4299" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":280,"completed":210,"skipped":3334,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:15:51.430: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8609
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:15:58.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8609" for this suite.

• [SLOW TEST:7.311 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":280,"completed":211,"skipped":3348,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:15:58.741: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-3117
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:15:58.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3117" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":280,"completed":212,"skipped":3350,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:15:58.978: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1816
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Starting the proxy
Feb 17 17:15:59.178: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-855842601 proxy --unix-socket=/tmp/kubectl-proxy-unix812108724/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:15:59.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1816" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":280,"completed":213,"skipped":3353,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:15:59.272: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9665
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1788
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 17 17:15:59.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-9665'
Feb 17 17:15:59.572: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 17 17:15:59.572: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1793
Feb 17 17:15:59.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 delete jobs e2e-test-httpd-job --namespace=kubectl-9665'
Feb 17 17:15:59.704: INFO: stderr: ""
Feb 17 17:15:59.704: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:15:59.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9665" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run job should create a job from an image when restart is OnFailure  [Conformance]","total":280,"completed":214,"skipped":3362,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:15:59.730: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-5433
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 17:15:59.928: INFO: Creating ReplicaSet my-hostname-basic-ec138442-f228-443e-92f7-3de7365161da
Feb 17 17:15:59.952: INFO: Pod name my-hostname-basic-ec138442-f228-443e-92f7-3de7365161da: Found 0 pods out of 1
Feb 17 17:16:04.964: INFO: Pod name my-hostname-basic-ec138442-f228-443e-92f7-3de7365161da: Found 1 pods out of 1
Feb 17 17:16:04.964: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-ec138442-f228-443e-92f7-3de7365161da" is running
Feb 17 17:16:04.976: INFO: Pod "my-hostname-basic-ec138442-f228-443e-92f7-3de7365161da-vd4lr" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-17 17:15:59 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-17 17:16:01 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-17 17:16:01 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-17 17:15:59 +0000 UTC Reason: Message:}])
Feb 17 17:16:04.976: INFO: Trying to dial the pod
Feb 17 17:16:10.018: INFO: Controller my-hostname-basic-ec138442-f228-443e-92f7-3de7365161da: Got expected result from replica 1 [my-hostname-basic-ec138442-f228-443e-92f7-3de7365161da-vd4lr]: "my-hostname-basic-ec138442-f228-443e-92f7-3de7365161da-vd4lr", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:16:10.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5433" for this suite.

• [SLOW TEST:10.323 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":215,"skipped":3388,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:16:10.053: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8616
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8616
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8616
STEP: creating replication controller externalsvc in namespace services-8616
I0217 17:16:10.332488      24 runners.go:189] Created replication controller with name: externalsvc, namespace: services-8616, replica count: 2
I0217 17:16:13.383019      24 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Feb 17 17:16:13.438: INFO: Creating new exec pod
Feb 17 17:16:15.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=services-8616 execpodjt7jz -- /bin/sh -x -c nslookup nodeport-service'
Feb 17 17:16:15.779: INFO: stderr: "+ nslookup nodeport-service\n"
Feb 17 17:16:15.779: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-8616.svc.cluster.local\tcanonical name = externalsvc.services-8616.svc.cluster.local.\nName:\texternalsvc.services-8616.svc.cluster.local\nAddress: 172.21.185.221\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8616, will wait for the garbage collector to delete the pods
Feb 17 17:16:15.864: INFO: Deleting ReplicationController externalsvc took: 24.356964ms
Feb 17 17:16:15.965: INFO: Terminating ReplicationController externalsvc pods took: 100.272873ms
Feb 17 17:16:31.512: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:16:31.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8616" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:21.516 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":280,"completed":216,"skipped":3412,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:16:31.569: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9467
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test env composition
Feb 17 17:16:31.789: INFO: Waiting up to 5m0s for pod "var-expansion-b9a69b27-abfd-4116-91e0-a86edf972329" in namespace "var-expansion-9467" to be "success or failure"
Feb 17 17:16:31.805: INFO: Pod "var-expansion-b9a69b27-abfd-4116-91e0-a86edf972329": Phase="Pending", Reason="", readiness=false. Elapsed: 15.700314ms
Feb 17 17:16:33.815: INFO: Pod "var-expansion-b9a69b27-abfd-4116-91e0-a86edf972329": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026422701s
STEP: Saw pod success
Feb 17 17:16:33.816: INFO: Pod "var-expansion-b9a69b27-abfd-4116-91e0-a86edf972329" satisfied condition "success or failure"
Feb 17 17:16:33.826: INFO: Trying to get logs from node 10.195.53.9 pod var-expansion-b9a69b27-abfd-4116-91e0-a86edf972329 container dapi-container: <nil>
STEP: delete the pod
Feb 17 17:16:33.882: INFO: Waiting for pod var-expansion-b9a69b27-abfd-4116-91e0-a86edf972329 to disappear
Feb 17 17:16:33.893: INFO: Pod var-expansion-b9a69b27-abfd-4116-91e0-a86edf972329 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:16:33.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9467" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":280,"completed":217,"skipped":3425,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:16:33.921: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9384
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:16:51.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9384" for this suite.

• [SLOW TEST:17.323 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":280,"completed":218,"skipped":3465,"failed":0}
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:16:51.244: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4365
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Feb 17 17:16:51.442: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:16:55.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4365" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":280,"completed":219,"skipped":3470,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:16:55.530: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-4462
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Feb 17 17:16:56.254: INFO: Pod name wrapped-volume-race-356e4a82-e461-4dec-8eeb-99f15011caff: Found 0 pods out of 5
Feb 17 17:17:01.270: INFO: Pod name wrapped-volume-race-356e4a82-e461-4dec-8eeb-99f15011caff: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-356e4a82-e461-4dec-8eeb-99f15011caff in namespace emptydir-wrapper-4462, will wait for the garbage collector to delete the pods
Feb 17 17:17:01.416: INFO: Deleting ReplicationController wrapped-volume-race-356e4a82-e461-4dec-8eeb-99f15011caff took: 29.409808ms
Feb 17 17:17:01.616: INFO: Terminating ReplicationController wrapped-volume-race-356e4a82-e461-4dec-8eeb-99f15011caff pods took: 200.765432ms
STEP: Creating RC which spawns configmap-volume pods
Feb 17 17:17:11.059: INFO: Pod name wrapped-volume-race-10fe3b54-0d4a-48e8-8773-da7b8799f5c4: Found 0 pods out of 5
Feb 17 17:17:16.077: INFO: Pod name wrapped-volume-race-10fe3b54-0d4a-48e8-8773-da7b8799f5c4: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-10fe3b54-0d4a-48e8-8773-da7b8799f5c4 in namespace emptydir-wrapper-4462, will wait for the garbage collector to delete the pods
Feb 17 17:17:16.227: INFO: Deleting ReplicationController wrapped-volume-race-10fe3b54-0d4a-48e8-8773-da7b8799f5c4 took: 27.192238ms
Feb 17 17:17:16.335: INFO: Terminating ReplicationController wrapped-volume-race-10fe3b54-0d4a-48e8-8773-da7b8799f5c4 pods took: 107.90192ms
STEP: Creating RC which spawns configmap-volume pods
Feb 17 17:17:26.680: INFO: Pod name wrapped-volume-race-b588b670-a9c6-479c-bc26-2c652d475bc3: Found 0 pods out of 5
Feb 17 17:17:31.698: INFO: Pod name wrapped-volume-race-b588b670-a9c6-479c-bc26-2c652d475bc3: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b588b670-a9c6-479c-bc26-2c652d475bc3 in namespace emptydir-wrapper-4462, will wait for the garbage collector to delete the pods
Feb 17 17:17:31.842: INFO: Deleting ReplicationController wrapped-volume-race-b588b670-a9c6-479c-bc26-2c652d475bc3 took: 27.3606ms
Feb 17 17:17:31.942: INFO: Terminating ReplicationController wrapped-volume-race-b588b670-a9c6-479c-bc26-2c652d475bc3 pods took: 100.180044ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:17:42.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4462" for this suite.

• [SLOW TEST:47.027 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":280,"completed":220,"skipped":3483,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:17:42.558: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7794
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Feb 17 17:17:42.780: INFO: Waiting up to 5m0s for pod "downward-api-ee0e5ce2-c3d3-43c0-8b18-0af28f7ba7a2" in namespace "downward-api-7794" to be "success or failure"
Feb 17 17:17:42.790: INFO: Pod "downward-api-ee0e5ce2-c3d3-43c0-8b18-0af28f7ba7a2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.50344ms
Feb 17 17:17:44.802: INFO: Pod "downward-api-ee0e5ce2-c3d3-43c0-8b18-0af28f7ba7a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022442222s
STEP: Saw pod success
Feb 17 17:17:44.802: INFO: Pod "downward-api-ee0e5ce2-c3d3-43c0-8b18-0af28f7ba7a2" satisfied condition "success or failure"
Feb 17 17:17:44.812: INFO: Trying to get logs from node 10.195.53.9 pod downward-api-ee0e5ce2-c3d3-43c0-8b18-0af28f7ba7a2 container dapi-container: <nil>
STEP: delete the pod
Feb 17 17:17:44.872: INFO: Waiting for pod downward-api-ee0e5ce2-c3d3-43c0-8b18-0af28f7ba7a2 to disappear
Feb 17 17:17:44.883: INFO: Pod downward-api-ee0e5ce2-c3d3-43c0-8b18-0af28f7ba7a2 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:17:44.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7794" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":280,"completed":221,"skipped":3495,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:17:44.909: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1058
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:17:45.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1058" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":280,"completed":222,"skipped":3502,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:17:45.177: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-2770
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:17:47.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2770" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":280,"completed":223,"skipped":3532,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:17:47.572: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6235
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-c14f71c9-48d0-4b1d-a003-065c9e23c926
STEP: Creating configMap with name cm-test-opt-upd-cd3790a5-40ed-4531-846c-4755281b3e2f
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-c14f71c9-48d0-4b1d-a003-065c9e23c926
STEP: Updating configmap cm-test-opt-upd-cd3790a5-40ed-4531-846c-4755281b3e2f
STEP: Creating configMap with name cm-test-opt-create-3d99bde6-a584-4996-9c96-b6027ab6d567
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:17:52.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6235" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":224,"skipped":3538,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:17:52.083: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1628
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1628
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1628
STEP: creating replication controller externalsvc in namespace services-1628
I0217 17:17:52.342895      24 runners.go:189] Created replication controller with name: externalsvc, namespace: services-1628, replica count: 2
I0217 17:17:55.393297      24 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Feb 17 17:17:55.441: INFO: Creating new exec pod
Feb 17 17:17:57.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=services-1628 execpodthj5x -- /bin/sh -x -c nslookup clusterip-service'
Feb 17 17:17:57.739: INFO: stderr: "+ nslookup clusterip-service\n"
Feb 17 17:17:57.739: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-1628.svc.cluster.local\tcanonical name = externalsvc.services-1628.svc.cluster.local.\nName:\texternalsvc.services-1628.svc.cluster.local\nAddress: 172.21.167.241\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1628, will wait for the garbage collector to delete the pods
Feb 17 17:17:57.824: INFO: Deleting ReplicationController externalsvc took: 25.553887ms
Feb 17 17:17:57.925: INFO: Terminating ReplicationController externalsvc pods took: 100.189784ms
Feb 17 17:18:01.774: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:18:01.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1628" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:9.748 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":280,"completed":225,"skipped":3553,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:18:01.832: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-425
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 17 17:18:02.115: INFO: Number of nodes with available pods: 0
Feb 17 17:18:02.115: INFO: Node 10.195.53.14 is running more than one daemon pod
Feb 17 17:18:03.141: INFO: Number of nodes with available pods: 0
Feb 17 17:18:03.141: INFO: Node 10.195.53.14 is running more than one daemon pod
Feb 17 17:18:04.139: INFO: Number of nodes with available pods: 3
Feb 17 17:18:04.139: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb 17 17:18:04.198: INFO: Number of nodes with available pods: 2
Feb 17 17:18:04.198: INFO: Node 10.195.53.47 is running more than one daemon pod
Feb 17 17:18:05.223: INFO: Number of nodes with available pods: 2
Feb 17 17:18:05.223: INFO: Node 10.195.53.47 is running more than one daemon pod
Feb 17 17:18:06.468: INFO: Number of nodes with available pods: 2
Feb 17 17:18:06.468: INFO: Node 10.195.53.47 is running more than one daemon pod
Feb 17 17:18:07.222: INFO: Number of nodes with available pods: 2
Feb 17 17:18:07.222: INFO: Node 10.195.53.47 is running more than one daemon pod
Feb 17 17:18:08.222: INFO: Number of nodes with available pods: 2
Feb 17 17:18:08.222: INFO: Node 10.195.53.47 is running more than one daemon pod
Feb 17 17:18:09.222: INFO: Number of nodes with available pods: 2
Feb 17 17:18:09.222: INFO: Node 10.195.53.47 is running more than one daemon pod
Feb 17 17:18:10.222: INFO: Number of nodes with available pods: 3
Feb 17 17:18:10.222: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-425, will wait for the garbage collector to delete the pods
Feb 17 17:18:10.311: INFO: Deleting DaemonSet.extensions daemon-set took: 19.631325ms
Feb 17 17:18:10.411: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.184153ms
Feb 17 17:18:21.323: INFO: Number of nodes with available pods: 0
Feb 17 17:18:21.323: INFO: Number of running nodes: 0, number of available pods: 0
Feb 17 17:18:21.331: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-425/daemonsets","resourceVersion":"53875"},"items":null}

Feb 17 17:18:21.341: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-425/pods","resourceVersion":"53875"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:18:21.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-425" for this suite.

• [SLOW TEST:19.574 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":280,"completed":226,"skipped":3622,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:18:21.406: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9268
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 17:18:21.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 version'
Feb 17 17:18:21.697: INFO: stderr: ""
Feb 17 17:18:21.697: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.3\", GitCommit:\"06ad960bfd03b39c8310aaf92d1e7c12ce618213\", GitTreeState:\"clean\", BuildDate:\"2020-02-11T18:14:22Z\", GoVersion:\"go1.13.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.3+IKS\", GitCommit:\"f60ec7e9eb2b798894c7c3997d0d9d506bbd9615\", GitTreeState:\"clean\", BuildDate:\"2020-02-12T10:08:16Z\", GoVersion:\"go1.13.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:18:21.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9268" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":280,"completed":227,"skipped":3642,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:18:21.726: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2193
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 17:18:21.940: INFO: Waiting up to 5m0s for pod "downwardapi-volume-56626c17-077d-4dd3-a48d-b44a1479fe7d" in namespace "downward-api-2193" to be "success or failure"
Feb 17 17:18:21.952: INFO: Pod "downwardapi-volume-56626c17-077d-4dd3-a48d-b44a1479fe7d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.679802ms
Feb 17 17:18:23.964: INFO: Pod "downwardapi-volume-56626c17-077d-4dd3-a48d-b44a1479fe7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023156812s
STEP: Saw pod success
Feb 17 17:18:23.964: INFO: Pod "downwardapi-volume-56626c17-077d-4dd3-a48d-b44a1479fe7d" satisfied condition "success or failure"
Feb 17 17:18:23.975: INFO: Trying to get logs from node 10.195.53.9 pod downwardapi-volume-56626c17-077d-4dd3-a48d-b44a1479fe7d container client-container: <nil>
STEP: delete the pod
Feb 17 17:18:24.032: INFO: Waiting for pod downwardapi-volume-56626c17-077d-4dd3-a48d-b44a1479fe7d to disappear
Feb 17 17:18:24.043: INFO: Pod downwardapi-volume-56626c17-077d-4dd3-a48d-b44a1479fe7d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:18:24.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2193" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":228,"skipped":3645,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:18:24.073: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1554
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 17 17:18:26.328: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:18:26.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1554" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":229,"skipped":3685,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:18:26.402: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1639
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Feb 17 17:18:27.230: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0217 17:18:27.230643      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:18:27.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1639" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":280,"completed":230,"skipped":3735,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:18:27.258: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8673
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:18:40.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8673" for this suite.

• [SLOW TEST:13.357 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":280,"completed":231,"skipped":3765,"failed":0}
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:18:40.615: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6233
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-6233
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 17 17:18:40.814: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 17 17:19:05.064: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.20.69:8080/dial?request=hostname&protocol=udp&host=172.30.89.203&port=8081&tries=1'] Namespace:pod-network-test-6233 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 17:19:05.064: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 17:19:05.217: INFO: Waiting for responses: map[]
Feb 17 17:19:05.228: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.20.69:8080/dial?request=hostname&protocol=udp&host=172.30.117.199&port=8081&tries=1'] Namespace:pod-network-test-6233 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 17:19:05.228: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 17:19:05.382: INFO: Waiting for responses: map[]
Feb 17 17:19:05.393: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.20.69:8080/dial?request=hostname&protocol=udp&host=172.30.20.125&port=8081&tries=1'] Namespace:pod-network-test-6233 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 17:19:05.393: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 17:19:05.537: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:19:05.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6233" for this suite.

• [SLOW TEST:24.950 seconds]
[sig-network] Networking
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":232,"skipped":3768,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:19:05.566: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2286
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-2286
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating stateful set ss in namespace statefulset-2286
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2286
Feb 17 17:19:05.792: INFO: Found 0 stateful pods, waiting for 1
Feb 17 17:19:15.805: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb 17 17:19:15.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 17:19:16.117: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 17:19:16.117: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 17:19:16.117: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 17 17:19:16.130: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 17 17:19:26.144: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 17 17:19:26.144: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 17:19:26.184: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Feb 17 17:19:26.184: INFO: ss-0  10.195.53.9  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:05 +0000 UTC  }]
Feb 17 17:19:26.185: INFO: ss-1               Pending         []
Feb 17 17:19:26.185: INFO: 
Feb 17 17:19:26.185: INFO: StatefulSet ss has not reached scale 3, at 2
Feb 17 17:19:27.198: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.988908341s
Feb 17 17:19:28.210: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.975599104s
Feb 17 17:19:29.223: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.963325497s
Feb 17 17:19:30.235: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.95028616s
Feb 17 17:19:31.248: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.937963474s
Feb 17 17:19:32.264: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.925323116s
Feb 17 17:19:33.276: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.909316978s
Feb 17 17:19:34.289: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.897131506s
Feb 17 17:19:35.301: INFO: Verifying statefulset ss doesn't scale past 3 for another 884.331472ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2286
Feb 17 17:19:36.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:19:36.575: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 17 17:19:36.575: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 17 17:19:36.575: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 17 17:19:36.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:19:36.824: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 17 17:19:36.824: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 17 17:19:36.824: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 17 17:19:36.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:19:37.085: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 17 17:19:37.085: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 17 17:19:37.085: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 17 17:19:37.098: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Feb 17 17:19:47.111: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 17:19:47.111: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 17:19:47.111: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb 17 17:19:47.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 17:19:47.408: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 17:19:47.408: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 17:19:47.408: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 17 17:19:47.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 17:19:47.684: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 17:19:47.684: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 17:19:47.684: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 17 17:19:47.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 17:19:47.936: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 17:19:47.936: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 17:19:47.936: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 17 17:19:47.936: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 17:19:47.945: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Feb 17 17:19:57.967: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 17 17:19:57.967: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 17 17:19:57.967: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 17 17:19:57.998: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Feb 17 17:19:57.998: INFO: ss-0  10.195.53.9   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:05 +0000 UTC  }]
Feb 17 17:19:57.998: INFO: ss-1  10.195.53.47  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  }]
Feb 17 17:19:57.998: INFO: ss-2  10.195.53.14  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  }]
Feb 17 17:19:57.998: INFO: 
Feb 17 17:19:57.998: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 17 17:19:59.010: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Feb 17 17:19:59.010: INFO: ss-0  10.195.53.9   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:05 +0000 UTC  }]
Feb 17 17:19:59.010: INFO: ss-1  10.195.53.47  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  }]
Feb 17 17:19:59.010: INFO: ss-2  10.195.53.14  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  }]
Feb 17 17:19:59.010: INFO: 
Feb 17 17:19:59.010: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 17 17:20:00.023: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Feb 17 17:20:00.023: INFO: ss-1  10.195.53.47  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  }]
Feb 17 17:20:00.023: INFO: ss-2  10.195.53.14  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  }]
Feb 17 17:20:00.023: INFO: 
Feb 17 17:20:00.023: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 17 17:20:01.108: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Feb 17 17:20:01.108: INFO: ss-1  10.195.53.47  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  }]
Feb 17 17:20:01.108: INFO: ss-2  10.195.53.14  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  }]
Feb 17 17:20:01.108: INFO: 
Feb 17 17:20:01.108: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 17 17:20:02.120: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Feb 17 17:20:02.120: INFO: ss-1  10.195.53.47  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  }]
Feb 17 17:20:02.120: INFO: ss-2  10.195.53.14  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  }]
Feb 17 17:20:02.120: INFO: 
Feb 17 17:20:02.120: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 17 17:20:03.133: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Feb 17 17:20:03.133: INFO: ss-1  10.195.53.47  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  }]
Feb 17 17:20:03.133: INFO: ss-2  10.195.53.14  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  }]
Feb 17 17:20:03.133: INFO: 
Feb 17 17:20:03.133: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 17 17:20:04.145: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Feb 17 17:20:04.145: INFO: ss-1  10.195.53.47  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  }]
Feb 17 17:20:04.146: INFO: ss-2  10.195.53.14  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  }]
Feb 17 17:20:04.146: INFO: 
Feb 17 17:20:04.146: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 17 17:20:05.158: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Feb 17 17:20:05.158: INFO: ss-1  10.195.53.47  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  }]
Feb 17 17:20:05.158: INFO: ss-2  10.195.53.14  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  }]
Feb 17 17:20:05.158: INFO: 
Feb 17 17:20:05.158: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 17 17:20:06.170: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Feb 17 17:20:06.170: INFO: ss-1  10.195.53.47  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  }]
Feb 17 17:20:06.170: INFO: ss-2  10.195.53.14  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  }]
Feb 17 17:20:06.170: INFO: 
Feb 17 17:20:06.170: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 17 17:20:07.183: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Feb 17 17:20:07.183: INFO: ss-1  10.195.53.47  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:19:26 +0000 UTC  }]
Feb 17 17:20:07.183: INFO: 
Feb 17 17:20:07.183: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2286
Feb 17 17:20:08.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:20:08.381: INFO: rc: 1
Feb 17 17:20:08.381: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Feb 17 17:20:18.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:20:18.556: INFO: rc: 1
Feb 17 17:20:18.556: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Feb 17 17:20:28.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:20:28.660: INFO: rc: 1
Feb 17 17:20:28.660: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Feb 17 17:20:38.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:20:38.784: INFO: rc: 1
Feb 17 17:20:38.784: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Feb 17 17:20:48.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:20:48.889: INFO: rc: 1
Feb 17 17:20:48.889: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Feb 17 17:20:58.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:20:58.990: INFO: rc: 1
Feb 17 17:20:58.990: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Feb 17 17:21:08.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:21:09.098: INFO: rc: 1
Feb 17 17:21:09.098: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Feb 17 17:21:19.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:21:19.204: INFO: rc: 1
Feb 17 17:21:19.204: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Feb 17 17:21:29.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:21:29.319: INFO: rc: 1
Feb 17 17:21:29.319: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Feb 17 17:21:39.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:21:39.425: INFO: rc: 1
Feb 17 17:21:39.425: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Feb 17 17:21:49.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:21:49.529: INFO: rc: 1
Feb 17 17:21:49.529: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Feb 17 17:21:59.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:21:59.649: INFO: rc: 1
Feb 17 17:21:59.649: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Feb 17 17:22:09.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:22:09.774: INFO: rc: 1
Feb 17 17:22:09.774: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Feb 17 17:22:19.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:22:19.885: INFO: rc: 1
Feb 17 17:22:19.885: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Feb 17 17:22:29.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:22:29.995: INFO: rc: 1
Feb 17 17:22:29.995: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Feb 17 17:22:39.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:22:40.095: INFO: rc: 1
Feb 17 17:22:40.095: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Feb 17 17:22:50.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:22:50.205: INFO: rc: 1
Feb 17 17:22:50.205: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Feb 17 17:23:00.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:23:00.323: INFO: rc: 1
Feb 17 17:23:00.323: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Feb 17 17:23:10.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:23:10.435: INFO: rc: 1
Feb 17 17:23:10.435: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Feb 17 17:23:20.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:23:20.554: INFO: rc: 1
Feb 17 17:23:20.554: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Feb 17 17:23:30.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:23:30.667: INFO: rc: 1
Feb 17 17:23:30.667: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Feb 17 17:23:40.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:23:40.788: INFO: rc: 1
Feb 17 17:23:40.788: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Feb 17 17:23:50.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:23:50.900: INFO: rc: 1
Feb 17 17:23:50.900: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Feb 17 17:24:00.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:24:01.007: INFO: rc: 1
Feb 17 17:24:01.007: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Feb 17 17:24:11.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:24:11.111: INFO: rc: 1
Feb 17 17:24:11.111: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Feb 17 17:24:21.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:24:21.223: INFO: rc: 1
Feb 17 17:24:21.223: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Feb 17 17:24:31.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:24:31.335: INFO: rc: 1
Feb 17 17:24:31.335: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Feb 17 17:24:41.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:24:41.446: INFO: rc: 1
Feb 17 17:24:41.446: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Feb 17 17:24:51.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:24:51.561: INFO: rc: 1
Feb 17 17:24:51.561: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Feb 17 17:25:01.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:25:01.682: INFO: rc: 1
Feb 17 17:25:01.682: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Feb 17 17:25:11.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 exec --namespace=statefulset-2286 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:25:11.797: INFO: rc: 1
Feb 17 17:25:11.797: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: 
Feb 17 17:25:11.797: INFO: Scaling statefulset ss to 0
Feb 17 17:25:11.826: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Feb 17 17:25:11.834: INFO: Deleting all statefulset in ns statefulset-2286
Feb 17 17:25:11.842: INFO: Scaling statefulset ss to 0
Feb 17 17:25:11.872: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 17:25:11.881: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:25:11.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2286" for this suite.

• [SLOW TEST:366.374 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":280,"completed":233,"skipped":3771,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:25:11.940: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-4737
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 17:25:12.200: INFO: (0) /api/v1/nodes/10.195.53.47:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 53.717822ms)
Feb 17 17:25:12.214: INFO: (1) /api/v1/nodes/10.195.53.47:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 13.31142ms)
Feb 17 17:25:12.228: INFO: (2) /api/v1/nodes/10.195.53.47:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 14.265582ms)
Feb 17 17:25:12.244: INFO: (3) /api/v1/nodes/10.195.53.47:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 15.585847ms)
Feb 17 17:25:12.257: INFO: (4) /api/v1/nodes/10.195.53.47:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 12.97008ms)
Feb 17 17:25:12.270: INFO: (5) /api/v1/nodes/10.195.53.47:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 12.573791ms)
Feb 17 17:25:12.283: INFO: (6) /api/v1/nodes/10.195.53.47:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 13.373643ms)
Feb 17 17:25:12.296: INFO: (7) /api/v1/nodes/10.195.53.47:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 13.123812ms)
Feb 17 17:25:12.311: INFO: (8) /api/v1/nodes/10.195.53.47:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 14.538548ms)
Feb 17 17:25:12.324: INFO: (9) /api/v1/nodes/10.195.53.47:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 12.928246ms)
Feb 17 17:25:12.337: INFO: (10) /api/v1/nodes/10.195.53.47:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 12.589063ms)
Feb 17 17:25:12.354: INFO: (11) /api/v1/nodes/10.195.53.47:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 17.245924ms)
Feb 17 17:25:12.367: INFO: (12) /api/v1/nodes/10.195.53.47:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 13.065954ms)
Feb 17 17:25:12.382: INFO: (13) /api/v1/nodes/10.195.53.47:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 14.179698ms)
Feb 17 17:25:12.394: INFO: (14) /api/v1/nodes/10.195.53.47:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 12.705228ms)
Feb 17 17:25:12.408: INFO: (15) /api/v1/nodes/10.195.53.47:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 13.143083ms)
Feb 17 17:25:12.420: INFO: (16) /api/v1/nodes/10.195.53.47:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 12.719014ms)
Feb 17 17:25:12.434: INFO: (17) /api/v1/nodes/10.195.53.47:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 13.132622ms)
Feb 17 17:25:12.447: INFO: (18) /api/v1/nodes/10.195.53.47:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 12.760676ms)
Feb 17 17:25:12.459: INFO: (19) /api/v1/nodes/10.195.53.47:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 12.695166ms)
[AfterEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:25:12.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4737" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":280,"completed":234,"skipped":3782,"failed":0}

------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:25:12.487: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2727
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-2727
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating statefulset ss in namespace statefulset-2727
Feb 17 17:25:12.716: INFO: Found 0 stateful pods, waiting for 1
Feb 17 17:25:22.730: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Feb 17 17:25:22.780: INFO: Deleting all statefulset in ns statefulset-2727
Feb 17 17:25:22.790: INFO: Scaling statefulset ss to 0
Feb 17 17:25:42.835: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 17:25:42.844: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:25:42.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2727" for this suite.

• [SLOW TEST:30.418 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":280,"completed":235,"skipped":3782,"failed":0}
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:25:42.906: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8324
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 17:25:43.135: INFO: Waiting up to 5m0s for pod "downwardapi-volume-20e9e69b-eb1d-4138-a467-c254275cd93a" in namespace "projected-8324" to be "success or failure"
Feb 17 17:25:43.149: INFO: Pod "downwardapi-volume-20e9e69b-eb1d-4138-a467-c254275cd93a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.267578ms
Feb 17 17:25:45.160: INFO: Pod "downwardapi-volume-20e9e69b-eb1d-4138-a467-c254275cd93a": Phase="Running", Reason="", readiness=true. Elapsed: 2.024938064s
Feb 17 17:25:47.173: INFO: Pod "downwardapi-volume-20e9e69b-eb1d-4138-a467-c254275cd93a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037119635s
STEP: Saw pod success
Feb 17 17:25:47.173: INFO: Pod "downwardapi-volume-20e9e69b-eb1d-4138-a467-c254275cd93a" satisfied condition "success or failure"
Feb 17 17:25:47.183: INFO: Trying to get logs from node 10.195.53.9 pod downwardapi-volume-20e9e69b-eb1d-4138-a467-c254275cd93a container client-container: <nil>
STEP: delete the pod
Feb 17 17:25:47.297: INFO: Waiting for pod downwardapi-volume-20e9e69b-eb1d-4138-a467-c254275cd93a to disappear
Feb 17 17:25:47.307: INFO: Pod downwardapi-volume-20e9e69b-eb1d-4138-a467-c254275cd93a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:25:47.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8324" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":280,"completed":236,"skipped":3782,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:25:47.334: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1278
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating replication controller my-hostname-basic-7adda4df-f8a9-4c34-9af2-29cf8243f7be
Feb 17 17:25:47.549: INFO: Pod name my-hostname-basic-7adda4df-f8a9-4c34-9af2-29cf8243f7be: Found 0 pods out of 1
Feb 17 17:25:52.560: INFO: Pod name my-hostname-basic-7adda4df-f8a9-4c34-9af2-29cf8243f7be: Found 1 pods out of 1
Feb 17 17:25:52.560: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-7adda4df-f8a9-4c34-9af2-29cf8243f7be" are running
Feb 17 17:25:52.573: INFO: Pod "my-hostname-basic-7adda4df-f8a9-4c34-9af2-29cf8243f7be-x4bbr" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-17 17:25:47 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-17 17:25:49 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-17 17:25:49 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-17 17:25:47 +0000 UTC Reason: Message:}])
Feb 17 17:25:52.573: INFO: Trying to dial the pod
Feb 17 17:25:57.615: INFO: Controller my-hostname-basic-7adda4df-f8a9-4c34-9af2-29cf8243f7be: Got expected result from replica 1 [my-hostname-basic-7adda4df-f8a9-4c34-9af2-29cf8243f7be-x4bbr]: "my-hostname-basic-7adda4df-f8a9-4c34-9af2-29cf8243f7be-x4bbr", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:25:57.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1278" for this suite.

• [SLOW TEST:10.311 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":237,"skipped":3834,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:25:57.645: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9023
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating cluster-info
Feb 17 17:25:57.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 cluster-info'
Feb 17 17:25:58.188: INFO: stderr: ""
Feb 17 17:25:58.188: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\x1b[0;32mNodeLocalDNS\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/node-local-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:25:58.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9023" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":280,"completed":238,"skipped":3839,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:25:58.217: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6254
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 17:25:58.861: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 17:26:01.919: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:26:02.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6254" for this suite.
STEP: Destroying namespace "webhook-6254-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":280,"completed":239,"skipped":3844,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:26:02.190: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4334
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 17:26:03.110: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 17:26:06.175: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:26:18.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4334" for this suite.
STEP: Destroying namespace "webhook-4334-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.455 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":280,"completed":240,"skipped":3867,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:26:18.646: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7272
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Feb 17 17:26:21.444: INFO: Successfully updated pod "labelsupdatee64988f2-30ef-4948-a710-f4aa54118cdf"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:26:23.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7272" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":241,"skipped":3891,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:26:23.519: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5545
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:26:34.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5545" for this suite.

• [SLOW TEST:11.314 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":280,"completed":242,"skipped":3896,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:26:34.835: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8818
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-8818/configmap-test-6b401736-53f9-4db7-bef9-ece8e9311951
STEP: Creating a pod to test consume configMaps
Feb 17 17:26:35.063: INFO: Waiting up to 5m0s for pod "pod-configmaps-19adba2b-90c0-4002-82f6-039d130b1337" in namespace "configmap-8818" to be "success or failure"
Feb 17 17:26:35.074: INFO: Pod "pod-configmaps-19adba2b-90c0-4002-82f6-039d130b1337": Phase="Pending", Reason="", readiness=false. Elapsed: 10.951335ms
Feb 17 17:26:37.091: INFO: Pod "pod-configmaps-19adba2b-90c0-4002-82f6-039d130b1337": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028069306s
Feb 17 17:26:39.106: INFO: Pod "pod-configmaps-19adba2b-90c0-4002-82f6-039d130b1337": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043454285s
STEP: Saw pod success
Feb 17 17:26:39.106: INFO: Pod "pod-configmaps-19adba2b-90c0-4002-82f6-039d130b1337" satisfied condition "success or failure"
Feb 17 17:26:39.116: INFO: Trying to get logs from node 10.195.53.9 pod pod-configmaps-19adba2b-90c0-4002-82f6-039d130b1337 container env-test: <nil>
STEP: delete the pod
Feb 17 17:26:39.177: INFO: Waiting for pod pod-configmaps-19adba2b-90c0-4002-82f6-039d130b1337 to disappear
Feb 17 17:26:39.189: INFO: Pod pod-configmaps-19adba2b-90c0-4002-82f6-039d130b1337 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:26:39.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8818" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":243,"skipped":3910,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:26:39.219: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2215
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl replace
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1897
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 17 17:26:39.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-2215'
Feb 17 17:26:39.527: INFO: stderr: ""
Feb 17 17:26:39.527: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Feb 17 17:26:44.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 get pod e2e-test-httpd-pod --namespace=kubectl-2215 -o json'
Feb 17 17:26:44.684: INFO: stderr: ""
Feb 17 17:26:44.684: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2020-02-17T17:26:39Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2215\",\n        \"resourceVersion\": \"56166\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-2215/pods/e2e-test-httpd-pod\",\n        \"uid\": \"82e28982-c213-453b-be9d-d6f5e181702e\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-rdbqx\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.195.53.9\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-rdbqx\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-rdbqx\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-17T17:26:39Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-17T17:26:41Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-17T17:26:41Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-17T17:26:39Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://36e0f34f10d7d6e7bc7ff06782ddc9a6364d7906c269a804320f87ff64e6bc9a\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-02-17T17:26:40Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.195.53.9\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.20.65\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.20.65\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-02-17T17:26:39Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb 17 17:26:44.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 replace -f - --namespace=kubectl-2215'
Feb 17 17:26:45.006: INFO: stderr: ""
Feb 17 17:26:45.006: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1902
Feb 17 17:26:45.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 delete pods e2e-test-httpd-pod --namespace=kubectl-2215'
Feb 17 17:26:51.295: INFO: stderr: ""
Feb 17 17:26:51.295: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:26:51.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2215" for this suite.

• [SLOW TEST:12.233 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1893
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":280,"completed":244,"skipped":3911,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:26:51.453: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2912
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-5497
STEP: Creating secret with name secret-test-14d5b4d4-7f10-46d5-b2d4-642610a2e8f7
STEP: Creating a pod to test consume secrets
Feb 17 17:26:51.877: INFO: Waiting up to 5m0s for pod "pod-secrets-f0af12c5-7b4f-4151-ad9e-4b71a938f1f2" in namespace "secrets-2912" to be "success or failure"
Feb 17 17:26:51.887: INFO: Pod "pod-secrets-f0af12c5-7b4f-4151-ad9e-4b71a938f1f2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.253953ms
Feb 17 17:26:53.898: INFO: Pod "pod-secrets-f0af12c5-7b4f-4151-ad9e-4b71a938f1f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021053411s
STEP: Saw pod success
Feb 17 17:26:53.898: INFO: Pod "pod-secrets-f0af12c5-7b4f-4151-ad9e-4b71a938f1f2" satisfied condition "success or failure"
Feb 17 17:26:53.909: INFO: Trying to get logs from node 10.195.53.9 pod pod-secrets-f0af12c5-7b4f-4151-ad9e-4b71a938f1f2 container secret-volume-test: <nil>
STEP: delete the pod
Feb 17 17:26:53.965: INFO: Waiting for pod pod-secrets-f0af12c5-7b4f-4151-ad9e-4b71a938f1f2 to disappear
Feb 17 17:26:53.975: INFO: Pod pod-secrets-f0af12c5-7b4f-4151-ad9e-4b71a938f1f2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:26:53.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2912" for this suite.
STEP: Destroying namespace "secret-namespace-5497" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":280,"completed":245,"skipped":3930,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:26:54.019: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3285
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-830c0551-afa8-40f4-a08b-efe548dcf23e
STEP: Creating secret with name s-test-opt-upd-9ebfce78-451a-499b-8590-8554b14e47fc
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-830c0551-afa8-40f4-a08b-efe548dcf23e
STEP: Updating secret s-test-opt-upd-9ebfce78-451a-499b-8590-8554b14e47fc
STEP: Creating secret with name s-test-opt-create-b7efbef7-05c4-451e-a92b-dd35930e184a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:28:09.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3285" for this suite.

• [SLOW TEST:75.556 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":246,"skipped":3968,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:28:09.576: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3635
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating all guestbook components
Feb 17 17:28:09.772: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Feb 17 17:28:09.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 create -f - --namespace=kubectl-3635'
Feb 17 17:28:10.081: INFO: stderr: ""
Feb 17 17:28:10.081: INFO: stdout: "service/agnhost-slave created\n"
Feb 17 17:28:10.081: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Feb 17 17:28:10.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 create -f - --namespace=kubectl-3635'
Feb 17 17:28:10.409: INFO: stderr: ""
Feb 17 17:28:10.409: INFO: stdout: "service/agnhost-master created\n"
Feb 17 17:28:10.409: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 17 17:28:10.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 create -f - --namespace=kubectl-3635'
Feb 17 17:28:10.712: INFO: stderr: ""
Feb 17 17:28:10.712: INFO: stdout: "service/frontend created\n"
Feb 17 17:28:10.712: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Feb 17 17:28:10.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 create -f - --namespace=kubectl-3635'
Feb 17 17:28:10.896: INFO: stderr: ""
Feb 17 17:28:10.896: INFO: stdout: "deployment.apps/frontend created\n"
Feb 17 17:28:10.897: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 17 17:28:10.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 create -f - --namespace=kubectl-3635'
Feb 17 17:28:11.201: INFO: stderr: ""
Feb 17 17:28:11.201: INFO: stdout: "deployment.apps/agnhost-master created\n"
Feb 17 17:28:11.202: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 17 17:28:11.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 create -f - --namespace=kubectl-3635'
Feb 17 17:28:11.405: INFO: stderr: ""
Feb 17 17:28:11.405: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Feb 17 17:28:11.406: INFO: Waiting for all frontend pods to be Running.
Feb 17 17:28:16.456: INFO: Waiting for frontend to serve content.
Feb 17 17:28:16.484: INFO: Trying to add a new entry to the guestbook.
Feb 17 17:28:16.509: INFO: Verifying that added entry can be retrieved.
Feb 17 17:28:16.530: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Feb 17 17:28:21.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 delete --grace-period=0 --force -f - --namespace=kubectl-3635'
Feb 17 17:28:21.705: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 17 17:28:21.706: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Feb 17 17:28:21.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 delete --grace-period=0 --force -f - --namespace=kubectl-3635'
Feb 17 17:28:21.859: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 17 17:28:21.859: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 17 17:28:21.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 delete --grace-period=0 --force -f - --namespace=kubectl-3635'
Feb 17 17:28:22.009: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 17 17:28:22.009: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 17 17:28:22.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 delete --grace-period=0 --force -f - --namespace=kubectl-3635'
Feb 17 17:28:22.124: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 17 17:28:22.124: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 17 17:28:22.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 delete --grace-period=0 --force -f - --namespace=kubectl-3635'
Feb 17 17:28:22.256: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 17 17:28:22.256: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 17 17:28:22.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 delete --grace-period=0 --force -f - --namespace=kubectl-3635'
Feb 17 17:28:22.396: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 17 17:28:22.397: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:28:22.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3635" for this suite.

• [SLOW TEST:12.850 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:386
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":280,"completed":247,"skipped":3973,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:28:22.427: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-307
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 17:28:22.630: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:28:23.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-307" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":280,"completed":248,"skipped":3981,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:28:23.710: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8544
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-9627b08a-9e09-43a6-9e88-05f90e04bc54
STEP: Creating a pod to test consume configMaps
Feb 17 17:28:23.936: INFO: Waiting up to 5m0s for pod "pod-configmaps-44333ab7-9afa-4cbe-b896-217bcfec9ce3" in namespace "configmap-8544" to be "success or failure"
Feb 17 17:28:23.948: INFO: Pod "pod-configmaps-44333ab7-9afa-4cbe-b896-217bcfec9ce3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.50299ms
Feb 17 17:28:25.960: INFO: Pod "pod-configmaps-44333ab7-9afa-4cbe-b896-217bcfec9ce3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023540116s
STEP: Saw pod success
Feb 17 17:28:25.960: INFO: Pod "pod-configmaps-44333ab7-9afa-4cbe-b896-217bcfec9ce3" satisfied condition "success or failure"
Feb 17 17:28:25.971: INFO: Trying to get logs from node 10.195.53.9 pod pod-configmaps-44333ab7-9afa-4cbe-b896-217bcfec9ce3 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 17:28:26.027: INFO: Waiting for pod pod-configmaps-44333ab7-9afa-4cbe-b896-217bcfec9ce3 to disappear
Feb 17 17:28:26.037: INFO: Pod pod-configmaps-44333ab7-9afa-4cbe-b896-217bcfec9ce3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:28:26.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8544" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":249,"skipped":3990,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:28:26.066: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6369
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Feb 17 17:28:26.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 create -f - --namespace=kubectl-6369'
Feb 17 17:28:26.561: INFO: stderr: ""
Feb 17 17:28:26.561: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Feb 17 17:28:27.573: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 17 17:28:27.573: INFO: Found 0 / 1
Feb 17 17:28:28.571: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 17 17:28:28.571: INFO: Found 0 / 1
Feb 17 17:28:29.573: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 17 17:28:29.573: INFO: Found 1 / 1
Feb 17 17:28:29.573: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb 17 17:28:29.586: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 17 17:28:29.586: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 17 17:28:29.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 patch pod agnhost-master-jcbj5 --namespace=kubectl-6369 -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 17 17:28:29.695: INFO: stderr: ""
Feb 17 17:28:29.695: INFO: stdout: "pod/agnhost-master-jcbj5 patched\n"
STEP: checking annotations
Feb 17 17:28:29.706: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 17 17:28:29.706: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:28:29.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6369" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":280,"completed":250,"skipped":4019,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:28:29.737: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-6030
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb 17 17:28:31.997: INFO: &Pod{ObjectMeta:{send-events-f3858cf4-2e9d-4f3c-bf80-53d28b6107a1  events-6030 /api/v1/namespaces/events-6030/pods/send-events-f3858cf4-2e9d-4f3c-bf80-53d28b6107a1 3f4d3aab-e93f-4fe8-91be-ce97149ea63e 56946 0 2020-02-17 17:28:29 +0000 UTC <nil> <nil> map[name:foo time:929990303] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8b82z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8b82z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8b82z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:28:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:28:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:28:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:28:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.9,PodIP:172.30.20.91,StartTime:2020-02-17 17:28:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 17:28:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:containerd://f9eba1d85f085c883ae4a73125fa2b771a73c53b5d1af66243e82dd7a4102623,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.20.91,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Feb 17 17:28:34.005: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb 17 17:28:36.013: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:28:36.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6030" for this suite.

• [SLOW TEST:6.328 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":280,"completed":251,"skipped":4046,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:28:36.066: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7678
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 17:28:36.276: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f3abe1c8-b17d-4a5f-8a9f-a5d3ebe01661" in namespace "projected-7678" to be "success or failure"
Feb 17 17:28:36.288: INFO: Pod "downwardapi-volume-f3abe1c8-b17d-4a5f-8a9f-a5d3ebe01661": Phase="Pending", Reason="", readiness=false. Elapsed: 11.906576ms
Feb 17 17:28:38.300: INFO: Pod "downwardapi-volume-f3abe1c8-b17d-4a5f-8a9f-a5d3ebe01661": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024349528s
STEP: Saw pod success
Feb 17 17:28:38.300: INFO: Pod "downwardapi-volume-f3abe1c8-b17d-4a5f-8a9f-a5d3ebe01661" satisfied condition "success or failure"
Feb 17 17:28:38.312: INFO: Trying to get logs from node 10.195.53.9 pod downwardapi-volume-f3abe1c8-b17d-4a5f-8a9f-a5d3ebe01661 container client-container: <nil>
STEP: delete the pod
Feb 17 17:28:38.372: INFO: Waiting for pod downwardapi-volume-f3abe1c8-b17d-4a5f-8a9f-a5d3ebe01661 to disappear
Feb 17 17:28:38.382: INFO: Pod downwardapi-volume-f3abe1c8-b17d-4a5f-8a9f-a5d3ebe01661 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:28:38.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7678" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":252,"skipped":4055,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:28:38.413: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2186
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 17:28:39.334: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 17:28:42.394: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:28:42.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2186" for this suite.
STEP: Destroying namespace "webhook-2186-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":280,"completed":253,"skipped":4059,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:28:42.568: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5394
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 17:29:02.810: INFO: Container started at 2020-02-17 17:28:44 +0000 UTC, pod became ready at 2020-02-17 17:29:02 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:29:02.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5394" for this suite.

• [SLOW TEST:20.269 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":280,"completed":254,"skipped":4087,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:29:02.837: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5068
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Feb 17 17:29:03.033: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 17 17:29:03.070: INFO: Waiting for terminating namespaces to be deleted...
Feb 17 17:29:03.079: INFO: 
Logging pods the kubelet thinks is on node 10.195.53.14 before test
Feb 17 17:29:03.143: INFO: calico-node-94c2k from kube-system started at 2020-02-17 14:36:27 +0000 UTC (1 container statuses recorded)
Feb 17 17:29:03.143: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 17:29:03.143: INFO: calico-kube-controllers-866cf6f69c-fm97n from kube-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 17:29:03.143: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 17 17:29:03.143: INFO: kubernetes-dashboard-bbcc67fc-x8mnz from kube-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 17:29:03.143: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 17 17:29:03.143: INFO: ibm-cloud-provider-ip-135-90-78-211-7555ccd494-hhqb2 from ibm-system started at 2020-02-17 14:41:58 +0000 UTC (1 container statuses recorded)
Feb 17 17:29:03.143: INFO: 	Container ibm-cloud-provider-ip-135-90-78-211 ready: true, restart count 0
Feb 17 17:29:03.143: INFO: ibm-file-plugin-6694f985b8-s5cjp from kube-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 17:29:03.143: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Feb 17 17:29:03.143: INFO: vpn-b5cd9dc8b-fp9qk from kube-system started at 2020-02-17 17:05:48 +0000 UTC (1 container statuses recorded)
Feb 17 17:29:03.143: INFO: 	Container vpn ready: true, restart count 0
Feb 17 17:29:03.143: INFO: public-crbp5a3ais0mfj6cn3dl60-alb1-79849b9bd6-4z2xg from kube-system started at 2020-02-17 14:44:46 +0000 UTC (4 container statuses recorded)
Feb 17 17:29:03.143: INFO: 	Container ingress-auth-1 ready: true, restart count 1
Feb 17 17:29:03.143: INFO: 	Container ingress-auth-2 ready: true, restart count 1
Feb 17 17:29:03.143: INFO: 	Container ingress-auth-3 ready: true, restart count 1
Feb 17 17:29:03.143: INFO: 	Container nginx-ingress ready: true, restart count 0
Feb 17 17:29:03.143: INFO: sonobuoy-systemd-logs-daemon-set-bc697ed487d7421d-st8jd from sonobuoy started at 2020-02-17 16:14:42 +0000 UTC (2 container statuses recorded)
Feb 17 17:29:03.143: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 17 17:29:03.143: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 17:29:03.143: INFO: ibm-storage-watcher-74f895486d-k5zxb from kube-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 17:29:03.143: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Feb 17 17:29:03.143: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-02-17 14:39:39 +0000 UTC (1 container statuses recorded)
Feb 17 17:29:03.143: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Feb 17 17:29:03.143: INFO: catalog-operator-7d9cb6cf74-qr9nb from ibm-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 17:29:03.143: INFO: 	Container catalog-operator ready: true, restart count 0
Feb 17 17:29:03.143: INFO: coredns-autoscaler-7dddb6f87c-g6nrf from kube-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 17:29:03.143: INFO: 	Container autoscaler ready: true, restart count 0
Feb 17 17:29:03.143: INFO: metrics-server-647cf95c9b-5j4cj from kube-system started at 2020-02-17 17:05:48 +0000 UTC (2 container statuses recorded)
Feb 17 17:29:03.143: INFO: 	Container metrics-server ready: true, restart count 0
Feb 17 17:29:03.143: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Feb 17 17:29:03.143: INFO: ibm-master-proxy-static-10.195.53.14 from kube-system started at 2020-02-17 14:36:26 +0000 UTC (2 container statuses recorded)
Feb 17 17:29:03.143: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 17:29:03.143: INFO: 	Container pause ready: true, restart count 0
Feb 17 17:29:03.143: INFO: ibm-keepalived-watcher-8pbd8 from kube-system started at 2020-02-17 14:36:27 +0000 UTC (1 container statuses recorded)
Feb 17 17:29:03.143: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 17:29:03.143: INFO: coredns-5b567488dd-kckdk from kube-system started at 2020-02-17 14:59:13 +0000 UTC (1 container statuses recorded)
Feb 17 17:29:03.143: INFO: 	Container coredns ready: true, restart count 0
Feb 17 17:29:03.143: INFO: 
Logging pods the kubelet thinks is on node 10.195.53.47 before test
Feb 17 17:29:03.215: INFO: coredns-5b567488dd-qtx5q from kube-system started at 2020-02-17 14:59:13 +0000 UTC (1 container statuses recorded)
Feb 17 17:29:03.215: INFO: 	Container coredns ready: true, restart count 0
Feb 17 17:29:03.215: INFO: ibm-master-proxy-static-10.195.53.47 from kube-system started at 2020-02-17 14:36:20 +0000 UTC (2 container statuses recorded)
Feb 17 17:29:03.215: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 17:29:03.215: INFO: 	Container pause ready: true, restart count 0
Feb 17 17:29:03.215: INFO: ibm-keepalived-watcher-bl4pr from kube-system started at 2020-02-17 14:36:22 +0000 UTC (1 container statuses recorded)
Feb 17 17:29:03.215: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 17:29:03.215: INFO: calico-node-kmwlv from kube-system started at 2020-02-17 14:36:22 +0000 UTC (1 container statuses recorded)
Feb 17 17:29:03.215: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 17:29:03.215: INFO: addon-catalog-source-c5sks from ibm-system started at 2020-02-17 14:38:40 +0000 UTC (1 container statuses recorded)
Feb 17 17:29:03.215: INFO: 	Container configmap-registry-server ready: true, restart count 0
Feb 17 17:29:03.215: INFO: ibm-cloud-provider-ip-135-90-78-211-7555ccd494-fz6gg from ibm-system started at 2020-02-17 14:41:58 +0000 UTC (1 container statuses recorded)
Feb 17 17:29:03.215: INFO: 	Container ibm-cloud-provider-ip-135-90-78-211 ready: true, restart count 0
Feb 17 17:29:03.215: INFO: sonobuoy-e2e-job-c9def2901e004587 from sonobuoy started at 2020-02-17 16:14:42 +0000 UTC (2 container statuses recorded)
Feb 17 17:29:03.215: INFO: 	Container e2e ready: true, restart count 0
Feb 17 17:29:03.215: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 17:29:03.215: INFO: olm-operator-587889d75d-29ltv from ibm-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 17:29:03.215: INFO: 	Container olm-operator ready: true, restart count 0
Feb 17 17:29:03.215: INFO: dashboard-metrics-scraper-69468c6b44-gqtkk from kube-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 17:29:03.215: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 17 17:29:03.215: INFO: sonobuoy-systemd-logs-daemon-set-bc697ed487d7421d-5b85k from sonobuoy started at 2020-02-17 16:14:42 +0000 UTC (2 container statuses recorded)
Feb 17 17:29:03.215: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 17 17:29:03.215: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 17:29:03.215: INFO: public-crbp5a3ais0mfj6cn3dl60-alb1-79849b9bd6-8r8qk from kube-system started at 2020-02-17 14:44:46 +0000 UTC (4 container statuses recorded)
Feb 17 17:29:03.215: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Feb 17 17:29:03.215: INFO: 	Container ingress-auth-2 ready: true, restart count 1
Feb 17 17:29:03.215: INFO: 	Container ingress-auth-3 ready: true, restart count 1
Feb 17 17:29:03.215: INFO: 	Container nginx-ingress ready: true, restart count 0
Feb 17 17:29:03.215: INFO: coredns-5b567488dd-n2wt7 from kube-system started at 2020-02-17 17:05:48 +0000 UTC (1 container statuses recorded)
Feb 17 17:29:03.215: INFO: 	Container coredns ready: true, restart count 0
Feb 17 17:29:03.215: INFO: 
Logging pods the kubelet thinks is on node 10.195.53.9 before test
Feb 17 17:29:03.260: INFO: ibm-master-proxy-static-10.195.53.9 from kube-system started at 2020-02-17 14:36:11 +0000 UTC (2 container statuses recorded)
Feb 17 17:29:03.260: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 17:29:03.260: INFO: 	Container pause ready: true, restart count 0
Feb 17 17:29:03.260: INFO: send-events-f3858cf4-2e9d-4f3c-bf80-53d28b6107a1 from events-6030 started at 2020-02-17 17:28:29 +0000 UTC (1 container statuses recorded)
Feb 17 17:29:03.260: INFO: 	Container p ready: true, restart count 0
Feb 17 17:29:03.260: INFO: test-webserver-0d86bffa-c2ac-4b2b-a563-34a94460c3a2 from container-probe-5394 started at 2020-02-17 17:28:42 +0000 UTC (1 container statuses recorded)
Feb 17 17:29:03.260: INFO: 	Container test-webserver ready: true, restart count 0
Feb 17 17:29:03.260: INFO: calico-node-r8w4v from kube-system started at 2020-02-17 14:36:12 +0000 UTC (1 container statuses recorded)
Feb 17 17:29:03.260: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 17:29:03.260: INFO: ibm-keepalived-watcher-7c6j2 from kube-system started at 2020-02-17 14:36:12 +0000 UTC (1 container statuses recorded)
Feb 17 17:29:03.260: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 17:29:03.260: INFO: sonobuoy-systemd-logs-daemon-set-bc697ed487d7421d-ntb66 from sonobuoy started at 2020-02-17 16:14:42 +0000 UTC (2 container statuses recorded)
Feb 17 17:29:03.260: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 17 17:29:03.260: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 17:29:03.260: INFO: sonobuoy from sonobuoy started at 2020-02-17 16:14:33 +0000 UTC (1 container statuses recorded)
Feb 17 17:29:03.260: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-dbe0b27a-b21f-4ade-8798-12800f137d9c 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-dbe0b27a-b21f-4ade-8798-12800f137d9c off the node 10.195.53.9
STEP: verifying the node doesn't have the label kubernetes.io/e2e-dbe0b27a-b21f-4ade-8798-12800f137d9c
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:29:07.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5068" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":280,"completed":255,"skipped":4090,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:29:07.490: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4604
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-421a7c03-0ea3-4664-a409-fa9da331b39d
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:29:11.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4604" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":256,"skipped":4119,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:29:11.827: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-5684
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 17:29:12.019: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Creating first CR 
Feb 17 17:29:12.666: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-17T17:29:12Z generation:1 name:name1 resourceVersion:57335 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:a604f7f1-6e56-485b-8d70-b42e4ec364b4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Feb 17 17:29:22.681: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-17T17:29:22Z generation:1 name:name2 resourceVersion:57406 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:ac260e8b-1547-4038-b44f-ffdd9fe4b2d7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Feb 17 17:29:32.696: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-17T17:29:12Z generation:2 name:name1 resourceVersion:57440 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:a604f7f1-6e56-485b-8d70-b42e4ec364b4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Feb 17 17:29:42.711: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-17T17:29:22Z generation:2 name:name2 resourceVersion:57465 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:ac260e8b-1547-4038-b44f-ffdd9fe4b2d7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Feb 17 17:29:52.740: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-17T17:29:12Z generation:2 name:name1 resourceVersion:57496 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:a604f7f1-6e56-485b-8d70-b42e4ec364b4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Feb 17 17:30:02.771: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-17T17:29:22Z generation:2 name:name2 resourceVersion:57521 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:ac260e8b-1547-4038-b44f-ffdd9fe4b2d7] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:30:13.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-5684" for this suite.

• [SLOW TEST:61.498 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:41
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":280,"completed":257,"skipped":4121,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:30:13.327: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6421
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:30:15.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6421" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":280,"completed":258,"skipped":4134,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:30:15.650: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-666
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-f9250c8c-62f7-4b56-b022-0e85ebe6a926
STEP: Creating a pod to test consume configMaps
Feb 17 17:30:15.881: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c46c8256-3999-48d1-84c0-5d7f4f0ddea0" in namespace "projected-666" to be "success or failure"
Feb 17 17:30:15.891: INFO: Pod "pod-projected-configmaps-c46c8256-3999-48d1-84c0-5d7f4f0ddea0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.488633ms
Feb 17 17:30:17.903: INFO: Pod "pod-projected-configmaps-c46c8256-3999-48d1-84c0-5d7f4f0ddea0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022456228s
Feb 17 17:30:19.916: INFO: Pod "pod-projected-configmaps-c46c8256-3999-48d1-84c0-5d7f4f0ddea0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034690668s
STEP: Saw pod success
Feb 17 17:30:19.916: INFO: Pod "pod-projected-configmaps-c46c8256-3999-48d1-84c0-5d7f4f0ddea0" satisfied condition "success or failure"
Feb 17 17:30:19.927: INFO: Trying to get logs from node 10.195.53.9 pod pod-projected-configmaps-c46c8256-3999-48d1-84c0-5d7f4f0ddea0 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 17:30:19.991: INFO: Waiting for pod pod-projected-configmaps-c46c8256-3999-48d1-84c0-5d7f4f0ddea0 to disappear
Feb 17 17:30:20.002: INFO: Pod pod-projected-configmaps-c46c8256-3999-48d1-84c0-5d7f4f0ddea0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:30:20.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-666" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":280,"completed":259,"skipped":4146,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:30:20.029: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-709
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 17:30:20.245: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a4853100-8be6-4f47-abe4-7edcb00b3aec" in namespace "downward-api-709" to be "success or failure"
Feb 17 17:30:20.256: INFO: Pod "downwardapi-volume-a4853100-8be6-4f47-abe4-7edcb00b3aec": Phase="Pending", Reason="", readiness=false. Elapsed: 10.903761ms
Feb 17 17:30:22.477: INFO: Pod "downwardapi-volume-a4853100-8be6-4f47-abe4-7edcb00b3aec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.232013383s
STEP: Saw pod success
Feb 17 17:30:22.477: INFO: Pod "downwardapi-volume-a4853100-8be6-4f47-abe4-7edcb00b3aec" satisfied condition "success or failure"
Feb 17 17:30:22.488: INFO: Trying to get logs from node 10.195.53.9 pod downwardapi-volume-a4853100-8be6-4f47-abe4-7edcb00b3aec container client-container: <nil>
STEP: delete the pod
Feb 17 17:30:22.548: INFO: Waiting for pod downwardapi-volume-a4853100-8be6-4f47-abe4-7edcb00b3aec to disappear
Feb 17 17:30:22.559: INFO: Pod downwardapi-volume-a4853100-8be6-4f47-abe4-7edcb00b3aec no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:30:22.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-709" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":260,"skipped":4161,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:30:22.587: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1476
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 17 17:30:22.803: INFO: Waiting up to 5m0s for pod "pod-17ba2e91-5427-4ff2-8a46-9fb79a4944fb" in namespace "emptydir-1476" to be "success or failure"
Feb 17 17:30:22.815: INFO: Pod "pod-17ba2e91-5427-4ff2-8a46-9fb79a4944fb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.209248ms
Feb 17 17:30:24.827: INFO: Pod "pod-17ba2e91-5427-4ff2-8a46-9fb79a4944fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023909052s
Feb 17 17:30:26.840: INFO: Pod "pod-17ba2e91-5427-4ff2-8a46-9fb79a4944fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036939114s
STEP: Saw pod success
Feb 17 17:30:26.840: INFO: Pod "pod-17ba2e91-5427-4ff2-8a46-9fb79a4944fb" satisfied condition "success or failure"
Feb 17 17:30:26.851: INFO: Trying to get logs from node 10.195.53.9 pod pod-17ba2e91-5427-4ff2-8a46-9fb79a4944fb container test-container: <nil>
STEP: delete the pod
Feb 17 17:30:26.906: INFO: Waiting for pod pod-17ba2e91-5427-4ff2-8a46-9fb79a4944fb to disappear
Feb 17 17:30:26.916: INFO: Pod pod-17ba2e91-5427-4ff2-8a46-9fb79a4944fb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:30:26.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1476" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":261,"skipped":4172,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:30:26.944: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6489
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating api versions
Feb 17 17:30:27.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855842601 api-versions'
Feb 17 17:30:27.252: INFO: stderr: ""
Feb 17 17:30:27.252: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:30:27.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6489" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":280,"completed":262,"skipped":4181,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:30:27.280: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4022
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:31:27.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4022" for this suite.

• [SLOW TEST:60.255 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":280,"completed":263,"skipped":4264,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:31:27.536: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-9676
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
Feb 17 17:31:28.297: INFO: created pod pod-service-account-defaultsa
Feb 17 17:31:28.297: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 17 17:31:28.310: INFO: created pod pod-service-account-mountsa
Feb 17 17:31:28.310: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 17 17:31:28.324: INFO: created pod pod-service-account-nomountsa
Feb 17 17:31:28.324: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 17 17:31:28.336: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 17 17:31:28.336: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 17 17:31:28.349: INFO: created pod pod-service-account-mountsa-mountspec
Feb 17 17:31:28.349: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 17 17:31:28.363: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 17 17:31:28.363: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 17 17:31:28.376: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 17 17:31:28.376: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 17 17:31:28.388: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 17 17:31:28.388: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 17 17:31:28.401: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 17 17:31:28.401: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:31:28.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9676" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":280,"completed":264,"skipped":4278,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:31:28.433: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1664
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 17:31:28.647: INFO: Waiting up to 5m0s for pod "downwardapi-volume-92faffe1-ee9d-4513-991f-bda79b754134" in namespace "downward-api-1664" to be "success or failure"
Feb 17 17:31:28.659: INFO: Pod "downwardapi-volume-92faffe1-ee9d-4513-991f-bda79b754134": Phase="Pending", Reason="", readiness=false. Elapsed: 12.409043ms
Feb 17 17:31:30.672: INFO: Pod "downwardapi-volume-92faffe1-ee9d-4513-991f-bda79b754134": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024606893s
STEP: Saw pod success
Feb 17 17:31:30.672: INFO: Pod "downwardapi-volume-92faffe1-ee9d-4513-991f-bda79b754134" satisfied condition "success or failure"
Feb 17 17:31:30.683: INFO: Trying to get logs from node 10.195.53.47 pod downwardapi-volume-92faffe1-ee9d-4513-991f-bda79b754134 container client-container: <nil>
STEP: delete the pod
Feb 17 17:31:30.787: INFO: Waiting for pod downwardapi-volume-92faffe1-ee9d-4513-991f-bda79b754134 to disappear
Feb 17 17:31:30.797: INFO: Pod downwardapi-volume-92faffe1-ee9d-4513-991f-bda79b754134 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:31:30.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1664" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":265,"skipped":4279,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:31:30.828: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-1236
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:172
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating server pod server in namespace prestop-1236
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1236
STEP: Deleting pre-stop pod
Feb 17 17:31:44.153: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:31:44.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1236" for this suite.

• [SLOW TEST:13.377 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":280,"completed":266,"skipped":4293,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:31:44.210: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-54
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 17:31:44.425: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-b5b144f3-427d-4002-b604-2885d2641885" in namespace "security-context-test-54" to be "success or failure"
Feb 17 17:31:44.435: INFO: Pod "busybox-privileged-false-b5b144f3-427d-4002-b604-2885d2641885": Phase="Pending", Reason="", readiness=false. Elapsed: 10.656412ms
Feb 17 17:31:46.447: INFO: Pod "busybox-privileged-false-b5b144f3-427d-4002-b604-2885d2641885": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022280414s
Feb 17 17:31:46.447: INFO: Pod "busybox-privileged-false-b5b144f3-427d-4002-b604-2885d2641885" satisfied condition "success or failure"
Feb 17 17:31:46.475: INFO: Got logs for pod "busybox-privileged-false-b5b144f3-427d-4002-b604-2885d2641885": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:31:46.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-54" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":267,"skipped":4331,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:31:46.503: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2625
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:32:02.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2625" for this suite.

• [SLOW TEST:16.333 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":280,"completed":268,"skipped":4344,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:32:02.837: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9423
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-9423
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 17 17:32:03.030: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 17 17:32:27.266: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.20.109:8080/dial?request=hostname&protocol=http&host=172.30.89.206&port=8080&tries=1'] Namespace:pod-network-test-9423 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 17:32:27.266: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 17:32:27.416: INFO: Waiting for responses: map[]
Feb 17 17:32:27.427: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.20.109:8080/dial?request=hostname&protocol=http&host=172.30.117.206&port=8080&tries=1'] Namespace:pod-network-test-9423 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 17:32:27.427: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 17:32:27.584: INFO: Waiting for responses: map[]
Feb 17 17:32:27.595: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.20.109:8080/dial?request=hostname&protocol=http&host=172.30.20.110&port=8080&tries=1'] Namespace:pod-network-test-9423 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 17:32:27.595: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
Feb 17 17:32:27.757: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:32:27.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9423" for this suite.

• [SLOW TEST:24.953 seconds]
[sig-network] Networking
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":269,"skipped":4374,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:32:27.792: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-6643
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:46
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:32:27.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-6643" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":280,"completed":270,"skipped":4389,"failed":0}

------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:32:28.026: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4718
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 17:32:28.242: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb 17 17:32:33.255: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 17 17:32:33.255: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 17 17:32:35.266: INFO: Creating deployment "test-rollover-deployment"
Feb 17 17:32:35.290: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 17 17:32:37.314: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 17 17:32:37.335: INFO: Ensure that both replica sets have 1 created replica
Feb 17 17:32:37.355: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 17 17:32:37.377: INFO: Updating deployment test-rollover-deployment
Feb 17 17:32:37.378: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 17 17:32:39.400: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 17 17:32:39.422: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 17 17:32:39.444: INFO: all replica sets need to contain the pod-template-hash label
Feb 17 17:32:39.444: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557555, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557555, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557559, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557555, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:32:41.467: INFO: all replica sets need to contain the pod-template-hash label
Feb 17 17:32:41.468: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557555, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557555, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557559, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557555, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:32:43.470: INFO: all replica sets need to contain the pod-template-hash label
Feb 17 17:32:43.470: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557555, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557555, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557559, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557555, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:32:45.467: INFO: all replica sets need to contain the pod-template-hash label
Feb 17 17:32:45.467: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557555, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557555, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557559, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557555, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:32:47.475: INFO: all replica sets need to contain the pod-template-hash label
Feb 17 17:32:47.475: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557555, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557555, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557559, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557555, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:32:49.468: INFO: 
Feb 17 17:32:49.468: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Feb 17 17:32:49.501: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-4718 /apis/apps/v1/namespaces/deployment-4718/deployments/test-rollover-deployment 4d75a0ae-be6f-4425-8264-a51a1231147f 58718 2 2020-02-17 17:32:35 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0039a9198 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-02-17 17:32:35 +0000 UTC,LastTransitionTime:2020-02-17 17:32:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-574d6dfbff" has successfully progressed.,LastUpdateTime:2020-02-17 17:32:49 +0000 UTC,LastTransitionTime:2020-02-17 17:32:35 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 17 17:32:49.536: INFO: New ReplicaSet "test-rollover-deployment-574d6dfbff" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-574d6dfbff  deployment-4718 /apis/apps/v1/namespaces/deployment-4718/replicasets/test-rollover-deployment-574d6dfbff 3c91450d-35a4-437f-8c0e-9a98141f181c 58708 2 2020-02-17 17:32:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 4d75a0ae-be6f-4425-8264-a51a1231147f 0xc0039a9627 0xc0039a9628}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 574d6dfbff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0039a9698 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 17 17:32:49.536: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 17 17:32:49.536: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4718 /apis/apps/v1/namespaces/deployment-4718/replicasets/test-rollover-controller a11b8d71-5911-4b45-8180-a747c6ac0c71 58717 2 2020-02-17 17:32:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 4d75a0ae-be6f-4425-8264-a51a1231147f 0xc0039a9557 0xc0039a9558}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0039a95b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 17 17:32:49.536: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-4718 /apis/apps/v1/namespaces/deployment-4718/replicasets/test-rollover-deployment-f6c94f66c 0a52cf12-a89a-4f52-9fc3-67adae11f74f 58659 2 2020-02-17 17:32:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 4d75a0ae-be6f-4425-8264-a51a1231147f 0xc0039a9700 0xc0039a9701}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0039a9778 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 17 17:32:49.547: INFO: Pod "test-rollover-deployment-574d6dfbff-gs86p" is available:
&Pod{ObjectMeta:{test-rollover-deployment-574d6dfbff-gs86p test-rollover-deployment-574d6dfbff- deployment-4718 /api/v1/namespaces/deployment-4718/pods/test-rollover-deployment-574d6dfbff-gs86p 4ceaf656-629c-49c4-a646-93cec834e1f4 58677 0 2020-02-17 17:32:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-574d6dfbff 3c91450d-35a4-437f-8c0e-9a98141f181c 0xc0039a9cc7 0xc0039a9cc8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5qk6p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5qk6p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5qk6p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.195.53.9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:32:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:32:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:32:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:32:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.53.9,PodIP:172.30.20.105,StartTime:2020-02-17 17:32:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 17:32:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:containerd://96299eba5a5851b36a0af73ab4286385eb388bf3d4a80b7dcf77274f106bc67d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.20.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:32:49.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4718" for this suite.

• [SLOW TEST:21.549 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":280,"completed":271,"skipped":4389,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:32:49.575: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5959
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 17:32:49.791: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9eb32936-e0ec-445e-bcb3-97774df4cdf9" in namespace "projected-5959" to be "success or failure"
Feb 17 17:32:49.802: INFO: Pod "downwardapi-volume-9eb32936-e0ec-445e-bcb3-97774df4cdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.465896ms
Feb 17 17:32:51.813: INFO: Pod "downwardapi-volume-9eb32936-e0ec-445e-bcb3-97774df4cdf9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021901388s
STEP: Saw pod success
Feb 17 17:32:51.813: INFO: Pod "downwardapi-volume-9eb32936-e0ec-445e-bcb3-97774df4cdf9" satisfied condition "success or failure"
Feb 17 17:32:51.828: INFO: Trying to get logs from node 10.195.53.9 pod downwardapi-volume-9eb32936-e0ec-445e-bcb3-97774df4cdf9 container client-container: <nil>
STEP: delete the pod
Feb 17 17:32:51.884: INFO: Waiting for pod downwardapi-volume-9eb32936-e0ec-445e-bcb3-97774df4cdf9 to disappear
Feb 17 17:32:51.894: INFO: Pod downwardapi-volume-9eb32936-e0ec-445e-bcb3-97774df4cdf9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:32:51.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5959" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":272,"skipped":4404,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:32:51.923: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3833
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb 17 17:32:52.143: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3833 /api/v1/namespaces/watch-3833/configmaps/e2e-watch-test-watch-closed 15c1daf8-6e2a-48dc-9034-f7fd41e33220 58765 0 2020-02-17 17:32:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 17 17:32:52.143: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3833 /api/v1/namespaces/watch-3833/configmaps/e2e-watch-test-watch-closed 15c1daf8-6e2a-48dc-9034-f7fd41e33220 58766 0 2020-02-17 17:32:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb 17 17:32:52.184: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3833 /api/v1/namespaces/watch-3833/configmaps/e2e-watch-test-watch-closed 15c1daf8-6e2a-48dc-9034-f7fd41e33220 58767 0 2020-02-17 17:32:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 17 17:32:52.184: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3833 /api/v1/namespaces/watch-3833/configmaps/e2e-watch-test-watch-closed 15c1daf8-6e2a-48dc-9034-f7fd41e33220 58768 0 2020-02-17 17:32:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:32:52.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3833" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":280,"completed":273,"skipped":4420,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:32:52.209: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2926
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 17 17:32:55.002: INFO: Successfully updated pod "pod-update-activedeadlineseconds-7de9eb47-5777-4452-bada-9037fcaa4f9c"
Feb 17 17:32:55.002: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-7de9eb47-5777-4452-bada-9037fcaa4f9c" in namespace "pods-2926" to be "terminated due to deadline exceeded"
Feb 17 17:32:55.011: INFO: Pod "pod-update-activedeadlineseconds-7de9eb47-5777-4452-bada-9037fcaa4f9c": Phase="Running", Reason="", readiness=true. Elapsed: 9.409623ms
Feb 17 17:32:57.022: INFO: Pod "pod-update-activedeadlineseconds-7de9eb47-5777-4452-bada-9037fcaa4f9c": Phase="Running", Reason="", readiness=true. Elapsed: 2.020220802s
Feb 17 17:32:59.034: INFO: Pod "pod-update-activedeadlineseconds-7de9eb47-5777-4452-bada-9037fcaa4f9c": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.032506905s
Feb 17 17:32:59.034: INFO: Pod "pod-update-activedeadlineseconds-7de9eb47-5777-4452-bada-9037fcaa4f9c" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:32:59.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2926" for this suite.

• [SLOW TEST:6.853 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":280,"completed":274,"skipped":4441,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:32:59.065: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3364
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Feb 17 17:32:59.266: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 17 17:32:59.297: INFO: Waiting for terminating namespaces to be deleted...
Feb 17 17:32:59.306: INFO: 
Logging pods the kubelet thinks is on node 10.195.53.14 before test
Feb 17 17:32:59.590: INFO: public-crbp5a3ais0mfj6cn3dl60-alb1-79849b9bd6-4z2xg from kube-system started at 2020-02-17 14:44:46 +0000 UTC (4 container statuses recorded)
Feb 17 17:32:59.590: INFO: 	Container ingress-auth-1 ready: true, restart count 1
Feb 17 17:32:59.590: INFO: 	Container ingress-auth-2 ready: true, restart count 1
Feb 17 17:32:59.590: INFO: 	Container ingress-auth-3 ready: true, restart count 1
Feb 17 17:32:59.590: INFO: 	Container nginx-ingress ready: true, restart count 0
Feb 17 17:32:59.590: INFO: sonobuoy-systemd-logs-daemon-set-bc697ed487d7421d-st8jd from sonobuoy started at 2020-02-17 16:14:42 +0000 UTC (2 container statuses recorded)
Feb 17 17:32:59.590: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 17 17:32:59.590: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 17:32:59.590: INFO: ibm-storage-watcher-74f895486d-k5zxb from kube-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 17:32:59.590: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Feb 17 17:32:59.590: INFO: coredns-autoscaler-7dddb6f87c-g6nrf from kube-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 17:32:59.590: INFO: 	Container autoscaler ready: true, restart count 0
Feb 17 17:32:59.590: INFO: metrics-server-647cf95c9b-5j4cj from kube-system started at 2020-02-17 17:05:48 +0000 UTC (2 container statuses recorded)
Feb 17 17:32:59.590: INFO: 	Container metrics-server ready: true, restart count 0
Feb 17 17:32:59.590: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Feb 17 17:32:59.590: INFO: ibm-master-proxy-static-10.195.53.14 from kube-system started at 2020-02-17 14:36:26 +0000 UTC (2 container statuses recorded)
Feb 17 17:32:59.590: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 17:32:59.590: INFO: 	Container pause ready: true, restart count 0
Feb 17 17:32:59.590: INFO: ibm-keepalived-watcher-8pbd8 from kube-system started at 2020-02-17 14:36:27 +0000 UTC (1 container statuses recorded)
Feb 17 17:32:59.590: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 17:32:59.590: INFO: coredns-5b567488dd-kckdk from kube-system started at 2020-02-17 14:59:13 +0000 UTC (1 container statuses recorded)
Feb 17 17:32:59.590: INFO: 	Container coredns ready: true, restart count 0
Feb 17 17:32:59.590: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-02-17 14:39:39 +0000 UTC (1 container statuses recorded)
Feb 17 17:32:59.590: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Feb 17 17:32:59.590: INFO: catalog-operator-7d9cb6cf74-qr9nb from ibm-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 17:32:59.590: INFO: 	Container catalog-operator ready: true, restart count 0
Feb 17 17:32:59.590: INFO: calico-node-94c2k from kube-system started at 2020-02-17 14:36:27 +0000 UTC (1 container statuses recorded)
Feb 17 17:32:59.590: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 17:32:59.590: INFO: calico-kube-controllers-866cf6f69c-fm97n from kube-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 17:32:59.590: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 17 17:32:59.590: INFO: kubernetes-dashboard-bbcc67fc-x8mnz from kube-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 17:32:59.590: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 17 17:32:59.590: INFO: ibm-cloud-provider-ip-135-90-78-211-7555ccd494-hhqb2 from ibm-system started at 2020-02-17 14:41:58 +0000 UTC (1 container statuses recorded)
Feb 17 17:32:59.590: INFO: 	Container ibm-cloud-provider-ip-135-90-78-211 ready: true, restart count 0
Feb 17 17:32:59.590: INFO: ibm-file-plugin-6694f985b8-s5cjp from kube-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 17:32:59.590: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Feb 17 17:32:59.590: INFO: vpn-b5cd9dc8b-fp9qk from kube-system started at 2020-02-17 17:05:48 +0000 UTC (1 container statuses recorded)
Feb 17 17:32:59.590: INFO: 	Container vpn ready: true, restart count 0
Feb 17 17:32:59.590: INFO: 
Logging pods the kubelet thinks is on node 10.195.53.47 before test
Feb 17 17:32:59.617: INFO: coredns-5b567488dd-qtx5q from kube-system started at 2020-02-17 14:59:13 +0000 UTC (1 container statuses recorded)
Feb 17 17:32:59.618: INFO: 	Container coredns ready: true, restart count 0
Feb 17 17:32:59.618: INFO: ibm-master-proxy-static-10.195.53.47 from kube-system started at 2020-02-17 14:36:20 +0000 UTC (2 container statuses recorded)
Feb 17 17:32:59.618: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 17:32:59.618: INFO: 	Container pause ready: true, restart count 0
Feb 17 17:32:59.618: INFO: ibm-keepalived-watcher-bl4pr from kube-system started at 2020-02-17 14:36:22 +0000 UTC (1 container statuses recorded)
Feb 17 17:32:59.618: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 17:32:59.618: INFO: calico-node-kmwlv from kube-system started at 2020-02-17 14:36:22 +0000 UTC (1 container statuses recorded)
Feb 17 17:32:59.618: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 17:32:59.618: INFO: addon-catalog-source-c5sks from ibm-system started at 2020-02-17 14:38:40 +0000 UTC (1 container statuses recorded)
Feb 17 17:32:59.618: INFO: 	Container configmap-registry-server ready: true, restart count 0
Feb 17 17:32:59.618: INFO: ibm-cloud-provider-ip-135-90-78-211-7555ccd494-fz6gg from ibm-system started at 2020-02-17 14:41:58 +0000 UTC (1 container statuses recorded)
Feb 17 17:32:59.618: INFO: 	Container ibm-cloud-provider-ip-135-90-78-211 ready: true, restart count 0
Feb 17 17:32:59.618: INFO: sonobuoy-e2e-job-c9def2901e004587 from sonobuoy started at 2020-02-17 16:14:42 +0000 UTC (2 container statuses recorded)
Feb 17 17:32:59.618: INFO: 	Container e2e ready: true, restart count 0
Feb 17 17:32:59.618: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 17:32:59.618: INFO: olm-operator-587889d75d-29ltv from ibm-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 17:32:59.618: INFO: 	Container olm-operator ready: true, restart count 0
Feb 17 17:32:59.618: INFO: dashboard-metrics-scraper-69468c6b44-gqtkk from kube-system started at 2020-02-17 16:42:01 +0000 UTC (1 container statuses recorded)
Feb 17 17:32:59.618: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 17 17:32:59.618: INFO: sonobuoy-systemd-logs-daemon-set-bc697ed487d7421d-5b85k from sonobuoy started at 2020-02-17 16:14:42 +0000 UTC (2 container statuses recorded)
Feb 17 17:32:59.618: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 17 17:32:59.618: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 17:32:59.618: INFO: public-crbp5a3ais0mfj6cn3dl60-alb1-79849b9bd6-8r8qk from kube-system started at 2020-02-17 14:44:46 +0000 UTC (4 container statuses recorded)
Feb 17 17:32:59.618: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Feb 17 17:32:59.618: INFO: 	Container ingress-auth-2 ready: true, restart count 1
Feb 17 17:32:59.618: INFO: 	Container ingress-auth-3 ready: true, restart count 1
Feb 17 17:32:59.618: INFO: 	Container nginx-ingress ready: true, restart count 0
Feb 17 17:32:59.618: INFO: coredns-5b567488dd-n2wt7 from kube-system started at 2020-02-17 17:05:48 +0000 UTC (1 container statuses recorded)
Feb 17 17:32:59.618: INFO: 	Container coredns ready: true, restart count 0
Feb 17 17:32:59.618: INFO: 
Logging pods the kubelet thinks is on node 10.195.53.9 before test
Feb 17 17:32:59.640: INFO: calico-node-r8w4v from kube-system started at 2020-02-17 14:36:12 +0000 UTC (1 container statuses recorded)
Feb 17 17:32:59.640: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 17:32:59.640: INFO: ibm-keepalived-watcher-7c6j2 from kube-system started at 2020-02-17 14:36:12 +0000 UTC (1 container statuses recorded)
Feb 17 17:32:59.640: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 17:32:59.640: INFO: pod-update-activedeadlineseconds-7de9eb47-5777-4452-bada-9037fcaa4f9c from pods-2926 started at 2020-02-17 17:32:52 +0000 UTC (1 container statuses recorded)
Feb 17 17:32:59.640: INFO: 	Container nginx ready: false, restart count 0
Feb 17 17:32:59.640: INFO: sonobuoy-systemd-logs-daemon-set-bc697ed487d7421d-ntb66 from sonobuoy started at 2020-02-17 16:14:42 +0000 UTC (2 container statuses recorded)
Feb 17 17:32:59.640: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 17 17:32:59.640: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 17:32:59.640: INFO: sonobuoy from sonobuoy started at 2020-02-17 16:14:33 +0000 UTC (1 container statuses recorded)
Feb 17 17:32:59.640: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 17 17:32:59.640: INFO: ibm-master-proxy-static-10.195.53.9 from kube-system started at 2020-02-17 14:36:11 +0000 UTC (2 container statuses recorded)
Feb 17 17:32:59.640: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 17:32:59.640: INFO: 	Container pause ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15f440f2923cd217], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:33:00.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3364" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":280,"completed":275,"skipped":4524,"failed":0}

------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:33:00.736: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2573
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-downwardapi-f9xp
STEP: Creating a pod to test atomic-volume-subpath
Feb 17 17:33:00.972: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-f9xp" in namespace "subpath-2573" to be "success or failure"
Feb 17 17:33:00.983: INFO: Pod "pod-subpath-test-downwardapi-f9xp": Phase="Pending", Reason="", readiness=false. Elapsed: 11.073377ms
Feb 17 17:33:02.997: INFO: Pod "pod-subpath-test-downwardapi-f9xp": Phase="Running", Reason="", readiness=true. Elapsed: 2.025012509s
Feb 17 17:33:05.010: INFO: Pod "pod-subpath-test-downwardapi-f9xp": Phase="Running", Reason="", readiness=true. Elapsed: 4.037891454s
Feb 17 17:33:07.021: INFO: Pod "pod-subpath-test-downwardapi-f9xp": Phase="Running", Reason="", readiness=true. Elapsed: 6.048882871s
Feb 17 17:33:09.033: INFO: Pod "pod-subpath-test-downwardapi-f9xp": Phase="Running", Reason="", readiness=true. Elapsed: 8.060579526s
Feb 17 17:33:11.045: INFO: Pod "pod-subpath-test-downwardapi-f9xp": Phase="Running", Reason="", readiness=true. Elapsed: 10.072705372s
Feb 17 17:33:13.056: INFO: Pod "pod-subpath-test-downwardapi-f9xp": Phase="Running", Reason="", readiness=true. Elapsed: 12.084251408s
Feb 17 17:33:15.070: INFO: Pod "pod-subpath-test-downwardapi-f9xp": Phase="Running", Reason="", readiness=true. Elapsed: 14.097479454s
Feb 17 17:33:17.081: INFO: Pod "pod-subpath-test-downwardapi-f9xp": Phase="Running", Reason="", readiness=true. Elapsed: 16.109232054s
Feb 17 17:33:19.092: INFO: Pod "pod-subpath-test-downwardapi-f9xp": Phase="Running", Reason="", readiness=true. Elapsed: 18.12019047s
Feb 17 17:33:21.104: INFO: Pod "pod-subpath-test-downwardapi-f9xp": Phase="Running", Reason="", readiness=true. Elapsed: 20.131606762s
Feb 17 17:33:23.115: INFO: Pod "pod-subpath-test-downwardapi-f9xp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.143316443s
STEP: Saw pod success
Feb 17 17:33:23.115: INFO: Pod "pod-subpath-test-downwardapi-f9xp" satisfied condition "success or failure"
Feb 17 17:33:23.126: INFO: Trying to get logs from node 10.195.53.9 pod pod-subpath-test-downwardapi-f9xp container test-container-subpath-downwardapi-f9xp: <nil>
STEP: delete the pod
Feb 17 17:33:23.184: INFO: Waiting for pod pod-subpath-test-downwardapi-f9xp to disappear
Feb 17 17:33:23.194: INFO: Pod pod-subpath-test-downwardapi-f9xp no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-f9xp
Feb 17 17:33:23.194: INFO: Deleting pod "pod-subpath-test-downwardapi-f9xp" in namespace "subpath-2573"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:33:23.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2573" for this suite.

• [SLOW TEST:22.495 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":280,"completed":276,"skipped":4524,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:33:23.232: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3180
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 17:33:23.480: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb 17 17:33:23.500: INFO: Number of nodes with available pods: 0
Feb 17 17:33:23.500: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb 17 17:33:23.543: INFO: Number of nodes with available pods: 0
Feb 17 17:33:23.543: INFO: Node 10.195.53.9 is running more than one daemon pod
Feb 17 17:33:24.555: INFO: Number of nodes with available pods: 0
Feb 17 17:33:24.555: INFO: Node 10.195.53.9 is running more than one daemon pod
Feb 17 17:33:25.559: INFO: Number of nodes with available pods: 1
Feb 17 17:33:25.559: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb 17 17:33:25.600: INFO: Number of nodes with available pods: 1
Feb 17 17:33:25.600: INFO: Number of running nodes: 0, number of available pods: 1
Feb 17 17:33:26.612: INFO: Number of nodes with available pods: 0
Feb 17 17:33:26.612: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb 17 17:33:26.635: INFO: Number of nodes with available pods: 0
Feb 17 17:33:26.635: INFO: Node 10.195.53.9 is running more than one daemon pod
Feb 17 17:33:27.647: INFO: Number of nodes with available pods: 0
Feb 17 17:33:27.647: INFO: Node 10.195.53.9 is running more than one daemon pod
Feb 17 17:33:28.646: INFO: Number of nodes with available pods: 0
Feb 17 17:33:28.646: INFO: Node 10.195.53.9 is running more than one daemon pod
Feb 17 17:33:29.648: INFO: Number of nodes with available pods: 0
Feb 17 17:33:29.648: INFO: Node 10.195.53.9 is running more than one daemon pod
Feb 17 17:33:30.647: INFO: Number of nodes with available pods: 0
Feb 17 17:33:30.647: INFO: Node 10.195.53.9 is running more than one daemon pod
Feb 17 17:33:31.646: INFO: Number of nodes with available pods: 1
Feb 17 17:33:31.646: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3180, will wait for the garbage collector to delete the pods
Feb 17 17:33:31.744: INFO: Deleting DaemonSet.extensions daemon-set took: 20.24461ms
Feb 17 17:33:31.844: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.285863ms
Feb 17 17:33:35.455: INFO: Number of nodes with available pods: 0
Feb 17 17:33:35.455: INFO: Number of running nodes: 0, number of available pods: 0
Feb 17 17:33:35.464: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3180/daemonsets","resourceVersion":"59119"},"items":null}

Feb 17 17:33:35.475: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3180/pods","resourceVersion":"59119"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:33:35.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3180" for this suite.

• [SLOW TEST:12.319 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":280,"completed":277,"skipped":4529,"failed":0}
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:33:35.551: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1614
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name projected-secret-test-e62544fe-1d9d-477b-b87c-799262909fe2
STEP: Creating a pod to test consume secrets
Feb 17 17:33:35.773: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-40f9f1a3-6efd-488d-9e0f-579b6121e30c" in namespace "projected-1614" to be "success or failure"
Feb 17 17:33:35.783: INFO: Pod "pod-projected-secrets-40f9f1a3-6efd-488d-9e0f-579b6121e30c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.080244ms
Feb 17 17:33:37.795: INFO: Pod "pod-projected-secrets-40f9f1a3-6efd-488d-9e0f-579b6121e30c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021551674s
STEP: Saw pod success
Feb 17 17:33:37.795: INFO: Pod "pod-projected-secrets-40f9f1a3-6efd-488d-9e0f-579b6121e30c" satisfied condition "success or failure"
Feb 17 17:33:37.806: INFO: Trying to get logs from node 10.195.53.9 pod pod-projected-secrets-40f9f1a3-6efd-488d-9e0f-579b6121e30c container secret-volume-test: <nil>
STEP: delete the pod
Feb 17 17:33:37.864: INFO: Waiting for pod pod-projected-secrets-40f9f1a3-6efd-488d-9e0f-579b6121e30c to disappear
Feb 17 17:33:37.875: INFO: Pod pod-projected-secrets-40f9f1a3-6efd-488d-9e0f-579b6121e30c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:33:37.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1614" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":278,"skipped":4529,"failed":0}
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:33:37.903: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-998
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:33:40.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-998" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":280,"completed":279,"skipped":4534,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:33:40.199: INFO: >>> kubeConfig: /tmp/kubeconfig-855842601
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2388
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 17:33:40.960: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 17:33:43.003: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557620, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557620, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557620, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557620, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 17:33:46.041: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:33:56.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2388" for this suite.
STEP: Destroying namespace "webhook-2388-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.234 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":280,"completed":280,"skipped":4563,"failed":0}
Feb 17 17:33:56.433: INFO: Running AfterSuite actions on all nodes
Feb 17 17:33:56.433: INFO: Running AfterSuite actions on node 1
Feb 17 17:33:56.433: INFO: Skipping dumping logs from cluster
{"msg":"Test Suite completed","total":280,"completed":280,"skipped":4563,"failed":0}

Ran 280 of 4843 Specs in 4718.907 seconds
SUCCESS! -- 280 Passed | 0 Failed | 0 Pending | 4563 Skipped
PASS

Ginkgo ran 1 suite in 1h18m40.241236926s
Test Suite Passed
