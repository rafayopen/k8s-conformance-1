I0114 14:40:00.283894      23 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-321483740
I0114 14:40:00.283919      23 test_context.go:419] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0114 14:40:00.284100      23 e2e.go:109] Starting e2e run "172ede82-b4eb-481c-a852-7ea994fdc7e5" on Ginkgo node 1
{"msg":"Test Suite starting","total":278,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1579012797 - Will randomize all specs
Will run 278 of 4814 specs

Jan 14 14:40:00.353: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 14:40:00.356: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0114 14:40:00.356624      23 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post http://localhost:8099/progress: dial tcp 127.0.0.1:8099: connect: connection refused
Jan 14 14:40:00.600: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 14 14:40:00.643: INFO: 40 / 40 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 14 14:40:00.643: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Jan 14 14:40:00.643: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 14 14:40:00.658: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jan 14 14:40:00.658: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cinder-provisioner' (0 seconds elapsed)
Jan 14 14:40:00.658: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'coredns' (0 seconds elapsed)
Jan 14 14:40:00.658: INFO: e2e test version: v1.17.0
Jan 14 14:40:00.659: INFO: kube-apiserver version: v1.17.0
Jan 14 14:40:00.659: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 14:40:00.670: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:40:00.671: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename job
Jan 14 14:40:04.370: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Jan 14 14:40:04.391: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-5003
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-5003, will wait for the garbage collector to delete the pods
Jan 14 14:40:19.500: INFO: Deleting Job.batch foo took: 228.681393ms
Jan 14 14:40:20.700: INFO: Terminating Job.batch foo pods took: 1.200173971s
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 14:41:05.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5003" for this suite.

â€¢ [SLOW TEST:66.278 seconds]
[sig-apps] Job
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":278,"completed":1,"skipped":89,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:41:06.954: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-707
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jan 14 14:41:07.995: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 14 14:41:08.109: INFO: Waiting for terminating namespaces to be deleted...
Jan 14 14:41:08.207: INFO: 
Logging pods the kubelet thinks is on node master1 before test
Jan 14 14:41:08.232: INFO: coredns-52ppc from kube-system started at 2020-01-14 12:58:20 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.234: INFO: 	Container coredns ready: true, restart count 0
Jan 14 14:41:08.236: INFO: daemon-set-gg2wn from daemonsets-9852 started at 2020-01-14 13:36:55 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.236: INFO: 	Container app ready: true, restart count 0
Jan 14 14:41:08.236: INFO: resource-reserver-master1 from kube-system started at 2020-01-14 11:40:33 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.236: INFO: 	Container sleep-forever ready: true, restart count 0
Jan 14 14:41:08.236: INFO: kube-scheduler-master1 from kube-system started at 2020-01-14 11:40:33 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.236: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan 14 14:41:08.236: INFO: cinder-provisioner-xxxjh from kube-system started at 2020-01-14 05:31:24 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.236: INFO: 	Container cinder-provisioner ready: true, restart count 1
Jan 14 14:41:08.236: INFO: oss-provisioner-7bb5d4c769-vhrt6 from kube-system started at 2020-01-14 05:31:38 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.236: INFO: 	Container oss-provisioner ready: true, restart count 1
Jan 14 14:41:08.236: INFO: calico-node-c275v from kube-system started at 2020-01-14 05:30:06 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.236: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 14:41:08.236: INFO: kube-state-metrics-8988b595-gq7dc from monitoring started at 2020-01-14 12:57:51 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.236: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 14 14:41:08.236: INFO: kube-apiserver-master1 from kube-system started at 2020-01-14 10:55:20 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.236: INFO: 	Container kube-apiserver ready: true, restart count 1
Jan 14 14:41:08.236: INFO: kube-controller-manager-master1 from kube-system started at 2020-01-14 11:40:33 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.236: INFO: 	Container kube-controller-manager ready: true, restart count 3
Jan 14 14:41:08.236: INFO: kube-proxy-master1 from kube-system started at 2020-01-14 11:40:33 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.236: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 14:41:08.236: INFO: nginx-proxy-master1 from kube-system started at 2020-01-14 11:40:33 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.236: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 14 14:41:08.236: INFO: 
Logging pods the kubelet thinks is on node master2 before test
Jan 14 14:41:08.252: INFO: calico-node-vhmj5 from kube-system started at 2020-01-14 05:30:04 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.252: INFO: 	Container calico-node ready: true, restart count 2
Jan 14 14:41:08.252: INFO: kube-controller-manager-master2 from kube-system started at 2020-01-14 08:23:38 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.252: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jan 14 14:41:08.252: INFO: kube-proxy-master2 from kube-system started at 2020-01-14 08:24:11 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.252: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 14:41:08.252: INFO: kube-scheduler-master2 from kube-system started at 2020-01-14 08:24:38 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.252: INFO: 	Container kube-scheduler ready: true, restart count 1
Jan 14 14:41:08.252: INFO: daemon-set-5jq8c from daemonsets-9852 started at 2020-01-14 13:36:55 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.252: INFO: 	Container app ready: true, restart count 0
Jan 14 14:41:08.252: INFO: cinder-provisioner-9fk6f from kube-system started at 2020-01-14 05:31:23 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.252: INFO: 	Container cinder-provisioner ready: true, restart count 2
Jan 14 14:41:08.252: INFO: cinder-snapshot-58545f46f8-d2m7m from kube-system started at 2020-01-14 05:31:52 +0000 UTC (2 container statuses recorded)
Jan 14 14:41:08.252: INFO: 	Container cinder-snapshot-controller ready: true, restart count 2
Jan 14 14:41:08.252: INFO: 	Container cinder-snapshot-provisioner ready: true, restart count 2
Jan 14 14:41:08.252: INFO: kube-apiserver-master2 from kube-system started at 2020-01-14 08:26:26 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.252: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 14 14:41:08.252: INFO: resource-reserver-master2 from kube-system started at 2020-01-14 05:27:40 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.252: INFO: 	Container sleep-forever ready: true, restart count 0
Jan 14 14:41:08.252: INFO: nginx-proxy-master2 from kube-system started at 2020-01-14 05:27:39 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.252: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 14 14:41:08.252: INFO: coredns-cnrrm from kube-system started at 2020-01-14 12:58:20 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.252: INFO: 	Container coredns ready: true, restart count 0
Jan 14 14:41:08.252: INFO: 
Logging pods the kubelet thinks is on node master3 before test
Jan 14 14:41:08.266: INFO: kube-apiserver-master3 from kube-system started at 2020-01-14 08:29:42 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.267: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 14 14:41:08.267: INFO: kube-controller-manager-master3 from kube-system started at 2020-01-14 08:30:08 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.267: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jan 14 14:41:08.267: INFO: kube-proxy-master3 from kube-system started at 2020-01-14 08:30:38 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.267: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 14:41:08.267: INFO: resource-reserver-master3 from kube-system started at 2020-01-14 08:31:44 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.267: INFO: 	Container sleep-forever ready: true, restart count 0
Jan 14 14:41:08.267: INFO: calico-node-p7t5k from kube-system started at 2020-01-14 05:30:03 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.267: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 14:41:08.267: INFO: cinder-provisioner-wbhvj from kube-system started at 2020-01-14 05:31:22 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.267: INFO: 	Container cinder-provisioner ready: true, restart count 1
Jan 14 14:41:08.267: INFO: localpv-provisioner-6c5467f94-q9cvw from kube-system started at 2020-01-14 05:31:32 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.267: INFO: 	Container localpv-provisioner ready: true, restart count 1
Jan 14 14:41:08.267: INFO: coredns-f24kh from kube-system started at 2020-01-14 12:58:20 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.267: INFO: 	Container coredns ready: true, restart count 0
Jan 14 14:41:08.267: INFO: kube-scheduler-master3 from kube-system started at 2020-01-14 08:31:06 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.267: INFO: 	Container kube-scheduler ready: true, restart count 1
Jan 14 14:41:08.267: INFO: nginx-proxy-master3 from kube-system started at 2020-01-14 05:27:40 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.267: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 14 14:41:08.267: INFO: daemon-set-247jg from daemonsets-9852 started at 2020-01-14 13:36:56 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.267: INFO: 	Container app ready: true, restart count 0
Jan 14 14:41:08.267: INFO: 
Logging pods the kubelet thinks is on node slave1 before test
Jan 14 14:41:08.280: INFO: dns-test-b39ed90a-d7ff-4205-b2f7-ffe423eff36b from dns-1195 started at 2020-01-14 12:33:00 +0000 UTC (3 container statuses recorded)
Jan 14 14:41:08.280: INFO: 	Container jessie-querier ready: true, restart count 8
Jan 14 14:41:08.280: INFO: 	Container querier ready: true, restart count 10
Jan 14 14:41:08.280: INFO: 	Container webserver ready: true, restart count 0
Jan 14 14:41:08.280: INFO: kube-proxy-slave1 from kube-system started at 2020-01-14 08:17:36 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.280: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 14:41:08.280: INFO: calico-node-wzpws from kube-system started at 2020-01-14 05:30:06 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.280: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 14:41:08.280: INFO: calico-kube-controllers-6ff9f48ccd-phjqn from kube-system started at 2020-01-14 05:30:37 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.280: INFO: 	Container calico-kube-controllers ready: true, restart count 1
Jan 14 14:41:08.280: INFO: tiller-deploy-77d97f584c-l4s9r from kube-system started at 2020-01-14 05:32:44 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.280: INFO: 	Container tiller ready: true, restart count 1
Jan 14 14:41:08.280: INFO: daemon-set-d6b8h from daemonsets-9852 started at 2020-01-14 13:36:56 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.280: INFO: 	Container app ready: true, restart count 0
Jan 14 14:41:08.280: INFO: 
Logging pods the kubelet thinks is on node slave2 before test
Jan 14 14:41:08.294: INFO: daemon-set-zww4m from daemonsets-9852 started at 2020-01-14 13:36:55 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.294: INFO: 	Container app ready: true, restart count 0
Jan 14 14:41:08.294: INFO: calico-node-gwd6p from kube-system started at 2020-01-14 05:30:03 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.294: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 14:41:08.294: INFO: kube-proxy-slave2 from kube-system started at 2020-01-14 08:17:48 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.294: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 14:41:08.294: INFO: cirros-26408-6bb4b5b58b-kfs9n from default started at 2020-01-14 11:01:57 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.294: INFO: 	Container cirros-26408 ready: true, restart count 0
Jan 14 14:41:08.294: INFO: agnhost-deployment-54964f567d-498qd from default started at 2020-01-14 11:05:35 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.294: INFO: 	Container agnhost ready: true, restart count 0
Jan 14 14:41:08.294: INFO: annotationupdate8b560796-65d3-444d-b2a8-95487af080c1 from downward-api-9309 started at 2020-01-14 12:08:41 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.294: INFO: 	Container client-container ready: false, restart count 0
Jan 14 14:41:08.294: INFO: 
Logging pods the kubelet thinks is on node slave3 before test
Jan 14 14:41:08.315: INFO: calico-node-zprlr from kube-system started at 2020-01-14 05:30:06 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.315: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 14:41:08.315: INFO: kube-proxy-slave3 from kube-system started at 2020-01-14 08:14:15 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.315: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 14:41:08.315: INFO: sonobuoy from sonobuoy started at 2020-01-14 14:39:37 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.315: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 14 14:41:08.315: INFO: sonobuoy-e2e-job-5f91618b21714006 from sonobuoy started at 2020-01-14 14:39:48 +0000 UTC (2 container statuses recorded)
Jan 14 14:41:08.315: INFO: 	Container e2e ready: true, restart count 0
Jan 14 14:41:08.315: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 14 14:41:08.315: INFO: daemon-set-gn9px from daemonsets-9852 started at 2020-01-14 13:36:55 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.315: INFO: 	Container app ready: true, restart count 0
Jan 14 14:41:08.315: INFO: 
Logging pods the kubelet thinks is on node slave4 before test
Jan 14 14:41:08.352: INFO: cirros-15453-7f8dd5d978-gjlvv from default started at 2020-01-14 11:03:42 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.352: INFO: 	Container cirros-15453 ready: true, restart count 0
Jan 14 14:41:08.352: INFO: dns-test-53c7521e-6d65-4cd3-b40e-b0e2058d5107 from dns-5986 started at 2020-01-14 12:27:30 +0000 UTC (3 container statuses recorded)
Jan 14 14:41:08.352: INFO: 	Container jessie-querier ready: true, restart count 8
Jan 14 14:41:08.352: INFO: 	Container querier ready: true, restart count 10
Jan 14 14:41:08.352: INFO: 	Container webserver ready: true, restart count 0
Jan 14 14:41:08.352: INFO: daemon-set-b8jkm from daemonsets-9852 started at 2020-01-14 13:36:54 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.352: INFO: 	Container app ready: true, restart count 0
Jan 14 14:41:08.352: INFO: calico-node-hc57z from kube-system started at 2020-01-14 05:30:06 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.352: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 14:41:08.352: INFO: kube-proxy-slave4 from kube-system started at 2020-01-14 08:13:30 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.352: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 14:41:08.352: INFO: cirros-19353-6f67878d75-p7mck from default started at 2020-01-14 11:01:24 +0000 UTC (1 container statuses recorded)
Jan 14 14:41:08.352: INFO: 	Container cirros-19353 ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-6ecf7d1c-b758-4fbd-8a31-b62d5a2e9434 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-6ecf7d1c-b758-4fbd-8a31-b62d5a2e9434 off the node slave2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-6ecf7d1c-b758-4fbd-8a31-b62d5a2e9434
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 14:41:34.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-707" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

â€¢ [SLOW TEST:28.244 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":278,"completed":2,"skipped":101,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:41:35.199: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4444
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 14 14:41:39.388: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 14 14:41:41.713: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609700, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609700, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609700, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609699, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 14:41:43.790: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609700, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609700, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609700, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609699, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 14:41:45.902: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609700, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609700, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609700, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609699, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 14:41:47.821: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609700, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609700, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609700, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609699, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 14:41:49.859: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609700, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609700, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609700, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609699, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 14 14:41:53.143: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jan 14 14:42:02.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 attach --namespace=webhook-4444 to-be-attached-pod -i -c=container1'
Jan 14 14:42:09.087: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 14:42:09.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4444" for this suite.
STEP: Destroying namespace "webhook-4444-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:36.211 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":278,"completed":3,"skipped":118,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:42:11.411: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1877
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:178
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 14:42:12.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1877" for this suite.
â€¢{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":278,"completed":4,"skipped":203,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:42:13.201: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8582
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8582.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8582.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8582.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8582.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 14 14:42:36.484: INFO: DNS probes using dns-test-e52ab5f8-cbaf-4b95-a1e0-2a9b7de45b97 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8582.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8582.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8582.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8582.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 14 14:42:58.269: INFO: DNS probes using dns-test-08685f4d-e646-4c08-b87b-1f5a8f6bc0aa succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8582.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8582.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8582.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8582.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 14 14:43:17.062: INFO: DNS probes using dns-test-e1fcc630-afcb-4eda-aaea-937279b96805 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 14:43:17.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8582" for this suite.

â€¢ [SLOW TEST:64.860 seconds]
[sig-network] DNS
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":278,"completed":5,"skipped":207,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:43:18.066: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2550
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 14 14:43:23.636: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 14 14:43:25.782: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609804, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609804, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609804, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609803, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 14:43:27.846: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609804, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609804, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609804, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609803, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 14:43:30.389: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609804, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609804, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609804, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609803, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 14:43:31.854: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609804, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609804, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609804, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609803, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 14:43:34.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609804, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609804, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609804, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714609803, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 14 14:43:38.406: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 14:43:40.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2550" for this suite.
STEP: Destroying namespace "webhook-2550-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:24.949 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":278,"completed":6,"skipped":220,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:43:43.016: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-3047
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jan 14 14:44:02.688: INFO: Successfully updated pod "adopt-release-8gzjd"
STEP: Checking that the Job readopts the Pod
Jan 14 14:44:02.688: INFO: Waiting up to 15m0s for pod "adopt-release-8gzjd" in namespace "job-3047" to be "adopted"
Jan 14 14:44:02.692: INFO: Pod "adopt-release-8gzjd": Phase="Running", Reason="", readiness=true. Elapsed: 4.229838ms
Jan 14 14:44:04.890: INFO: Pod "adopt-release-8gzjd": Phase="Running", Reason="", readiness=true. Elapsed: 2.201378729s
Jan 14 14:44:04.890: INFO: Pod "adopt-release-8gzjd" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jan 14 14:44:05.977: INFO: Successfully updated pod "adopt-release-8gzjd"
STEP: Checking that the Job releases the Pod
Jan 14 14:44:05.977: INFO: Waiting up to 15m0s for pod "adopt-release-8gzjd" in namespace "job-3047" to be "released"
Jan 14 14:44:06.693: INFO: Pod "adopt-release-8gzjd": Phase="Running", Reason="", readiness=true. Elapsed: 716.113545ms
Jan 14 14:44:06.694: INFO: Pod "adopt-release-8gzjd" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 14:44:06.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3047" for this suite.

â€¢ [SLOW TEST:24.073 seconds]
[sig-apps] Job
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":278,"completed":7,"skipped":237,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:44:07.088: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6543
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6543.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6543.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6543.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6543.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6543.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6543.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6543.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6543.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6543.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6543.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6543.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6543.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6543.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6543.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6543.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6543.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6543.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6543.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 14 14:44:27.968: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6543.svc.cluster.local from pod dns-6543/dns-test-7ae5e3c4-93c1-42f6-be71-83265971f1d3: the server could not find the requested resource (get pods dns-test-7ae5e3c4-93c1-42f6-be71-83265971f1d3)
Jan 14 14:44:28.128: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6543.svc.cluster.local from pod dns-6543/dns-test-7ae5e3c4-93c1-42f6-be71-83265971f1d3: the server could not find the requested resource (get pods dns-test-7ae5e3c4-93c1-42f6-be71-83265971f1d3)
Jan 14 14:44:28.135: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6543.svc.cluster.local from pod dns-6543/dns-test-7ae5e3c4-93c1-42f6-be71-83265971f1d3: the server could not find the requested resource (get pods dns-test-7ae5e3c4-93c1-42f6-be71-83265971f1d3)
Jan 14 14:44:28.139: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6543.svc.cluster.local from pod dns-6543/dns-test-7ae5e3c4-93c1-42f6-be71-83265971f1d3: the server could not find the requested resource (get pods dns-test-7ae5e3c4-93c1-42f6-be71-83265971f1d3)
Jan 14 14:44:28.156: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6543.svc.cluster.local from pod dns-6543/dns-test-7ae5e3c4-93c1-42f6-be71-83265971f1d3: the server could not find the requested resource (get pods dns-test-7ae5e3c4-93c1-42f6-be71-83265971f1d3)
Jan 14 14:44:28.160: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6543.svc.cluster.local from pod dns-6543/dns-test-7ae5e3c4-93c1-42f6-be71-83265971f1d3: the server could not find the requested resource (get pods dns-test-7ae5e3c4-93c1-42f6-be71-83265971f1d3)
Jan 14 14:44:28.164: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6543.svc.cluster.local from pod dns-6543/dns-test-7ae5e3c4-93c1-42f6-be71-83265971f1d3: the server could not find the requested resource (get pods dns-test-7ae5e3c4-93c1-42f6-be71-83265971f1d3)
Jan 14 14:44:28.168: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6543.svc.cluster.local from pod dns-6543/dns-test-7ae5e3c4-93c1-42f6-be71-83265971f1d3: the server could not find the requested resource (get pods dns-test-7ae5e3c4-93c1-42f6-be71-83265971f1d3)
Jan 14 14:44:28.175: INFO: Lookups using dns-6543/dns-test-7ae5e3c4-93c1-42f6-be71-83265971f1d3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6543.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6543.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6543.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6543.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6543.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6543.svc.cluster.local jessie_udp@dns-test-service-2.dns-6543.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6543.svc.cluster.local]

Jan 14 14:44:33.298: INFO: DNS probes using dns-6543/dns-test-7ae5e3c4-93c1-42f6-be71-83265971f1d3 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 14:44:35.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6543" for this suite.

â€¢ [SLOW TEST:28.804 seconds]
[sig-network] DNS
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":278,"completed":8,"skipped":239,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:44:35.900: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3777
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-3777
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 14 14:44:37.560: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 14 14:45:30.306: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.49.86:8080/dial?request=hostname&protocol=udp&host=10.151.161.14&port=8081&tries=1'] Namespace:pod-network-test-3777 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 14:45:30.306: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 14:45:31.106: INFO: Waiting for responses: map[]
Jan 14 14:45:31.168: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.49.86:8080/dial?request=hostname&protocol=udp&host=10.151.208.10&port=8081&tries=1'] Namespace:pod-network-test-3777 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 14:45:31.168: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 14:45:31.469: INFO: Waiting for responses: map[]
Jan 14 14:45:31.473: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.49.86:8080/dial?request=hostname&protocol=udp&host=10.151.32.11&port=8081&tries=1'] Namespace:pod-network-test-3777 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 14:45:31.473: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 14:45:31.786: INFO: Waiting for responses: map[]
Jan 14 14:45:31.790: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.49.86:8080/dial?request=hostname&protocol=udp&host=10.151.51.26&port=8081&tries=1'] Namespace:pod-network-test-3777 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 14:45:31.790: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 14:45:32.104: INFO: Waiting for responses: map[]
Jan 14 14:45:32.187: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.49.86:8080/dial?request=hostname&protocol=udp&host=10.151.49.85&port=8081&tries=1'] Namespace:pod-network-test-3777 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 14:45:32.187: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 14:45:32.468: INFO: Waiting for responses: map[]
Jan 14 14:45:32.490: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.49.86:8080/dial?request=hostname&protocol=udp&host=10.151.194.48&port=8081&tries=1'] Namespace:pod-network-test-3777 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 14:45:32.490: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 14:45:32.902: INFO: Waiting for responses: map[]
Jan 14 14:45:32.906: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.49.86:8080/dial?request=hostname&protocol=udp&host=10.151.26.44&port=8081&tries=1'] Namespace:pod-network-test-3777 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 14:45:32.906: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 14:45:33.300: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 14:45:33.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3777" for this suite.

â€¢ [SLOW TEST:57.587 seconds]
[sig-network] Networking
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":9,"skipped":255,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:45:33.487: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9203
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-9203
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-9203
STEP: Creating statefulset with conflicting port in namespace statefulset-9203
STEP: Waiting until pod test-pod will start running in namespace statefulset-9203
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9203
Jan 14 14:45:53.812: INFO: Observed stateful pod in namespace: statefulset-9203, name: ss-0, uid: 236e2217-1c66-4c16-8113-8b6ab876bd8f, status phase: Failed. Waiting for statefulset controller to delete.
Jan 14 14:45:54.013: INFO: Observed stateful pod in namespace: statefulset-9203, name: ss-0, uid: 236e2217-1c66-4c16-8113-8b6ab876bd8f, status phase: Failed. Waiting for statefulset controller to delete.
Jan 14 14:45:54.056: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9203
STEP: Removing pod with conflicting port in namespace statefulset-9203
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9203 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jan 14 14:46:11.023: INFO: Deleting all statefulset in ns statefulset-9203
Jan 14 14:46:11.027: INFO: Scaling statefulset ss to 0
Jan 14 14:46:31.686: INFO: Waiting for statefulset status.replicas updated to 0
Jan 14 14:46:31.691: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 14:46:32.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9203" for this suite.

â€¢ [SLOW TEST:59.067 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":278,"completed":10,"skipped":265,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:46:32.556: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6031
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-601ac04c-3fb0-4111-ba05-3c06c1cf6b8f
STEP: Creating a pod to test consume configMaps
Jan 14 14:46:34.079: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-261066ff-102e-4604-afd4-a137d54b17dc" in namespace "projected-6031" to be "success or failure"
Jan 14 14:46:34.083: INFO: Pod "pod-projected-configmaps-261066ff-102e-4604-afd4-a137d54b17dc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.282905ms
Jan 14 14:46:36.091: INFO: Pod "pod-projected-configmaps-261066ff-102e-4604-afd4-a137d54b17dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011160303s
Jan 14 14:46:38.730: INFO: Pod "pod-projected-configmaps-261066ff-102e-4604-afd4-a137d54b17dc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.650998306s
Jan 14 14:46:41.369: INFO: Pod "pod-projected-configmaps-261066ff-102e-4604-afd4-a137d54b17dc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.289299843s
Jan 14 14:46:43.834: INFO: Pod "pod-projected-configmaps-261066ff-102e-4604-afd4-a137d54b17dc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.754720081s
Jan 14 14:46:45.888: INFO: Pod "pod-projected-configmaps-261066ff-102e-4604-afd4-a137d54b17dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.808804761s
STEP: Saw pod success
Jan 14 14:46:45.888: INFO: Pod "pod-projected-configmaps-261066ff-102e-4604-afd4-a137d54b17dc" satisfied condition "success or failure"
Jan 14 14:46:45.892: INFO: Trying to get logs from node slave2 pod pod-projected-configmaps-261066ff-102e-4604-afd4-a137d54b17dc container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 14 14:46:45.934: INFO: Waiting for pod pod-projected-configmaps-261066ff-102e-4604-afd4-a137d54b17dc to disappear
Jan 14 14:46:45.937: INFO: Pod pod-projected-configmaps-261066ff-102e-4604-afd4-a137d54b17dc no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 14:46:45.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6031" for this suite.

â€¢ [SLOW TEST:13.404 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":11,"skipped":275,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:46:45.960: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8185
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 14 14:46:50.000: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 14 14:46:52.480: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610010, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610010, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610011, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610010, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 14:46:54.487: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610010, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610010, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610011, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610010, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 14:46:56.696: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610010, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610010, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610011, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610010, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 14:46:58.492: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610010, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610010, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610011, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610010, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 14 14:47:02.279: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 14:47:02.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8185" for this suite.
STEP: Destroying namespace "webhook-8185-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:17.982 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":278,"completed":12,"skipped":277,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:47:03.943: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-4815
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
Jan 14 14:47:08.687: INFO: created pod pod-service-account-defaultsa
Jan 14 14:47:08.687: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 14 14:47:08.716: INFO: created pod pod-service-account-mountsa
Jan 14 14:47:08.716: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 14 14:47:09.080: INFO: created pod pod-service-account-nomountsa
Jan 14 14:47:09.080: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 14 14:47:10.178: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 14 14:47:10.182: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 14 14:47:10.653: INFO: created pod pod-service-account-mountsa-mountspec
Jan 14 14:47:10.656: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 14 14:47:11.065: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 14 14:47:11.065: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 14 14:47:11.639: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 14 14:47:11.639: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 14 14:47:11.884: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 14 14:47:11.885: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 14 14:47:12.615: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 14 14:47:12.615: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 14:47:12.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4815" for this suite.

â€¢ [SLOW TEST:9.470 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":278,"completed":13,"skipped":308,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:47:13.413: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3510
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 14 14:47:17.562: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 14 14:47:20.665: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610038, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610038, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610039, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610037, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 14:47:23.026: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610038, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610038, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610039, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610037, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 14:47:24.804: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610038, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610038, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610039, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610037, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 14:47:26.884: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610038, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610038, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610039, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610037, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 14:47:29.026: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610038, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610038, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610039, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610037, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 14:47:30.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610038, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610038, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610039, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610037, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 14 14:47:34.421: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 14:47:36.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3510" for this suite.
STEP: Destroying namespace "webhook-3510-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:24.091 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":278,"completed":14,"skipped":316,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:47:37.504: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7318
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 14 14:47:39.892: INFO: Waiting up to 5m0s for pod "downwardapi-volume-85f7766c-0426-493a-8821-a4fd72ee648d" in namespace "projected-7318" to be "success or failure"
Jan 14 14:47:40.112: INFO: Pod "downwardapi-volume-85f7766c-0426-493a-8821-a4fd72ee648d": Phase="Pending", Reason="", readiness=false. Elapsed: 220.227386ms
Jan 14 14:47:42.384: INFO: Pod "downwardapi-volume-85f7766c-0426-493a-8821-a4fd72ee648d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.492303235s
Jan 14 14:47:44.804: INFO: Pod "downwardapi-volume-85f7766c-0426-493a-8821-a4fd72ee648d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.912399434s
Jan 14 14:47:46.990: INFO: Pod "downwardapi-volume-85f7766c-0426-493a-8821-a4fd72ee648d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.098441897s
Jan 14 14:47:49.375: INFO: Pod "downwardapi-volume-85f7766c-0426-493a-8821-a4fd72ee648d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.482878399s
Jan 14 14:47:51.484: INFO: Pod "downwardapi-volume-85f7766c-0426-493a-8821-a4fd72ee648d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.592198999s
STEP: Saw pod success
Jan 14 14:47:51.484: INFO: Pod "downwardapi-volume-85f7766c-0426-493a-8821-a4fd72ee648d" satisfied condition "success or failure"
Jan 14 14:47:51.783: INFO: Trying to get logs from node slave2 pod downwardapi-volume-85f7766c-0426-493a-8821-a4fd72ee648d container client-container: <nil>
STEP: delete the pod
Jan 14 14:47:52.376: INFO: Waiting for pod downwardapi-volume-85f7766c-0426-493a-8821-a4fd72ee648d to disappear
Jan 14 14:47:52.766: INFO: Pod downwardapi-volume-85f7766c-0426-493a-8821-a4fd72ee648d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 14:47:52.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7318" for this suite.

â€¢ [SLOW TEST:15.390 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":278,"completed":15,"skipped":317,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:47:52.894: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-582
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Jan 14 14:48:05.221: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 14:48:05.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0114 14:48:05.221844      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-582" for this suite.

â€¢ [SLOW TEST:12.523 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":278,"completed":16,"skipped":327,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:48:05.417: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4469
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Jan 14 14:48:21.110: INFO: 10 pods remaining
Jan 14 14:48:21.111: INFO: 10 pods has nil DeletionTimestamp
Jan 14 14:48:21.111: INFO: 
Jan 14 14:48:22.351: INFO: 10 pods remaining
Jan 14 14:48:22.353: INFO: 1 pods has nil DeletionTimestamp
Jan 14 14:48:22.353: INFO: 
Jan 14 14:48:25.205: INFO: 8 pods remaining
Jan 14 14:48:25.205: INFO: 0 pods has nil DeletionTimestamp
Jan 14 14:48:25.205: INFO: 
Jan 14 14:48:26.720: INFO: 1 pods remaining
Jan 14 14:48:26.720: INFO: 0 pods has nil DeletionTimestamp
Jan 14 14:48:26.720: INFO: 
STEP: Gathering metrics
W0114 14:48:30.114141      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 14 14:48:30.114: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 14:48:30.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4469" for this suite.

â€¢ [SLOW TEST:25.549 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":278,"completed":17,"skipped":333,"failed":0}
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:48:30.968: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8894
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's args
Jan 14 14:48:33.618: INFO: Waiting up to 5m0s for pod "var-expansion-509e1f63-42f5-4f10-85f3-ef0ddb29e986" in namespace "var-expansion-8894" to be "success or failure"
Jan 14 14:48:33.682: INFO: Pod "var-expansion-509e1f63-42f5-4f10-85f3-ef0ddb29e986": Phase="Pending", Reason="", readiness=false. Elapsed: 63.443243ms
Jan 14 14:48:35.743: INFO: Pod "var-expansion-509e1f63-42f5-4f10-85f3-ef0ddb29e986": Phase="Pending", Reason="", readiness=false. Elapsed: 2.124666258s
Jan 14 14:48:38.157: INFO: Pod "var-expansion-509e1f63-42f5-4f10-85f3-ef0ddb29e986": Phase="Pending", Reason="", readiness=false. Elapsed: 4.538457198s
Jan 14 14:48:40.250: INFO: Pod "var-expansion-509e1f63-42f5-4f10-85f3-ef0ddb29e986": Phase="Pending", Reason="", readiness=false. Elapsed: 6.631792344s
Jan 14 14:48:42.278: INFO: Pod "var-expansion-509e1f63-42f5-4f10-85f3-ef0ddb29e986": Phase="Pending", Reason="", readiness=false. Elapsed: 8.659706603s
Jan 14 14:48:44.402: INFO: Pod "var-expansion-509e1f63-42f5-4f10-85f3-ef0ddb29e986": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.7835795s
STEP: Saw pod success
Jan 14 14:48:44.402: INFO: Pod "var-expansion-509e1f63-42f5-4f10-85f3-ef0ddb29e986" satisfied condition "success or failure"
Jan 14 14:48:44.671: INFO: Trying to get logs from node slave2 pod var-expansion-509e1f63-42f5-4f10-85f3-ef0ddb29e986 container dapi-container: <nil>
STEP: delete the pod
Jan 14 14:48:44.962: INFO: Waiting for pod var-expansion-509e1f63-42f5-4f10-85f3-ef0ddb29e986 to disappear
Jan 14 14:48:44.968: INFO: Pod var-expansion-509e1f63-42f5-4f10-85f3-ef0ddb29e986 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 14:48:44.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8894" for this suite.

â€¢ [SLOW TEST:14.291 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":278,"completed":18,"skipped":336,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:48:45.259: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2303
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-2303
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Jan 14 14:48:47.204: INFO: Found 0 stateful pods, waiting for 3
Jan 14 14:48:57.486: INFO: Found 2 stateful pods, waiting for 3
Jan 14 14:49:07.227: INFO: Found 2 stateful pods, waiting for 3
Jan 14 14:49:17.519: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 14 14:49:17.519: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 14 14:49:17.519: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
Jan 14 14:49:27.247: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 14 14:49:27.247: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 14 14:49:27.247: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 14 14:49:27.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2303 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 14 14:49:28.445: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 14 14:49:28.445: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 14 14:49:28.445: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jan 14 14:49:39.282: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jan 14 14:49:49.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2303 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 14:49:50.656: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 14 14:49:50.656: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 14 14:49:50.656: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 14 14:49:50.685: INFO: Waiting for StatefulSet statefulset-2303/ss2 to complete update
Jan 14 14:49:50.685: INFO: Waiting for Pod statefulset-2303/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan 14 14:49:50.685: INFO: Waiting for Pod statefulset-2303/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan 14 14:49:50.685: INFO: Waiting for Pod statefulset-2303/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan 14 14:50:00.693: INFO: Waiting for StatefulSet statefulset-2303/ss2 to complete update
Jan 14 14:50:00.693: INFO: Waiting for Pod statefulset-2303/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan 14 14:50:00.693: INFO: Waiting for Pod statefulset-2303/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan 14 14:50:10.709: INFO: Waiting for StatefulSet statefulset-2303/ss2 to complete update
Jan 14 14:50:10.709: INFO: Waiting for Pod statefulset-2303/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan 14 14:50:10.709: INFO: Waiting for Pod statefulset-2303/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan 14 14:50:20.881: INFO: Waiting for StatefulSet statefulset-2303/ss2 to complete update
Jan 14 14:50:20.881: INFO: Waiting for Pod statefulset-2303/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan 14 14:50:20.881: INFO: Waiting for Pod statefulset-2303/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan 14 14:50:30.706: INFO: Waiting for StatefulSet statefulset-2303/ss2 to complete update
Jan 14 14:50:30.706: INFO: Waiting for Pod statefulset-2303/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan 14 14:50:40.981: INFO: Waiting for StatefulSet statefulset-2303/ss2 to complete update
Jan 14 14:50:40.981: INFO: Waiting for Pod statefulset-2303/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan 14 14:50:51.830: INFO: Waiting for StatefulSet statefulset-2303/ss2 to complete update
STEP: Rolling back to a previous revision
Jan 14 14:51:01.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2303 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 14 14:51:02.505: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 14 14:51:02.505: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 14 14:51:02.505: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 14 14:51:04.346: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jan 14 14:51:05.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2303 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 14:51:06.183: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 14 14:51:06.183: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 14 14:51:06.183: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 14 14:51:17.413: INFO: Waiting for StatefulSet statefulset-2303/ss2 to complete update
Jan 14 14:51:17.413: INFO: Waiting for Pod statefulset-2303/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jan 14 14:51:17.413: INFO: Waiting for Pod statefulset-2303/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jan 14 14:51:17.413: INFO: Waiting for Pod statefulset-2303/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jan 14 14:51:28.481: INFO: Waiting for StatefulSet statefulset-2303/ss2 to complete update
Jan 14 14:51:28.481: INFO: Waiting for Pod statefulset-2303/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jan 14 14:51:28.481: INFO: Waiting for Pod statefulset-2303/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jan 14 14:51:37.551: INFO: Waiting for StatefulSet statefulset-2303/ss2 to complete update
Jan 14 14:51:37.551: INFO: Waiting for Pod statefulset-2303/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jan 14 14:51:37.551: INFO: Waiting for Pod statefulset-2303/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jan 14 14:51:47.599: INFO: Waiting for StatefulSet statefulset-2303/ss2 to complete update
Jan 14 14:51:47.599: INFO: Waiting for Pod statefulset-2303/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jan 14 14:51:47.599: INFO: Waiting for Pod statefulset-2303/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jan 14 14:51:57.942: INFO: Waiting for StatefulSet statefulset-2303/ss2 to complete update
Jan 14 14:51:57.942: INFO: Waiting for Pod statefulset-2303/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jan 14 14:52:07.506: INFO: Waiting for StatefulSet statefulset-2303/ss2 to complete update
Jan 14 14:52:07.507: INFO: Waiting for Pod statefulset-2303/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jan 14 14:52:17.698: INFO: Waiting for StatefulSet statefulset-2303/ss2 to complete update
Jan 14 14:52:17.698: INFO: Waiting for Pod statefulset-2303/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jan 14 14:52:27.782: INFO: Waiting for StatefulSet statefulset-2303/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jan 14 14:52:37.783: INFO: Deleting all statefulset in ns statefulset-2303
Jan 14 14:52:37.786: INFO: Scaling statefulset ss2 to 0
Jan 14 14:53:28.088: INFO: Waiting for statefulset status.replicas updated to 0
Jan 14 14:53:28.091: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 14:53:28.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2303" for this suite.

â€¢ [SLOW TEST:284.361 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":278,"completed":19,"skipped":349,"failed":0}
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:53:29.620: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6848
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 14:53:34.643: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jan 14 14:53:35.176: INFO: Number of nodes with available pods: 0
Jan 14 14:53:35.176: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jan 14 14:53:36.503: INFO: Number of nodes with available pods: 0
Jan 14 14:53:36.503: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:53:37.508: INFO: Number of nodes with available pods: 0
Jan 14 14:53:37.508: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:53:38.509: INFO: Number of nodes with available pods: 0
Jan 14 14:53:38.509: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:53:39.600: INFO: Number of nodes with available pods: 0
Jan 14 14:53:39.600: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:53:41.557: INFO: Number of nodes with available pods: 0
Jan 14 14:53:41.557: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:53:43.630: INFO: Number of nodes with available pods: 0
Jan 14 14:53:43.630: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:53:44.782: INFO: Number of nodes with available pods: 0
Jan 14 14:53:44.782: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:53:45.804: INFO: Number of nodes with available pods: 0
Jan 14 14:53:45.804: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:53:47.070: INFO: Number of nodes with available pods: 0
Jan 14 14:53:47.070: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:53:47.837: INFO: Number of nodes with available pods: 0
Jan 14 14:53:47.838: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:53:48.521: INFO: Number of nodes with available pods: 0
Jan 14 14:53:48.521: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:53:49.509: INFO: Number of nodes with available pods: 0
Jan 14 14:53:49.509: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:53:50.681: INFO: Number of nodes with available pods: 1
Jan 14 14:53:50.681: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jan 14 14:53:51.789: INFO: Number of nodes with available pods: 1
Jan 14 14:53:51.789: INFO: Number of running nodes: 0, number of available pods: 1
Jan 14 14:53:53.081: INFO: Number of nodes with available pods: 0
Jan 14 14:53:53.082: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jan 14 14:53:54.321: INFO: Number of nodes with available pods: 0
Jan 14 14:53:54.321: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:53:55.325: INFO: Number of nodes with available pods: 0
Jan 14 14:53:55.325: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:53:56.330: INFO: Number of nodes with available pods: 0
Jan 14 14:53:56.330: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:53:57.516: INFO: Number of nodes with available pods: 0
Jan 14 14:53:57.516: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:53:58.327: INFO: Number of nodes with available pods: 0
Jan 14 14:53:58.327: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:53:59.325: INFO: Number of nodes with available pods: 0
Jan 14 14:53:59.325: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:54:00.396: INFO: Number of nodes with available pods: 0
Jan 14 14:54:00.396: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:54:01.326: INFO: Number of nodes with available pods: 0
Jan 14 14:54:01.326: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:54:02.431: INFO: Number of nodes with available pods: 0
Jan 14 14:54:02.431: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:54:03.764: INFO: Number of nodes with available pods: 0
Jan 14 14:54:03.764: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:54:04.733: INFO: Number of nodes with available pods: 0
Jan 14 14:54:04.733: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:54:05.539: INFO: Number of nodes with available pods: 0
Jan 14 14:54:05.539: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:54:06.326: INFO: Number of nodes with available pods: 0
Jan 14 14:54:06.326: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:54:07.326: INFO: Number of nodes with available pods: 0
Jan 14 14:54:07.326: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:54:08.384: INFO: Number of nodes with available pods: 0
Jan 14 14:54:08.384: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:54:09.482: INFO: Number of nodes with available pods: 0
Jan 14 14:54:09.482: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:54:10.454: INFO: Number of nodes with available pods: 0
Jan 14 14:54:10.454: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:54:11.397: INFO: Number of nodes with available pods: 0
Jan 14 14:54:11.397: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:54:12.572: INFO: Number of nodes with available pods: 0
Jan 14 14:54:12.572: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:54:13.326: INFO: Number of nodes with available pods: 0
Jan 14 14:54:13.326: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:54:14.648: INFO: Number of nodes with available pods: 0
Jan 14 14:54:14.648: INFO: Node slave4 is running more than one daemon pod
Jan 14 14:54:15.699: INFO: Number of nodes with available pods: 1
Jan 14 14:54:15.699: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6848, will wait for the garbage collector to delete the pods
Jan 14 14:54:16.651: INFO: Deleting DaemonSet.extensions daemon-set took: 414.332701ms
Jan 14 14:54:17.751: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.100318781s
Jan 14 14:54:26.777: INFO: Number of nodes with available pods: 0
Jan 14 14:54:26.777: INFO: Number of running nodes: 0, number of available pods: 0
Jan 14 14:54:26.783: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6848/daemonsets","resourceVersion":"96486"},"items":null}

Jan 14 14:54:26.787: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6848/pods","resourceVersion":"96486"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 14:54:27.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6848" for this suite.

â€¢ [SLOW TEST:58.160 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":278,"completed":20,"skipped":349,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:54:27.781: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7606
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 14 14:54:31.036: INFO: Number of nodes with available pods: 0
Jan 14 14:54:31.036: INFO: Node master1 is running more than one daemon pod
Jan 14 14:54:32.525: INFO: Number of nodes with available pods: 0
Jan 14 14:54:32.525: INFO: Node master1 is running more than one daemon pod
Jan 14 14:54:34.668: INFO: Number of nodes with available pods: 0
Jan 14 14:54:34.669: INFO: Node master1 is running more than one daemon pod
Jan 14 14:54:37.001: INFO: Number of nodes with available pods: 0
Jan 14 14:54:37.001: INFO: Node master1 is running more than one daemon pod
Jan 14 14:54:37.771: INFO: Number of nodes with available pods: 0
Jan 14 14:54:37.772: INFO: Node master1 is running more than one daemon pod
Jan 14 14:54:38.926: INFO: Number of nodes with available pods: 0
Jan 14 14:54:38.929: INFO: Node master1 is running more than one daemon pod
Jan 14 14:54:39.681: INFO: Number of nodes with available pods: 0
Jan 14 14:54:39.681: INFO: Node master1 is running more than one daemon pod
Jan 14 14:54:40.164: INFO: Number of nodes with available pods: 0
Jan 14 14:54:40.164: INFO: Node master1 is running more than one daemon pod
Jan 14 14:54:41.849: INFO: Number of nodes with available pods: 0
Jan 14 14:54:41.849: INFO: Node master1 is running more than one daemon pod
Jan 14 14:54:42.901: INFO: Number of nodes with available pods: 0
Jan 14 14:54:42.901: INFO: Node master1 is running more than one daemon pod
Jan 14 14:54:45.783: INFO: Number of nodes with available pods: 2
Jan 14 14:54:45.783: INFO: Node master2 is running more than one daemon pod
Jan 14 14:54:47.896: INFO: Number of nodes with available pods: 2
Jan 14 14:54:47.897: INFO: Node master2 is running more than one daemon pod
Jan 14 14:54:49.840: INFO: Number of nodes with available pods: 4
Jan 14 14:54:49.840: INFO: Node master2 is running more than one daemon pod
Jan 14 14:54:51.062: INFO: Number of nodes with available pods: 4
Jan 14 14:54:51.062: INFO: Node master2 is running more than one daemon pod
Jan 14 14:54:52.496: INFO: Number of nodes with available pods: 6
Jan 14 14:54:52.496: INFO: Node master2 is running more than one daemon pod
Jan 14 14:54:54.162: INFO: Number of nodes with available pods: 7
Jan 14 14:54:54.162: INFO: Number of running nodes: 7, number of available pods: 7
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jan 14 14:54:54.653: INFO: Number of nodes with available pods: 6
Jan 14 14:54:54.653: INFO: Node master3 is running more than one daemon pod
Jan 14 14:54:55.810: INFO: Number of nodes with available pods: 6
Jan 14 14:54:55.810: INFO: Node master3 is running more than one daemon pod
Jan 14 14:54:56.767: INFO: Number of nodes with available pods: 6
Jan 14 14:54:56.767: INFO: Node master3 is running more than one daemon pod
Jan 14 14:54:57.877: INFO: Number of nodes with available pods: 6
Jan 14 14:54:57.877: INFO: Node master3 is running more than one daemon pod
Jan 14 14:54:58.688: INFO: Number of nodes with available pods: 6
Jan 14 14:54:58.688: INFO: Node master3 is running more than one daemon pod
Jan 14 14:55:00.249: INFO: Number of nodes with available pods: 6
Jan 14 14:55:00.249: INFO: Node master3 is running more than one daemon pod
Jan 14 14:55:01.088: INFO: Number of nodes with available pods: 6
Jan 14 14:55:01.088: INFO: Node master3 is running more than one daemon pod
Jan 14 14:55:01.936: INFO: Number of nodes with available pods: 6
Jan 14 14:55:01.936: INFO: Node master3 is running more than one daemon pod
Jan 14 14:55:03.640: INFO: Number of nodes with available pods: 6
Jan 14 14:55:03.640: INFO: Node master3 is running more than one daemon pod
Jan 14 14:55:04.137: INFO: Number of nodes with available pods: 6
Jan 14 14:55:04.137: INFO: Node master3 is running more than one daemon pod
Jan 14 14:55:06.247: INFO: Number of nodes with available pods: 6
Jan 14 14:55:06.247: INFO: Node master3 is running more than one daemon pod
Jan 14 14:55:07.782: INFO: Number of nodes with available pods: 6
Jan 14 14:55:07.782: INFO: Node master3 is running more than one daemon pod
Jan 14 14:55:08.743: INFO: Number of nodes with available pods: 6
Jan 14 14:55:08.743: INFO: Node master3 is running more than one daemon pod
Jan 14 14:55:09.851: INFO: Number of nodes with available pods: 6
Jan 14 14:55:09.851: INFO: Node master3 is running more than one daemon pod
Jan 14 14:55:10.749: INFO: Number of nodes with available pods: 6
Jan 14 14:55:10.749: INFO: Node master3 is running more than one daemon pod
Jan 14 14:55:11.753: INFO: Number of nodes with available pods: 6
Jan 14 14:55:11.753: INFO: Node master3 is running more than one daemon pod
Jan 14 14:55:12.911: INFO: Number of nodes with available pods: 6
Jan 14 14:55:12.911: INFO: Node master3 is running more than one daemon pod
Jan 14 14:55:13.982: INFO: Number of nodes with available pods: 6
Jan 14 14:55:13.982: INFO: Node master3 is running more than one daemon pod
Jan 14 14:55:14.945: INFO: Number of nodes with available pods: 6
Jan 14 14:55:14.945: INFO: Node master3 is running more than one daemon pod
Jan 14 14:55:15.810: INFO: Number of nodes with available pods: 6
Jan 14 14:55:15.810: INFO: Node master3 is running more than one daemon pod
Jan 14 14:55:16.915: INFO: Number of nodes with available pods: 7
Jan 14 14:55:16.915: INFO: Number of running nodes: 7, number of available pods: 7
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7606, will wait for the garbage collector to delete the pods
Jan 14 14:55:17.180: INFO: Deleting DaemonSet.extensions daemon-set took: 208.24674ms
Jan 14 14:55:20.381: INFO: Terminating DaemonSet.extensions daemon-set pods took: 3.20101334s
Jan 14 14:55:35.187: INFO: Number of nodes with available pods: 0
Jan 14 14:55:35.187: INFO: Number of running nodes: 0, number of available pods: 0
Jan 14 14:55:35.191: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7606/daemonsets","resourceVersion":"96843"},"items":null}

Jan 14 14:55:35.194: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7606/pods","resourceVersion":"96843"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 14:55:35.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7606" for this suite.

â€¢ [SLOW TEST:67.741 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":278,"completed":21,"skipped":356,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:55:35.522: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-773
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating all guestbook components
Jan 14 14:55:36.882: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Jan 14 14:55:36.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 create -f - --namespace=kubectl-773'
Jan 14 14:55:45.027: INFO: stderr: ""
Jan 14 14:55:45.027: INFO: stdout: "service/agnhost-slave created\n"
Jan 14 14:55:45.027: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Jan 14 14:55:45.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 create -f - --namespace=kubectl-773'
Jan 14 14:55:46.756: INFO: stderr: ""
Jan 14 14:55:46.756: INFO: stdout: "service/agnhost-master created\n"
Jan 14 14:55:46.757: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 14 14:55:46.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 create -f - --namespace=kubectl-773'
Jan 14 14:55:48.050: INFO: stderr: ""
Jan 14 14:55:48.050: INFO: stdout: "service/frontend created\n"
Jan 14 14:55:48.050: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 14 14:55:48.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 create -f - --namespace=kubectl-773'
Jan 14 14:55:49.410: INFO: stderr: ""
Jan 14 14:55:49.410: INFO: stdout: "deployment.apps/frontend created\n"
Jan 14 14:55:49.412: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 14 14:55:49.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 create -f - --namespace=kubectl-773'
Jan 14 14:55:50.667: INFO: stderr: ""
Jan 14 14:55:50.667: INFO: stdout: "deployment.apps/agnhost-master created\n"
Jan 14 14:55:50.667: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 14 14:55:50.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 create -f - --namespace=kubectl-773'
Jan 14 14:55:52.091: INFO: stderr: ""
Jan 14 14:55:52.091: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Jan 14 14:55:52.091: INFO: Waiting for all frontend pods to be Running.
Jan 14 14:56:07.143: INFO: Waiting for frontend to serve content.
Jan 14 14:56:12.650: INFO: Failed to get response from guestbook. err: the server responded with the status code 417 but did not return more information (get services frontend), response: 
Jan 14 14:56:17.674: INFO: Trying to add a new entry to the guestbook.
Jan 14 14:56:17.702: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jan 14 14:56:17.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 delete --grace-period=0 --force -f - --namespace=kubectl-773'
Jan 14 14:56:19.261: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 14 14:56:19.261: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Jan 14 14:56:19.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 delete --grace-period=0 --force -f - --namespace=kubectl-773'
Jan 14 14:56:20.889: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 14 14:56:20.889: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Jan 14 14:56:20.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 delete --grace-period=0 --force -f - --namespace=kubectl-773'
Jan 14 14:56:22.683: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 14 14:56:22.683: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 14 14:56:22.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 delete --grace-period=0 --force -f - --namespace=kubectl-773'
Jan 14 14:56:23.611: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 14 14:56:23.611: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 14 14:56:23.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 delete --grace-period=0 --force -f - --namespace=kubectl-773'
Jan 14 14:56:24.515: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 14 14:56:24.515: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Jan 14 14:56:24.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 delete --grace-period=0 --force -f - --namespace=kubectl-773'
Jan 14 14:56:25.354: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 14 14:56:25.354: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 14:56:25.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-773" for this suite.

â€¢ [SLOW TEST:50.640 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:385
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":278,"completed":22,"skipped":365,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:56:26.163: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2187
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Update Demo
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:329
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Jan 14 14:56:28.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 create -f - --namespace=kubectl-2187'
Jan 14 14:56:30.131: INFO: stderr: ""
Jan 14 14:56:30.131: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 14 14:56:30.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2187'
Jan 14 14:56:30.748: INFO: stderr: ""
Jan 14 14:56:30.748: INFO: stdout: ""
STEP: Replicas for name=update-demo: expected=2 actual=0
Jan 14 14:56:35.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2187'
Jan 14 14:56:36.590: INFO: stderr: ""
Jan 14 14:56:36.590: INFO: stdout: "update-demo-nautilus-nqclx update-demo-nautilus-qpvjg "
Jan 14 14:56:36.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-nautilus-nqclx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2187'
Jan 14 14:56:36.877: INFO: stderr: ""
Jan 14 14:56:36.877: INFO: stdout: ""
Jan 14 14:56:36.877: INFO: update-demo-nautilus-nqclx is created but not running
Jan 14 14:56:41.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2187'
Jan 14 14:56:42.516: INFO: stderr: ""
Jan 14 14:56:42.517: INFO: stdout: "update-demo-nautilus-nqclx update-demo-nautilus-qpvjg "
Jan 14 14:56:42.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-nautilus-nqclx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2187'
Jan 14 14:56:43.063: INFO: stderr: ""
Jan 14 14:56:43.063: INFO: stdout: ""
Jan 14 14:56:43.063: INFO: update-demo-nautilus-nqclx is created but not running
Jan 14 14:56:48.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2187'
Jan 14 14:56:48.388: INFO: stderr: ""
Jan 14 14:56:48.388: INFO: stdout: "update-demo-nautilus-nqclx update-demo-nautilus-qpvjg "
Jan 14 14:56:48.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-nautilus-nqclx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2187'
Jan 14 14:56:49.336: INFO: stderr: ""
Jan 14 14:56:49.336: INFO: stdout: "true"
Jan 14 14:56:49.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-nautilus-nqclx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2187'
Jan 14 14:56:49.787: INFO: stderr: ""
Jan 14 14:56:49.787: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 14 14:56:49.787: INFO: validating pod update-demo-nautilus-nqclx
Jan 14 14:56:50.273: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 14 14:56:50.274: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 14 14:56:50.274: INFO: update-demo-nautilus-nqclx is verified up and running
Jan 14 14:56:50.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-nautilus-qpvjg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2187'
Jan 14 14:56:50.596: INFO: stderr: ""
Jan 14 14:56:50.596: INFO: stdout: "true"
Jan 14 14:56:50.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-nautilus-qpvjg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2187'
Jan 14 14:56:51.601: INFO: stderr: ""
Jan 14 14:56:51.601: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 14 14:56:51.601: INFO: validating pod update-demo-nautilus-qpvjg
Jan 14 14:56:51.954: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 14 14:56:51.954: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 14 14:56:51.954: INFO: update-demo-nautilus-qpvjg is verified up and running
STEP: scaling down the replication controller
Jan 14 14:56:51.957: INFO: scanned /root for discovery docs: <nil>
Jan 14 14:56:51.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-2187'
Jan 14 14:56:56.266: INFO: stderr: ""
Jan 14 14:56:56.266: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 14 14:56:56.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2187'
Jan 14 14:56:56.652: INFO: stderr: ""
Jan 14 14:56:56.652: INFO: stdout: "update-demo-nautilus-nqclx update-demo-nautilus-qpvjg "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jan 14 14:57:01.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2187'
Jan 14 14:57:02.148: INFO: stderr: ""
Jan 14 14:57:02.148: INFO: stdout: "update-demo-nautilus-nqclx update-demo-nautilus-qpvjg "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jan 14 14:57:07.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2187'
Jan 14 14:57:07.700: INFO: stderr: ""
Jan 14 14:57:07.700: INFO: stdout: "update-demo-nautilus-nqclx "
Jan 14 14:57:07.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-nautilus-nqclx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2187'
Jan 14 14:57:07.939: INFO: stderr: ""
Jan 14 14:57:07.947: INFO: stdout: "true"
Jan 14 14:57:07.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-nautilus-nqclx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2187'
Jan 14 14:57:08.158: INFO: stderr: ""
Jan 14 14:57:08.158: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 14 14:57:08.158: INFO: validating pod update-demo-nautilus-nqclx
Jan 14 14:57:08.437: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 14 14:57:08.437: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 14 14:57:08.438: INFO: update-demo-nautilus-nqclx is verified up and running
STEP: scaling up the replication controller
Jan 14 14:57:08.441: INFO: scanned /root for discovery docs: <nil>
Jan 14 14:57:08.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-2187'
Jan 14 14:57:11.173: INFO: stderr: ""
Jan 14 14:57:11.173: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 14 14:57:11.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2187'
Jan 14 14:57:11.840: INFO: stderr: ""
Jan 14 14:57:11.840: INFO: stdout: "update-demo-nautilus-nqclx update-demo-nautilus-wclm9 "
Jan 14 14:57:11.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-nautilus-nqclx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2187'
Jan 14 14:57:12.397: INFO: stderr: ""
Jan 14 14:57:12.397: INFO: stdout: "true"
Jan 14 14:57:12.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-nautilus-nqclx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2187'
Jan 14 14:57:13.113: INFO: stderr: ""
Jan 14 14:57:13.118: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 14 14:57:13.118: INFO: validating pod update-demo-nautilus-nqclx
Jan 14 14:57:13.451: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 14 14:57:13.451: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 14 14:57:13.451: INFO: update-demo-nautilus-nqclx is verified up and running
Jan 14 14:57:13.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-nautilus-wclm9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2187'
Jan 14 14:57:13.937: INFO: stderr: ""
Jan 14 14:57:13.937: INFO: stdout: ""
Jan 14 14:57:13.937: INFO: update-demo-nautilus-wclm9 is created but not running
Jan 14 14:57:18.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2187'
Jan 14 14:57:19.217: INFO: stderr: ""
Jan 14 14:57:19.217: INFO: stdout: "update-demo-nautilus-nqclx update-demo-nautilus-wclm9 "
Jan 14 14:57:19.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-nautilus-nqclx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2187'
Jan 14 14:57:19.946: INFO: stderr: ""
Jan 14 14:57:19.946: INFO: stdout: "true"
Jan 14 14:57:19.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-nautilus-nqclx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2187'
Jan 14 14:57:20.199: INFO: stderr: ""
Jan 14 14:57:20.199: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 14 14:57:20.199: INFO: validating pod update-demo-nautilus-nqclx
Jan 14 14:57:20.257: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 14 14:57:20.257: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 14 14:57:20.257: INFO: update-demo-nautilus-nqclx is verified up and running
Jan 14 14:57:20.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-nautilus-wclm9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2187'
Jan 14 14:57:20.414: INFO: stderr: ""
Jan 14 14:57:20.417: INFO: stdout: "true"
Jan 14 14:57:20.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-nautilus-wclm9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2187'
Jan 14 14:57:21.018: INFO: stderr: ""
Jan 14 14:57:21.018: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 14 14:57:21.018: INFO: validating pod update-demo-nautilus-wclm9
Jan 14 14:57:21.029: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 14 14:57:21.029: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 14 14:57:21.029: INFO: update-demo-nautilus-wclm9 is verified up and running
STEP: using delete to clean up resources
Jan 14 14:57:21.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 delete --grace-period=0 --force -f - --namespace=kubectl-2187'
Jan 14 14:57:21.224: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 14 14:57:21.224: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 14 14:57:21.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2187'
Jan 14 14:57:22.061: INFO: stderr: "No resources found in kubectl-2187 namespace.\n"
Jan 14 14:57:22.061: INFO: stdout: ""
Jan 14 14:57:22.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods -l name=update-demo --namespace=kubectl-2187 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 14 14:57:22.577: INFO: stderr: ""
Jan 14 14:57:22.577: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 14:57:22.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2187" for this suite.

â€¢ [SLOW TEST:56.930 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:327
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":278,"completed":23,"skipped":379,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:57:23.093: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2779
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2779.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2779.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 14 14:57:46.805: INFO: DNS probes using dns-2779/dns-test-8bb2aa44-5589-46d5-9d1d-09cdfece9da3 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 14:57:48.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2779" for this suite.

â€¢ [SLOW TEST:25.927 seconds]
[sig-network] DNS
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":278,"completed":24,"skipped":388,"failed":0}
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:57:49.020: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6219
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-874fe605-ebae-4db8-82c1-678c6efe9e3d
STEP: Creating a pod to test consume secrets
Jan 14 14:57:52.405: INFO: Waiting up to 5m0s for pod "pod-secrets-b660d62b-1a58-4953-8350-82fa1280e5e9" in namespace "secrets-6219" to be "success or failure"
Jan 14 14:57:52.654: INFO: Pod "pod-secrets-b660d62b-1a58-4953-8350-82fa1280e5e9": Phase="Pending", Reason="", readiness=false. Elapsed: 249.135758ms
Jan 14 14:57:55.138: INFO: Pod "pod-secrets-b660d62b-1a58-4953-8350-82fa1280e5e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.732494467s
Jan 14 14:57:57.305: INFO: Pod "pod-secrets-b660d62b-1a58-4953-8350-82fa1280e5e9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.899936071s
Jan 14 14:57:59.913: INFO: Pod "pod-secrets-b660d62b-1a58-4953-8350-82fa1280e5e9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.507463211s
Jan 14 14:58:01.952: INFO: Pod "pod-secrets-b660d62b-1a58-4953-8350-82fa1280e5e9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.547436619s
Jan 14 14:58:04.576: INFO: Pod "pod-secrets-b660d62b-1a58-4953-8350-82fa1280e5e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.170512763s
STEP: Saw pod success
Jan 14 14:58:04.576: INFO: Pod "pod-secrets-b660d62b-1a58-4953-8350-82fa1280e5e9" satisfied condition "success or failure"
Jan 14 14:58:05.203: INFO: Trying to get logs from node slave2 pod pod-secrets-b660d62b-1a58-4953-8350-82fa1280e5e9 container secret-volume-test: <nil>
STEP: delete the pod
Jan 14 14:58:05.581: INFO: Waiting for pod pod-secrets-b660d62b-1a58-4953-8350-82fa1280e5e9 to disappear
Jan 14 14:58:05.997: INFO: Pod pod-secrets-b660d62b-1a58-4953-8350-82fa1280e5e9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 14:58:05.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6219" for this suite.

â€¢ [SLOW TEST:17.169 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":278,"completed":25,"skipped":388,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:58:06.190: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6063
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-f6aaa436-a84f-472c-b7ce-af4625c1a162
STEP: Creating a pod to test consume secrets
Jan 14 14:58:09.481: INFO: Waiting up to 5m0s for pod "pod-secrets-3809f143-7626-45ae-b913-041dd87aa6bf" in namespace "secrets-6063" to be "success or failure"
Jan 14 14:58:09.830: INFO: Pod "pod-secrets-3809f143-7626-45ae-b913-041dd87aa6bf": Phase="Pending", Reason="", readiness=false. Elapsed: 348.978623ms
Jan 14 14:58:12.382: INFO: Pod "pod-secrets-3809f143-7626-45ae-b913-041dd87aa6bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.901271569s
Jan 14 14:58:15.431: INFO: Pod "pod-secrets-3809f143-7626-45ae-b913-041dd87aa6bf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.950416027s
Jan 14 14:58:17.697: INFO: Pod "pod-secrets-3809f143-7626-45ae-b913-041dd87aa6bf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.215982362s
Jan 14 14:58:20.224: INFO: Pod "pod-secrets-3809f143-7626-45ae-b913-041dd87aa6bf": Phase="Pending", Reason="", readiness=false. Elapsed: 10.742886942s
Jan 14 14:58:22.268: INFO: Pod "pod-secrets-3809f143-7626-45ae-b913-041dd87aa6bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.787069145s
STEP: Saw pod success
Jan 14 14:58:22.268: INFO: Pod "pod-secrets-3809f143-7626-45ae-b913-041dd87aa6bf" satisfied condition "success or failure"
Jan 14 14:58:22.329: INFO: Trying to get logs from node slave2 pod pod-secrets-3809f143-7626-45ae-b913-041dd87aa6bf container secret-volume-test: <nil>
STEP: delete the pod
Jan 14 14:58:23.918: INFO: Waiting for pod pod-secrets-3809f143-7626-45ae-b913-041dd87aa6bf to disappear
Jan 14 14:58:23.923: INFO: Pod pod-secrets-3809f143-7626-45ae-b913-041dd87aa6bf no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 14:58:23.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6063" for this suite.

â€¢ [SLOW TEST:17.747 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":278,"completed":26,"skipped":405,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:58:23.937: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9883
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 14 14:58:28.686: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 14 14:58:32.179: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610709, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610709, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610710, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610708, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 14:58:34.298: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610709, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610709, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610710, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610708, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 14:58:36.349: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610709, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610709, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610710, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610708, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 14:58:39.153: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610709, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610709, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610710, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610708, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 14:58:40.230: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610709, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610709, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610710, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714610708, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 14 14:58:44.527: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 14:58:58.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9883" for this suite.
STEP: Destroying namespace "webhook-9883-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:35.456 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":278,"completed":27,"skipped":415,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:58:59.393: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-5473
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 14 14:59:35.081: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 14 14:59:35.496: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 14 14:59:37.496: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 14 14:59:37.950: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 14 14:59:39.496: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 14 14:59:39.553: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 14 14:59:41.496: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 14 14:59:41.514: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 14 14:59:43.496: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 14 14:59:43.703: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 14 14:59:45.496: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 14 14:59:45.500: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 14:59:45.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5473" for this suite.

â€¢ [SLOW TEST:46.415 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":278,"completed":28,"skipped":438,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:59:45.809: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-849
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Starting the proxy
Jan 14 14:59:48.300: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-321483740 proxy --unix-socket=/tmp/kubectl-proxy-unix849829887/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 14:59:48.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-849" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":278,"completed":29,"skipped":444,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 14:59:48.440: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-7093
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:00:06.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7093" for this suite.

â€¢ [SLOW TEST:19.459 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":278,"completed":30,"skipped":475,"failed":0}
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:00:07.900: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2707
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jan 14 15:00:13.628: INFO: Waiting up to 5m0s for pod "downward-api-0765a861-3522-419d-b6b0-2a1de9b52596" in namespace "downward-api-2707" to be "success or failure"
Jan 14 15:00:14.241: INFO: Pod "downward-api-0765a861-3522-419d-b6b0-2a1de9b52596": Phase="Pending", Reason="", readiness=false. Elapsed: 612.611824ms
Jan 14 15:00:16.636: INFO: Pod "downward-api-0765a861-3522-419d-b6b0-2a1de9b52596": Phase="Pending", Reason="", readiness=false. Elapsed: 3.007124519s
Jan 14 15:00:19.281: INFO: Pod "downward-api-0765a861-3522-419d-b6b0-2a1de9b52596": Phase="Pending", Reason="", readiness=false. Elapsed: 5.652953602s
Jan 14 15:00:21.877: INFO: Pod "downward-api-0765a861-3522-419d-b6b0-2a1de9b52596": Phase="Pending", Reason="", readiness=false. Elapsed: 8.248964642s
Jan 14 15:00:25.211: INFO: Pod "downward-api-0765a861-3522-419d-b6b0-2a1de9b52596": Phase="Pending", Reason="", readiness=false. Elapsed: 11.582814163s
Jan 14 15:00:27.248: INFO: Pod "downward-api-0765a861-3522-419d-b6b0-2a1de9b52596": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.619586247s
STEP: Saw pod success
Jan 14 15:00:27.248: INFO: Pod "downward-api-0765a861-3522-419d-b6b0-2a1de9b52596" satisfied condition "success or failure"
Jan 14 15:00:27.251: INFO: Trying to get logs from node slave2 pod downward-api-0765a861-3522-419d-b6b0-2a1de9b52596 container dapi-container: <nil>
STEP: delete the pod
Jan 14 15:00:29.462: INFO: Waiting for pod downward-api-0765a861-3522-419d-b6b0-2a1de9b52596 to disappear
Jan 14 15:00:29.488: INFO: Pod downward-api-0765a861-3522-419d-b6b0-2a1de9b52596 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:00:29.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2707" for this suite.

â€¢ [SLOW TEST:22.092 seconds]
[sig-node] Downward API
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:33
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":278,"completed":31,"skipped":475,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:00:29.992: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6336
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-ddcedbbb-c8be-4ad7-9c5d-66d625d9f767 in namespace container-probe-6336
Jan 14 15:00:45.385: INFO: Started pod busybox-ddcedbbb-c8be-4ad7-9c5d-66d625d9f767 in namespace container-probe-6336
STEP: checking the pod's current state and verifying that restartCount is present
Jan 14 15:00:45.641: INFO: Initial restart count of pod busybox-ddcedbbb-c8be-4ad7-9c5d-66d625d9f767 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:04:47.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6336" for this suite.

â€¢ [SLOW TEST:258.077 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":278,"completed":32,"skipped":486,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:04:48.070: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8522
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating cluster-info
Jan 14 15:04:48.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 cluster-info'
Jan 14 15:04:49.113: INFO: stderr: ""
Jan 14 15:04:49.113: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.150.0.1:443\x1b[0m\n\x1b[0;32mcoredns\x1b[0m is running at \x1b[0;33mhttps://10.150.0.1:443/api/v1/namespaces/kube-system/services/coredns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:04:49.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8522" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":278,"completed":33,"skipped":487,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:04:49.131: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6292
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 14 15:04:50.347: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fec86bc9-2ed9-42d6-8a48-bc6dca35d4e5" in namespace "downward-api-6292" to be "success or failure"
Jan 14 15:04:50.353: INFO: Pod "downwardapi-volume-fec86bc9-2ed9-42d6-8a48-bc6dca35d4e5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.497887ms
Jan 14 15:04:52.384: INFO: Pod "downwardapi-volume-fec86bc9-2ed9-42d6-8a48-bc6dca35d4e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03666778s
Jan 14 15:04:54.582: INFO: Pod "downwardapi-volume-fec86bc9-2ed9-42d6-8a48-bc6dca35d4e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.234454029s
Jan 14 15:04:56.624: INFO: Pod "downwardapi-volume-fec86bc9-2ed9-42d6-8a48-bc6dca35d4e5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.276421772s
Jan 14 15:04:58.655: INFO: Pod "downwardapi-volume-fec86bc9-2ed9-42d6-8a48-bc6dca35d4e5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.307108268s
Jan 14 15:05:00.683: INFO: Pod "downwardapi-volume-fec86bc9-2ed9-42d6-8a48-bc6dca35d4e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.335679969s
STEP: Saw pod success
Jan 14 15:05:00.683: INFO: Pod "downwardapi-volume-fec86bc9-2ed9-42d6-8a48-bc6dca35d4e5" satisfied condition "success or failure"
Jan 14 15:05:00.694: INFO: Trying to get logs from node slave2 pod downwardapi-volume-fec86bc9-2ed9-42d6-8a48-bc6dca35d4e5 container client-container: <nil>
STEP: delete the pod
Jan 14 15:05:01.145: INFO: Waiting for pod downwardapi-volume-fec86bc9-2ed9-42d6-8a48-bc6dca35d4e5 to disappear
Jan 14 15:05:01.148: INFO: Pod downwardapi-volume-fec86bc9-2ed9-42d6-8a48-bc6dca35d4e5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:05:01.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6292" for this suite.

â€¢ [SLOW TEST:12.079 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":278,"completed":34,"skipped":494,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:05:01.211: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-99
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-42ca11be-3eb4-4d5d-a7d1-d68b10e41442
STEP: Creating a pod to test consume secrets
Jan 14 15:05:04.019: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3bbfc3e7-fca3-43c9-a684-b235c3d6bdf7" in namespace "projected-99" to be "success or failure"
Jan 14 15:05:04.340: INFO: Pod "pod-projected-secrets-3bbfc3e7-fca3-43c9-a684-b235c3d6bdf7": Phase="Pending", Reason="", readiness=false. Elapsed: 321.759056ms
Jan 14 15:05:06.347: INFO: Pod "pod-projected-secrets-3bbfc3e7-fca3-43c9-a684-b235c3d6bdf7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.327788929s
Jan 14 15:05:08.351: INFO: Pod "pod-projected-secrets-3bbfc3e7-fca3-43c9-a684-b235c3d6bdf7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.332295986s
Jan 14 15:05:10.519: INFO: Pod "pod-projected-secrets-3bbfc3e7-fca3-43c9-a684-b235c3d6bdf7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.499844782s
Jan 14 15:05:12.804: INFO: Pod "pod-projected-secrets-3bbfc3e7-fca3-43c9-a684-b235c3d6bdf7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.785536029s
Jan 14 15:05:15.072: INFO: Pod "pod-projected-secrets-3bbfc3e7-fca3-43c9-a684-b235c3d6bdf7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.052984995s
STEP: Saw pod success
Jan 14 15:05:15.072: INFO: Pod "pod-projected-secrets-3bbfc3e7-fca3-43c9-a684-b235c3d6bdf7" satisfied condition "success or failure"
Jan 14 15:05:15.165: INFO: Trying to get logs from node slave2 pod pod-projected-secrets-3bbfc3e7-fca3-43c9-a684-b235c3d6bdf7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 14 15:05:16.682: INFO: Waiting for pod pod-projected-secrets-3bbfc3e7-fca3-43c9-a684-b235c3d6bdf7 to disappear
Jan 14 15:05:16.691: INFO: Pod pod-projected-secrets-3bbfc3e7-fca3-43c9-a684-b235c3d6bdf7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:05:16.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-99" for this suite.

â€¢ [SLOW TEST:15.616 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":35,"skipped":509,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:05:16.828: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-372
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 15:05:18.667: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:06:24.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-372" for this suite.

â€¢ [SLOW TEST:67.837 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:47
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":278,"completed":36,"skipped":515,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:06:24.666: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8399
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name secret-emptykey-test-0733055a-1bd9-49a7-aac6-39fe2d38aea6
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:06:25.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8399" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":278,"completed":37,"skipped":522,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:06:25.859: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7153
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:06:39.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7153" for this suite.

â€¢ [SLOW TEST:13.724 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":278,"completed":38,"skipped":533,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:06:39.585: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6644
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 15:06:41.129: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:06:49.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6644" for this suite.

â€¢ [SLOW TEST:9.882 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":278,"completed":39,"skipped":543,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:06:49.465: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8931
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 14 15:06:51.501: INFO: Waiting up to 5m0s for pod "pod-77ca2553-4976-4363-9c74-6b48dbcf17b6" in namespace "emptydir-8931" to be "success or failure"
Jan 14 15:06:51.767: INFO: Pod "pod-77ca2553-4976-4363-9c74-6b48dbcf17b6": Phase="Pending", Reason="", readiness=false. Elapsed: 265.631317ms
Jan 14 15:06:53.844: INFO: Pod "pod-77ca2553-4976-4363-9c74-6b48dbcf17b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.34323256s
Jan 14 15:06:56.109: INFO: Pod "pod-77ca2553-4976-4363-9c74-6b48dbcf17b6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.607909079s
Jan 14 15:06:58.114: INFO: Pod "pod-77ca2553-4976-4363-9c74-6b48dbcf17b6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.6124405s
Jan 14 15:07:00.501: INFO: Pod "pod-77ca2553-4976-4363-9c74-6b48dbcf17b6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.000282052s
Jan 14 15:07:02.782: INFO: Pod "pod-77ca2553-4976-4363-9c74-6b48dbcf17b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.280998222s
STEP: Saw pod success
Jan 14 15:07:02.782: INFO: Pod "pod-77ca2553-4976-4363-9c74-6b48dbcf17b6" satisfied condition "success or failure"
Jan 14 15:07:03.138: INFO: Trying to get logs from node slave2 pod pod-77ca2553-4976-4363-9c74-6b48dbcf17b6 container test-container: <nil>
STEP: delete the pod
Jan 14 15:07:04.164: INFO: Waiting for pod pod-77ca2553-4976-4363-9c74-6b48dbcf17b6 to disappear
Jan 14 15:07:04.683: INFO: Pod pod-77ca2553-4976-4363-9c74-6b48dbcf17b6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:07:04.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8931" for this suite.

â€¢ [SLOW TEST:15.646 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":40,"skipped":544,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:07:05.148: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9416
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:08:07.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9416" for this suite.

â€¢ [SLOW TEST:62.981 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":278,"completed":41,"skipped":546,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:08:08.130: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6051
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jan 14 15:08:10.686: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6051 /api/v1/namespaces/watch-6051/configmaps/e2e-watch-test-configmap-a 5a86b2e7-bdd2-40bd-a3b4-293d4aca933e 99926 0 2020-01-14 15:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 14 15:08:10.686: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6051 /api/v1/namespaces/watch-6051/configmaps/e2e-watch-test-configmap-a 5a86b2e7-bdd2-40bd-a3b4-293d4aca933e 99926 0 2020-01-14 15:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jan 14 15:08:20.937: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6051 /api/v1/namespaces/watch-6051/configmaps/e2e-watch-test-configmap-a 5a86b2e7-bdd2-40bd-a3b4-293d4aca933e 99974 0 2020-01-14 15:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jan 14 15:08:20.937: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6051 /api/v1/namespaces/watch-6051/configmaps/e2e-watch-test-configmap-a 5a86b2e7-bdd2-40bd-a3b4-293d4aca933e 99974 0 2020-01-14 15:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jan 14 15:08:31.115: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6051 /api/v1/namespaces/watch-6051/configmaps/e2e-watch-test-configmap-a 5a86b2e7-bdd2-40bd-a3b4-293d4aca933e 100004 0 2020-01-14 15:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 14 15:08:31.115: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6051 /api/v1/namespaces/watch-6051/configmaps/e2e-watch-test-configmap-a 5a86b2e7-bdd2-40bd-a3b4-293d4aca933e 100004 0 2020-01-14 15:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jan 14 15:08:41.431: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6051 /api/v1/namespaces/watch-6051/configmaps/e2e-watch-test-configmap-a 5a86b2e7-bdd2-40bd-a3b4-293d4aca933e 100036 0 2020-01-14 15:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 14 15:08:41.431: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6051 /api/v1/namespaces/watch-6051/configmaps/e2e-watch-test-configmap-a 5a86b2e7-bdd2-40bd-a3b4-293d4aca933e 100036 0 2020-01-14 15:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jan 14 15:08:51.500: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6051 /api/v1/namespaces/watch-6051/configmaps/e2e-watch-test-configmap-b 5893c6cf-b7da-4e55-af6a-67de231cf0ea 100065 0 2020-01-14 15:08:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 14 15:08:51.500: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6051 /api/v1/namespaces/watch-6051/configmaps/e2e-watch-test-configmap-b 5893c6cf-b7da-4e55-af6a-67de231cf0ea 100065 0 2020-01-14 15:08:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jan 14 15:09:01.792: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6051 /api/v1/namespaces/watch-6051/configmaps/e2e-watch-test-configmap-b 5893c6cf-b7da-4e55-af6a-67de231cf0ea 100094 0 2020-01-14 15:08:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 14 15:09:01.792: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6051 /api/v1/namespaces/watch-6051/configmaps/e2e-watch-test-configmap-b 5893c6cf-b7da-4e55-af6a-67de231cf0ea 100094 0 2020-01-14 15:08:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:09:11.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6051" for this suite.

â€¢ [SLOW TEST:63.677 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":278,"completed":42,"skipped":556,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:09:11.810: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3503
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 15:09:13.439: INFO: Create a RollingUpdate DaemonSet
Jan 14 15:09:13.802: INFO: Check that daemon pods launch on every node of the cluster
Jan 14 15:09:13.964: INFO: Number of nodes with available pods: 0
Jan 14 15:09:13.964: INFO: Node master1 is running more than one daemon pod
Jan 14 15:09:15.676: INFO: Number of nodes with available pods: 0
Jan 14 15:09:15.676: INFO: Node master1 is running more than one daemon pod
Jan 14 15:09:18.084: INFO: Number of nodes with available pods: 0
Jan 14 15:09:18.084: INFO: Node master1 is running more than one daemon pod
Jan 14 15:09:19.119: INFO: Number of nodes with available pods: 0
Jan 14 15:09:19.120: INFO: Node master1 is running more than one daemon pod
Jan 14 15:09:20.631: INFO: Number of nodes with available pods: 0
Jan 14 15:09:20.631: INFO: Node master1 is running more than one daemon pod
Jan 14 15:09:21.549: INFO: Number of nodes with available pods: 0
Jan 14 15:09:21.549: INFO: Node master1 is running more than one daemon pod
Jan 14 15:09:22.407: INFO: Number of nodes with available pods: 0
Jan 14 15:09:22.407: INFO: Node master1 is running more than one daemon pod
Jan 14 15:09:23.616: INFO: Number of nodes with available pods: 0
Jan 14 15:09:23.616: INFO: Node master1 is running more than one daemon pod
Jan 14 15:09:26.358: INFO: Number of nodes with available pods: 0
Jan 14 15:09:26.359: INFO: Node master1 is running more than one daemon pod
Jan 14 15:09:28.296: INFO: Number of nodes with available pods: 3
Jan 14 15:09:28.296: INFO: Node master1 is running more than one daemon pod
Jan 14 15:09:29.184: INFO: Number of nodes with available pods: 5
Jan 14 15:09:29.187: INFO: Node master1 is running more than one daemon pod
Jan 14 15:09:29.975: INFO: Number of nodes with available pods: 7
Jan 14 15:09:29.975: INFO: Number of running nodes: 7, number of available pods: 7
Jan 14 15:09:29.975: INFO: Update the DaemonSet to trigger a rollout
Jan 14 15:09:29.985: INFO: Updating DaemonSet daemon-set
Jan 14 15:09:40.012: INFO: Roll back the DaemonSet before rollout is complete
Jan 14 15:09:40.032: INFO: Updating DaemonSet daemon-set
Jan 14 15:09:40.035: INFO: Make sure DaemonSet rollback is complete
Jan 14 15:09:40.048: INFO: Wrong image for pod: daemon-set-sfqsl. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jan 14 15:09:40.048: INFO: Pod daemon-set-sfqsl is not available
Jan 14 15:09:41.486: INFO: Wrong image for pod: daemon-set-sfqsl. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jan 14 15:09:41.487: INFO: Pod daemon-set-sfqsl is not available
Jan 14 15:09:42.414: INFO: Wrong image for pod: daemon-set-sfqsl. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jan 14 15:09:42.414: INFO: Pod daemon-set-sfqsl is not available
Jan 14 15:09:43.410: INFO: Wrong image for pod: daemon-set-sfqsl. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jan 14 15:09:43.413: INFO: Pod daemon-set-sfqsl is not available
Jan 14 15:09:44.230: INFO: Wrong image for pod: daemon-set-sfqsl. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jan 14 15:09:44.232: INFO: Pod daemon-set-sfqsl is not available
Jan 14 15:09:45.488: INFO: Wrong image for pod: daemon-set-sfqsl. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jan 14 15:09:45.491: INFO: Pod daemon-set-sfqsl is not available
Jan 14 15:09:47.584: INFO: Pod daemon-set-9jr7c is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3503, will wait for the garbage collector to delete the pods
Jan 14 15:09:47.949: INFO: Deleting DaemonSet.extensions daemon-set took: 46.35537ms
Jan 14 15:09:49.050: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.100833203s
Jan 14 15:10:09.755: INFO: Number of nodes with available pods: 0
Jan 14 15:10:09.757: INFO: Number of running nodes: 0, number of available pods: 0
Jan 14 15:10:09.765: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3503/daemonsets","resourceVersion":"100471"},"items":null}

Jan 14 15:10:09.768: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3503/pods","resourceVersion":"100471"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:10:10.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3503" for this suite.

â€¢ [SLOW TEST:58.539 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":278,"completed":43,"skipped":581,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:10:10.357: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4284
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override command
Jan 14 15:10:13.953: INFO: Waiting up to 5m0s for pod "client-containers-207b4e85-0908-43c3-8b82-bf5d1ff26df3" in namespace "containers-4284" to be "success or failure"
Jan 14 15:10:13.989: INFO: Pod "client-containers-207b4e85-0908-43c3-8b82-bf5d1ff26df3": Phase="Pending", Reason="", readiness=false. Elapsed: 35.917587ms
Jan 14 15:10:16.137: INFO: Pod "client-containers-207b4e85-0908-43c3-8b82-bf5d1ff26df3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.183399667s
Jan 14 15:10:18.162: INFO: Pod "client-containers-207b4e85-0908-43c3-8b82-bf5d1ff26df3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.207938633s
Jan 14 15:10:20.769: INFO: Pod "client-containers-207b4e85-0908-43c3-8b82-bf5d1ff26df3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.815711905s
Jan 14 15:10:22.773: INFO: Pod "client-containers-207b4e85-0908-43c3-8b82-bf5d1ff26df3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.819745707s
Jan 14 15:10:24.845: INFO: Pod "client-containers-207b4e85-0908-43c3-8b82-bf5d1ff26df3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.891615192s
STEP: Saw pod success
Jan 14 15:10:24.845: INFO: Pod "client-containers-207b4e85-0908-43c3-8b82-bf5d1ff26df3" satisfied condition "success or failure"
Jan 14 15:10:24.849: INFO: Trying to get logs from node slave2 pod client-containers-207b4e85-0908-43c3-8b82-bf5d1ff26df3 container test-container: <nil>
STEP: delete the pod
Jan 14 15:10:25.831: INFO: Waiting for pod client-containers-207b4e85-0908-43c3-8b82-bf5d1ff26df3 to disappear
Jan 14 15:10:25.835: INFO: Pod client-containers-207b4e85-0908-43c3-8b82-bf5d1ff26df3 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:10:25.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4284" for this suite.

â€¢ [SLOW TEST:15.736 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":278,"completed":44,"skipped":602,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:10:26.094: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-6351
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-2szxp in namespace proxy-6351
I0114 15:10:29.404405      23 runners.go:189] Created replication controller with name: proxy-service-2szxp, namespace: proxy-6351, replica count: 1
I0114 15:10:30.454915      23 runners.go:189] proxy-service-2szxp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 15:10:31.455149      23 runners.go:189] proxy-service-2szxp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 15:10:32.455360      23 runners.go:189] proxy-service-2szxp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 15:10:33.455613      23 runners.go:189] proxy-service-2szxp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 15:10:34.455834      23 runners.go:189] proxy-service-2szxp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 15:10:35.456049      23 runners.go:189] proxy-service-2szxp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 15:10:36.456309      23 runners.go:189] proxy-service-2szxp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 15:10:37.456537      23 runners.go:189] proxy-service-2szxp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 15:10:38.456794      23 runners.go:189] proxy-service-2szxp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 15:10:39.457041      23 runners.go:189] proxy-service-2szxp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 15:10:40.458338      23 runners.go:189] proxy-service-2szxp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0114 15:10:41.458676      23 runners.go:189] proxy-service-2szxp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0114 15:10:42.458911      23 runners.go:189] proxy-service-2szxp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0114 15:10:43.459192      23 runners.go:189] proxy-service-2szxp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0114 15:10:44.459493      23 runners.go:189] proxy-service-2szxp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0114 15:10:45.459874      23 runners.go:189] proxy-service-2szxp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0114 15:10:46.460177      23 runners.go:189] proxy-service-2szxp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0114 15:10:47.460438      23 runners.go:189] proxy-service-2szxp Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 14 15:10:47.578: INFO: setup took 19.752202907s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jan 14 15:10:47.656: INFO: (0) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname2/proxy/: bar (200; 77.707692ms)
Jan 14 15:10:47.658: INFO: (0) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname1/proxy/: foo (200; 79.19394ms)
Jan 14 15:10:47.658: INFO: (0) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 80.003825ms)
Jan 14 15:10:47.658: INFO: (0) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname2/proxy/: bar (200; 79.622814ms)
Jan 14 15:10:47.666: INFO: (0) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 87.284434ms)
Jan 14 15:10:47.671: INFO: (0) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 93.154722ms)
Jan 14 15:10:47.671: INFO: (0) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 93.256399ms)
Jan 14 15:10:47.671: INFO: (0) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname1/proxy/: foo (200; 92.377135ms)
Jan 14 15:10:47.685: INFO: (0) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/rewriteme">test</a> (200; 106.662787ms)
Jan 14 15:10:47.688: INFO: (0) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">test<... (200; 109.445498ms)
Jan 14 15:10:47.689: INFO: (0) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">... (200; 110.044429ms)
Jan 14 15:10:47.702: INFO: (0) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname1/proxy/: tls baz (200; 123.626294ms)
Jan 14 15:10:47.705: INFO: (0) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/tlsrewritem... (200; 125.95665ms)
Jan 14 15:10:47.713: INFO: (0) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname2/proxy/: tls qux (200; 135.51741ms)
Jan 14 15:10:47.718: INFO: (0) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:460/proxy/: tls baz (200; 139.048951ms)
Jan 14 15:10:47.718: INFO: (0) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:462/proxy/: tls qux (200; 139.855854ms)
Jan 14 15:10:47.731: INFO: (1) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 12.288571ms)
Jan 14 15:10:47.732: INFO: (1) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">test<... (200; 10.857892ms)
Jan 14 15:10:47.732: INFO: (1) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 11.109723ms)
Jan 14 15:10:47.732: INFO: (1) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname2/proxy/: bar (200; 13.262081ms)
Jan 14 15:10:47.732: INFO: (1) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:460/proxy/: tls baz (200; 13.040141ms)
Jan 14 15:10:47.732: INFO: (1) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:462/proxy/: tls qux (200; 11.441741ms)
Jan 14 15:10:47.732: INFO: (1) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/rewriteme">test</a> (200; 13.321474ms)
Jan 14 15:10:47.732: INFO: (1) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname2/proxy/: tls qux (200; 11.134699ms)
Jan 14 15:10:47.732: INFO: (1) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname2/proxy/: bar (200; 11.633822ms)
Jan 14 15:10:47.732: INFO: (1) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/tlsrewritem... (200; 11.274147ms)
Jan 14 15:10:47.732: INFO: (1) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">... (200; 11.263757ms)
Jan 14 15:10:47.732: INFO: (1) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 13.928708ms)
Jan 14 15:10:47.733: INFO: (1) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname1/proxy/: tls baz (200; 12.395878ms)
Jan 14 15:10:47.733: INFO: (1) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 14.895629ms)
Jan 14 15:10:47.733: INFO: (1) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname1/proxy/: foo (200; 15.208881ms)
Jan 14 15:10:47.733: INFO: (1) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname1/proxy/: foo (200; 12.630197ms)
Jan 14 15:10:47.747: INFO: (2) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">test<... (200; 13.275788ms)
Jan 14 15:10:47.747: INFO: (2) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname1/proxy/: foo (200; 13.241552ms)
Jan 14 15:10:47.748: INFO: (2) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 13.969352ms)
Jan 14 15:10:47.748: INFO: (2) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 13.893095ms)
Jan 14 15:10:47.749: INFO: (2) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname1/proxy/: foo (200; 15.445835ms)
Jan 14 15:10:47.749: INFO: (2) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:462/proxy/: tls qux (200; 14.899287ms)
Jan 14 15:10:47.749: INFO: (2) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname2/proxy/: tls qux (200; 15.519018ms)
Jan 14 15:10:47.749: INFO: (2) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 15.160789ms)
Jan 14 15:10:47.749: INFO: (2) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/rewriteme">test</a> (200; 15.251114ms)
Jan 14 15:10:47.749: INFO: (2) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 15.166064ms)
Jan 14 15:10:47.750: INFO: (2) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">... (200; 15.716366ms)
Jan 14 15:10:47.750: INFO: (2) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/tlsrewritem... (200; 16.865511ms)
Jan 14 15:10:47.751: INFO: (2) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname2/proxy/: bar (200; 16.70683ms)
Jan 14 15:10:47.752: INFO: (2) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname2/proxy/: bar (200; 17.503081ms)
Jan 14 15:10:47.752: INFO: (2) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname1/proxy/: tls baz (200; 17.688835ms)
Jan 14 15:10:47.752: INFO: (2) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:460/proxy/: tls baz (200; 17.764419ms)
Jan 14 15:10:47.758: INFO: (3) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/tlsrewritem... (200; 5.963268ms)
Jan 14 15:10:47.761: INFO: (3) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">... (200; 8.104717ms)
Jan 14 15:10:47.761: INFO: (3) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 8.589708ms)
Jan 14 15:10:47.761: INFO: (3) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 8.379245ms)
Jan 14 15:10:47.761: INFO: (3) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:460/proxy/: tls baz (200; 8.324414ms)
Jan 14 15:10:47.761: INFO: (3) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 8.121666ms)
Jan 14 15:10:47.762: INFO: (3) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 10.33909ms)
Jan 14 15:10:47.763: INFO: (3) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname1/proxy/: foo (200; 10.794587ms)
Jan 14 15:10:47.763: INFO: (3) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname1/proxy/: tls baz (200; 10.672728ms)
Jan 14 15:10:47.763: INFO: (3) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:462/proxy/: tls qux (200; 9.969765ms)
Jan 14 15:10:47.763: INFO: (3) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/rewriteme">test</a> (200; 11.361159ms)
Jan 14 15:10:47.764: INFO: (3) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname2/proxy/: bar (200; 12.197821ms)
Jan 14 15:10:47.765: INFO: (3) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname2/proxy/: bar (200; 12.493605ms)
Jan 14 15:10:47.765: INFO: (3) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">test<... (200; 12.601665ms)
Jan 14 15:10:47.765: INFO: (3) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname2/proxy/: tls qux (200; 12.187109ms)
Jan 14 15:10:47.765: INFO: (3) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname1/proxy/: foo (200; 12.757196ms)
Jan 14 15:10:47.772: INFO: (4) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">... (200; 6.847516ms)
Jan 14 15:10:47.772: INFO: (4) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 7.403182ms)
Jan 14 15:10:47.772: INFO: (4) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:462/proxy/: tls qux (200; 7.65605ms)
Jan 14 15:10:47.772: INFO: (4) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 7.209587ms)
Jan 14 15:10:47.772: INFO: (4) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/tlsrewritem... (200; 7.166928ms)
Jan 14 15:10:47.772: INFO: (4) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 6.561642ms)
Jan 14 15:10:47.772: INFO: (4) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 7.46264ms)
Jan 14 15:10:47.772: INFO: (4) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/rewriteme">test</a> (200; 6.821962ms)
Jan 14 15:10:47.773: INFO: (4) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname1/proxy/: tls baz (200; 7.283496ms)
Jan 14 15:10:47.773: INFO: (4) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname2/proxy/: bar (200; 7.971153ms)
Jan 14 15:10:47.773: INFO: (4) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">test<... (200; 7.268407ms)
Jan 14 15:10:47.773: INFO: (4) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname2/proxy/: tls qux (200; 7.878876ms)
Jan 14 15:10:47.773: INFO: (4) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname1/proxy/: foo (200; 7.423446ms)
Jan 14 15:10:47.773: INFO: (4) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:460/proxy/: tls baz (200; 7.764587ms)
Jan 14 15:10:47.773: INFO: (4) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname1/proxy/: foo (200; 8.031624ms)
Jan 14 15:10:47.775: INFO: (4) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname2/proxy/: bar (200; 9.082567ms)
Jan 14 15:10:47.780: INFO: (5) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 5.484264ms)
Jan 14 15:10:47.782: INFO: (5) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">test<... (200; 6.109198ms)
Jan 14 15:10:47.782: INFO: (5) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:460/proxy/: tls baz (200; 6.804491ms)
Jan 14 15:10:47.782: INFO: (5) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">... (200; 6.657624ms)
Jan 14 15:10:47.782: INFO: (5) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 6.938128ms)
Jan 14 15:10:47.782: INFO: (5) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:462/proxy/: tls qux (200; 6.833653ms)
Jan 14 15:10:47.782: INFO: (5) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/tlsrewritem... (200; 6.824407ms)
Jan 14 15:10:47.788: INFO: (5) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname2/proxy/: bar (200; 12.727962ms)
Jan 14 15:10:47.788: INFO: (5) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 11.523047ms)
Jan 14 15:10:47.788: INFO: (5) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname2/proxy/: tls qux (200; 12.86391ms)
Jan 14 15:10:47.788: INFO: (5) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/rewriteme">test</a> (200; 11.886352ms)
Jan 14 15:10:47.788: INFO: (5) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 12.438197ms)
Jan 14 15:10:47.791: INFO: (5) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname1/proxy/: foo (200; 15.817829ms)
Jan 14 15:10:47.791: INFO: (5) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname1/proxy/: foo (200; 15.932354ms)
Jan 14 15:10:47.791: INFO: (5) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname2/proxy/: bar (200; 15.609662ms)
Jan 14 15:10:47.791: INFO: (5) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname1/proxy/: tls baz (200; 15.784101ms)
Jan 14 15:10:47.798: INFO: (6) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 5.910954ms)
Jan 14 15:10:47.800: INFO: (6) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:462/proxy/: tls qux (200; 8.495822ms)
Jan 14 15:10:47.800: INFO: (6) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/rewriteme">test</a> (200; 7.983714ms)
Jan 14 15:10:47.800: INFO: (6) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/tlsrewritem... (200; 8.174447ms)
Jan 14 15:10:47.800: INFO: (6) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">test<... (200; 7.888359ms)
Jan 14 15:10:47.801: INFO: (6) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 8.502644ms)
Jan 14 15:10:47.801: INFO: (6) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname1/proxy/: foo (200; 8.888806ms)
Jan 14 15:10:47.801: INFO: (6) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 8.507551ms)
Jan 14 15:10:47.801: INFO: (6) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:460/proxy/: tls baz (200; 8.514552ms)
Jan 14 15:10:47.801: INFO: (6) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">... (200; 9.286826ms)
Jan 14 15:10:47.801: INFO: (6) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 9.185079ms)
Jan 14 15:10:47.802: INFO: (6) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname2/proxy/: bar (200; 9.306318ms)
Jan 14 15:10:47.802: INFO: (6) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname2/proxy/: tls qux (200; 9.944902ms)
Jan 14 15:10:47.803: INFO: (6) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname2/proxy/: bar (200; 10.645662ms)
Jan 14 15:10:47.803: INFO: (6) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname1/proxy/: foo (200; 10.56715ms)
Jan 14 15:10:47.804: INFO: (6) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname1/proxy/: tls baz (200; 11.278838ms)
Jan 14 15:10:47.811: INFO: (7) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/tlsrewritem... (200; 7.651321ms)
Jan 14 15:10:47.814: INFO: (7) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/rewriteme">test</a> (200; 9.947004ms)
Jan 14 15:10:47.822: INFO: (7) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 17.545841ms)
Jan 14 15:10:47.822: INFO: (7) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 17.24316ms)
Jan 14 15:10:47.822: INFO: (7) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">test<... (200; 17.790594ms)
Jan 14 15:10:47.822: INFO: (7) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname1/proxy/: tls baz (200; 17.878471ms)
Jan 14 15:10:47.822: INFO: (7) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname2/proxy/: bar (200; 17.444784ms)
Jan 14 15:10:47.822: INFO: (7) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname1/proxy/: foo (200; 18.002752ms)
Jan 14 15:10:47.822: INFO: (7) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 17.621383ms)
Jan 14 15:10:47.822: INFO: (7) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname1/proxy/: foo (200; 18.126477ms)
Jan 14 15:10:47.822: INFO: (7) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">... (200; 17.288436ms)
Jan 14 15:10:47.822: INFO: (7) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname2/proxy/: tls qux (200; 17.480839ms)
Jan 14 15:10:47.822: INFO: (7) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:460/proxy/: tls baz (200; 17.381727ms)
Jan 14 15:10:47.822: INFO: (7) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 17.284983ms)
Jan 14 15:10:47.822: INFO: (7) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname2/proxy/: bar (200; 17.927842ms)
Jan 14 15:10:47.822: INFO: (7) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:462/proxy/: tls qux (200; 17.698974ms)
Jan 14 15:10:47.827: INFO: (8) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/rewriteme">test</a> (200; 4.773008ms)
Jan 14 15:10:47.827: INFO: (8) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/tlsrewritem... (200; 5.353068ms)
Jan 14 15:10:47.828: INFO: (8) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">test<... (200; 5.402506ms)
Jan 14 15:10:47.828: INFO: (8) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 4.837853ms)
Jan 14 15:10:47.828: INFO: (8) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 5.130737ms)
Jan 14 15:10:47.831: INFO: (8) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 7.212259ms)
Jan 14 15:10:47.831: INFO: (8) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:462/proxy/: tls qux (200; 7.415892ms)
Jan 14 15:10:47.831: INFO: (8) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 8.04859ms)
Jan 14 15:10:47.831: INFO: (8) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">... (200; 7.63773ms)
Jan 14 15:10:47.831: INFO: (8) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname1/proxy/: foo (200; 8.794624ms)
Jan 14 15:10:47.831: INFO: (8) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:460/proxy/: tls baz (200; 8.272146ms)
Jan 14 15:10:47.832: INFO: (8) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname1/proxy/: foo (200; 9.385769ms)
Jan 14 15:10:47.832: INFO: (8) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname2/proxy/: tls qux (200; 9.080016ms)
Jan 14 15:10:47.832: INFO: (8) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname2/proxy/: bar (200; 9.698032ms)
Jan 14 15:10:47.832: INFO: (8) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname2/proxy/: bar (200; 9.391153ms)
Jan 14 15:10:47.832: INFO: (8) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname1/proxy/: tls baz (200; 9.917662ms)
Jan 14 15:10:47.853: INFO: (9) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname1/proxy/: foo (200; 19.972773ms)
Jan 14 15:10:47.853: INFO: (9) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:462/proxy/: tls qux (200; 19.580858ms)
Jan 14 15:10:47.853: INFO: (9) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/rewriteme">test</a> (200; 19.80642ms)
Jan 14 15:10:47.853: INFO: (9) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname2/proxy/: tls qux (200; 19.484609ms)
Jan 14 15:10:47.853: INFO: (9) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">... (200; 20.348661ms)
Jan 14 15:10:47.853: INFO: (9) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">test<... (200; 19.979268ms)
Jan 14 15:10:47.853: INFO: (9) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 19.444138ms)
Jan 14 15:10:47.853: INFO: (9) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 19.830912ms)
Jan 14 15:10:47.853: INFO: (9) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 20.336004ms)
Jan 14 15:10:47.853: INFO: (9) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/tlsrewritem... (200; 20.283871ms)
Jan 14 15:10:47.853: INFO: (9) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 19.803305ms)
Jan 14 15:10:47.853: INFO: (9) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname1/proxy/: tls baz (200; 20.14652ms)
Jan 14 15:10:47.853: INFO: (9) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:460/proxy/: tls baz (200; 20.592818ms)
Jan 14 15:10:47.853: INFO: (9) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname1/proxy/: foo (200; 20.291395ms)
Jan 14 15:10:47.853: INFO: (9) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname2/proxy/: bar (200; 20.070867ms)
Jan 14 15:10:47.853: INFO: (9) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname2/proxy/: bar (200; 19.737366ms)
Jan 14 15:10:47.878: INFO: (10) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 23.344158ms)
Jan 14 15:10:47.878: INFO: (10) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/tlsrewritem... (200; 24.949208ms)
Jan 14 15:10:47.878: INFO: (10) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/rewriteme">test</a> (200; 24.31676ms)
Jan 14 15:10:47.878: INFO: (10) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">test<... (200; 25.248106ms)
Jan 14 15:10:47.879: INFO: (10) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:460/proxy/: tls baz (200; 24.404943ms)
Jan 14 15:10:47.879: INFO: (10) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 24.788423ms)
Jan 14 15:10:47.879: INFO: (10) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:462/proxy/: tls qux (200; 24.379469ms)
Jan 14 15:10:47.879: INFO: (10) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname2/proxy/: tls qux (200; 24.210461ms)
Jan 14 15:10:47.879: INFO: (10) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 24.560972ms)
Jan 14 15:10:47.879: INFO: (10) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname2/proxy/: bar (200; 24.734786ms)
Jan 14 15:10:47.879: INFO: (10) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname2/proxy/: bar (200; 24.368736ms)
Jan 14 15:10:47.879: INFO: (10) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname1/proxy/: foo (200; 25.241282ms)
Jan 14 15:10:47.879: INFO: (10) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">... (200; 24.214256ms)
Jan 14 15:10:47.879: INFO: (10) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname1/proxy/: foo (200; 25.365748ms)
Jan 14 15:10:47.879: INFO: (10) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 24.118957ms)
Jan 14 15:10:47.879: INFO: (10) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname1/proxy/: tls baz (200; 25.203148ms)
Jan 14 15:10:47.888: INFO: (11) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 8.09641ms)
Jan 14 15:10:47.888: INFO: (11) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 9.33981ms)
Jan 14 15:10:47.888: INFO: (11) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 8.442049ms)
Jan 14 15:10:47.888: INFO: (11) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname1/proxy/: foo (200; 8.687504ms)
Jan 14 15:10:47.890: INFO: (11) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname1/proxy/: foo (200; 9.989415ms)
Jan 14 15:10:47.890: INFO: (11) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">test<... (200; 10.177371ms)
Jan 14 15:10:47.890: INFO: (11) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname2/proxy/: bar (200; 9.780432ms)
Jan 14 15:10:47.890: INFO: (11) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/rewriteme">test</a> (200; 9.735168ms)
Jan 14 15:10:47.890: INFO: (11) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname2/proxy/: tls qux (200; 9.353243ms)
Jan 14 15:10:47.890: INFO: (11) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname1/proxy/: tls baz (200; 10.357758ms)
Jan 14 15:10:47.890: INFO: (11) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:460/proxy/: tls baz (200; 9.628591ms)
Jan 14 15:10:47.890: INFO: (11) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 9.940583ms)
Jan 14 15:10:47.890: INFO: (11) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/tlsrewritem... (200; 9.591619ms)
Jan 14 15:10:47.890: INFO: (11) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname2/proxy/: bar (200; 9.827583ms)
Jan 14 15:10:47.890: INFO: (11) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:462/proxy/: tls qux (200; 9.923894ms)
Jan 14 15:10:47.890: INFO: (11) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">... (200; 9.81383ms)
Jan 14 15:10:47.899: INFO: (12) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/tlsrewritem... (200; 8.789279ms)
Jan 14 15:10:47.904: INFO: (12) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 12.818719ms)
Jan 14 15:10:47.904: INFO: (12) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">test<... (200; 13.346299ms)
Jan 14 15:10:47.904: INFO: (12) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 12.663193ms)
Jan 14 15:10:47.905: INFO: (12) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/rewriteme">test</a> (200; 13.50478ms)
Jan 14 15:10:47.905: INFO: (12) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">... (200; 13.19045ms)
Jan 14 15:10:47.905: INFO: (12) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:460/proxy/: tls baz (200; 13.452978ms)
Jan 14 15:10:47.905: INFO: (12) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname1/proxy/: tls baz (200; 14.605105ms)
Jan 14 15:10:47.906: INFO: (12) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:462/proxy/: tls qux (200; 14.437207ms)
Jan 14 15:10:47.906: INFO: (12) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname2/proxy/: bar (200; 14.545509ms)
Jan 14 15:10:47.906: INFO: (12) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 14.745897ms)
Jan 14 15:10:47.906: INFO: (12) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 15.040195ms)
Jan 14 15:10:47.906: INFO: (12) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname1/proxy/: foo (200; 15.63984ms)
Jan 14 15:10:47.906: INFO: (12) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname2/proxy/: tls qux (200; 15.078885ms)
Jan 14 15:10:47.907: INFO: (12) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname2/proxy/: bar (200; 16.19195ms)
Jan 14 15:10:47.907: INFO: (12) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname1/proxy/: foo (200; 16.698193ms)
Jan 14 15:10:47.943: INFO: (13) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname1/proxy/: foo (200; 34.322858ms)
Jan 14 15:10:47.943: INFO: (13) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:460/proxy/: tls baz (200; 33.619847ms)
Jan 14 15:10:47.943: INFO: (13) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname2/proxy/: bar (200; 34.250839ms)
Jan 14 15:10:47.943: INFO: (13) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/rewriteme">test</a> (200; 34.189359ms)
Jan 14 15:10:47.943: INFO: (13) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname2/proxy/: bar (200; 33.905096ms)
Jan 14 15:10:47.943: INFO: (13) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:462/proxy/: tls qux (200; 35.342054ms)
Jan 14 15:10:47.943: INFO: (13) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname1/proxy/: foo (200; 34.7308ms)
Jan 14 15:10:47.943: INFO: (13) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">test<... (200; 34.498434ms)
Jan 14 15:10:47.943: INFO: (13) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 35.09521ms)
Jan 14 15:10:47.943: INFO: (13) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">... (200; 35.216136ms)
Jan 14 15:10:47.943: INFO: (13) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/tlsrewritem... (200; 35.011388ms)
Jan 14 15:10:47.943: INFO: (13) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 34.338877ms)
Jan 14 15:10:47.943: INFO: (13) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 34.272992ms)
Jan 14 15:10:47.943: INFO: (13) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 33.88539ms)
Jan 14 15:10:47.943: INFO: (13) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname1/proxy/: tls baz (200; 34.801254ms)
Jan 14 15:10:47.943: INFO: (13) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname2/proxy/: tls qux (200; 34.105171ms)
Jan 14 15:10:48.495: INFO: (14) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 550.57974ms)
Jan 14 15:10:48.503: INFO: (14) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname1/proxy/: tls baz (200; 556.884129ms)
Jan 14 15:10:48.503: INFO: (14) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname1/proxy/: foo (200; 557.067986ms)
Jan 14 15:10:48.503: INFO: (14) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname2/proxy/: bar (200; 557.823856ms)
Jan 14 15:10:48.503: INFO: (14) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/tlsrewritem... (200; 557.286662ms)
Jan 14 15:10:48.503: INFO: (14) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 556.686261ms)
Jan 14 15:10:48.503: INFO: (14) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname2/proxy/: tls qux (200; 557.814739ms)
Jan 14 15:10:48.503: INFO: (14) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/rewriteme">test</a> (200; 556.843185ms)
Jan 14 15:10:48.503: INFO: (14) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:460/proxy/: tls baz (200; 557.7125ms)
Jan 14 15:10:48.504: INFO: (14) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">test<... (200; 558.086865ms)
Jan 14 15:10:48.504: INFO: (14) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">... (200; 558.58998ms)
Jan 14 15:10:48.504: INFO: (14) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:462/proxy/: tls qux (200; 558.700327ms)
Jan 14 15:10:48.504: INFO: (14) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname2/proxy/: bar (200; 558.112482ms)
Jan 14 15:10:48.510: INFO: (14) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 565.002788ms)
Jan 14 15:10:48.511: INFO: (14) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname1/proxy/: foo (200; 565.704483ms)
Jan 14 15:10:48.515: INFO: (14) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 569.629684ms)
Jan 14 15:10:48.524: INFO: (15) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/tlsrewritem... (200; 8.52119ms)
Jan 14 15:10:48.524: INFO: (15) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/rewriteme">test</a> (200; 8.07242ms)
Jan 14 15:10:48.524: INFO: (15) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:462/proxy/: tls qux (200; 8.111128ms)
Jan 14 15:10:48.525: INFO: (15) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 9.252604ms)
Jan 14 15:10:48.525: INFO: (15) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">... (200; 9.533973ms)
Jan 14 15:10:48.525: INFO: (15) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 10.030805ms)
Jan 14 15:10:48.525: INFO: (15) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">test<... (200; 10.275815ms)
Jan 14 15:10:48.526: INFO: (15) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:460/proxy/: tls baz (200; 9.871974ms)
Jan 14 15:10:48.526: INFO: (15) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 9.851609ms)
Jan 14 15:10:48.526: INFO: (15) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 10.264025ms)
Jan 14 15:10:48.544: INFO: (15) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname1/proxy/: foo (200; 29.150191ms)
Jan 14 15:10:48.546: INFO: (15) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname2/proxy/: bar (200; 30.46925ms)
Jan 14 15:10:48.546: INFO: (15) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname2/proxy/: bar (200; 30.12222ms)
Jan 14 15:10:48.546: INFO: (15) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname1/proxy/: foo (200; 30.976273ms)
Jan 14 15:10:48.546: INFO: (15) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname1/proxy/: tls baz (200; 31.065174ms)
Jan 14 15:10:48.547: INFO: (15) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname2/proxy/: tls qux (200; 31.691147ms)
Jan 14 15:10:48.559: INFO: (16) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 11.845606ms)
Jan 14 15:10:48.559: INFO: (16) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 12.21057ms)
Jan 14 15:10:48.559: INFO: (16) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">test<... (200; 12.144521ms)
Jan 14 15:10:48.560: INFO: (16) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/tlsrewritem... (200; 12.796675ms)
Jan 14 15:10:48.561: INFO: (16) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/rewriteme">test</a> (200; 13.361764ms)
Jan 14 15:10:48.561: INFO: (16) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 13.664837ms)
Jan 14 15:10:48.561: INFO: (16) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">... (200; 13.60523ms)
Jan 14 15:10:48.561: INFO: (16) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 13.600097ms)
Jan 14 15:10:48.561: INFO: (16) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:462/proxy/: tls qux (200; 13.517751ms)
Jan 14 15:10:48.561: INFO: (16) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:460/proxy/: tls baz (200; 13.775605ms)
Jan 14 15:10:48.564: INFO: (16) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname2/proxy/: bar (200; 16.926148ms)
Jan 14 15:10:48.564: INFO: (16) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname2/proxy/: tls qux (200; 17.255457ms)
Jan 14 15:10:48.564: INFO: (16) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname1/proxy/: foo (200; 16.909826ms)
Jan 14 15:10:48.565: INFO: (16) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname1/proxy/: tls baz (200; 17.689328ms)
Jan 14 15:10:48.565: INFO: (16) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname1/proxy/: foo (200; 17.745924ms)
Jan 14 15:10:48.565: INFO: (16) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname2/proxy/: bar (200; 17.65531ms)
Jan 14 15:10:48.570: INFO: (17) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:460/proxy/: tls baz (200; 4.743724ms)
Jan 14 15:10:48.572: INFO: (17) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/tlsrewritem... (200; 6.131342ms)
Jan 14 15:10:48.572: INFO: (17) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">... (200; 6.837611ms)
Jan 14 15:10:48.572: INFO: (17) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 6.730778ms)
Jan 14 15:10:48.573: INFO: (17) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">test<... (200; 6.781518ms)
Jan 14 15:10:48.574: INFO: (17) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/rewriteme">test</a> (200; 7.138053ms)
Jan 14 15:10:48.575: INFO: (17) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:462/proxy/: tls qux (200; 8.053736ms)
Jan 14 15:10:48.575: INFO: (17) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 7.898916ms)
Jan 14 15:10:48.576: INFO: (17) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname2/proxy/: bar (200; 9.536665ms)
Jan 14 15:10:48.576: INFO: (17) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 9.164911ms)
Jan 14 15:10:48.576: INFO: (17) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 9.148766ms)
Jan 14 15:10:48.576: INFO: (17) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname1/proxy/: foo (200; 10.506528ms)
Jan 14 15:10:48.577: INFO: (17) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname2/proxy/: tls qux (200; 9.196826ms)
Jan 14 15:10:48.577: INFO: (17) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname2/proxy/: bar (200; 9.547267ms)
Jan 14 15:10:48.577: INFO: (17) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname1/proxy/: foo (200; 11.200192ms)
Jan 14 15:10:48.578: INFO: (17) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname1/proxy/: tls baz (200; 11.509836ms)
Jan 14 15:10:48.583: INFO: (18) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">test<... (200; 5.213517ms)
Jan 14 15:10:48.585: INFO: (18) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/rewriteme">test</a> (200; 7.19106ms)
Jan 14 15:10:48.585: INFO: (18) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 7.101456ms)
Jan 14 15:10:48.585: INFO: (18) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/tlsrewritem... (200; 6.772554ms)
Jan 14 15:10:48.586: INFO: (18) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 7.302841ms)
Jan 14 15:10:48.587: INFO: (18) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 8.692724ms)
Jan 14 15:10:48.588: INFO: (18) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:462/proxy/: tls qux (200; 9.344047ms)
Jan 14 15:10:48.588: INFO: (18) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname2/proxy/: bar (200; 10.363981ms)
Jan 14 15:10:48.588: INFO: (18) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:460/proxy/: tls baz (200; 9.593672ms)
Jan 14 15:10:48.589: INFO: (18) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">... (200; 9.552276ms)
Jan 14 15:10:48.589: INFO: (18) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 9.685395ms)
Jan 14 15:10:48.589: INFO: (18) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname2/proxy/: tls qux (200; 10.59993ms)
Jan 14 15:10:48.590: INFO: (18) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname2/proxy/: bar (200; 11.292113ms)
Jan 14 15:10:48.592: INFO: (18) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname1/proxy/: foo (200; 13.586197ms)
Jan 14 15:10:48.592: INFO: (18) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname1/proxy/: tls baz (200; 13.564711ms)
Jan 14 15:10:48.592: INFO: (18) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname1/proxy/: foo (200; 13.64033ms)
Jan 14 15:10:48.673: INFO: (19) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname2/proxy/: bar (200; 80.032981ms)
Jan 14 15:10:48.673: INFO: (19) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 80.157707ms)
Jan 14 15:10:48.673: INFO: (19) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 79.667509ms)
Jan 14 15:10:48.673: INFO: (19) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">test<... (200; 79.977938ms)
Jan 14 15:10:48.673: INFO: (19) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:462/proxy/: tls qux (200; 80.267418ms)
Jan 14 15:10:48.673: INFO: (19) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:1080/proxy/rewriteme">... (200; 80.209355ms)
Jan 14 15:10:48.674: INFO: (19) /api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/proxy-service-2szxp-9m9pv/proxy/rewriteme">test</a> (200; 80.66245ms)
Jan 14 15:10:48.674: INFO: (19) /api/v1/namespaces/proxy-6351/services/http:proxy-service-2szxp:portname1/proxy/: foo (200; 80.163842ms)
Jan 14 15:10:48.674: INFO: (19) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/: <a href="/api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:443/proxy/tlsrewritem... (200; 80.974011ms)
Jan 14 15:10:48.674: INFO: (19) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname1/proxy/: tls baz (200; 81.339089ms)
Jan 14 15:10:48.674: INFO: (19) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname1/proxy/: foo (200; 80.641041ms)
Jan 14 15:10:48.674: INFO: (19) /api/v1/namespaces/proxy-6351/pods/https:proxy-service-2szxp-9m9pv:460/proxy/: tls baz (200; 80.852242ms)
Jan 14 15:10:48.674: INFO: (19) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:160/proxy/: foo (200; 80.910931ms)
Jan 14 15:10:48.674: INFO: (19) /api/v1/namespaces/proxy-6351/services/proxy-service-2szxp:portname2/proxy/: bar (200; 80.85237ms)
Jan 14 15:10:48.674: INFO: (19) /api/v1/namespaces/proxy-6351/pods/http:proxy-service-2szxp-9m9pv:162/proxy/: bar (200; 81.127921ms)
Jan 14 15:10:48.674: INFO: (19) /api/v1/namespaces/proxy-6351/services/https:proxy-service-2szxp:tlsportname2/proxy/: tls qux (200; 80.951076ms)
STEP: deleting ReplicationController proxy-service-2szxp in namespace proxy-6351, will wait for the garbage collector to delete the pods
Jan 14 15:10:48.743: INFO: Deleting ReplicationController proxy-service-2szxp took: 13.3821ms
Jan 14 15:10:49.243: INFO: Terminating ReplicationController proxy-service-2szxp pods took: 500.272318ms
[AfterEach] version v1
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:10:59.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6351" for this suite.

â€¢ [SLOW TEST:33.749 seconds]
[sig-network] Proxy
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":278,"completed":45,"skipped":623,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:10:59.845: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5496
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 14 15:11:04.820: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 14 15:11:06.832: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611465, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611465, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611465, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611463, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 15:11:08.998: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611465, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611465, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611465, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611463, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 15:11:11.821: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611465, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611465, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611465, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611463, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 15:11:12.909: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611465, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611465, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611465, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611463, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 15:11:15.042: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611465, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611465, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611465, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611463, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 14 15:11:18.481: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 15:11:18.582: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9061-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:11:26.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5496" for this suite.
STEP: Destroying namespace "webhook-5496-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:28.326 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":278,"completed":46,"skipped":653,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:11:28.172: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7909
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 15:11:30.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 create -f - --namespace=kubectl-7909'
Jan 14 15:11:37.965: INFO: stderr: ""
Jan 14 15:11:37.965: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Jan 14 15:11:37.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 create -f - --namespace=kubectl-7909'
Jan 14 15:11:39.606: INFO: stderr: ""
Jan 14 15:11:39.606: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Jan 14 15:11:40.612: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 15:11:40.612: INFO: Found 0 / 1
Jan 14 15:11:41.700: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 15:11:41.700: INFO: Found 0 / 1
Jan 14 15:11:42.646: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 15:11:42.646: INFO: Found 0 / 1
Jan 14 15:11:43.900: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 15:11:43.900: INFO: Found 0 / 1
Jan 14 15:11:44.623: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 15:11:44.623: INFO: Found 0 / 1
Jan 14 15:11:45.611: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 15:11:45.611: INFO: Found 0 / 1
Jan 14 15:11:46.683: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 15:11:46.683: INFO: Found 1 / 1
Jan 14 15:11:46.683: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 14 15:11:46.688: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 15:11:46.688: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 14 15:11:46.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 describe pod agnhost-master-5qfmc --namespace=kubectl-7909'
Jan 14 15:11:47.128: INFO: stderr: ""
Jan 14 15:11:47.128: INFO: stdout: "Name:         agnhost-master-5qfmc\nNamespace:    kubectl-7909\nPriority:     0\nNode:         slave2/192.168.0.142\nStart Time:   Tue, 14 Jan 2020 15:11:38 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nStatus:       Running\nIP:           10.151.49.121\nIPs:\n  IP:           10.151.49.121\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   docker://83f86ab4715b4f76024a60a3393a2b684f25ef88e5507e1a1c82af8910278a54\n    Image:          gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Image ID:       docker://sha256:a450ec43af3738f7fc05227a9a778feb9f90e08bfee4362b6a59224bcbf172cf\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 14 Jan 2020 15:11:45 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-4xqlm (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-4xqlm:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-4xqlm\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From               Message\n  ----    ------     ----       ----               -------\n  Normal  Scheduled  <unknown>  default-scheduler  Successfully assigned kubectl-7909/agnhost-master-5qfmc to slave2\n  Normal  Pulled     4s         kubelet, slave2    Container image \"gcr.io/kubernetes-e2e-test-images/agnhost:2.8\" already present on machine\n  Normal  Created    3s         kubelet, slave2    Created container agnhost-master\n  Normal  Started    2s         kubelet, slave2    Started container agnhost-master\n"
Jan 14 15:11:47.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 describe rc agnhost-master --namespace=kubectl-7909'
Jan 14 15:11:47.522: INFO: stderr: ""
Jan 14 15:11:47.523: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-7909\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  9s    replication-controller  Created pod: agnhost-master-5qfmc\n"
Jan 14 15:11:47.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 describe service agnhost-master --namespace=kubectl-7909'
Jan 14 15:11:47.865: INFO: stderr: ""
Jan 14 15:11:47.865: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-7909\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                10.150.15.151\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.151.49.121:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 14 15:11:47.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 describe node master1'
Jan 14 15:11:48.329: INFO: stderr: ""
Jan 14 15:11:48.329: INFO: stdout: "Name:               master1\nRoles:              master,monitor,node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=master1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=true\n                    node-role.kubernetes.io/monitor=true\n                    node-role.kubernetes.io/node=true\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\nCreationTimestamp:  Tue, 14 Jan 2020 05:26:29 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  master1\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 14 Jan 2020 15:11:45 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 14 Jan 2020 08:19:25 +0000   Tue, 14 Jan 2020 08:19:25 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 14 Jan 2020 15:06:52 +0000   Tue, 14 Jan 2020 05:26:29 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 14 Jan 2020 15:06:52 +0000   Tue, 14 Jan 2020 05:26:29 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 14 Jan 2020 15:06:52 +0000   Tue, 14 Jan 2020 05:26:29 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 14 Jan 2020 15:06:52 +0000   Tue, 14 Jan 2020 10:55:27 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.0.112\n  Hostname:    master1\nCapacity:\n  cpu:                8\n  ephemeral-storage:  206357460Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16424664Ki\n  pods:               50\nAllocatable:\n  cpu:                7\n  ephemeral-storage:  195871700Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             14900376Ki\n  pods:               50\nSystem Info:\n  Machine ID:                 a87d37f9c4d74e9eb672b97b5dd0c818\n  System UUID:                554B207C-8DB1-4A6C-A51A-D21C741B1318\n  Boot ID:                    c1f14c2b-4ad7-47a9-9f3b-b379ef778524\n  Kernel Version:             4.15.0-45-generic\n  OS Image:                   Ubuntu 18.04.2 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://18.6.3\n  Kubelet Version:            v1.17.0\n  Kube-Proxy Version:         v1.17.0\nNon-terminated Pods:          (12 in total)\n  Namespace                   Name                                 CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                 ------------  ----------  ---------------  -------------  ---\n  daemonsets-9852             daemon-set-gg2wn                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         94m\n  kube-system                 calico-node-c275v                    150m (2%)     1 (14%)     64Mi (0%)        4Gi (28%)      9h\n  kube-system                 cinder-provisioner-xxxjh             0 (0%)        0 (0%)      0 (0%)           0 (0%)         9h\n  kube-system                 coredns-52ppc                        100m (1%)     0 (0%)      70Mi (0%)        500Mi (3%)     133m\n  kube-system                 kube-apiserver-master1               100m (1%)     2 (28%)     256Mi (1%)       6Gi (42%)      6h51m\n  kube-system                 kube-controller-manager-master1      100m (1%)     2 (28%)     100Mi (0%)       6Gi (42%)      6h51m\n  kube-system                 kube-proxy-master1                   150m (2%)     500m (7%)   64M (0%)         2G (13%)       6h51m\n  kube-system                 kube-scheduler-master1               80m (1%)      2 (28%)     170Mi (1%)       6Gi (42%)      6h51m\n  kube-system                 nginx-proxy-master1                  25m (0%)      300m (4%)   32M (0%)         512M (3%)      6h51m\n  kube-system                 oss-provisioner-7bb5d4c769-vhrt6     0 (0%)        0 (0%)      0 (0%)           0 (0%)         9h\n  kube-system                 resource-reserver-master1            800m (11%)    800m (11%)  512Mi (3%)       512Mi (3%)     6h51m\n  monitoring                  kube-state-metrics-8988b595-gq7dc    0 (0%)        0 (0%)      0 (0%)           0 (0%)         133m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests        Limits\n  --------           --------        ------\n  cpu                1505m (21%)     8600m (122%)\n  memory             1293878Ki (8%)  26558085Ki (178%)\n  ephemeral-storage  0 (0%)          0 (0%)\nEvents:              <none>\n"
Jan 14 15:11:48.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 describe namespace kubectl-7909'
Jan 14 15:11:48.663: INFO: stderr: ""
Jan 14 15:11:48.664: INFO: stdout: "Name:         kubectl-7909\nLabels:       e2e-framework=kubectl\n              e2e-run=172ede82-b4eb-481c-a852-7ea994fdc7e5\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:11:48.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7909" for this suite.

â€¢ [SLOW TEST:20.882 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":278,"completed":47,"skipped":656,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:11:49.054: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4928
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 14 15:12:02.597: INFO: Successfully updated pod "pod-update-62baed34-2161-458f-8618-5b2ff78a9290"
STEP: verifying the updated pod is in kubernetes
Jan 14 15:12:02.609: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:12:02.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4928" for this suite.

â€¢ [SLOW TEST:14.595 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":278,"completed":48,"skipped":658,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:12:03.649: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9622
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-91b9ba7f-ec3b-4463-8ba6-13f8b5a5c11f
STEP: Creating a pod to test consume configMaps
Jan 14 15:12:07.440: INFO: Waiting up to 5m0s for pod "pod-configmaps-8996bb0c-e3a9-4c65-bacc-8f524694ac0e" in namespace "configmap-9622" to be "success or failure"
Jan 14 15:12:07.445: INFO: Pod "pod-configmaps-8996bb0c-e3a9-4c65-bacc-8f524694ac0e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.312435ms
Jan 14 15:12:09.503: INFO: Pod "pod-configmaps-8996bb0c-e3a9-4c65-bacc-8f524694ac0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062826049s
Jan 14 15:12:12.111: INFO: Pod "pod-configmaps-8996bb0c-e3a9-4c65-bacc-8f524694ac0e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.670661539s
Jan 14 15:12:14.282: INFO: Pod "pod-configmaps-8996bb0c-e3a9-4c65-bacc-8f524694ac0e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.841942831s
Jan 14 15:12:16.346: INFO: Pod "pod-configmaps-8996bb0c-e3a9-4c65-bacc-8f524694ac0e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.905233461s
Jan 14 15:12:18.431: INFO: Pod "pod-configmaps-8996bb0c-e3a9-4c65-bacc-8f524694ac0e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.990861992s
Jan 14 15:12:20.489: INFO: Pod "pod-configmaps-8996bb0c-e3a9-4c65-bacc-8f524694ac0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.047975016s
STEP: Saw pod success
Jan 14 15:12:20.491: INFO: Pod "pod-configmaps-8996bb0c-e3a9-4c65-bacc-8f524694ac0e" satisfied condition "success or failure"
Jan 14 15:12:20.680: INFO: Trying to get logs from node slave3 pod pod-configmaps-8996bb0c-e3a9-4c65-bacc-8f524694ac0e container configmap-volume-test: <nil>
STEP: delete the pod
Jan 14 15:12:21.622: INFO: Waiting for pod pod-configmaps-8996bb0c-e3a9-4c65-bacc-8f524694ac0e to disappear
Jan 14 15:12:21.630: INFO: Pod pod-configmaps-8996bb0c-e3a9-4c65-bacc-8f524694ac0e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:12:21.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9622" for this suite.

â€¢ [SLOW TEST:18.712 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":49,"skipped":702,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:12:22.365: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9762
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting the proxy server
Jan 14 15:12:23.746: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-321483740 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:12:23.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9762" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":278,"completed":50,"skipped":725,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:12:24.176: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9599
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl logs
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
STEP: creating an pod
Jan 14 15:12:25.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.8 --namespace=kubectl-9599 -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 14 15:12:26.118: INFO: stderr: ""
Jan 14 15:12:26.118: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Waiting for log generator to start.
Jan 14 15:12:26.118: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 14 15:12:26.118: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9599" to be "running and ready, or succeeded"
Jan 14 15:12:26.360: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 241.421389ms
Jan 14 15:12:28.365: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.246638538s
Jan 14 15:12:30.515: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.396390313s
Jan 14 15:12:32.924: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.805683482s
Jan 14 15:12:35.153: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 9.034497644s
Jan 14 15:12:37.630: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 11.511584763s
Jan 14 15:12:39.697: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 13.578683229s
Jan 14 15:12:39.697: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 14 15:12:39.697: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jan 14 15:12:39.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 logs logs-generator logs-generator --namespace=kubectl-9599'
Jan 14 15:12:40.305: INFO: stderr: ""
Jan 14 15:12:40.305: INFO: stdout: "I0114 15:12:35.503376       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/lbl 395\nI0114 15:12:35.703549       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/w5xd 432\nI0114 15:12:35.903533       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/mdqn 524\nI0114 15:12:36.108336       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/574 333\nI0114 15:12:36.303548       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/d2n 473\nI0114 15:12:36.503596       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/thv 506\nI0114 15:12:36.703556       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/94f 375\nI0114 15:12:36.903529       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/bcp4 595\nI0114 15:12:37.103569       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/9jx 432\nI0114 15:12:37.303576       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/ddc 524\nI0114 15:12:37.503628       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/78w 549\nI0114 15:12:37.703547       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/v2m 383\nI0114 15:12:37.903527       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/c5ph 212\nI0114 15:12:38.103532       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/pbb 307\nI0114 15:12:38.303558       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/cg5 456\nI0114 15:12:38.504573       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/hf9m 256\nI0114 15:12:38.704710       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/5sh 246\nI0114 15:12:38.903556       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/x5c 442\nI0114 15:12:39.103555       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/dtv 271\nI0114 15:12:39.303549       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/hpbw 393\nI0114 15:12:39.503956       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/tlq 403\nI0114 15:12:39.704557       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/2hc 410\nI0114 15:12:39.904662       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/default/pods/6f7 542\nI0114 15:12:40.103539       1 logs_generator.go:76] 23 POST /api/v1/namespaces/default/pods/xfjl 248\n"
STEP: limiting log lines
Jan 14 15:12:40.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 logs logs-generator logs-generator --namespace=kubectl-9599 --tail=1'
Jan 14 15:12:40.512: INFO: stderr: ""
Jan 14 15:12:40.513: INFO: stdout: "I0114 15:12:40.303555       1 logs_generator.go:76] 24 POST /api/v1/namespaces/default/pods/fxts 346\n"
Jan 14 15:12:40.513: INFO: got output "I0114 15:12:40.303555       1 logs_generator.go:76] 24 POST /api/v1/namespaces/default/pods/fxts 346\n"
STEP: limiting log bytes
Jan 14 15:12:40.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 logs logs-generator logs-generator --namespace=kubectl-9599 --limit-bytes=1'
Jan 14 15:12:41.198: INFO: stderr: ""
Jan 14 15:12:41.198: INFO: stdout: "I"
Jan 14 15:12:41.198: INFO: got output "I"
STEP: exposing timestamps
Jan 14 15:12:41.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 logs logs-generator logs-generator --namespace=kubectl-9599 --tail=1 --timestamps'
Jan 14 15:12:42.073: INFO: stderr: ""
Jan 14 15:12:42.073: INFO: stdout: "2020-01-14T15:12:41.903783521Z I0114 15:12:41.903544       1 logs_generator.go:76] 32 GET /api/v1/namespaces/ns/pods/szd 520\n"
Jan 14 15:12:42.073: INFO: got output "2020-01-14T15:12:41.903783521Z I0114 15:12:41.903544       1 logs_generator.go:76] 32 GET /api/v1/namespaces/ns/pods/szd 520\n"
STEP: restricting to a time range
Jan 14 15:12:44.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 logs logs-generator logs-generator --namespace=kubectl-9599 --since=1s'
Jan 14 15:12:45.110: INFO: stderr: ""
Jan 14 15:12:45.110: INFO: stdout: "I0114 15:12:44.103607       1 logs_generator.go:76] 43 PUT /api/v1/namespaces/kube-system/pods/4n4c 239\nI0114 15:12:44.303562       1 logs_generator.go:76] 44 PUT /api/v1/namespaces/default/pods/gqj 536\nI0114 15:12:44.503544       1 logs_generator.go:76] 45 POST /api/v1/namespaces/ns/pods/bqzk 464\nI0114 15:12:44.703538       1 logs_generator.go:76] 46 PUT /api/v1/namespaces/ns/pods/2b42 506\nI0114 15:12:44.903562       1 logs_generator.go:76] 47 PUT /api/v1/namespaces/kube-system/pods/w5hf 251\n"
Jan 14 15:12:45.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 logs logs-generator logs-generator --namespace=kubectl-9599 --since=24h'
Jan 14 15:12:45.996: INFO: stderr: ""
Jan 14 15:12:45.996: INFO: stdout: "I0114 15:12:35.503376       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/lbl 395\nI0114 15:12:35.703549       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/w5xd 432\nI0114 15:12:35.903533       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/mdqn 524\nI0114 15:12:36.108336       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/574 333\nI0114 15:12:36.303548       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/d2n 473\nI0114 15:12:36.503596       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/thv 506\nI0114 15:12:36.703556       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/94f 375\nI0114 15:12:36.903529       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/bcp4 595\nI0114 15:12:37.103569       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/9jx 432\nI0114 15:12:37.303576       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/ddc 524\nI0114 15:12:37.503628       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/78w 549\nI0114 15:12:37.703547       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/v2m 383\nI0114 15:12:37.903527       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/c5ph 212\nI0114 15:12:38.103532       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/pbb 307\nI0114 15:12:38.303558       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/cg5 456\nI0114 15:12:38.504573       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/hf9m 256\nI0114 15:12:38.704710       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/5sh 246\nI0114 15:12:38.903556       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/x5c 442\nI0114 15:12:39.103555       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/dtv 271\nI0114 15:12:39.303549       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/hpbw 393\nI0114 15:12:39.503956       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/tlq 403\nI0114 15:12:39.704557       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/2hc 410\nI0114 15:12:39.904662       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/default/pods/6f7 542\nI0114 15:12:40.103539       1 logs_generator.go:76] 23 POST /api/v1/namespaces/default/pods/xfjl 248\nI0114 15:12:40.303555       1 logs_generator.go:76] 24 POST /api/v1/namespaces/default/pods/fxts 346\nI0114 15:12:40.503532       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/kube-system/pods/rlf 382\nI0114 15:12:40.703542       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/kube-system/pods/kp7j 533\nI0114 15:12:40.903542       1 logs_generator.go:76] 27 POST /api/v1/namespaces/default/pods/494 526\nI0114 15:12:41.103544       1 logs_generator.go:76] 28 POST /api/v1/namespaces/ns/pods/ngb4 490\nI0114 15:12:41.303565       1 logs_generator.go:76] 29 POST /api/v1/namespaces/ns/pods/7lvq 255\nI0114 15:12:41.503543       1 logs_generator.go:76] 30 POST /api/v1/namespaces/ns/pods/vl2t 454\nI0114 15:12:41.703568       1 logs_generator.go:76] 31 PUT /api/v1/namespaces/kube-system/pods/4jp 277\nI0114 15:12:41.903544       1 logs_generator.go:76] 32 GET /api/v1/namespaces/ns/pods/szd 520\nI0114 15:12:42.103525       1 logs_generator.go:76] 33 POST /api/v1/namespaces/kube-system/pods/26n 388\nI0114 15:12:42.303555       1 logs_generator.go:76] 34 POST /api/v1/namespaces/ns/pods/kf4 246\nI0114 15:12:42.503536       1 logs_generator.go:76] 35 PUT /api/v1/namespaces/ns/pods/bgxl 306\nI0114 15:12:42.703535       1 logs_generator.go:76] 36 PUT /api/v1/namespaces/default/pods/ch5 315\nI0114 15:12:42.904798       1 logs_generator.go:76] 37 GET /api/v1/namespaces/default/pods/8ftc 201\nI0114 15:12:43.106612       1 logs_generator.go:76] 38 GET /api/v1/namespaces/kube-system/pods/vb6 356\nI0114 15:12:43.305997       1 logs_generator.go:76] 39 PUT /api/v1/namespaces/default/pods/4ffz 327\nI0114 15:12:43.505832       1 logs_generator.go:76] 40 PUT /api/v1/namespaces/kube-system/pods/xxc 218\nI0114 15:12:43.704902       1 logs_generator.go:76] 41 GET /api/v1/namespaces/default/pods/48sw 568\nI0114 15:12:43.903575       1 logs_generator.go:76] 42 GET /api/v1/namespaces/default/pods/crh 375\nI0114 15:12:44.103607       1 logs_generator.go:76] 43 PUT /api/v1/namespaces/kube-system/pods/4n4c 239\nI0114 15:12:44.303562       1 logs_generator.go:76] 44 PUT /api/v1/namespaces/default/pods/gqj 536\nI0114 15:12:44.503544       1 logs_generator.go:76] 45 POST /api/v1/namespaces/ns/pods/bqzk 464\nI0114 15:12:44.703538       1 logs_generator.go:76] 46 PUT /api/v1/namespaces/ns/pods/2b42 506\nI0114 15:12:44.903562       1 logs_generator.go:76] 47 PUT /api/v1/namespaces/kube-system/pods/w5hf 251\nI0114 15:12:45.103704       1 logs_generator.go:76] 48 GET /api/v1/namespaces/ns/pods/g2r 598\nI0114 15:12:45.304628       1 logs_generator.go:76] 49 GET /api/v1/namespaces/kube-system/pods/68x 417\nI0114 15:12:45.505074       1 logs_generator.go:76] 50 PUT /api/v1/namespaces/ns/pods/2g82 421\nI0114 15:12:45.703577       1 logs_generator.go:76] 51 POST /api/v1/namespaces/kube-system/pods/lm7 413\nI0114 15:12:45.903583       1 logs_generator.go:76] 52 POST /api/v1/namespaces/default/pods/vj48 243\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1450
Jan 14 15:12:45.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 delete pod logs-generator --namespace=kubectl-9599'
Jan 14 15:13:00.148: INFO: stderr: ""
Jan 14 15:13:00.148: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:13:00.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9599" for this suite.

â€¢ [SLOW TEST:36.891 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1440
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":278,"completed":51,"skipped":762,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:13:01.070: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3842
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 15:13:02.739: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jan 14 15:13:12.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 --namespace=crd-publish-openapi-3842 create -f -'
Jan 14 15:13:21.108: INFO: stderr: ""
Jan 14 15:13:21.108: INFO: stdout: "e2e-test-crd-publish-openapi-3248-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 14 15:13:21.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 --namespace=crd-publish-openapi-3842 delete e2e-test-crd-publish-openapi-3248-crds test-foo'
Jan 14 15:13:21.523: INFO: stderr: ""
Jan 14 15:13:21.523: INFO: stdout: "e2e-test-crd-publish-openapi-3248-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 14 15:13:21.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 --namespace=crd-publish-openapi-3842 apply -f -'
Jan 14 15:13:22.323: INFO: stderr: ""
Jan 14 15:13:22.324: INFO: stdout: "e2e-test-crd-publish-openapi-3248-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 14 15:13:22.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 --namespace=crd-publish-openapi-3842 delete e2e-test-crd-publish-openapi-3248-crds test-foo'
Jan 14 15:13:22.612: INFO: stderr: ""
Jan 14 15:13:22.612: INFO: stdout: "e2e-test-crd-publish-openapi-3248-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jan 14 15:13:22.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 --namespace=crd-publish-openapi-3842 create -f -'
Jan 14 15:13:23.688: INFO: rc: 1
Jan 14 15:13:23.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 --namespace=crd-publish-openapi-3842 apply -f -'
Jan 14 15:13:24.525: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jan 14 15:13:24.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 --namespace=crd-publish-openapi-3842 create -f -'
Jan 14 15:13:25.336: INFO: rc: 1
Jan 14 15:13:25.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 --namespace=crd-publish-openapi-3842 apply -f -'
Jan 14 15:13:26.390: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jan 14 15:13:26.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 explain e2e-test-crd-publish-openapi-3248-crds'
Jan 14 15:13:26.977: INFO: stderr: ""
Jan 14 15:13:26.978: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3248-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jan 14 15:13:26.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 explain e2e-test-crd-publish-openapi-3248-crds.metadata'
Jan 14 15:13:27.791: INFO: stderr: ""
Jan 14 15:13:27.791: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3248-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 14 15:13:27.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 explain e2e-test-crd-publish-openapi-3248-crds.spec'
Jan 14 15:13:28.544: INFO: stderr: ""
Jan 14 15:13:28.544: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3248-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 14 15:13:28.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 explain e2e-test-crd-publish-openapi-3248-crds.spec.bars'
Jan 14 15:13:29.529: INFO: stderr: ""
Jan 14 15:13:29.529: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3248-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jan 14 15:13:29.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 explain e2e-test-crd-publish-openapi-3248-crds.spec.bars2'
Jan 14 15:13:30.644: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:13:36.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3842" for this suite.

â€¢ [SLOW TEST:35.491 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":278,"completed":52,"skipped":809,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:13:36.561: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2260
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 14 15:13:41.269: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 14 15:13:44.063: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611622, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611622, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611622, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611621, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 15:13:46.068: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611622, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611622, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611622, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611621, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 15:13:48.080: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611622, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611622, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611622, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611621, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 15:13:50.079: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611622, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611622, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611622, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611621, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 15:13:52.073: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611622, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611622, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611622, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611621, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 15:13:54.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611622, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611622, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611622, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714611621, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 14 15:13:57.537: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 15:13:57.543: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1827-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:14:05.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2260" for this suite.
STEP: Destroying namespace "webhook-2260-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:30.802 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":278,"completed":53,"skipped":809,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:14:07.372: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4269
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:14:16.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4269" for this suite.

â€¢ [SLOW TEST:9.556 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":278,"completed":54,"skipped":809,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:14:16.929: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9434
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test env composition
Jan 14 15:14:19.538: INFO: Waiting up to 5m0s for pod "var-expansion-93f82d01-a8da-4fa5-b278-5c59cac96120" in namespace "var-expansion-9434" to be "success or failure"
Jan 14 15:14:19.543: INFO: Pod "var-expansion-93f82d01-a8da-4fa5-b278-5c59cac96120": Phase="Pending", Reason="", readiness=false. Elapsed: 4.785606ms
Jan 14 15:14:21.553: INFO: Pod "var-expansion-93f82d01-a8da-4fa5-b278-5c59cac96120": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014962196s
Jan 14 15:14:23.558: INFO: Pod "var-expansion-93f82d01-a8da-4fa5-b278-5c59cac96120": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020500159s
Jan 14 15:14:25.761: INFO: Pod "var-expansion-93f82d01-a8da-4fa5-b278-5c59cac96120": Phase="Pending", Reason="", readiness=false. Elapsed: 6.223073989s
Jan 14 15:14:28.119: INFO: Pod "var-expansion-93f82d01-a8da-4fa5-b278-5c59cac96120": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.580897014s
STEP: Saw pod success
Jan 14 15:14:28.119: INFO: Pod "var-expansion-93f82d01-a8da-4fa5-b278-5c59cac96120" satisfied condition "success or failure"
Jan 14 15:14:28.128: INFO: Trying to get logs from node slave2 pod var-expansion-93f82d01-a8da-4fa5-b278-5c59cac96120 container dapi-container: <nil>
STEP: delete the pod
Jan 14 15:14:29.946: INFO: Waiting for pod var-expansion-93f82d01-a8da-4fa5-b278-5c59cac96120 to disappear
Jan 14 15:14:29.950: INFO: Pod var-expansion-93f82d01-a8da-4fa5-b278-5c59cac96120 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:14:29.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9434" for this suite.

â€¢ [SLOW TEST:13.040 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":278,"completed":55,"skipped":816,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:14:29.969: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6066
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1672
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan 14 15:14:32.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-6066'
Jan 14 15:14:32.637: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 14 15:14:32.674: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Jan 14 15:14:32.737: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Jan 14 15:14:33.141: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jan 14 15:14:33.571: INFO: scanned /root for discovery docs: <nil>
Jan 14 15:14:33.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-6066'
Jan 14 15:15:00.492: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jan 14 15:15:00.492: INFO: stdout: "Created e2e-test-httpd-rc-6765820c8ed1e9702463c39c201f06b8\nScaling up e2e-test-httpd-rc-6765820c8ed1e9702463c39c201f06b8 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-6765820c8ed1e9702463c39c201f06b8 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-6765820c8ed1e9702463c39c201f06b8 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Jan 14 15:15:00.492: INFO: stdout: "Created e2e-test-httpd-rc-6765820c8ed1e9702463c39c201f06b8\nScaling up e2e-test-httpd-rc-6765820c8ed1e9702463c39c201f06b8 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-6765820c8ed1e9702463c39c201f06b8 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-6765820c8ed1e9702463c39c201f06b8 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Jan 14 15:15:00.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-6066'
Jan 14 15:15:00.840: INFO: stderr: ""
Jan 14 15:15:00.840: INFO: stdout: "e2e-test-httpd-rc-6765820c8ed1e9702463c39c201f06b8-ztn9g "
Jan 14 15:15:00.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods e2e-test-httpd-rc-6765820c8ed1e9702463c39c201f06b8-ztn9g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6066'
Jan 14 15:15:01.275: INFO: stderr: ""
Jan 14 15:15:01.275: INFO: stdout: "true"
Jan 14 15:15:01.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods e2e-test-httpd-rc-6765820c8ed1e9702463c39c201f06b8-ztn9g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6066'
Jan 14 15:15:01.717: INFO: stderr: ""
Jan 14 15:15:01.717: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Jan 14 15:15:01.717: INFO: e2e-test-httpd-rc-6765820c8ed1e9702463c39c201f06b8-ztn9g is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1678
Jan 14 15:15:01.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 delete rc e2e-test-httpd-rc --namespace=kubectl-6066'
Jan 14 15:15:02.514: INFO: stderr: ""
Jan 14 15:15:02.518: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:15:02.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6066" for this suite.

â€¢ [SLOW TEST:33.302 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1667
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl rolling-update should support rolling-update to same image  [Conformance]","total":278,"completed":56,"skipped":818,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:15:03.271: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7856
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 14 15:15:16.917: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:15:17.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7856" for this suite.

â€¢ [SLOW TEST:14.417 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:131
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":278,"completed":57,"skipped":822,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:15:17.688: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1968
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 15:15:19.584: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jan 14 15:15:29.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 --namespace=crd-publish-openapi-1968 create -f -'
Jan 14 15:15:38.133: INFO: stderr: ""
Jan 14 15:15:38.133: INFO: stdout: "e2e-test-crd-publish-openapi-3841-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 14 15:15:38.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 --namespace=crd-publish-openapi-1968 delete e2e-test-crd-publish-openapi-3841-crds test-cr'
Jan 14 15:15:38.475: INFO: stderr: ""
Jan 14 15:15:38.475: INFO: stdout: "e2e-test-crd-publish-openapi-3841-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 14 15:15:38.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 --namespace=crd-publish-openapi-1968 apply -f -'
Jan 14 15:15:39.671: INFO: stderr: ""
Jan 14 15:15:39.671: INFO: stdout: "e2e-test-crd-publish-openapi-3841-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 14 15:15:39.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 --namespace=crd-publish-openapi-1968 delete e2e-test-crd-publish-openapi-3841-crds test-cr'
Jan 14 15:15:40.841: INFO: stderr: ""
Jan 14 15:15:40.841: INFO: stdout: "e2e-test-crd-publish-openapi-3841-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jan 14 15:15:40.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 explain e2e-test-crd-publish-openapi-3841-crds'
Jan 14 15:15:41.609: INFO: stderr: ""
Jan 14 15:15:41.609: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3841-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:15:48.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1968" for this suite.

â€¢ [SLOW TEST:30.970 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":278,"completed":58,"skipped":837,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:15:48.659: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-9151
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2309
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6311
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:16:35.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9151" for this suite.
STEP: Destroying namespace "nsdeletetest-2309" for this suite.
Jan 14 15:16:35.674: INFO: Namespace nsdeletetest-2309 was already deleted
STEP: Destroying namespace "nsdeletetest-6311" for this suite.

â€¢ [SLOW TEST:47.024 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":278,"completed":59,"skipped":854,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:16:35.683: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1009
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0114 15:16:51.372996      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 14 15:16:51.373: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:16:51.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1009" for this suite.

â€¢ [SLOW TEST:16.324 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":278,"completed":60,"skipped":858,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:16:52.007: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7853
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:17:56.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7853" for this suite.

â€¢ [SLOW TEST:64.581 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":278,"completed":61,"skipped":871,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:17:56.588: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9185
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-30ed12ba-fb29-4d1a-8242-5069c916bc1d
STEP: Creating a pod to test consume configMaps
Jan 14 15:17:58.731: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e892a43d-712b-4fd5-90e9-59e187c1742b" in namespace "projected-9185" to be "success or failure"
Jan 14 15:17:58.893: INFO: Pod "pod-projected-configmaps-e892a43d-712b-4fd5-90e9-59e187c1742b": Phase="Pending", Reason="", readiness=false. Elapsed: 161.796081ms
Jan 14 15:18:00.897: INFO: Pod "pod-projected-configmaps-e892a43d-712b-4fd5-90e9-59e187c1742b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.166215349s
Jan 14 15:18:03.408: INFO: Pod "pod-projected-configmaps-e892a43d-712b-4fd5-90e9-59e187c1742b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.677262366s
Jan 14 15:18:05.517: INFO: Pod "pod-projected-configmaps-e892a43d-712b-4fd5-90e9-59e187c1742b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.786032515s
Jan 14 15:18:07.771: INFO: Pod "pod-projected-configmaps-e892a43d-712b-4fd5-90e9-59e187c1742b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.040440672s
Jan 14 15:18:09.776: INFO: Pod "pod-projected-configmaps-e892a43d-712b-4fd5-90e9-59e187c1742b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.045445844s
Jan 14 15:18:12.176: INFO: Pod "pod-projected-configmaps-e892a43d-712b-4fd5-90e9-59e187c1742b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.44460394s
STEP: Saw pod success
Jan 14 15:18:12.176: INFO: Pod "pod-projected-configmaps-e892a43d-712b-4fd5-90e9-59e187c1742b" satisfied condition "success or failure"
Jan 14 15:18:12.272: INFO: Trying to get logs from node slave2 pod pod-projected-configmaps-e892a43d-712b-4fd5-90e9-59e187c1742b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 14 15:18:12.823: INFO: Waiting for pod pod-projected-configmaps-e892a43d-712b-4fd5-90e9-59e187c1742b to disappear
Jan 14 15:18:12.983: INFO: Pod pod-projected-configmaps-e892a43d-712b-4fd5-90e9-59e187c1742b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:18:12.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9185" for this suite.

â€¢ [SLOW TEST:16.431 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":278,"completed":62,"skipped":872,"failed":0}
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:18:13.018: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6369
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-551b3a1a-bc43-4a61-a13d-80d63c8b8aeb
STEP: Creating a pod to test consume secrets
Jan 14 15:18:16.059: INFO: Waiting up to 5m0s for pod "pod-secrets-501ab0cd-2d67-4bfb-a97e-ab41f0956d6b" in namespace "secrets-6369" to be "success or failure"
Jan 14 15:18:16.071: INFO: Pod "pod-secrets-501ab0cd-2d67-4bfb-a97e-ab41f0956d6b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.873599ms
Jan 14 15:18:18.166: INFO: Pod "pod-secrets-501ab0cd-2d67-4bfb-a97e-ab41f0956d6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.107491436s
Jan 14 15:18:20.348: INFO: Pod "pod-secrets-501ab0cd-2d67-4bfb-a97e-ab41f0956d6b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.289212702s
Jan 14 15:18:22.740: INFO: Pod "pod-secrets-501ab0cd-2d67-4bfb-a97e-ab41f0956d6b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.681227661s
Jan 14 15:18:24.745: INFO: Pod "pod-secrets-501ab0cd-2d67-4bfb-a97e-ab41f0956d6b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.686134735s
Jan 14 15:18:27.384: INFO: Pod "pod-secrets-501ab0cd-2d67-4bfb-a97e-ab41f0956d6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.325468549s
STEP: Saw pod success
Jan 14 15:18:27.385: INFO: Pod "pod-secrets-501ab0cd-2d67-4bfb-a97e-ab41f0956d6b" satisfied condition "success or failure"
Jan 14 15:18:27.395: INFO: Trying to get logs from node slave2 pod pod-secrets-501ab0cd-2d67-4bfb-a97e-ab41f0956d6b container secret-volume-test: <nil>
STEP: delete the pod
Jan 14 15:18:28.151: INFO: Waiting for pod pod-secrets-501ab0cd-2d67-4bfb-a97e-ab41f0956d6b to disappear
Jan 14 15:18:28.334: INFO: Pod pod-secrets-501ab0cd-2d67-4bfb-a97e-ab41f0956d6b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:18:28.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6369" for this suite.

â€¢ [SLOW TEST:15.356 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":63,"skipped":872,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:18:28.375: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-2230
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 15:18:30.682: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-8bf270c9-5bec-4975-86bc-2f0badcdab32" in namespace "security-context-test-2230" to be "success or failure"
Jan 14 15:18:30.689: INFO: Pod "busybox-privileged-false-8bf270c9-5bec-4975-86bc-2f0badcdab32": Phase="Pending", Reason="", readiness=false. Elapsed: 6.586908ms
Jan 14 15:18:32.896: INFO: Pod "busybox-privileged-false-8bf270c9-5bec-4975-86bc-2f0badcdab32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.213149567s
Jan 14 15:18:35.077: INFO: Pod "busybox-privileged-false-8bf270c9-5bec-4975-86bc-2f0badcdab32": Phase="Pending", Reason="", readiness=false. Elapsed: 4.394446391s
Jan 14 15:18:37.497: INFO: Pod "busybox-privileged-false-8bf270c9-5bec-4975-86bc-2f0badcdab32": Phase="Pending", Reason="", readiness=false. Elapsed: 6.813989952s
Jan 14 15:18:39.633: INFO: Pod "busybox-privileged-false-8bf270c9-5bec-4975-86bc-2f0badcdab32": Phase="Pending", Reason="", readiness=false. Elapsed: 8.950920607s
Jan 14 15:18:41.808: INFO: Pod "busybox-privileged-false-8bf270c9-5bec-4975-86bc-2f0badcdab32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.125291771s
Jan 14 15:18:41.808: INFO: Pod "busybox-privileged-false-8bf270c9-5bec-4975-86bc-2f0badcdab32" satisfied condition "success or failure"
Jan 14 15:18:41.820: INFO: Got logs for pod "busybox-privileged-false-8bf270c9-5bec-4975-86bc-2f0badcdab32": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:18:41.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2230" for this suite.

â€¢ [SLOW TEST:14.348 seconds]
[k8s.io] Security Context
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  When creating a pod with privileged
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:225
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":64,"skipped":886,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:18:42.741: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2070
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 15:18:45.448: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jan 14 15:18:55.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 --namespace=crd-publish-openapi-2070 create -f -'
Jan 14 15:19:04.050: INFO: stderr: ""
Jan 14 15:19:04.050: INFO: stdout: "e2e-test-crd-publish-openapi-7554-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 14 15:19:04.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 --namespace=crd-publish-openapi-2070 delete e2e-test-crd-publish-openapi-7554-crds test-cr'
Jan 14 15:19:04.753: INFO: stderr: ""
Jan 14 15:19:04.754: INFO: stdout: "e2e-test-crd-publish-openapi-7554-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 14 15:19:04.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 --namespace=crd-publish-openapi-2070 apply -f -'
Jan 14 15:19:06.019: INFO: stderr: ""
Jan 14 15:19:06.019: INFO: stdout: "e2e-test-crd-publish-openapi-7554-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 14 15:19:06.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 --namespace=crd-publish-openapi-2070 delete e2e-test-crd-publish-openapi-7554-crds test-cr'
Jan 14 15:19:06.455: INFO: stderr: ""
Jan 14 15:19:06.455: INFO: stdout: "e2e-test-crd-publish-openapi-7554-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jan 14 15:19:06.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 explain e2e-test-crd-publish-openapi-7554-crds'
Jan 14 15:19:07.519: INFO: stderr: ""
Jan 14 15:19:07.519: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7554-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:19:12.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2070" for this suite.

â€¢ [SLOW TEST:30.856 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":278,"completed":65,"skipped":901,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:19:13.594: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-6190
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test hostPath mode
Jan 14 15:19:15.968: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-6190" to be "success or failure"
Jan 14 15:19:16.000: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 31.988507ms
Jan 14 15:19:18.068: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.099661467s
Jan 14 15:19:20.072: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.103736891s
Jan 14 15:19:22.544: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.576395863s
Jan 14 15:19:24.690: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 8.721649092s
Jan 14 15:19:27.153: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 11.185336448s
Jan 14 15:19:29.158: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.190130612s
STEP: Saw pod success
Jan 14 15:19:29.158: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jan 14 15:19:29.162: INFO: Trying to get logs from node slave2 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jan 14 15:19:29.736: INFO: Waiting for pod pod-host-path-test to disappear
Jan 14 15:19:29.739: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:19:29.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-6190" for this suite.

â€¢ [SLOW TEST:16.521 seconds]
[sig-storage] HostPath
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":66,"skipped":910,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:19:30.116: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-3085
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 15:19:33.224: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-1918a471-4f87-479b-b13d-4fb583254794" in namespace "security-context-test-3085" to be "success or failure"
Jan 14 15:19:33.671: INFO: Pod "busybox-readonly-false-1918a471-4f87-479b-b13d-4fb583254794": Phase="Pending", Reason="", readiness=false. Elapsed: 446.471413ms
Jan 14 15:19:35.750: INFO: Pod "busybox-readonly-false-1918a471-4f87-479b-b13d-4fb583254794": Phase="Pending", Reason="", readiness=false. Elapsed: 2.525753088s
Jan 14 15:19:38.008: INFO: Pod "busybox-readonly-false-1918a471-4f87-479b-b13d-4fb583254794": Phase="Pending", Reason="", readiness=false. Elapsed: 4.78349s
Jan 14 15:19:40.561: INFO: Pod "busybox-readonly-false-1918a471-4f87-479b-b13d-4fb583254794": Phase="Pending", Reason="", readiness=false. Elapsed: 7.336293346s
Jan 14 15:19:43.044: INFO: Pod "busybox-readonly-false-1918a471-4f87-479b-b13d-4fb583254794": Phase="Pending", Reason="", readiness=false. Elapsed: 9.819646502s
Jan 14 15:19:45.161: INFO: Pod "busybox-readonly-false-1918a471-4f87-479b-b13d-4fb583254794": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.9363697s
Jan 14 15:19:45.161: INFO: Pod "busybox-readonly-false-1918a471-4f87-479b-b13d-4fb583254794" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:19:45.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3085" for this suite.

â€¢ [SLOW TEST:15.296 seconds]
[k8s.io] Security Context
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:164
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":278,"completed":67,"skipped":967,"failed":0}
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:19:45.414: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-461
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-secret-q84n
STEP: Creating a pod to test atomic-volume-subpath
Jan 14 15:19:47.916: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-q84n" in namespace "subpath-461" to be "success or failure"
Jan 14 15:19:48.366: INFO: Pod "pod-subpath-test-secret-q84n": Phase="Pending", Reason="", readiness=false. Elapsed: 449.484155ms
Jan 14 15:19:50.372: INFO: Pod "pod-subpath-test-secret-q84n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.455925653s
Jan 14 15:19:52.673: INFO: Pod "pod-subpath-test-secret-q84n": Phase="Pending", Reason="", readiness=false. Elapsed: 4.756075812s
Jan 14 15:19:54.778: INFO: Pod "pod-subpath-test-secret-q84n": Phase="Pending", Reason="", readiness=false. Elapsed: 6.861703628s
Jan 14 15:19:57.069: INFO: Pod "pod-subpath-test-secret-q84n": Phase="Pending", Reason="", readiness=false. Elapsed: 9.152317852s
Jan 14 15:19:59.307: INFO: Pod "pod-subpath-test-secret-q84n": Phase="Running", Reason="", readiness=true. Elapsed: 11.390896525s
Jan 14 15:20:02.006: INFO: Pod "pod-subpath-test-secret-q84n": Phase="Running", Reason="", readiness=true. Elapsed: 14.089801785s
Jan 14 15:20:04.012: INFO: Pod "pod-subpath-test-secret-q84n": Phase="Running", Reason="", readiness=true. Elapsed: 16.095277782s
Jan 14 15:20:07.166: INFO: Pod "pod-subpath-test-secret-q84n": Phase="Running", Reason="", readiness=true. Elapsed: 19.249225946s
Jan 14 15:20:09.584: INFO: Pod "pod-subpath-test-secret-q84n": Phase="Running", Reason="", readiness=true. Elapsed: 21.667566003s
Jan 14 15:20:11.778: INFO: Pod "pod-subpath-test-secret-q84n": Phase="Running", Reason="", readiness=true. Elapsed: 23.861114989s
Jan 14 15:20:13.789: INFO: Pod "pod-subpath-test-secret-q84n": Phase="Running", Reason="", readiness=true. Elapsed: 25.872456387s
Jan 14 15:20:16.123: INFO: Pod "pod-subpath-test-secret-q84n": Phase="Running", Reason="", readiness=true. Elapsed: 28.206119575s
Jan 14 15:20:18.651: INFO: Pod "pod-subpath-test-secret-q84n": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.734335487s
STEP: Saw pod success
Jan 14 15:20:18.651: INFO: Pod "pod-subpath-test-secret-q84n" satisfied condition "success or failure"
Jan 14 15:20:18.983: INFO: Trying to get logs from node slave2 pod pod-subpath-test-secret-q84n container test-container-subpath-secret-q84n: <nil>
STEP: delete the pod
Jan 14 15:20:19.827: INFO: Waiting for pod pod-subpath-test-secret-q84n to disappear
Jan 14 15:20:20.265: INFO: Pod pod-subpath-test-secret-q84n no longer exists
STEP: Deleting pod pod-subpath-test-secret-q84n
Jan 14 15:20:20.265: INFO: Deleting pod "pod-subpath-test-secret-q84n" in namespace "subpath-461"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:20:20.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-461" for this suite.

â€¢ [SLOW TEST:35.508 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":278,"completed":68,"skipped":969,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:20:20.923: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7633
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7633.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7633.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7633.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7633.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7633.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7633.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 14 15:20:40.789: INFO: DNS probes using dns-7633/dns-test-f7d3ef40-f8b8-487d-9b91-1586f38b61ed succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:20:41.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7633" for this suite.

â€¢ [SLOW TEST:20.183 seconds]
[sig-network] DNS
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":278,"completed":69,"skipped":979,"failed":0}
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:20:41.106: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6836
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jan 14 15:20:43.025: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:21:00.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6836" for this suite.

â€¢ [SLOW TEST:19.983 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":278,"completed":70,"skipped":983,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:21:01.090: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9678
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-ede54bec-7c57-4cad-b513-5b9ac7ffb241
STEP: Creating a pod to test consume secrets
Jan 14 15:21:06.271: INFO: Waiting up to 5m0s for pod "pod-secrets-b2a11ae7-1787-4832-ab31-982e5a5c58d3" in namespace "secrets-9678" to be "success or failure"
Jan 14 15:21:06.614: INFO: Pod "pod-secrets-b2a11ae7-1787-4832-ab31-982e5a5c58d3": Phase="Pending", Reason="", readiness=false. Elapsed: 342.222467ms
Jan 14 15:21:08.863: INFO: Pod "pod-secrets-b2a11ae7-1787-4832-ab31-982e5a5c58d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.591882482s
Jan 14 15:21:11.363: INFO: Pod "pod-secrets-b2a11ae7-1787-4832-ab31-982e5a5c58d3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.091877194s
Jan 14 15:21:13.478: INFO: Pod "pod-secrets-b2a11ae7-1787-4832-ab31-982e5a5c58d3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.2068586s
Jan 14 15:21:15.576: INFO: Pod "pod-secrets-b2a11ae7-1787-4832-ab31-982e5a5c58d3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.304390253s
Jan 14 15:21:17.611: INFO: Pod "pod-secrets-b2a11ae7-1787-4832-ab31-982e5a5c58d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.339202148s
STEP: Saw pod success
Jan 14 15:21:17.611: INFO: Pod "pod-secrets-b2a11ae7-1787-4832-ab31-982e5a5c58d3" satisfied condition "success or failure"
Jan 14 15:21:17.626: INFO: Trying to get logs from node slave2 pod pod-secrets-b2a11ae7-1787-4832-ab31-982e5a5c58d3 container secret-env-test: <nil>
STEP: delete the pod
Jan 14 15:21:20.463: INFO: Waiting for pod pod-secrets-b2a11ae7-1787-4832-ab31-982e5a5c58d3 to disappear
Jan 14 15:21:20.472: INFO: Pod pod-secrets-b2a11ae7-1787-4832-ab31-982e5a5c58d3 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:21:20.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9678" for this suite.

â€¢ [SLOW TEST:19.962 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":278,"completed":71,"skipped":987,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:21:21.065: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9545
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 14 15:21:33.963: INFO: Successfully updated pod "pod-update-activedeadlineseconds-fcd84670-c824-4eb8-acd2-1ba739cdab40"
Jan 14 15:21:33.963: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-fcd84670-c824-4eb8-acd2-1ba739cdab40" in namespace "pods-9545" to be "terminated due to deadline exceeded"
Jan 14 15:21:34.059: INFO: Pod "pod-update-activedeadlineseconds-fcd84670-c824-4eb8-acd2-1ba739cdab40": Phase="Running", Reason="", readiness=true. Elapsed: 96.272816ms
Jan 14 15:21:36.216: INFO: Pod "pod-update-activedeadlineseconds-fcd84670-c824-4eb8-acd2-1ba739cdab40": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.253102582s
Jan 14 15:21:36.216: INFO: Pod "pod-update-activedeadlineseconds-fcd84670-c824-4eb8-acd2-1ba739cdab40" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:21:36.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9545" for this suite.

â€¢ [SLOW TEST:16.039 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":278,"completed":72,"skipped":1040,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:21:37.104: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6761
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 14 15:21:43.634: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 14 15:21:45.967: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714612104, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714612104, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714612105, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714612103, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 15:21:47.971: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714612104, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714612104, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714612105, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714612103, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 15:21:50.344: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714612104, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714612104, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714612105, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714612103, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 15:21:52.062: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714612104, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714612104, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714612105, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714612103, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 15:21:54.337: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714612104, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714612104, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714612105, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714612103, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 14 15:21:58.594: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:21:59.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6761" for this suite.
STEP: Destroying namespace "webhook-6761-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:25.641 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":278,"completed":73,"skipped":1046,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:22:02.744: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5444
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 14 15:22:05.393: INFO: Waiting up to 5m0s for pod "downwardapi-volume-198b350b-001e-4dfa-ba48-318f1187c7ce" in namespace "projected-5444" to be "success or failure"
Jan 14 15:22:05.757: INFO: Pod "downwardapi-volume-198b350b-001e-4dfa-ba48-318f1187c7ce": Phase="Pending", Reason="", readiness=false. Elapsed: 363.568347ms
Jan 14 15:22:07.811: INFO: Pod "downwardapi-volume-198b350b-001e-4dfa-ba48-318f1187c7ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.41758998s
Jan 14 15:22:09.822: INFO: Pod "downwardapi-volume-198b350b-001e-4dfa-ba48-318f1187c7ce": Phase="Pending", Reason="", readiness=false. Elapsed: 4.428428799s
Jan 14 15:22:11.922: INFO: Pod "downwardapi-volume-198b350b-001e-4dfa-ba48-318f1187c7ce": Phase="Pending", Reason="", readiness=false. Elapsed: 6.528931846s
Jan 14 15:22:14.283: INFO: Pod "downwardapi-volume-198b350b-001e-4dfa-ba48-318f1187c7ce": Phase="Pending", Reason="", readiness=false. Elapsed: 8.889847585s
Jan 14 15:22:16.289: INFO: Pod "downwardapi-volume-198b350b-001e-4dfa-ba48-318f1187c7ce": Phase="Pending", Reason="", readiness=false. Elapsed: 10.895491489s
Jan 14 15:22:18.322: INFO: Pod "downwardapi-volume-198b350b-001e-4dfa-ba48-318f1187c7ce": Phase="Pending", Reason="", readiness=false. Elapsed: 12.928924912s
Jan 14 15:22:20.349: INFO: Pod "downwardapi-volume-198b350b-001e-4dfa-ba48-318f1187c7ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.95546405s
STEP: Saw pod success
Jan 14 15:22:20.349: INFO: Pod "downwardapi-volume-198b350b-001e-4dfa-ba48-318f1187c7ce" satisfied condition "success or failure"
Jan 14 15:22:20.355: INFO: Trying to get logs from node slave3 pod downwardapi-volume-198b350b-001e-4dfa-ba48-318f1187c7ce container client-container: <nil>
STEP: delete the pod
Jan 14 15:22:21.262: INFO: Waiting for pod downwardapi-volume-198b350b-001e-4dfa-ba48-318f1187c7ce to disappear
Jan 14 15:22:21.280: INFO: Pod downwardapi-volume-198b350b-001e-4dfa-ba48-318f1187c7ce no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:22:21.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5444" for this suite.

â€¢ [SLOW TEST:18.563 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":278,"completed":74,"skipped":1056,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:22:21.312: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6887
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:22:42.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6887" for this suite.

â€¢ [SLOW TEST:20.951 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":278,"completed":75,"skipped":1061,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:22:42.264: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6226
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 14 15:22:44.948: INFO: Waiting up to 5m0s for pod "pod-721a0212-360f-47d5-87ff-c26055fdd18b" in namespace "emptydir-6226" to be "success or failure"
Jan 14 15:22:44.959: INFO: Pod "pod-721a0212-360f-47d5-87ff-c26055fdd18b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.392301ms
Jan 14 15:22:46.964: INFO: Pod "pod-721a0212-360f-47d5-87ff-c26055fdd18b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015019565s
Jan 14 15:22:49.411: INFO: Pod "pod-721a0212-360f-47d5-87ff-c26055fdd18b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.462111959s
Jan 14 15:22:51.557: INFO: Pod "pod-721a0212-360f-47d5-87ff-c26055fdd18b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.608040655s
Jan 14 15:22:53.563: INFO: Pod "pod-721a0212-360f-47d5-87ff-c26055fdd18b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.614199345s
Jan 14 15:22:55.569: INFO: Pod "pod-721a0212-360f-47d5-87ff-c26055fdd18b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.620304709s
Jan 14 15:22:57.764: INFO: Pod "pod-721a0212-360f-47d5-87ff-c26055fdd18b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.81571145s
STEP: Saw pod success
Jan 14 15:22:57.764: INFO: Pod "pod-721a0212-360f-47d5-87ff-c26055fdd18b" satisfied condition "success or failure"
Jan 14 15:22:57.775: INFO: Trying to get logs from node slave2 pod pod-721a0212-360f-47d5-87ff-c26055fdd18b container test-container: <nil>
STEP: delete the pod
Jan 14 15:22:58.084: INFO: Waiting for pod pod-721a0212-360f-47d5-87ff-c26055fdd18b to disappear
Jan 14 15:22:58.088: INFO: Pod pod-721a0212-360f-47d5-87ff-c26055fdd18b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:22:58.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6226" for this suite.

â€¢ [SLOW TEST:15.841 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":76,"skipped":1070,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:22:58.105: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3715
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jan 14 15:23:00.273: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3715 /api/v1/namespaces/watch-3715/configmaps/e2e-watch-test-resource-version 815fb450-64b7-412d-b7e1-bdd63f5a0c67 104261 0 2020-01-14 15:23:00 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 14 15:23:00.273: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3715 /api/v1/namespaces/watch-3715/configmaps/e2e-watch-test-resource-version 815fb450-64b7-412d-b7e1-bdd63f5a0c67 104262 0 2020-01-14 15:23:00 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:23:00.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3715" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":278,"completed":77,"skipped":1073,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:23:00.655: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4000
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 14 15:23:02.576: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b0b3a583-83d4-4fcc-b8c3-15aa1af6a86a" in namespace "projected-4000" to be "success or failure"
Jan 14 15:23:03.359: INFO: Pod "downwardapi-volume-b0b3a583-83d4-4fcc-b8c3-15aa1af6a86a": Phase="Pending", Reason="", readiness=false. Elapsed: 782.946367ms
Jan 14 15:23:05.478: INFO: Pod "downwardapi-volume-b0b3a583-83d4-4fcc-b8c3-15aa1af6a86a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.90152754s
Jan 14 15:23:07.800: INFO: Pod "downwardapi-volume-b0b3a583-83d4-4fcc-b8c3-15aa1af6a86a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.223391534s
Jan 14 15:23:09.861: INFO: Pod "downwardapi-volume-b0b3a583-83d4-4fcc-b8c3-15aa1af6a86a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.284615002s
Jan 14 15:23:11.890: INFO: Pod "downwardapi-volume-b0b3a583-83d4-4fcc-b8c3-15aa1af6a86a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.313445902s
Jan 14 15:23:13.912: INFO: Pod "downwardapi-volume-b0b3a583-83d4-4fcc-b8c3-15aa1af6a86a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.335736941s
Jan 14 15:23:15.935: INFO: Pod "downwardapi-volume-b0b3a583-83d4-4fcc-b8c3-15aa1af6a86a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.358689463s
STEP: Saw pod success
Jan 14 15:23:15.935: INFO: Pod "downwardapi-volume-b0b3a583-83d4-4fcc-b8c3-15aa1af6a86a" satisfied condition "success or failure"
Jan 14 15:23:15.940: INFO: Trying to get logs from node slave2 pod downwardapi-volume-b0b3a583-83d4-4fcc-b8c3-15aa1af6a86a container client-container: <nil>
STEP: delete the pod
Jan 14 15:23:17.286: INFO: Waiting for pod downwardapi-volume-b0b3a583-83d4-4fcc-b8c3-15aa1af6a86a to disappear
Jan 14 15:23:17.549: INFO: Pod downwardapi-volume-b0b3a583-83d4-4fcc-b8c3-15aa1af6a86a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:23:17.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4000" for this suite.

â€¢ [SLOW TEST:17.300 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":78,"skipped":1079,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:23:17.954: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2628
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:23:33.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2628" for this suite.

â€¢ [SLOW TEST:15.516 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":278,"completed":79,"skipped":1080,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:23:33.470: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-523
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:23:55.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-523" for this suite.

â€¢ [SLOW TEST:22.768 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":278,"completed":80,"skipped":1087,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:23:56.245: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4079
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jan 14 15:24:11.588: INFO: Successfully updated pod "labelsupdate51ee6dfe-a025-4c59-bc09-b172e984fdd3"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:24:14.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4079" for this suite.

â€¢ [SLOW TEST:17.798 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":278,"completed":81,"skipped":1089,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:24:14.044: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8403
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 15:24:16.017: INFO: Creating deployment "webserver-deployment"
Jan 14 15:24:16.024: INFO: Waiting for observed generation 1
Jan 14 15:24:19.593: INFO: Waiting for all required pods to come up
Jan 14 15:24:21.883: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jan 14 15:24:38.074: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 14 15:24:38.281: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 14 15:24:38.632: INFO: Updating deployment webserver-deployment
Jan 14 15:24:38.633: INFO: Waiting for observed generation 2
Jan 14 15:24:44.903: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 14 15:24:44.909: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 14 15:24:45.249: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 14 15:24:45.747: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 14 15:24:45.747: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 14 15:24:48.126: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 14 15:24:48.580: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 14 15:24:48.580: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 14 15:24:49.386: INFO: Updating deployment webserver-deployment
Jan 14 15:24:49.386: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 14 15:24:49.475: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 14 15:24:50.174: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jan 14 15:24:52.057: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8403 /apis/apps/v1/namespaces/deployment-8403/deployments/webserver-deployment 17086bc1-41de-405b-8ad6-84840dc12bf1 104912 3 2020-01-14 15:24:16 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00225bfc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-01-14 15:24:47 +0000 UTC,LastTransitionTime:2020-01-14 15:24:16 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-01-14 15:24:49 +0000 UTC,LastTransitionTime:2020-01-14 15:24:49 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan 14 15:24:52.622: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-8403 /apis/apps/v1/namespaces/deployment-8403/replicasets/webserver-deployment-c7997dcc8 5e3d9eab-fb35-4a0a-b48c-89d7806a693b 104901 3 2020-01-14 15:24:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 17086bc1-41de-405b-8ad6-84840dc12bf1 0xc0029848f7 0xc0029848f8}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002984968 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 14 15:24:52.622: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 14 15:24:52.622: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-8403 /apis/apps/v1/namespaces/deployment-8403/replicasets/webserver-deployment-595b5b9587 f1c7043c-0af1-4216-9fdc-566d8c5d5ca7 104900 3 2020-01-14 15:24:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 17086bc1-41de-405b-8ad6-84840dc12bf1 0xc002984747 0xc002984748}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0029847a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan 14 15:24:56.190: INFO: Pod "webserver-deployment-595b5b9587-4ndt8" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-4ndt8 webserver-deployment-595b5b9587- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-595b5b9587-4ndt8 4bb51d94-f4e5-4a44-970d-519955d1a8ab 104754 0 2020-01-14 15:24:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f1c7043c-0af1-4216-9fdc-566d8c5d5ca7 0xc002984e37 0xc002984e38}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.161,PodIP:10.151.32.17,StartTime:2020-01-14 15:24:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-14 15:24:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://60f94c5000915ad638182c74741e755a9039db14d31e776d857d6d213d5b1c91,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.151.32.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.190: INFO: Pod "webserver-deployment-595b5b9587-4nfh6" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-4nfh6 webserver-deployment-595b5b9587- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-595b5b9587-4nfh6 a1e42f25-6b6b-4cc7-a4f3-e1646637a8b8 104947 0 2020-01-14 15:24:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f1c7043c-0af1-4216-9fdc-566d8c5d5ca7 0xc002984fb7 0xc002984fb8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.190: INFO: Pod "webserver-deployment-595b5b9587-6qpzj" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-6qpzj webserver-deployment-595b5b9587- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-595b5b9587-6qpzj e21226a0-19e6-440e-b73e-44bed9d892e3 104950 0 2020-01-14 15:24:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f1c7043c-0af1-4216-9fdc-566d8c5d5ca7 0xc0029850b0 0xc0029850b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.191: INFO: Pod "webserver-deployment-595b5b9587-6vr4m" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-6vr4m webserver-deployment-595b5b9587- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-595b5b9587-6vr4m ea72277e-a132-4f80-bce1-4fd7514d11fc 104958 0 2020-01-14 15:24:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f1c7043c-0af1-4216-9fdc-566d8c5d5ca7 0xc0029851a0 0xc0029851a1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.367: INFO: Pod "webserver-deployment-595b5b9587-92xst" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-92xst webserver-deployment-595b5b9587- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-595b5b9587-92xst 5e4c27de-f0cd-493b-b240-8bb593718f5c 104770 0 2020-01-14 15:24:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f1c7043c-0af1-4216-9fdc-566d8c5d5ca7 0xc0029852b0 0xc0029852b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.132,PodIP:10.151.194.65,StartTime:2020-01-14 15:24:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-14 15:24:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://b325190876ea71b7ba45642c5e9385dbfd5447730b0cfe69221d7ce91216964f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.151.194.65,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.369: INFO: Pod "webserver-deployment-595b5b9587-b4t6d" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-b4t6d webserver-deployment-595b5b9587- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-595b5b9587-b4t6d b590aa4c-87f2-4b90-88b1-fc9cfffad3bd 104957 0 2020-01-14 15:24:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f1c7043c-0af1-4216-9fdc-566d8c5d5ca7 0xc002985427 0xc002985428}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.369: INFO: Pod "webserver-deployment-595b5b9587-cggc9" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-cggc9 webserver-deployment-595b5b9587- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-595b5b9587-cggc9 6d05c2cd-f774-464a-9df0-61dc536de8a4 104949 0 2020-01-14 15:24:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f1c7043c-0af1-4216-9fdc-566d8c5d5ca7 0xc002985540 0xc002985541}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.370: INFO: Pod "webserver-deployment-595b5b9587-d522s" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-d522s webserver-deployment-595b5b9587- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-595b5b9587-d522s b7ffcb0b-b58d-4ebb-8754-31e831a92271 104777 0 2020-01-14 15:24:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f1c7043c-0af1-4216-9fdc-566d8c5d5ca7 0xc002985630 0xc002985631}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.112,PodIP:10.151.161.19,StartTime:2020-01-14 15:24:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-14 15:24:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://f60e3e00d168e444952bfe38edd3fa575558307b12923e13c793b2ad8700f77f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.151.161.19,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.370: INFO: Pod "webserver-deployment-595b5b9587-dblvt" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-dblvt webserver-deployment-595b5b9587- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-595b5b9587-dblvt 39410368-8782-4673-a086-5590939a13a4 104804 0 2020-01-14 15:24:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f1c7043c-0af1-4216-9fdc-566d8c5d5ca7 0xc0029857a7 0xc0029857a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.135,PodIP:10.151.26.54,StartTime:2020-01-14 15:24:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-14 15:24:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://860d9bdcbfeb0e1be2a1ee66ed0f97c08a5080c18b49b7aa2b0da68739fb523b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.151.26.54,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.370: INFO: Pod "webserver-deployment-595b5b9587-fxcwp" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-fxcwp webserver-deployment-595b5b9587- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-595b5b9587-fxcwp 86c156ae-c831-4d04-af1f-35d5b09b5c83 104956 0 2020-01-14 15:24:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f1c7043c-0af1-4216-9fdc-566d8c5d5ca7 0xc002985927 0xc002985928}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.374: INFO: Pod "webserver-deployment-595b5b9587-h7c4w" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-h7c4w webserver-deployment-595b5b9587- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-595b5b9587-h7c4w 9c9ae6a8-e710-4180-83a9-ce7d239eb66c 104789 0 2020-01-14 15:24:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f1c7043c-0af1-4216-9fdc-566d8c5d5ca7 0xc002985b20 0xc002985b21}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.132,PodIP:10.151.194.66,StartTime:2020-01-14 15:24:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-14 15:24:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://172787f9c49530defaa7deef58b065c7079bd802f34f1444b6266b61b8e8b935,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.151.194.66,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.374: INFO: Pod "webserver-deployment-595b5b9587-kb8x5" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-kb8x5 webserver-deployment-595b5b9587- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-595b5b9587-kb8x5 b08b839d-b05e-4126-a2e8-1bffeb5c5206 104758 0 2020-01-14 15:24:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f1c7043c-0af1-4216-9fdc-566d8c5d5ca7 0xc002985e57 0xc002985e58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.152,PodIP:10.151.51.40,StartTime:2020-01-14 15:24:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-14 15:24:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://62e36db39764fc94295d22f9088cbf304063b3113c652ccca88537f04c9c86bc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.151.51.40,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.374: INFO: Pod "webserver-deployment-595b5b9587-p9fgc" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-p9fgc webserver-deployment-595b5b9587- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-595b5b9587-p9fgc 5f086ed2-fcf3-46bb-828d-bc1f22ec91a0 104776 0 2020-01-14 15:24:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f1c7043c-0af1-4216-9fdc-566d8c5d5ca7 0xc0039c0087 0xc0039c0088}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.152,PodIP:10.151.51.41,StartTime:2020-01-14 15:24:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-14 15:24:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://2f907d65434c6557fb4259d6d96d9bd44b7c435f272131c8b404f4dd16111e36,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.151.51.41,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.375: INFO: Pod "webserver-deployment-595b5b9587-p9pfh" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-p9pfh webserver-deployment-595b5b9587- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-595b5b9587-p9pfh a9c0e615-afce-45c9-8f43-a9ffe231f6a0 104915 0 2020-01-14 15:24:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f1c7043c-0af1-4216-9fdc-566d8c5d5ca7 0xc0039c0207 0xc0039c0208}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.142,PodIP:,StartTime:2020-01-14 15:24:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.375: INFO: Pod "webserver-deployment-595b5b9587-qjt6f" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qjt6f webserver-deployment-595b5b9587- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-595b5b9587-qjt6f e1bf41f3-5ff7-490b-a9ac-2a21c3ec31f6 104948 0 2020-01-14 15:24:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f1c7043c-0af1-4216-9fdc-566d8c5d5ca7 0xc0039c0367 0xc0039c0368}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.375: INFO: Pod "webserver-deployment-595b5b9587-qzc2q" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qzc2q webserver-deployment-595b5b9587- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-595b5b9587-qzc2q 80405a9c-0a70-4f13-b1cb-15f12504bad3 104943 0 2020-01-14 15:24:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f1c7043c-0af1-4216-9fdc-566d8c5d5ca7 0xc0039c0460 0xc0039c0461}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.161,PodIP:,StartTime:2020-01-14 15:24:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.375: INFO: Pod "webserver-deployment-595b5b9587-rc9q7" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rc9q7 webserver-deployment-595b5b9587- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-595b5b9587-rc9q7 4bae6ca5-3d00-4cdc-95cc-cf39cb86d535 104796 0 2020-01-14 15:24:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f1c7043c-0af1-4216-9fdc-566d8c5d5ca7 0xc0039c05b7 0xc0039c05b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.158,PodIP:10.151.208.15,StartTime:2020-01-14 15:24:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-14 15:24:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://c6003d60d4c0c2f1b566e8a683399107674517bb3b8236c952b7e306ec8ff746,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.151.208.15,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.376: INFO: Pod "webserver-deployment-595b5b9587-sls6b" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-sls6b webserver-deployment-595b5b9587- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-595b5b9587-sls6b 56e8a9c3-aa04-4c79-b60a-2f05d6335c2e 104974 0 2020-01-14 15:24:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f1c7043c-0af1-4216-9fdc-566d8c5d5ca7 0xc0039c0737 0xc0039c0738}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.142,PodIP:,StartTime:2020-01-14 15:24:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.376: INFO: Pod "webserver-deployment-595b5b9587-tkprj" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-tkprj webserver-deployment-595b5b9587- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-595b5b9587-tkprj 24f05846-7c58-4e35-9d30-99a042fbfb14 104951 0 2020-01-14 15:24:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f1c7043c-0af1-4216-9fdc-566d8c5d5ca7 0xc0039c0897 0xc0039c0898}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.379: INFO: Pod "webserver-deployment-595b5b9587-xlbp5" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xlbp5 webserver-deployment-595b5b9587- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-595b5b9587-xlbp5 71b83132-b610-455a-ac0d-c5a92da0bdf2 104946 0 2020-01-14 15:24:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f1c7043c-0af1-4216-9fdc-566d8c5d5ca7 0xc0039c0990 0xc0039c0991}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.379: INFO: Pod "webserver-deployment-c7997dcc8-28ffw" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-28ffw webserver-deployment-c7997dcc8- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-c7997dcc8-28ffw 11198a23-b21c-41df-ab2d-61fc5470873a 104885 0 2020-01-14 15:24:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5e3d9eab-fb35-4a0a-b48c-89d7806a693b 0xc0039c0aa0 0xc0039c0aa1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.135,PodIP:,StartTime:2020-01-14 15:24:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.379: INFO: Pod "webserver-deployment-c7997dcc8-4b4s6" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-4b4s6 webserver-deployment-c7997dcc8- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-c7997dcc8-4b4s6 1d7d8ec7-a108-485a-947f-d4561649a43b 104940 0 2020-01-14 15:24:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5e3d9eab-fb35-4a0a-b48c-89d7806a693b 0xc0039c0c17 0xc0039c0c18}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.132,PodIP:,StartTime:2020-01-14 15:24:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.382: INFO: Pod "webserver-deployment-c7997dcc8-594zn" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-594zn webserver-deployment-c7997dcc8- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-c7997dcc8-594zn 23dd9384-f410-4617-8ea7-80afb1594985 104848 0 2020-01-14 15:24:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5e3d9eab-fb35-4a0a-b48c-89d7806a693b 0xc0039c0d97 0xc0039c0d98}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.132,PodIP:,StartTime:2020-01-14 15:24:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.388: INFO: Pod "webserver-deployment-c7997dcc8-5rjq5" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-5rjq5 webserver-deployment-c7997dcc8- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-c7997dcc8-5rjq5 a28a6b7f-693c-41cb-99f9-82c58ceebfa3 104889 0 2020-01-14 15:24:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5e3d9eab-fb35-4a0a-b48c-89d7806a693b 0xc0039c0f17 0xc0039c0f18}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.158,PodIP:,StartTime:2020-01-14 15:24:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.388: INFO: Pod "webserver-deployment-c7997dcc8-8b7rj" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-8b7rj webserver-deployment-c7997dcc8- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-c7997dcc8-8b7rj c553cc47-d00b-424d-97ea-a70847f10e8f 104846 0 2020-01-14 15:24:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5e3d9eab-fb35-4a0a-b48c-89d7806a693b 0xc0039c10b7 0xc0039c10b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.161,PodIP:,StartTime:2020-01-14 15:24:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.388: INFO: Pod "webserver-deployment-c7997dcc8-gszn6" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-gszn6 webserver-deployment-c7997dcc8- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-c7997dcc8-gszn6 49be582f-fd54-4e5a-bd16-63185c1d160d 104944 0 2020-01-14 15:24:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5e3d9eab-fb35-4a0a-b48c-89d7806a693b 0xc0039c1257 0xc0039c1258}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.389: INFO: Pod "webserver-deployment-c7997dcc8-hg4kq" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-hg4kq webserver-deployment-c7997dcc8- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-c7997dcc8-hg4kq 074e5bb5-e619-4d20-9e3f-74138156b92e 104955 0 2020-01-14 15:24:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5e3d9eab-fb35-4a0a-b48c-89d7806a693b 0xc0039c1370 0xc0039c1371}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.389: INFO: Pod "webserver-deployment-c7997dcc8-rp85l" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-rp85l webserver-deployment-c7997dcc8- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-c7997dcc8-rp85l 7cc02f66-9a9d-4854-a789-01276051726c 104952 0 2020-01-14 15:24:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5e3d9eab-fb35-4a0a-b48c-89d7806a693b 0xc0039c14b0 0xc0039c14b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.392: INFO: Pod "webserver-deployment-c7997dcc8-v6nxl" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-v6nxl webserver-deployment-c7997dcc8- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-c7997dcc8-v6nxl 2d7f3dd3-7dba-4e78-b68a-9427f8f9b0bf 104919 0 2020-01-14 15:24:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5e3d9eab-fb35-4a0a-b48c-89d7806a693b 0xc0039c15d0 0xc0039c15d1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.392: INFO: Pod "webserver-deployment-c7997dcc8-x8kxp" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-x8kxp webserver-deployment-c7997dcc8- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-c7997dcc8-x8kxp a6e8d621-b3b9-473f-bc0d-bc922aefa567 104954 0 2020-01-14 15:24:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5e3d9eab-fb35-4a0a-b48c-89d7806a693b 0xc0039c1700 0xc0039c1701}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.392: INFO: Pod "webserver-deployment-c7997dcc8-xczpb" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-xczpb webserver-deployment-c7997dcc8- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-c7997dcc8-xczpb 56d65041-8277-4c90-a0ea-38dd490ccaf4 104836 0 2020-01-14 15:24:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5e3d9eab-fb35-4a0a-b48c-89d7806a693b 0xc0039c1820 0xc0039c1821}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.142,PodIP:,StartTime:2020-01-14 15:24:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.393: INFO: Pod "webserver-deployment-c7997dcc8-z2l2x" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-z2l2x webserver-deployment-c7997dcc8- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-c7997dcc8-z2l2x c86aba20-94ca-4c18-a486-845492d4c5cf 104953 0 2020-01-14 15:24:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5e3d9eab-fb35-4a0a-b48c-89d7806a693b 0xc0039c1997 0xc0039c1998}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 14 15:24:56.393: INFO: Pod "webserver-deployment-c7997dcc8-znf9x" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-znf9x webserver-deployment-c7997dcc8- deployment-8403 /api/v1/namespaces/deployment-8403/pods/webserver-deployment-c7997dcc8-znf9x 255327bf-dc50-426a-a2a6-9d1a9238bd3f 104945 0 2020-01-14 15:24:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5e3d9eab-fb35-4a0a-b48c-89d7806a693b 0xc0039c1ac0 0xc0039c1ac1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2s47t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2s47t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2s47t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:24:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.152,PodIP:,StartTime:2020-01-14 15:24:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:24:56.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8403" for this suite.

â€¢ [SLOW TEST:49.113 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":278,"completed":82,"skipped":1105,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:25:03.157: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9659
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl replace
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1877
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan 14 15:26:00.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-9659'
Jan 14 15:26:03.244: INFO: stderr: ""
Jan 14 15:26:03.244: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jan 14 15:26:38.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pod e2e-test-httpd-pod --namespace=kubectl-9659 -o json'
Jan 14 15:26:38.653: INFO: stderr: ""
Jan 14 15:26:38.653: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2020-01-14T15:26:01Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9659\",\n        \"resourceVersion\": \"105646\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-9659/pods/e2e-test-httpd-pod\",\n        \"uid\": \"6d7b7266-f329-406a-82fb-ce689ef9af31\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-8w5zl\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"slave2\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-8w5zl\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-8w5zl\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-01-14T15:26:04Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-01-14T15:26:34Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-01-14T15:26:34Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-01-14T15:26:03Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://98f3d61ca82c633f065b942d9ce4c8be7910e331a17a5596aaa6824d89b63305\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-01-14T15:26:33Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.0.142\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.151.49.161\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.151.49.161\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-01-14T15:26:04Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jan 14 15:26:38.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 replace -f - --namespace=kubectl-9659'
Jan 14 15:26:40.260: INFO: stderr: ""
Jan 14 15:26:40.260: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1882
Jan 14 15:26:40.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 delete pods e2e-test-httpd-pod --namespace=kubectl-9659'
Jan 14 15:26:50.211: INFO: stderr: ""
Jan 14 15:26:50.211: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:26:50.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9659" for this suite.

â€¢ [SLOW TEST:107.316 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1873
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":278,"completed":83,"skipped":1108,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:26:50.474: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-587
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-587.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-587.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-587.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-587.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-587.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-587.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 14 15:27:08.324: INFO: DNS probes using dns-587/dns-test-106e7050-a3d8-40a5-9b9d-a9e9f1e03b75 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:27:08.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-587" for this suite.

â€¢ [SLOW TEST:18.226 seconds]
[sig-network] DNS
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":278,"completed":84,"skipped":1133,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:27:08.700: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3144
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-24b82c5a-e594-4ae1-89f4-421327d4516a
STEP: Creating a pod to test consume configMaps
Jan 14 15:27:11.249: INFO: Waiting up to 5m0s for pod "pod-configmaps-1f8f9ecb-48b2-4f83-b97c-323ccdba617f" in namespace "configmap-3144" to be "success or failure"
Jan 14 15:27:11.617: INFO: Pod "pod-configmaps-1f8f9ecb-48b2-4f83-b97c-323ccdba617f": Phase="Pending", Reason="", readiness=false. Elapsed: 367.440604ms
Jan 14 15:27:13.743: INFO: Pod "pod-configmaps-1f8f9ecb-48b2-4f83-b97c-323ccdba617f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.493700378s
Jan 14 15:27:15.830: INFO: Pod "pod-configmaps-1f8f9ecb-48b2-4f83-b97c-323ccdba617f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.580701481s
Jan 14 15:27:18.195: INFO: Pod "pod-configmaps-1f8f9ecb-48b2-4f83-b97c-323ccdba617f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.94603668s
Jan 14 15:27:20.266: INFO: Pod "pod-configmaps-1f8f9ecb-48b2-4f83-b97c-323ccdba617f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.016814194s
Jan 14 15:27:22.552: INFO: Pod "pod-configmaps-1f8f9ecb-48b2-4f83-b97c-323ccdba617f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.303150801s
STEP: Saw pod success
Jan 14 15:27:22.553: INFO: Pod "pod-configmaps-1f8f9ecb-48b2-4f83-b97c-323ccdba617f" satisfied condition "success or failure"
Jan 14 15:27:23.464: INFO: Trying to get logs from node slave2 pod pod-configmaps-1f8f9ecb-48b2-4f83-b97c-323ccdba617f container configmap-volume-test: <nil>
STEP: delete the pod
Jan 14 15:27:23.868: INFO: Waiting for pod pod-configmaps-1f8f9ecb-48b2-4f83-b97c-323ccdba617f to disappear
Jan 14 15:27:23.879: INFO: Pod pod-configmaps-1f8f9ecb-48b2-4f83-b97c-323ccdba617f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:27:23.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3144" for this suite.

â€¢ [SLOW TEST:15.579 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":85,"skipped":1139,"failed":0}
SSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:27:24.280: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-982
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-99d0fd4e-0839-4dc0-bab2-76c3314aa628 in namespace container-probe-982
Jan 14 15:27:36.048: INFO: Started pod busybox-99d0fd4e-0839-4dc0-bab2-76c3314aa628 in namespace container-probe-982
STEP: checking the pod's current state and verifying that restartCount is present
Jan 14 15:27:36.280: INFO: Initial restart count of pod busybox-99d0fd4e-0839-4dc0-bab2-76c3314aa628 is 0
Jan 14 15:28:24.365: INFO: Restart count of pod container-probe-982/busybox-99d0fd4e-0839-4dc0-bab2-76c3314aa628 is now 1 (48.084983856s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:28:24.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-982" for this suite.

â€¢ [SLOW TEST:60.844 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":278,"completed":86,"skipped":1142,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:28:25.124: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3663
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 15:28:27.269: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 14 15:28:32.336: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 14 15:28:38.985: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jan 14 15:28:39.238: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-3663 /apis/apps/v1/namespaces/deployment-3663/deployments/test-cleanup-deployment c9627cef-e1bc-4ccb-8228-50ad32d3d4e9 106229 1 2020-01-14 15:28:39 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0025acbb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jan 14 15:28:39.356: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jan 14 15:28:39.356: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jan 14 15:28:39.356: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-3663 /apis/apps/v1/namespaces/deployment-3663/replicasets/test-cleanup-controller a35d2289-07db-4ab4-bcc1-29aaec3cbeb1 106230 1 2020-01-14 15:28:27 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment c9627cef-e1bc-4ccb-8228-50ad32d3d4e9 0xc0025ad3f7 0xc0025ad3f8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0025ad458 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 14 15:28:39.469: INFO: Pod "test-cleanup-controller-4krdd" is available:
&Pod{ObjectMeta:{test-cleanup-controller-4krdd test-cleanup-controller- deployment-3663 /api/v1/namespaces/deployment-3663/pods/test-cleanup-controller-4krdd 707fab0a-0a1b-4996-8d22-a1874fff6ab5 106219 0 2020-01-14 15:28:27 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller a35d2289-07db-4ab4-bcc1-29aaec3cbeb1 0xc001df88c7 0xc001df88c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-47c82,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-47c82,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-47c82,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:28:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:28:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:28:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 15:28:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.142,PodIP:10.151.49.165,StartTime:2020-01-14 15:28:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-14 15:28:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://87d71f0df6279297a9a39634c8a43524fa25e0925e4783395c082823f0734f62,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.151.49.165,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:28:39.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3663" for this suite.

â€¢ [SLOW TEST:14.627 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":278,"completed":87,"skipped":1151,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:28:39.754: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4217
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-178e0502-f35b-4e9b-9b24-41dcab1e83fc
STEP: Creating a pod to test consume secrets
Jan 14 15:28:42.587: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1c785d16-ead0-4335-8de8-41dddf468a90" in namespace "projected-4217" to be "success or failure"
Jan 14 15:28:42.876: INFO: Pod "pod-projected-secrets-1c785d16-ead0-4335-8de8-41dddf468a90": Phase="Pending", Reason="", readiness=false. Elapsed: 288.494414ms
Jan 14 15:28:44.915: INFO: Pod "pod-projected-secrets-1c785d16-ead0-4335-8de8-41dddf468a90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.327444854s
Jan 14 15:28:47.856: INFO: Pod "pod-projected-secrets-1c785d16-ead0-4335-8de8-41dddf468a90": Phase="Pending", Reason="", readiness=false. Elapsed: 5.268828124s
Jan 14 15:28:50.400: INFO: Pod "pod-projected-secrets-1c785d16-ead0-4335-8de8-41dddf468a90": Phase="Pending", Reason="", readiness=false. Elapsed: 7.81235954s
Jan 14 15:28:52.621: INFO: Pod "pod-projected-secrets-1c785d16-ead0-4335-8de8-41dddf468a90": Phase="Pending", Reason="", readiness=false. Elapsed: 10.034168548s
Jan 14 15:28:54.789: INFO: Pod "pod-projected-secrets-1c785d16-ead0-4335-8de8-41dddf468a90": Phase="Pending", Reason="", readiness=false. Elapsed: 12.202248314s
Jan 14 15:28:56.909: INFO: Pod "pod-projected-secrets-1c785d16-ead0-4335-8de8-41dddf468a90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.321434926s
STEP: Saw pod success
Jan 14 15:28:56.909: INFO: Pod "pod-projected-secrets-1c785d16-ead0-4335-8de8-41dddf468a90" satisfied condition "success or failure"
Jan 14 15:28:57.017: INFO: Trying to get logs from node slave3 pod pod-projected-secrets-1c785d16-ead0-4335-8de8-41dddf468a90 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 14 15:28:58.351: INFO: Waiting for pod pod-projected-secrets-1c785d16-ead0-4335-8de8-41dddf468a90 to disappear
Jan 14 15:28:58.355: INFO: Pod pod-projected-secrets-1c785d16-ead0-4335-8de8-41dddf468a90 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:28:58.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4217" for this suite.

â€¢ [SLOW TEST:19.570 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":88,"skipped":1154,"failed":0}
SSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:28:59.324: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-358
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:29:42.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-358" for this suite.

â€¢ [SLOW TEST:42.769 seconds]
[sig-apps] Job
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":278,"completed":89,"skipped":1157,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:29:42.093: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2741
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override all
Jan 14 15:29:43.978: INFO: Waiting up to 5m0s for pod "client-containers-c04d022e-fc6a-4627-adba-eabe6c1ef0d1" in namespace "containers-2741" to be "success or failure"
Jan 14 15:29:44.198: INFO: Pod "client-containers-c04d022e-fc6a-4627-adba-eabe6c1ef0d1": Phase="Pending", Reason="", readiness=false. Elapsed: 219.838765ms
Jan 14 15:29:46.203: INFO: Pod "client-containers-c04d022e-fc6a-4627-adba-eabe6c1ef0d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.224912599s
Jan 14 15:29:48.209: INFO: Pod "client-containers-c04d022e-fc6a-4627-adba-eabe6c1ef0d1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.230295763s
Jan 14 15:29:50.277: INFO: Pod "client-containers-c04d022e-fc6a-4627-adba-eabe6c1ef0d1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.298351862s
Jan 14 15:29:52.450: INFO: Pod "client-containers-c04d022e-fc6a-4627-adba-eabe6c1ef0d1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.472030858s
Jan 14 15:29:54.472: INFO: Pod "client-containers-c04d022e-fc6a-4627-adba-eabe6c1ef0d1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.49395591s
Jan 14 15:29:56.494: INFO: Pod "client-containers-c04d022e-fc6a-4627-adba-eabe6c1ef0d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.516048908s
STEP: Saw pod success
Jan 14 15:29:56.495: INFO: Pod "client-containers-c04d022e-fc6a-4627-adba-eabe6c1ef0d1" satisfied condition "success or failure"
Jan 14 15:29:56.552: INFO: Trying to get logs from node slave2 pod client-containers-c04d022e-fc6a-4627-adba-eabe6c1ef0d1 container test-container: <nil>
STEP: delete the pod
Jan 14 15:29:57.098: INFO: Waiting for pod client-containers-c04d022e-fc6a-4627-adba-eabe6c1ef0d1 to disappear
Jan 14 15:29:57.104: INFO: Pod client-containers-c04d022e-fc6a-4627-adba-eabe6c1ef0d1 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:29:57.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2741" for this suite.

â€¢ [SLOW TEST:15.722 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":278,"completed":90,"skipped":1169,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:29:57.816: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4624
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 14 15:30:00.341: INFO: Waiting up to 5m0s for pod "pod-c222fb4a-6947-4ed9-bdc0-9f2784693faf" in namespace "emptydir-4624" to be "success or failure"
Jan 14 15:30:00.346: INFO: Pod "pod-c222fb4a-6947-4ed9-bdc0-9f2784693faf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.086012ms
Jan 14 15:30:02.500: INFO: Pod "pod-c222fb4a-6947-4ed9-bdc0-9f2784693faf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.158911451s
Jan 14 15:30:04.886: INFO: Pod "pod-c222fb4a-6947-4ed9-bdc0-9f2784693faf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.544967795s
Jan 14 15:30:06.908: INFO: Pod "pod-c222fb4a-6947-4ed9-bdc0-9f2784693faf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.566203614s
Jan 14 15:30:08.965: INFO: Pod "pod-c222fb4a-6947-4ed9-bdc0-9f2784693faf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.62345089s
Jan 14 15:30:11.235: INFO: Pod "pod-c222fb4a-6947-4ed9-bdc0-9f2784693faf": Phase="Pending", Reason="", readiness=false. Elapsed: 10.893909865s
Jan 14 15:30:13.297: INFO: Pod "pod-c222fb4a-6947-4ed9-bdc0-9f2784693faf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.955566976s
STEP: Saw pod success
Jan 14 15:30:13.297: INFO: Pod "pod-c222fb4a-6947-4ed9-bdc0-9f2784693faf" satisfied condition "success or failure"
Jan 14 15:30:13.301: INFO: Trying to get logs from node slave2 pod pod-c222fb4a-6947-4ed9-bdc0-9f2784693faf container test-container: <nil>
STEP: delete the pod
Jan 14 15:30:13.678: INFO: Waiting for pod pod-c222fb4a-6947-4ed9-bdc0-9f2784693faf to disappear
Jan 14 15:30:13.685: INFO: Pod pod-c222fb4a-6947-4ed9-bdc0-9f2784693faf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:30:13.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4624" for this suite.

â€¢ [SLOW TEST:16.193 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":91,"skipped":1179,"failed":0}
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:30:14.009: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5809
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-5809
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Jan 14 15:30:16.425: INFO: Found 0 stateful pods, waiting for 3
Jan 14 15:30:26.491: INFO: Found 1 stateful pods, waiting for 3
Jan 14 15:30:36.437: INFO: Found 2 stateful pods, waiting for 3
Jan 14 15:30:46.753: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 14 15:30:46.753: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 14 15:30:46.753: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jan 14 15:30:56.735: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 14 15:30:56.735: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 14 15:30:56.735: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jan 14 15:30:57.452: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jan 14 15:31:08.353: INFO: Updating stateful set ss2
Jan 14 15:31:08.427: INFO: Waiting for Pod statefulset-5809/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan 14 15:31:18.437: INFO: Waiting for Pod statefulset-5809/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Jan 14 15:31:28.865: INFO: Found 1 stateful pods, waiting for 3
Jan 14 15:31:38.886: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 14 15:31:38.886: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 14 15:31:38.886: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jan 14 15:31:49.080: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 14 15:31:49.080: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 14 15:31:49.080: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jan 14 15:31:49.115: INFO: Updating stateful set ss2
Jan 14 15:31:49.503: INFO: Waiting for Pod statefulset-5809/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan 14 15:31:59.554: INFO: Waiting for Pod statefulset-5809/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan 14 15:32:09.745: INFO: Updating stateful set ss2
Jan 14 15:32:09.756: INFO: Waiting for StatefulSet statefulset-5809/ss2 to complete update
Jan 14 15:32:09.756: INFO: Waiting for Pod statefulset-5809/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan 14 15:32:19.840: INFO: Waiting for StatefulSet statefulset-5809/ss2 to complete update
Jan 14 15:32:19.840: INFO: Waiting for Pod statefulset-5809/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jan 14 15:32:29.783: INFO: Deleting all statefulset in ns statefulset-5809
Jan 14 15:32:29.786: INFO: Scaling statefulset ss2 to 0
Jan 14 15:33:10.163: INFO: Waiting for statefulset status.replicas updated to 0
Jan 14 15:33:10.166: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:33:10.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5809" for this suite.

â€¢ [SLOW TEST:176.496 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":278,"completed":92,"skipped":1180,"failed":0}
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:33:10.507: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3783
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jan 14 15:33:12.517: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:33:30.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3783" for this suite.

â€¢ [SLOW TEST:19.561 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":278,"completed":93,"skipped":1182,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:33:30.068: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-4915
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
STEP: reading a file in the container
Jan 14 15:33:46.148: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4915 pod-service-account-65968517-75da-4b60-b12f-4bfefee5bafc -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jan 14 15:33:53.360: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4915 pod-service-account-65968517-75da-4b60-b12f-4bfefee5bafc -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jan 14 15:33:54.239: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4915 pod-service-account-65968517-75da-4b60-b12f-4bfefee5bafc -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:33:55.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4915" for this suite.

â€¢ [SLOW TEST:25.310 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":278,"completed":94,"skipped":1195,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:33:55.378: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-1372
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4489
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2571
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:34:12.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1372" for this suite.
STEP: Destroying namespace "nsdeletetest-4489" for this suite.
Jan 14 15:34:12.554: INFO: Namespace nsdeletetest-4489 was already deleted
STEP: Destroying namespace "nsdeletetest-2571" for this suite.

â€¢ [SLOW TEST:17.185 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":278,"completed":95,"skipped":1223,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:34:12.564: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-6201
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:34:15.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-6201" for this suite.
â€¢{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":278,"completed":96,"skipped":1263,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:34:15.396: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1755
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Update Demo
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:329
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the initial replication controller
Jan 14 15:34:17.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 create -f - --namespace=kubectl-1755'
Jan 14 15:34:18.375: INFO: stderr: ""
Jan 14 15:34:18.375: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 14 15:34:18.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1755'
Jan 14 15:34:18.594: INFO: stderr: ""
Jan 14 15:34:18.594: INFO: stdout: ""
STEP: Replicas for name=update-demo: expected=2 actual=0
Jan 14 15:34:23.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1755'
Jan 14 15:34:23.738: INFO: stderr: ""
Jan 14 15:34:23.738: INFO: stdout: "update-demo-nautilus-jkpfk update-demo-nautilus-wfxdg "
Jan 14 15:34:23.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-nautilus-jkpfk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1755'
Jan 14 15:34:23.975: INFO: stderr: ""
Jan 14 15:34:23.975: INFO: stdout: ""
Jan 14 15:34:23.975: INFO: update-demo-nautilus-jkpfk is created but not running
Jan 14 15:34:28.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1755'
Jan 14 15:34:29.588: INFO: stderr: ""
Jan 14 15:34:29.588: INFO: stdout: "update-demo-nautilus-jkpfk update-demo-nautilus-wfxdg "
Jan 14 15:34:29.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-nautilus-jkpfk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1755'
Jan 14 15:34:30.242: INFO: stderr: ""
Jan 14 15:34:30.242: INFO: stdout: ""
Jan 14 15:34:30.242: INFO: update-demo-nautilus-jkpfk is created but not running
Jan 14 15:34:35.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1755'
Jan 14 15:34:35.614: INFO: stderr: ""
Jan 14 15:34:35.616: INFO: stdout: "update-demo-nautilus-jkpfk update-demo-nautilus-wfxdg "
Jan 14 15:34:35.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-nautilus-jkpfk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1755'
Jan 14 15:34:35.769: INFO: stderr: ""
Jan 14 15:34:35.774: INFO: stdout: "true"
Jan 14 15:34:35.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-nautilus-jkpfk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1755'
Jan 14 15:34:36.035: INFO: stderr: ""
Jan 14 15:34:36.035: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 14 15:34:36.035: INFO: validating pod update-demo-nautilus-jkpfk
Jan 14 15:34:36.092: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 14 15:34:36.092: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 14 15:34:36.092: INFO: update-demo-nautilus-jkpfk is verified up and running
Jan 14 15:34:36.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-nautilus-wfxdg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1755'
Jan 14 15:34:36.599: INFO: stderr: ""
Jan 14 15:34:36.600: INFO: stdout: "true"
Jan 14 15:34:36.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-nautilus-wfxdg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1755'
Jan 14 15:34:36.928: INFO: stderr: ""
Jan 14 15:34:36.928: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 14 15:34:36.928: INFO: validating pod update-demo-nautilus-wfxdg
Jan 14 15:34:36.934: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 14 15:34:36.934: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 14 15:34:36.934: INFO: update-demo-nautilus-wfxdg is verified up and running
STEP: rolling-update to new replication controller
Jan 14 15:34:36.939: INFO: scanned /root for discovery docs: <nil>
Jan 14 15:34:36.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-1755'
Jan 14 15:35:19.947: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jan 14 15:35:19.948: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 14 15:35:19.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1755'
Jan 14 15:35:20.455: INFO: stderr: ""
Jan 14 15:35:20.455: INFO: stdout: "update-demo-kitten-dxthh update-demo-kitten-v7892 "
Jan 14 15:35:20.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-kitten-dxthh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1755'
Jan 14 15:35:20.702: INFO: stderr: ""
Jan 14 15:35:20.702: INFO: stdout: "true"
Jan 14 15:35:20.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-kitten-dxthh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1755'
Jan 14 15:35:21.075: INFO: stderr: ""
Jan 14 15:35:21.075: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jan 14 15:35:21.075: INFO: validating pod update-demo-kitten-dxthh
Jan 14 15:35:21.141: INFO: got data: {
  "image": "kitten.jpg"
}

Jan 14 15:35:21.141: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jan 14 15:35:21.141: INFO: update-demo-kitten-dxthh is verified up and running
Jan 14 15:35:21.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-kitten-v7892 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1755'
Jan 14 15:35:21.490: INFO: stderr: ""
Jan 14 15:35:21.490: INFO: stdout: "true"
Jan 14 15:35:21.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-kitten-v7892 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1755'
Jan 14 15:35:21.693: INFO: stderr: ""
Jan 14 15:35:21.707: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jan 14 15:35:21.707: INFO: validating pod update-demo-kitten-v7892
Jan 14 15:35:21.830: INFO: got data: {
  "image": "kitten.jpg"
}

Jan 14 15:35:21.830: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jan 14 15:35:21.830: INFO: update-demo-kitten-v7892 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:35:21.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1755" for this suite.

â€¢ [SLOW TEST:66.639 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:327
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should do a rolling update of a replication controller  [Conformance]","total":278,"completed":97,"skipped":1270,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:35:22.036: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3353
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jan 14 15:35:23.379: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 14 15:35:23.908: INFO: Waiting for terminating namespaces to be deleted...
Jan 14 15:35:23.997: INFO: 
Logging pods the kubelet thinks is on node master1 before test
Jan 14 15:35:24.073: INFO: resource-reserver-master1 from kube-system started at 2020-01-14 11:40:33 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.073: INFO: 	Container sleep-forever ready: true, restart count 0
Jan 14 15:35:24.073: INFO: oss-provisioner-7bb5d4c769-vhrt6 from kube-system started at 2020-01-14 05:31:38 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.073: INFO: 	Container oss-provisioner ready: true, restart count 1
Jan 14 15:35:24.073: INFO: daemon-set-gg2wn from daemonsets-9852 started at 2020-01-14 13:36:55 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.073: INFO: 	Container app ready: true, restart count 0
Jan 14 15:35:24.073: INFO: kube-controller-manager-master1 from kube-system started at 2020-01-14 11:40:33 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.073: INFO: 	Container kube-controller-manager ready: true, restart count 3
Jan 14 15:35:24.073: INFO: kube-scheduler-master1 from kube-system started at 2020-01-14 11:40:33 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.073: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan 14 15:35:24.073: INFO: cinder-provisioner-xxxjh from kube-system started at 2020-01-14 05:31:24 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.073: INFO: 	Container cinder-provisioner ready: true, restart count 1
Jan 14 15:35:24.073: INFO: coredns-52ppc from kube-system started at 2020-01-14 12:58:20 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.073: INFO: 	Container coredns ready: true, restart count 0
Jan 14 15:35:24.073: INFO: calico-node-c275v from kube-system started at 2020-01-14 05:30:06 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.073: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 15:35:24.073: INFO: kube-state-metrics-8988b595-gq7dc from monitoring started at 2020-01-14 12:57:51 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.073: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 14 15:35:24.073: INFO: kube-apiserver-master1 from kube-system started at 2020-01-14 10:55:20 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.073: INFO: 	Container kube-apiserver ready: true, restart count 1
Jan 14 15:35:24.073: INFO: kube-proxy-master1 from kube-system started at 2020-01-14 11:40:33 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.073: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 15:35:24.073: INFO: nginx-proxy-master1 from kube-system started at 2020-01-14 11:40:33 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.073: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 14 15:35:24.073: INFO: 
Logging pods the kubelet thinks is on node master2 before test
Jan 14 15:35:24.099: INFO: cinder-provisioner-9fk6f from kube-system started at 2020-01-14 05:31:23 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.106: INFO: 	Container cinder-provisioner ready: true, restart count 2
Jan 14 15:35:24.106: INFO: kube-apiserver-master2 from kube-system started at 2020-01-14 08:26:26 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.106: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 14 15:35:24.106: INFO: resource-reserver-master2 from kube-system started at 2020-01-14 05:27:40 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.107: INFO: 	Container sleep-forever ready: true, restart count 0
Jan 14 15:35:24.107: INFO: nginx-proxy-master2 from kube-system started at 2020-01-14 05:27:39 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.107: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 14 15:35:24.107: INFO: coredns-cnrrm from kube-system started at 2020-01-14 12:58:20 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.107: INFO: 	Container coredns ready: true, restart count 0
Jan 14 15:35:24.107: INFO: calico-node-vhmj5 from kube-system started at 2020-01-14 05:30:04 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.107: INFO: 	Container calico-node ready: true, restart count 2
Jan 14 15:35:24.107: INFO: kube-scheduler-master2 from kube-system started at 2020-01-14 08:24:38 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.107: INFO: 	Container kube-scheduler ready: true, restart count 2
Jan 14 15:35:24.107: INFO: daemon-set-5jq8c from daemonsets-9852 started at 2020-01-14 13:36:55 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.107: INFO: 	Container app ready: true, restart count 0
Jan 14 15:35:24.107: INFO: cinder-snapshot-58545f46f8-d2m7m from kube-system started at 2020-01-14 05:31:52 +0000 UTC (2 container statuses recorded)
Jan 14 15:35:24.107: INFO: 	Container cinder-snapshot-controller ready: true, restart count 2
Jan 14 15:35:24.107: INFO: 	Container cinder-snapshot-provisioner ready: true, restart count 2
Jan 14 15:35:24.107: INFO: kube-controller-manager-master2 from kube-system started at 2020-01-14 08:23:38 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.107: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jan 14 15:35:24.107: INFO: kube-proxy-master2 from kube-system started at 2020-01-14 08:24:11 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.107: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 15:35:24.107: INFO: 
Logging pods the kubelet thinks is on node master3 before test
Jan 14 15:35:24.170: INFO: kube-scheduler-master3 from kube-system started at 2020-01-14 08:31:06 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.170: INFO: 	Container kube-scheduler ready: true, restart count 1
Jan 14 15:35:24.170: INFO: daemon-set-247jg from daemonsets-9852 started at 2020-01-14 13:36:56 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.170: INFO: 	Container app ready: true, restart count 0
Jan 14 15:35:24.170: INFO: kube-proxy-master3 from kube-system started at 2020-01-14 08:30:38 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.170: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 15:35:24.170: INFO: calico-node-p7t5k from kube-system started at 2020-01-14 05:30:03 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.170: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 15:35:24.170: INFO: localpv-provisioner-6c5467f94-q9cvw from kube-system started at 2020-01-14 05:31:32 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.170: INFO: 	Container localpv-provisioner ready: true, restart count 1
Jan 14 15:35:24.170: INFO: coredns-f24kh from kube-system started at 2020-01-14 12:58:20 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.170: INFO: 	Container coredns ready: true, restart count 0
Jan 14 15:35:24.170: INFO: nginx-proxy-master3 from kube-system started at 2020-01-14 05:27:40 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.170: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 14 15:35:24.170: INFO: kube-apiserver-master3 from kube-system started at 2020-01-14 08:29:42 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.170: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 14 15:35:24.170: INFO: kube-controller-manager-master3 from kube-system started at 2020-01-14 08:30:08 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.170: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jan 14 15:35:24.171: INFO: resource-reserver-master3 from kube-system started at 2020-01-14 08:31:44 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.171: INFO: 	Container sleep-forever ready: true, restart count 0
Jan 14 15:35:24.171: INFO: cinder-provisioner-wbhvj from kube-system started at 2020-01-14 05:31:22 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.171: INFO: 	Container cinder-provisioner ready: true, restart count 1
Jan 14 15:35:24.171: INFO: 
Logging pods the kubelet thinks is on node slave1 before test
Jan 14 15:35:24.187: INFO: kube-proxy-slave1 from kube-system started at 2020-01-14 08:17:36 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.187: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 15:35:24.187: INFO: calico-node-wzpws from kube-system started at 2020-01-14 05:30:06 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.187: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 15:35:24.187: INFO: calico-kube-controllers-6ff9f48ccd-phjqn from kube-system started at 2020-01-14 05:30:37 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.187: INFO: 	Container calico-kube-controllers ready: true, restart count 1
Jan 14 15:35:24.187: INFO: tiller-deploy-77d97f584c-l4s9r from kube-system started at 2020-01-14 05:32:44 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.187: INFO: 	Container tiller ready: true, restart count 1
Jan 14 15:35:24.187: INFO: daemon-set-d6b8h from daemonsets-9852 started at 2020-01-14 13:36:56 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.187: INFO: 	Container app ready: true, restart count 0
Jan 14 15:35:24.187: INFO: dns-test-b39ed90a-d7ff-4205-b2f7-ffe423eff36b from dns-1195 started at 2020-01-14 12:33:00 +0000 UTC (3 container statuses recorded)
Jan 14 15:35:24.187: INFO: 	Container jessie-querier ready: true, restart count 13
Jan 14 15:35:24.187: INFO: 	Container querier ready: true, restart count 15
Jan 14 15:35:24.187: INFO: 	Container webserver ready: true, restart count 0
Jan 14 15:35:24.187: INFO: 
Logging pods the kubelet thinks is on node slave2 before test
Jan 14 15:35:24.202: INFO: daemon-set-zww4m from daemonsets-9852 started at 2020-01-14 13:36:55 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.204: INFO: 	Container app ready: true, restart count 0
Jan 14 15:35:24.204: INFO: kube-proxy-slave2 from kube-system started at 2020-01-14 08:17:48 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.204: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 15:35:24.204: INFO: cirros-26408-6bb4b5b58b-kfs9n from default started at 2020-01-14 11:01:57 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.204: INFO: 	Container cirros-26408 ready: true, restart count 0
Jan 14 15:35:24.204: INFO: agnhost-deployment-54964f567d-498qd from default started at 2020-01-14 11:05:35 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.204: INFO: 	Container agnhost ready: true, restart count 0
Jan 14 15:35:24.204: INFO: annotationupdate8b560796-65d3-444d-b2a8-95487af080c1 from downward-api-9309 started at 2020-01-14 12:08:41 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.204: INFO: 	Container client-container ready: false, restart count 0
Jan 14 15:35:24.204: INFO: calico-node-gwd6p from kube-system started at 2020-01-14 05:30:03 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.204: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 15:35:24.204: INFO: update-demo-kitten-dxthh from kubectl-1755 started at 2020-01-14 15:34:39 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.204: INFO: 	Container update-demo ready: true, restart count 0
Jan 14 15:35:24.204: INFO: 
Logging pods the kubelet thinks is on node slave3 before test
Jan 14 15:35:24.251: INFO: calico-node-zprlr from kube-system started at 2020-01-14 05:30:06 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.252: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 15:35:24.253: INFO: kube-proxy-slave3 from kube-system started at 2020-01-14 08:14:15 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.253: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 15:35:24.253: INFO: sonobuoy from sonobuoy started at 2020-01-14 14:39:37 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.253: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 14 15:35:24.253: INFO: sonobuoy-e2e-job-5f91618b21714006 from sonobuoy started at 2020-01-14 14:39:48 +0000 UTC (2 container statuses recorded)
Jan 14 15:35:24.253: INFO: 	Container e2e ready: true, restart count 0
Jan 14 15:35:24.253: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 14 15:35:24.253: INFO: daemon-set-gn9px from daemonsets-9852 started at 2020-01-14 13:36:55 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.253: INFO: 	Container app ready: true, restart count 0
Jan 14 15:35:24.253: INFO: update-demo-kitten-v7892 from kubectl-1755 started at 2020-01-14 15:34:53 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.253: INFO: 	Container update-demo ready: true, restart count 0
Jan 14 15:35:24.253: INFO: 
Logging pods the kubelet thinks is on node slave4 before test
Jan 14 15:35:24.334: INFO: kube-proxy-slave4 from kube-system started at 2020-01-14 08:13:30 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.334: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 15:35:24.334: INFO: cirros-15453-7f8dd5d978-gjlvv from default started at 2020-01-14 11:03:42 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.334: INFO: 	Container cirros-15453 ready: true, restart count 0
Jan 14 15:35:24.334: INFO: calico-node-hc57z from kube-system started at 2020-01-14 05:30:06 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.334: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 15:35:24.334: INFO: cirros-19353-6f67878d75-p7mck from default started at 2020-01-14 11:01:24 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.334: INFO: 	Container cirros-19353 ready: true, restart count 0
Jan 14 15:35:24.334: INFO: dns-test-53c7521e-6d65-4cd3-b40e-b0e2058d5107 from dns-5986 started at 2020-01-14 12:27:30 +0000 UTC (3 container statuses recorded)
Jan 14 15:35:24.334: INFO: 	Container jessie-querier ready: true, restart count 13
Jan 14 15:35:24.334: INFO: 	Container querier ready: true, restart count 15
Jan 14 15:35:24.334: INFO: 	Container webserver ready: true, restart count 0
Jan 14 15:35:24.334: INFO: daemon-set-b8jkm from daemonsets-9852 started at 2020-01-14 13:36:54 +0000 UTC (1 container statuses recorded)
Jan 14 15:35:24.334: INFO: 	Container app ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: verifying the node has the label node master1
STEP: verifying the node has the label node master2
STEP: verifying the node has the label node master3
STEP: verifying the node has the label node slave1
STEP: verifying the node has the label node slave2
STEP: verifying the node has the label node slave3
STEP: verifying the node has the label node slave4
Jan 14 15:35:29.131: INFO: Pod daemon-set-247jg requesting resource cpu=0m on Node master3
Jan 14 15:35:29.134: INFO: Pod daemon-set-5jq8c requesting resource cpu=0m on Node master2
Jan 14 15:35:29.136: INFO: Pod daemon-set-b8jkm requesting resource cpu=0m on Node slave4
Jan 14 15:35:29.143: INFO: Pod daemon-set-d6b8h requesting resource cpu=0m on Node slave1
Jan 14 15:35:29.143: INFO: Pod daemon-set-gg2wn requesting resource cpu=0m on Node master1
Jan 14 15:35:29.143: INFO: Pod daemon-set-gn9px requesting resource cpu=0m on Node slave3
Jan 14 15:35:29.143: INFO: Pod daemon-set-zww4m requesting resource cpu=0m on Node slave2
Jan 14 15:35:29.143: INFO: Pod agnhost-deployment-54964f567d-498qd requesting resource cpu=0m on Node slave2
Jan 14 15:35:29.143: INFO: Pod cirros-15453-7f8dd5d978-gjlvv requesting resource cpu=0m on Node slave4
Jan 14 15:35:29.143: INFO: Pod cirros-19353-6f67878d75-p7mck requesting resource cpu=0m on Node slave4
Jan 14 15:35:29.143: INFO: Pod cirros-26408-6bb4b5b58b-kfs9n requesting resource cpu=0m on Node slave2
Jan 14 15:35:29.143: INFO: Pod dns-test-b39ed90a-d7ff-4205-b2f7-ffe423eff36b requesting resource cpu=0m on Node slave1
Jan 14 15:35:29.143: INFO: Pod dns-test-53c7521e-6d65-4cd3-b40e-b0e2058d5107 requesting resource cpu=0m on Node slave4
Jan 14 15:35:29.143: INFO: Pod calico-kube-controllers-6ff9f48ccd-phjqn requesting resource cpu=30m on Node slave1
Jan 14 15:35:29.143: INFO: Pod calico-node-c275v requesting resource cpu=150m on Node master1
Jan 14 15:35:29.143: INFO: Pod calico-node-gwd6p requesting resource cpu=150m on Node slave2
Jan 14 15:35:29.143: INFO: Pod calico-node-hc57z requesting resource cpu=150m on Node slave4
Jan 14 15:35:29.143: INFO: Pod calico-node-p7t5k requesting resource cpu=150m on Node master3
Jan 14 15:35:29.143: INFO: Pod calico-node-vhmj5 requesting resource cpu=150m on Node master2
Jan 14 15:35:29.143: INFO: Pod calico-node-wzpws requesting resource cpu=150m on Node slave1
Jan 14 15:35:29.143: INFO: Pod calico-node-zprlr requesting resource cpu=150m on Node slave3
Jan 14 15:35:29.143: INFO: Pod cinder-provisioner-9fk6f requesting resource cpu=0m on Node master2
Jan 14 15:35:29.143: INFO: Pod cinder-provisioner-wbhvj requesting resource cpu=0m on Node master3
Jan 14 15:35:29.143: INFO: Pod cinder-provisioner-xxxjh requesting resource cpu=0m on Node master1
Jan 14 15:35:29.143: INFO: Pod cinder-snapshot-58545f46f8-d2m7m requesting resource cpu=0m on Node master2
Jan 14 15:35:29.143: INFO: Pod coredns-52ppc requesting resource cpu=100m on Node master1
Jan 14 15:35:29.143: INFO: Pod coredns-cnrrm requesting resource cpu=100m on Node master2
Jan 14 15:35:29.143: INFO: Pod coredns-f24kh requesting resource cpu=100m on Node master3
Jan 14 15:35:29.143: INFO: Pod kube-apiserver-master1 requesting resource cpu=100m on Node master1
Jan 14 15:35:29.143: INFO: Pod kube-apiserver-master2 requesting resource cpu=100m on Node master2
Jan 14 15:35:29.143: INFO: Pod kube-apiserver-master3 requesting resource cpu=100m on Node master3
Jan 14 15:35:29.143: INFO: Pod kube-controller-manager-master1 requesting resource cpu=100m on Node master1
Jan 14 15:35:29.143: INFO: Pod kube-controller-manager-master2 requesting resource cpu=100m on Node master2
Jan 14 15:35:29.143: INFO: Pod kube-controller-manager-master3 requesting resource cpu=100m on Node master3
Jan 14 15:35:29.143: INFO: Pod kube-proxy-master1 requesting resource cpu=150m on Node master1
Jan 14 15:35:29.143: INFO: Pod kube-proxy-master2 requesting resource cpu=150m on Node master2
Jan 14 15:35:29.143: INFO: Pod kube-proxy-master3 requesting resource cpu=150m on Node master3
Jan 14 15:35:29.143: INFO: Pod kube-proxy-slave1 requesting resource cpu=150m on Node slave1
Jan 14 15:35:29.143: INFO: Pod kube-proxy-slave2 requesting resource cpu=150m on Node slave2
Jan 14 15:35:29.143: INFO: Pod kube-proxy-slave3 requesting resource cpu=150m on Node slave3
Jan 14 15:35:29.143: INFO: Pod kube-proxy-slave4 requesting resource cpu=150m on Node slave4
Jan 14 15:35:29.143: INFO: Pod kube-scheduler-master1 requesting resource cpu=80m on Node master1
Jan 14 15:35:29.143: INFO: Pod kube-scheduler-master2 requesting resource cpu=80m on Node master2
Jan 14 15:35:29.143: INFO: Pod kube-scheduler-master3 requesting resource cpu=80m on Node master3
Jan 14 15:35:29.143: INFO: Pod localpv-provisioner-6c5467f94-q9cvw requesting resource cpu=0m on Node master3
Jan 14 15:35:29.143: INFO: Pod nginx-proxy-master1 requesting resource cpu=25m on Node master1
Jan 14 15:35:29.143: INFO: Pod nginx-proxy-master2 requesting resource cpu=25m on Node master2
Jan 14 15:35:29.143: INFO: Pod nginx-proxy-master3 requesting resource cpu=25m on Node master3
Jan 14 15:35:29.143: INFO: Pod oss-provisioner-7bb5d4c769-vhrt6 requesting resource cpu=0m on Node master1
Jan 14 15:35:29.143: INFO: Pod resource-reserver-master1 requesting resource cpu=800m on Node master1
Jan 14 15:35:29.143: INFO: Pod resource-reserver-master2 requesting resource cpu=800m on Node master2
Jan 14 15:35:29.143: INFO: Pod resource-reserver-master3 requesting resource cpu=800m on Node master3
Jan 14 15:35:29.143: INFO: Pod tiller-deploy-77d97f584c-l4s9r requesting resource cpu=0m on Node slave1
Jan 14 15:35:29.143: INFO: Pod update-demo-kitten-dxthh requesting resource cpu=0m on Node slave2
Jan 14 15:35:29.143: INFO: Pod update-demo-kitten-v7892 requesting resource cpu=0m on Node slave3
Jan 14 15:35:29.143: INFO: Pod kube-state-metrics-8988b595-gq7dc requesting resource cpu=0m on Node master1
Jan 14 15:35:29.143: INFO: Pod sonobuoy requesting resource cpu=0m on Node slave3
Jan 14 15:35:29.143: INFO: Pod sonobuoy-e2e-job-5f91618b21714006 requesting resource cpu=0m on Node slave3
STEP: Starting Pods to consume most of the cluster CPU.
Jan 14 15:35:29.143: INFO: Creating a pod which consumes cpu=3846m on Node master2
Jan 14 15:35:29.614: INFO: Creating a pod which consumes cpu=3846m on Node master3
Jan 14 15:35:29.883: INFO: Creating a pod which consumes cpu=2149m on Node slave1
Jan 14 15:35:30.217: INFO: Creating a pod which consumes cpu=2170m on Node slave2
Jan 14 15:35:30.974: INFO: Creating a pod which consumes cpu=2170m on Node slave3
Jan 14 15:35:31.800: INFO: Creating a pod which consumes cpu=2170m on Node slave4
Jan 14 15:35:32.655: INFO: Creating a pod which consumes cpu=3846m on Node master1
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-00a396c8-cf9e-435e-890b-758137e7c8dc.15e9cacdcfb92d3d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3353/filler-pod-00a396c8-cf9e-435e-890b-758137e7c8dc to master2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-00a396c8-cf9e-435e-890b-758137e7c8dc.15e9cad0956b6a63], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-00a396c8-cf9e-435e-890b-758137e7c8dc.15e9cad17675c07d], Reason = [Created], Message = [Created container filler-pod-00a396c8-cf9e-435e-890b-758137e7c8dc]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-00a396c8-cf9e-435e-890b-758137e7c8dc.15e9cad1bdee7e47], Reason = [Started], Message = [Started container filler-pod-00a396c8-cf9e-435e-890b-758137e7c8dc]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-29b0fe70-0193-480f-b627-3beda146086b.15e9cace6c58360a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3353/filler-pod-29b0fe70-0193-480f-b627-3beda146086b to slave3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-29b0fe70-0193-480f-b627-3beda146086b.15e9cad1b4a05e67], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-29b0fe70-0193-480f-b627-3beda146086b.15e9cad21897ced2], Reason = [Created], Message = [Created container filler-pod-29b0fe70-0193-480f-b627-3beda146086b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-29b0fe70-0193-480f-b627-3beda146086b.15e9cad285757772], Reason = [Started], Message = [Started container filler-pod-29b0fe70-0193-480f-b627-3beda146086b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2a35cb46-b1dd-4648-863a-d49b895e84a4.15e9cacdef947236], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3353/filler-pod-2a35cb46-b1dd-4648-863a-d49b895e84a4 to master3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2a35cb46-b1dd-4648-863a-d49b895e84a4.15e9cad03a88db86], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2a35cb46-b1dd-4648-863a-d49b895e84a4.15e9cad0e8553908], Reason = [Created], Message = [Created container filler-pod-2a35cb46-b1dd-4648-863a-d49b895e84a4]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2a35cb46-b1dd-4648-863a-d49b895e84a4.15e9cad1149101ae], Reason = [Started], Message = [Started container filler-pod-2a35cb46-b1dd-4648-863a-d49b895e84a4]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b03dfb2-a9ce-46c0-ae44-b745a8d28a70.15e9cace12f3d0a1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3353/filler-pod-5b03dfb2-a9ce-46c0-ae44-b745a8d28a70 to slave1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b03dfb2-a9ce-46c0-ae44-b745a8d28a70.15e9cad15be1c394], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b03dfb2-a9ce-46c0-ae44-b745a8d28a70.15e9cad1adbb1e2f], Reason = [Created], Message = [Created container filler-pod-5b03dfb2-a9ce-46c0-ae44-b745a8d28a70]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b03dfb2-a9ce-46c0-ae44-b745a8d28a70.15e9cad1e661b7ed], Reason = [Started], Message = [Started container filler-pod-5b03dfb2-a9ce-46c0-ae44-b745a8d28a70]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-797893bc-ffdf-4b1e-9752-f78e690b7516.15e9caced0bc9f6c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3353/filler-pod-797893bc-ffdf-4b1e-9752-f78e690b7516 to master1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-797893bc-ffdf-4b1e-9752-f78e690b7516.15e9cad241747da7], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-797893bc-ffdf-4b1e-9752-f78e690b7516.15e9cad325f0a982], Reason = [Created], Message = [Created container filler-pod-797893bc-ffdf-4b1e-9752-f78e690b7516]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-797893bc-ffdf-4b1e-9752-f78e690b7516.15e9cad36f2772ae], Reason = [Started], Message = [Started container filler-pod-797893bc-ffdf-4b1e-9752-f78e690b7516]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93c36db2-19d4-43c4-a649-d9c716f546ed.15e9caceb9d5597e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3353/filler-pod-93c36db2-19d4-43c4-a649-d9c716f546ed to slave4]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93c36db2-19d4-43c4-a649-d9c716f546ed.15e9cad24becd697], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93c36db2-19d4-43c4-a649-d9c716f546ed.15e9cad2b1953629], Reason = [Created], Message = [Created container filler-pod-93c36db2-19d4-43c4-a649-d9c716f546ed]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93c36db2-19d4-43c4-a649-d9c716f546ed.15e9cad35859d623], Reason = [Started], Message = [Started container filler-pod-93c36db2-19d4-43c4-a649-d9c716f546ed]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7ab2786-c5a5-466a-b74c-33dc0fe1ebac.15e9cace1374e858], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3353/filler-pod-c7ab2786-c5a5-466a-b74c-33dc0fe1ebac to slave2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7ab2786-c5a5-466a-b74c-33dc0fe1ebac.15e9cad0637e2b0d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7ab2786-c5a5-466a-b74c-33dc0fe1ebac.15e9cad0b33c387b], Reason = [Created], Message = [Created container filler-pod-c7ab2786-c5a5-466a-b74c-33dc0fe1ebac]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7ab2786-c5a5-466a-b74c-33dc0fe1ebac.15e9cad0ff15c11b], Reason = [Started], Message = [Started container filler-pod-c7ab2786-c5a5-466a-b74c-33dc0fe1ebac]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15e9cad3d7d64891], Reason = [FailedScheduling], Message = [0/7 nodes are available: 7 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15e9cad3f69f7957], Reason = [FailedScheduling], Message = [0/7 nodes are available: 7 Insufficient cpu.]
STEP: removing the label node off the node master2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node master3
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node slave1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node slave2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node slave3
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node slave4
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node master1
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:35:59.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3353" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

â€¢ [SLOW TEST:37.506 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":278,"completed":98,"skipped":1271,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:35:59.542: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5974
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Jan 14 15:36:02.444: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:36:36.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5974" for this suite.

â€¢ [SLOW TEST:38.162 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":278,"completed":99,"skipped":1272,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:36:37.704: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3763
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3763.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3763.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3763.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3763.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3763.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3763.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3763.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3763.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3763.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 97.232.150.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.150.232.97_udp@PTR;check="$$(dig +tcp +noall +answer +search 97.232.150.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.150.232.97_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3763.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3763.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3763.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3763.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3763.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3763.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3763.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3763.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3763.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 97.232.150.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.150.232.97_udp@PTR;check="$$(dig +tcp +noall +answer +search 97.232.150.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.150.232.97_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 14 15:36:58.838: INFO: Unable to read wheezy_udp@dns-test-service.dns-3763.svc.cluster.local from pod dns-3763/dns-test-b84718ed-a9fc-4cb7-96ef-a2d49f4e05e9: the server could not find the requested resource (get pods dns-test-b84718ed-a9fc-4cb7-96ef-a2d49f4e05e9)
Jan 14 15:36:58.843: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3763.svc.cluster.local from pod dns-3763/dns-test-b84718ed-a9fc-4cb7-96ef-a2d49f4e05e9: the server could not find the requested resource (get pods dns-test-b84718ed-a9fc-4cb7-96ef-a2d49f4e05e9)
Jan 14 15:36:58.848: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3763.svc.cluster.local from pod dns-3763/dns-test-b84718ed-a9fc-4cb7-96ef-a2d49f4e05e9: the server could not find the requested resource (get pods dns-test-b84718ed-a9fc-4cb7-96ef-a2d49f4e05e9)
Jan 14 15:36:58.852: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3763.svc.cluster.local from pod dns-3763/dns-test-b84718ed-a9fc-4cb7-96ef-a2d49f4e05e9: the server could not find the requested resource (get pods dns-test-b84718ed-a9fc-4cb7-96ef-a2d49f4e05e9)
Jan 14 15:36:59.152: INFO: Unable to read jessie_udp@dns-test-service.dns-3763.svc.cluster.local from pod dns-3763/dns-test-b84718ed-a9fc-4cb7-96ef-a2d49f4e05e9: the server could not find the requested resource (get pods dns-test-b84718ed-a9fc-4cb7-96ef-a2d49f4e05e9)
Jan 14 15:36:59.156: INFO: Unable to read jessie_tcp@dns-test-service.dns-3763.svc.cluster.local from pod dns-3763/dns-test-b84718ed-a9fc-4cb7-96ef-a2d49f4e05e9: the server could not find the requested resource (get pods dns-test-b84718ed-a9fc-4cb7-96ef-a2d49f4e05e9)
Jan 14 15:36:59.171: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3763.svc.cluster.local from pod dns-3763/dns-test-b84718ed-a9fc-4cb7-96ef-a2d49f4e05e9: the server could not find the requested resource (get pods dns-test-b84718ed-a9fc-4cb7-96ef-a2d49f4e05e9)
Jan 14 15:36:59.305: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3763.svc.cluster.local from pod dns-3763/dns-test-b84718ed-a9fc-4cb7-96ef-a2d49f4e05e9: the server could not find the requested resource (get pods dns-test-b84718ed-a9fc-4cb7-96ef-a2d49f4e05e9)
Jan 14 15:36:59.335: INFO: Lookups using dns-3763/dns-test-b84718ed-a9fc-4cb7-96ef-a2d49f4e05e9 failed for: [wheezy_udp@dns-test-service.dns-3763.svc.cluster.local wheezy_tcp@dns-test-service.dns-3763.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3763.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3763.svc.cluster.local jessie_udp@dns-test-service.dns-3763.svc.cluster.local jessie_tcp@dns-test-service.dns-3763.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3763.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3763.svc.cluster.local]

Jan 14 15:37:04.957: INFO: DNS probes using dns-3763/dns-test-b84718ed-a9fc-4cb7-96ef-a2d49f4e05e9 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:37:07.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3763" for this suite.

â€¢ [SLOW TEST:30.120 seconds]
[sig-network] DNS
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":278,"completed":100,"skipped":1283,"failed":0}
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:37:07.824: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3391
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Jan 14 15:37:10.682: INFO: namespace kubectl-3391
Jan 14 15:37:10.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 create -f - --namespace=kubectl-3391'
Jan 14 15:37:12.532: INFO: stderr: ""
Jan 14 15:37:12.532: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Jan 14 15:37:13.827: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 15:37:13.827: INFO: Found 0 / 1
Jan 14 15:37:14.829: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 15:37:14.831: INFO: Found 0 / 1
Jan 14 15:37:15.792: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 15:37:15.793: INFO: Found 0 / 1
Jan 14 15:37:16.548: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 15:37:16.548: INFO: Found 0 / 1
Jan 14 15:37:17.908: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 15:37:17.908: INFO: Found 0 / 1
Jan 14 15:37:18.573: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 15:37:18.573: INFO: Found 0 / 1
Jan 14 15:37:19.579: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 15:37:19.579: INFO: Found 0 / 1
Jan 14 15:37:20.852: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 15:37:20.852: INFO: Found 0 / 1
Jan 14 15:37:21.681: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 15:37:21.681: INFO: Found 0 / 1
Jan 14 15:37:22.660: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 15:37:22.660: INFO: Found 0 / 1
Jan 14 15:37:24.314: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 15:37:24.314: INFO: Found 1 / 1
Jan 14 15:37:24.314: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 14 15:37:24.320: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 15:37:24.320: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 14 15:37:24.320: INFO: wait on agnhost-master startup in kubectl-3391 
Jan 14 15:37:24.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 logs agnhost-master-vc77q agnhost-master --namespace=kubectl-3391'
Jan 14 15:37:24.893: INFO: stderr: ""
Jan 14 15:37:24.893: INFO: stdout: "Paused\n"
STEP: exposing RC
Jan 14 15:37:24.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-3391'
Jan 14 15:37:25.518: INFO: stderr: ""
Jan 14 15:37:25.518: INFO: stdout: "service/rm2 exposed\n"
Jan 14 15:37:25.524: INFO: Service rm2 in namespace kubectl-3391 found.
STEP: exposing service
Jan 14 15:37:27.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-3391'
Jan 14 15:37:28.450: INFO: stderr: ""
Jan 14 15:37:28.450: INFO: stdout: "service/rm3 exposed\n"
Jan 14 15:37:28.456: INFO: Service rm3 in namespace kubectl-3391 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:37:30.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3391" for this suite.

â€¢ [SLOW TEST:23.010 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1275
    should create services for rc  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":278,"completed":101,"skipped":1283,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:37:30.835: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3076
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 14 15:37:32.748: INFO: Waiting up to 5m0s for pod "pod-0a2eff98-46f0-4450-8597-19bfa7848b4d" in namespace "emptydir-3076" to be "success or failure"
Jan 14 15:37:33.109: INFO: Pod "pod-0a2eff98-46f0-4450-8597-19bfa7848b4d": Phase="Pending", Reason="", readiness=false. Elapsed: 361.092998ms
Jan 14 15:37:35.133: INFO: Pod "pod-0a2eff98-46f0-4450-8597-19bfa7848b4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.384630791s
Jan 14 15:37:37.860: INFO: Pod "pod-0a2eff98-46f0-4450-8597-19bfa7848b4d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.112481739s
Jan 14 15:37:41.076: INFO: Pod "pod-0a2eff98-46f0-4450-8597-19bfa7848b4d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.327808789s
Jan 14 15:37:43.362: INFO: Pod "pod-0a2eff98-46f0-4450-8597-19bfa7848b4d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.613812907s
Jan 14 15:37:46.235: INFO: Pod "pod-0a2eff98-46f0-4450-8597-19bfa7848b4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.487456639s
STEP: Saw pod success
Jan 14 15:37:46.235: INFO: Pod "pod-0a2eff98-46f0-4450-8597-19bfa7848b4d" satisfied condition "success or failure"
Jan 14 15:37:46.793: INFO: Trying to get logs from node slave2 pod pod-0a2eff98-46f0-4450-8597-19bfa7848b4d container test-container: <nil>
STEP: delete the pod
Jan 14 15:37:49.525: INFO: Waiting for pod pod-0a2eff98-46f0-4450-8597-19bfa7848b4d to disappear
Jan 14 15:37:49.896: INFO: Pod pod-0a2eff98-46f0-4450-8597-19bfa7848b4d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:37:49.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3076" for this suite.

â€¢ [SLOW TEST:19.496 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":102,"skipped":1295,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:37:50.331: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1208
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 15:38:23.651: INFO: Container started at 2020-01-14 15:38:03 +0000 UTC, pod became ready at 2020-01-14 15:38:22 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:38:23.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1208" for this suite.

â€¢ [SLOW TEST:33.853 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":278,"completed":103,"skipped":1344,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:38:24.184: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3827
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jan 14 15:38:41.452: INFO: Successfully updated pod "annotationupdate68517ec3-62c6-4664-9ad6-6be4fd8b5824"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:38:43.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3827" for this suite.

â€¢ [SLOW TEST:19.524 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":278,"completed":104,"skipped":1350,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:38:43.711: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2408
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:39:07.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2408" for this suite.

â€¢ [SLOW TEST:23.746 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":278,"completed":105,"skipped":1357,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:39:07.457: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4084
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on tmpfs
Jan 14 15:39:11.041: INFO: Waiting up to 5m0s for pod "pod-285142da-e5db-48b5-a72c-d7c32ffc0715" in namespace "emptydir-4084" to be "success or failure"
Jan 14 15:39:11.048: INFO: Pod "pod-285142da-e5db-48b5-a72c-d7c32ffc0715": Phase="Pending", Reason="", readiness=false. Elapsed: 7.355578ms
Jan 14 15:39:13.193: INFO: Pod "pod-285142da-e5db-48b5-a72c-d7c32ffc0715": Phase="Pending", Reason="", readiness=false. Elapsed: 2.152455766s
Jan 14 15:39:15.269: INFO: Pod "pod-285142da-e5db-48b5-a72c-d7c32ffc0715": Phase="Pending", Reason="", readiness=false. Elapsed: 4.228174408s
Jan 14 15:39:18.852: INFO: Pod "pod-285142da-e5db-48b5-a72c-d7c32ffc0715": Phase="Pending", Reason="", readiness=false. Elapsed: 7.810912465s
Jan 14 15:39:21.397: INFO: Pod "pod-285142da-e5db-48b5-a72c-d7c32ffc0715": Phase="Pending", Reason="", readiness=false. Elapsed: 10.356099719s
Jan 14 15:39:23.430: INFO: Pod "pod-285142da-e5db-48b5-a72c-d7c32ffc0715": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.389176733s
STEP: Saw pod success
Jan 14 15:39:23.430: INFO: Pod "pod-285142da-e5db-48b5-a72c-d7c32ffc0715" satisfied condition "success or failure"
Jan 14 15:39:23.435: INFO: Trying to get logs from node slave2 pod pod-285142da-e5db-48b5-a72c-d7c32ffc0715 container test-container: <nil>
STEP: delete the pod
Jan 14 15:39:23.772: INFO: Waiting for pod pod-285142da-e5db-48b5-a72c-d7c32ffc0715 to disappear
Jan 14 15:39:23.853: INFO: Pod pod-285142da-e5db-48b5-a72c-d7c32ffc0715 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:39:23.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4084" for this suite.

â€¢ [SLOW TEST:17.294 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":106,"skipped":1370,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:39:24.752: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5828
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 14 15:39:27.595: INFO: Waiting up to 5m0s for pod "pod-6dda7de2-c324-4a37-9a52-2ad7718faa8e" in namespace "emptydir-5828" to be "success or failure"
Jan 14 15:39:27.794: INFO: Pod "pod-6dda7de2-c324-4a37-9a52-2ad7718faa8e": Phase="Pending", Reason="", readiness=false. Elapsed: 199.085775ms
Jan 14 15:39:29.801: INFO: Pod "pod-6dda7de2-c324-4a37-9a52-2ad7718faa8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.205752365s
Jan 14 15:39:31.820: INFO: Pod "pod-6dda7de2-c324-4a37-9a52-2ad7718faa8e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.224207431s
Jan 14 15:39:34.329: INFO: Pod "pod-6dda7de2-c324-4a37-9a52-2ad7718faa8e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.734035448s
Jan 14 15:39:36.620: INFO: Pod "pod-6dda7de2-c324-4a37-9a52-2ad7718faa8e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.024979667s
Jan 14 15:39:38.625: INFO: Pod "pod-6dda7de2-c324-4a37-9a52-2ad7718faa8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.029815289s
STEP: Saw pod success
Jan 14 15:39:38.625: INFO: Pod "pod-6dda7de2-c324-4a37-9a52-2ad7718faa8e" satisfied condition "success or failure"
Jan 14 15:39:38.629: INFO: Trying to get logs from node slave2 pod pod-6dda7de2-c324-4a37-9a52-2ad7718faa8e container test-container: <nil>
STEP: delete the pod
Jan 14 15:39:38.920: INFO: Waiting for pod pod-6dda7de2-c324-4a37-9a52-2ad7718faa8e to disappear
Jan 14 15:39:38.926: INFO: Pod pod-6dda7de2-c324-4a37-9a52-2ad7718faa8e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:39:38.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5828" for this suite.

â€¢ [SLOW TEST:14.447 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":107,"skipped":1376,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:39:39.199: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8548
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-4b5c68e8-c267-4706-960e-fbc346791709
STEP: Creating a pod to test consume configMaps
Jan 14 15:39:41.600: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-046c4663-8618-4060-848b-28f1c65903ec" in namespace "projected-8548" to be "success or failure"
Jan 14 15:39:41.608: INFO: Pod "pod-projected-configmaps-046c4663-8618-4060-848b-28f1c65903ec": Phase="Pending", Reason="", readiness=false. Elapsed: 7.406249ms
Jan 14 15:39:43.662: INFO: Pod "pod-projected-configmaps-046c4663-8618-4060-848b-28f1c65903ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061889559s
Jan 14 15:39:46.233: INFO: Pod "pod-projected-configmaps-046c4663-8618-4060-848b-28f1c65903ec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.632339837s
Jan 14 15:39:48.941: INFO: Pod "pod-projected-configmaps-046c4663-8618-4060-848b-28f1c65903ec": Phase="Pending", Reason="", readiness=false. Elapsed: 7.340206468s
Jan 14 15:39:50.979: INFO: Pod "pod-projected-configmaps-046c4663-8618-4060-848b-28f1c65903ec": Phase="Pending", Reason="", readiness=false. Elapsed: 9.378119668s
Jan 14 15:39:53.189: INFO: Pod "pod-projected-configmaps-046c4663-8618-4060-848b-28f1c65903ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.587970568s
STEP: Saw pod success
Jan 14 15:39:53.189: INFO: Pod "pod-projected-configmaps-046c4663-8618-4060-848b-28f1c65903ec" satisfied condition "success or failure"
Jan 14 15:39:53.193: INFO: Trying to get logs from node slave2 pod pod-projected-configmaps-046c4663-8618-4060-848b-28f1c65903ec container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 14 15:39:54.555: INFO: Waiting for pod pod-projected-configmaps-046c4663-8618-4060-848b-28f1c65903ec to disappear
Jan 14 15:39:54.560: INFO: Pod pod-projected-configmaps-046c4663-8618-4060-848b-28f1c65903ec no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:39:54.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8548" for this suite.

â€¢ [SLOW TEST:15.862 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":278,"completed":108,"skipped":1378,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:39:55.061: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5723
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-7c8dafc4-4d9e-4dbe-9b03-1f87f60c9000
STEP: Creating a pod to test consume configMaps
Jan 14 15:39:58.445: INFO: Waiting up to 5m0s for pod "pod-configmaps-b42b08b3-f1ad-45dc-abb7-adcba4abc2ab" in namespace "configmap-5723" to be "success or failure"
Jan 14 15:39:58.480: INFO: Pod "pod-configmaps-b42b08b3-f1ad-45dc-abb7-adcba4abc2ab": Phase="Pending", Reason="", readiness=false. Elapsed: 35.001487ms
Jan 14 15:40:00.492: INFO: Pod "pod-configmaps-b42b08b3-f1ad-45dc-abb7-adcba4abc2ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04677489s
Jan 14 15:40:02.746: INFO: Pod "pod-configmaps-b42b08b3-f1ad-45dc-abb7-adcba4abc2ab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.301084193s
Jan 14 15:40:05.217: INFO: Pod "pod-configmaps-b42b08b3-f1ad-45dc-abb7-adcba4abc2ab": Phase="Pending", Reason="", readiness=false. Elapsed: 6.772291312s
Jan 14 15:40:07.689: INFO: Pod "pod-configmaps-b42b08b3-f1ad-45dc-abb7-adcba4abc2ab": Phase="Pending", Reason="", readiness=false. Elapsed: 9.244231676s
Jan 14 15:40:09.947: INFO: Pod "pod-configmaps-b42b08b3-f1ad-45dc-abb7-adcba4abc2ab": Phase="Pending", Reason="", readiness=false. Elapsed: 11.502029769s
Jan 14 15:40:11.952: INFO: Pod "pod-configmaps-b42b08b3-f1ad-45dc-abb7-adcba4abc2ab": Phase="Pending", Reason="", readiness=false. Elapsed: 13.506744179s
Jan 14 15:40:14.346: INFO: Pod "pod-configmaps-b42b08b3-f1ad-45dc-abb7-adcba4abc2ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 15.900766347s
STEP: Saw pod success
Jan 14 15:40:14.346: INFO: Pod "pod-configmaps-b42b08b3-f1ad-45dc-abb7-adcba4abc2ab" satisfied condition "success or failure"
Jan 14 15:40:14.351: INFO: Trying to get logs from node slave2 pod pod-configmaps-b42b08b3-f1ad-45dc-abb7-adcba4abc2ab container configmap-volume-test: <nil>
STEP: delete the pod
Jan 14 15:40:15.320: INFO: Waiting for pod pod-configmaps-b42b08b3-f1ad-45dc-abb7-adcba4abc2ab to disappear
Jan 14 15:40:15.380: INFO: Pod pod-configmaps-b42b08b3-f1ad-45dc-abb7-adcba4abc2ab no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:40:15.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5723" for this suite.

â€¢ [SLOW TEST:20.915 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":278,"completed":109,"skipped":1380,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:40:15.976: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4689
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:40:32.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4689" for this suite.

â€¢ [SLOW TEST:16.641 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when scheduling a read only busybox container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":110,"skipped":1408,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:40:32.618: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4508
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-3caeb875-8373-4c47-b127-572b62586efc
STEP: Creating a pod to test consume secrets
Jan 14 15:40:37.968: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9981dbfc-b2be-4601-9105-3509dd90074b" in namespace "projected-4508" to be "success or failure"
Jan 14 15:40:37.981: INFO: Pod "pod-projected-secrets-9981dbfc-b2be-4601-9105-3509dd90074b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.83781ms
Jan 14 15:40:40.326: INFO: Pod "pod-projected-secrets-9981dbfc-b2be-4601-9105-3509dd90074b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.357665002s
Jan 14 15:40:42.573: INFO: Pod "pod-projected-secrets-9981dbfc-b2be-4601-9105-3509dd90074b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.60481269s
Jan 14 15:40:44.583: INFO: Pod "pod-projected-secrets-9981dbfc-b2be-4601-9105-3509dd90074b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.614837397s
Jan 14 15:40:47.115: INFO: Pod "pod-projected-secrets-9981dbfc-b2be-4601-9105-3509dd90074b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.147174421s
Jan 14 15:40:49.145: INFO: Pod "pod-projected-secrets-9981dbfc-b2be-4601-9105-3509dd90074b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.177235441s
STEP: Saw pod success
Jan 14 15:40:49.145: INFO: Pod "pod-projected-secrets-9981dbfc-b2be-4601-9105-3509dd90074b" satisfied condition "success or failure"
Jan 14 15:40:49.157: INFO: Trying to get logs from node slave2 pod pod-projected-secrets-9981dbfc-b2be-4601-9105-3509dd90074b container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 14 15:40:49.203: INFO: Waiting for pod pod-projected-secrets-9981dbfc-b2be-4601-9105-3509dd90074b to disappear
Jan 14 15:40:49.207: INFO: Pod pod-projected-secrets-9981dbfc-b2be-4601-9105-3509dd90074b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:40:49.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4508" for this suite.

â€¢ [SLOW TEST:16.873 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":278,"completed":111,"skipped":1434,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:40:49.491: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2657
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jan 14 15:40:51.204: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jan 14 15:41:18.070: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 15:41:27.863: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:41:53.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2657" for this suite.

â€¢ [SLOW TEST:63.932 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":278,"completed":112,"skipped":1434,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:41:53.424: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2165
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jan 14 15:41:55.565: INFO: Created pod &Pod{ObjectMeta:{dns-2165  dns-2165 /api/v1/namespaces/dns-2165/pods/dns-2165 f56bd2d9-52f1-4e9c-b0f6-8a735e131440 109867 0 2020-01-14 15:41:55 +0000 UTC <nil> <nil> map[] map[] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bvhlc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bvhlc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bvhlc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
STEP: Verifying customized DNS suffix list is configured on pod...
Jan 14 15:42:06.635: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2165 PodName:dns-2165 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 15:42:06.635: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Verifying customized DNS server is configured on pod...
Jan 14 15:42:07.116: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2165 PodName:dns-2165 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 15:42:07.116: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 15:42:07.768: INFO: Deleting pod dns-2165...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:42:08.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2165" for this suite.

â€¢ [SLOW TEST:16.750 seconds]
[sig-network] DNS
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":278,"completed":113,"skipped":1446,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:42:10.174: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7080
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Jan 14 15:42:44.103: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0114 15:42:44.103660      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:42:44.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7080" for this suite.

â€¢ [SLOW TEST:33.961 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":278,"completed":114,"skipped":1465,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:42:44.135: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4646
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 14 15:42:52.355: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 14 15:42:54.968: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613372, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613372, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613373, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613371, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 15:42:57.027: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613372, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613372, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613373, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613371, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 15:42:59.021: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613372, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613372, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613373, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613371, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 15:43:01.090: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613372, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613372, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613373, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613371, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 15:43:03.452: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613372, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613372, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613373, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613371, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 15:43:05.297: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613372, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613372, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613373, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613371, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 14 15:43:08.471: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:43:12.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4646" for this suite.
STEP: Destroying namespace "webhook-4646-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:29.795 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":278,"completed":115,"skipped":1471,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:43:13.931: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-484
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Jan 14 15:43:16.253: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:43:48.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-484" for this suite.

â€¢ [SLOW TEST:35.143 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":278,"completed":116,"skipped":1520,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:43:49.074: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-2135
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 15:43:51.256: INFO: Creating ReplicaSet my-hostname-basic-a8c62b4f-61b6-490a-aa51-0b49d83e7905
Jan 14 15:43:52.019: INFO: Pod name my-hostname-basic-a8c62b4f-61b6-490a-aa51-0b49d83e7905: Found 1 pods out of 1
Jan 14 15:43:52.019: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-a8c62b4f-61b6-490a-aa51-0b49d83e7905" is running
Jan 14 15:44:02.031: INFO: Pod "my-hostname-basic-a8c62b4f-61b6-490a-aa51-0b49d83e7905-8jnx2" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-14 15:43:51 +0000 UTC Reason: Message:}])
Jan 14 15:44:02.031: INFO: Trying to dial the pod
Jan 14 15:44:07.313: INFO: Controller my-hostname-basic-a8c62b4f-61b6-490a-aa51-0b49d83e7905: Got expected result from replica 1 [my-hostname-basic-a8c62b4f-61b6-490a-aa51-0b49d83e7905-8jnx2]: "my-hostname-basic-a8c62b4f-61b6-490a-aa51-0b49d83e7905-8jnx2", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:44:07.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2135" for this suite.

â€¢ [SLOW TEST:18.429 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":278,"completed":117,"skipped":1525,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:44:07.504: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4733
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jan 14 15:44:09.855: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:44:30.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4733" for this suite.

â€¢ [SLOW TEST:24.472 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":278,"completed":118,"skipped":1552,"failed":0}
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:44:31.977: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6093
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-6b6da98b-2050-44ee-9029-fd506f4a3fff
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:44:48.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6093" for this suite.

â€¢ [SLOW TEST:16.806 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":119,"skipped":1552,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:44:48.783: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5077
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 14 15:44:51.498: INFO: Waiting up to 5m0s for pod "pod-aefea371-dcc8-4376-9d5c-d291939019ae" in namespace "emptydir-5077" to be "success or failure"
Jan 14 15:44:51.742: INFO: Pod "pod-aefea371-dcc8-4376-9d5c-d291939019ae": Phase="Pending", Reason="", readiness=false. Elapsed: 243.763305ms
Jan 14 15:44:53.750: INFO: Pod "pod-aefea371-dcc8-4376-9d5c-d291939019ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.251651434s
Jan 14 15:44:56.176: INFO: Pod "pod-aefea371-dcc8-4376-9d5c-d291939019ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.678024707s
Jan 14 15:44:58.733: INFO: Pod "pod-aefea371-dcc8-4376-9d5c-d291939019ae": Phase="Pending", Reason="", readiness=false. Elapsed: 7.235138168s
Jan 14 15:45:00.873: INFO: Pod "pod-aefea371-dcc8-4376-9d5c-d291939019ae": Phase="Pending", Reason="", readiness=false. Elapsed: 9.375006078s
Jan 14 15:45:03.175: INFO: Pod "pod-aefea371-dcc8-4376-9d5c-d291939019ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.676190678s
STEP: Saw pod success
Jan 14 15:45:03.175: INFO: Pod "pod-aefea371-dcc8-4376-9d5c-d291939019ae" satisfied condition "success or failure"
Jan 14 15:45:03.182: INFO: Trying to get logs from node slave3 pod pod-aefea371-dcc8-4376-9d5c-d291939019ae container test-container: <nil>
STEP: delete the pod
Jan 14 15:45:04.957: INFO: Waiting for pod pod-aefea371-dcc8-4376-9d5c-d291939019ae to disappear
Jan 14 15:45:05.057: INFO: Pod pod-aefea371-dcc8-4376-9d5c-d291939019ae no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:45:05.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5077" for this suite.

â€¢ [SLOW TEST:17.223 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":120,"skipped":1564,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:45:06.009: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6819
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 14 15:45:12.003: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 14 15:45:14.467: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613512, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613512, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613513, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613511, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 15:45:16.799: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613512, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613512, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613513, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613511, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 15:45:18.497: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613512, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613512, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613513, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613511, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 15:45:20.477: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613512, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613512, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613513, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613511, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 15:45:22.823: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613512, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613512, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613513, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714613511, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 14 15:45:25.867: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:45:31.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6819" for this suite.
STEP: Destroying namespace "webhook-6819-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:29.835 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":278,"completed":121,"skipped":1665,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:45:35.845: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4173
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 14 15:45:37.503: INFO: Waiting up to 5m0s for pod "pod-833c3a23-b1bf-4aa3-93eb-3cee32698b98" in namespace "emptydir-4173" to be "success or failure"
Jan 14 15:45:37.813: INFO: Pod "pod-833c3a23-b1bf-4aa3-93eb-3cee32698b98": Phase="Pending", Reason="", readiness=false. Elapsed: 310.72858ms
Jan 14 15:45:39.944: INFO: Pod "pod-833c3a23-b1bf-4aa3-93eb-3cee32698b98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.441605757s
Jan 14 15:45:41.989: INFO: Pod "pod-833c3a23-b1bf-4aa3-93eb-3cee32698b98": Phase="Pending", Reason="", readiness=false. Elapsed: 4.486577262s
Jan 14 15:45:44.415: INFO: Pod "pod-833c3a23-b1bf-4aa3-93eb-3cee32698b98": Phase="Pending", Reason="", readiness=false. Elapsed: 6.912237027s
Jan 14 15:45:46.572: INFO: Pod "pod-833c3a23-b1bf-4aa3-93eb-3cee32698b98": Phase="Pending", Reason="", readiness=false. Elapsed: 9.068891494s
Jan 14 15:45:48.752: INFO: Pod "pod-833c3a23-b1bf-4aa3-93eb-3cee32698b98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.2491678s
STEP: Saw pod success
Jan 14 15:45:48.752: INFO: Pod "pod-833c3a23-b1bf-4aa3-93eb-3cee32698b98" satisfied condition "success or failure"
Jan 14 15:45:48.758: INFO: Trying to get logs from node slave2 pod pod-833c3a23-b1bf-4aa3-93eb-3cee32698b98 container test-container: <nil>
STEP: delete the pod
Jan 14 15:45:49.469: INFO: Waiting for pod pod-833c3a23-b1bf-4aa3-93eb-3cee32698b98 to disappear
Jan 14 15:45:49.474: INFO: Pod pod-833c3a23-b1bf-4aa3-93eb-3cee32698b98 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:45:49.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4173" for this suite.

â€¢ [SLOW TEST:13.763 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":122,"skipped":1675,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:45:49.609: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5603
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 14 15:45:51.653: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0254b944-47b5-42ed-8b3b-1772dd01392c" in namespace "downward-api-5603" to be "success or failure"
Jan 14 15:45:51.702: INFO: Pod "downwardapi-volume-0254b944-47b5-42ed-8b3b-1772dd01392c": Phase="Pending", Reason="", readiness=false. Elapsed: 49.048265ms
Jan 14 15:45:53.803: INFO: Pod "downwardapi-volume-0254b944-47b5-42ed-8b3b-1772dd01392c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.150555891s
Jan 14 15:45:56.044: INFO: Pod "downwardapi-volume-0254b944-47b5-42ed-8b3b-1772dd01392c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.391441388s
Jan 14 15:45:58.057: INFO: Pod "downwardapi-volume-0254b944-47b5-42ed-8b3b-1772dd01392c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.404512039s
Jan 14 15:46:00.142: INFO: Pod "downwardapi-volume-0254b944-47b5-42ed-8b3b-1772dd01392c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.489200804s
Jan 14 15:46:02.744: INFO: Pod "downwardapi-volume-0254b944-47b5-42ed-8b3b-1772dd01392c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.09145985s
Jan 14 15:46:05.348: INFO: Pod "downwardapi-volume-0254b944-47b5-42ed-8b3b-1772dd01392c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.695060575s
STEP: Saw pod success
Jan 14 15:46:05.348: INFO: Pod "downwardapi-volume-0254b944-47b5-42ed-8b3b-1772dd01392c" satisfied condition "success or failure"
Jan 14 15:46:05.549: INFO: Trying to get logs from node slave2 pod downwardapi-volume-0254b944-47b5-42ed-8b3b-1772dd01392c container client-container: <nil>
STEP: delete the pod
Jan 14 15:46:06.582: INFO: Waiting for pod downwardapi-volume-0254b944-47b5-42ed-8b3b-1772dd01392c to disappear
Jan 14 15:46:06.644: INFO: Pod downwardapi-volume-0254b944-47b5-42ed-8b3b-1772dd01392c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:46:06.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5603" for this suite.

â€¢ [SLOW TEST:17.581 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":278,"completed":123,"skipped":1687,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:46:07.194: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9484
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jan 14 15:46:09.974: INFO: Waiting up to 5m0s for pod "downward-api-c0cfff23-900c-4f88-a9a5-a8c639273b79" in namespace "downward-api-9484" to be "success or failure"
Jan 14 15:46:10.329: INFO: Pod "downward-api-c0cfff23-900c-4f88-a9a5-a8c639273b79": Phase="Pending", Reason="", readiness=false. Elapsed: 354.304913ms
Jan 14 15:46:12.388: INFO: Pod "downward-api-c0cfff23-900c-4f88-a9a5-a8c639273b79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.413560822s
Jan 14 15:46:14.412: INFO: Pod "downward-api-c0cfff23-900c-4f88-a9a5-a8c639273b79": Phase="Pending", Reason="", readiness=false. Elapsed: 4.437306487s
Jan 14 15:46:16.416: INFO: Pod "downward-api-c0cfff23-900c-4f88-a9a5-a8c639273b79": Phase="Pending", Reason="", readiness=false. Elapsed: 6.441894135s
Jan 14 15:46:18.421: INFO: Pod "downward-api-c0cfff23-900c-4f88-a9a5-a8c639273b79": Phase="Pending", Reason="", readiness=false. Elapsed: 8.446629768s
Jan 14 15:46:20.426: INFO: Pod "downward-api-c0cfff23-900c-4f88-a9a5-a8c639273b79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.451278552s
STEP: Saw pod success
Jan 14 15:46:20.426: INFO: Pod "downward-api-c0cfff23-900c-4f88-a9a5-a8c639273b79" satisfied condition "success or failure"
Jan 14 15:46:20.429: INFO: Trying to get logs from node slave2 pod downward-api-c0cfff23-900c-4f88-a9a5-a8c639273b79 container dapi-container: <nil>
STEP: delete the pod
Jan 14 15:46:21.029: INFO: Waiting for pod downward-api-c0cfff23-900c-4f88-a9a5-a8c639273b79 to disappear
Jan 14 15:46:21.035: INFO: Pod downward-api-c0cfff23-900c-4f88-a9a5-a8c639273b79 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:46:21.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9484" for this suite.

â€¢ [SLOW TEST:14.090 seconds]
[sig-node] Downward API
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:33
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":278,"completed":124,"skipped":1731,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:46:21.284: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9500
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:46
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Jan 14 15:46:33.162: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-321483740 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jan 14 15:46:53.777: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:46:53.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9500" for this suite.

â€¢ [SLOW TEST:32.633 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should be submitted and removed [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period should be submitted and removed [Conformance]","total":278,"completed":125,"skipped":1752,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:46:53.918: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2257
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 14 15:46:56.620: INFO: Waiting up to 5m0s for pod "downwardapi-volume-60d8b82b-0800-4854-bc17-0e40623ff14f" in namespace "projected-2257" to be "success or failure"
Jan 14 15:46:56.733: INFO: Pod "downwardapi-volume-60d8b82b-0800-4854-bc17-0e40623ff14f": Phase="Pending", Reason="", readiness=false. Elapsed: 112.929544ms
Jan 14 15:46:58.750: INFO: Pod "downwardapi-volume-60d8b82b-0800-4854-bc17-0e40623ff14f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.129779254s
Jan 14 15:47:00.973: INFO: Pod "downwardapi-volume-60d8b82b-0800-4854-bc17-0e40623ff14f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.353111433s
Jan 14 15:47:03.221: INFO: Pod "downwardapi-volume-60d8b82b-0800-4854-bc17-0e40623ff14f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.601492582s
Jan 14 15:47:05.532: INFO: Pod "downwardapi-volume-60d8b82b-0800-4854-bc17-0e40623ff14f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.912227157s
Jan 14 15:47:07.542: INFO: Pod "downwardapi-volume-60d8b82b-0800-4854-bc17-0e40623ff14f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.921797808s
Jan 14 15:47:09.550: INFO: Pod "downwardapi-volume-60d8b82b-0800-4854-bc17-0e40623ff14f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.930319601s
STEP: Saw pod success
Jan 14 15:47:09.550: INFO: Pod "downwardapi-volume-60d8b82b-0800-4854-bc17-0e40623ff14f" satisfied condition "success or failure"
Jan 14 15:47:09.559: INFO: Trying to get logs from node slave2 pod downwardapi-volume-60d8b82b-0800-4854-bc17-0e40623ff14f container client-container: <nil>
STEP: delete the pod
Jan 14 15:47:10.094: INFO: Waiting for pod downwardapi-volume-60d8b82b-0800-4854-bc17-0e40623ff14f to disappear
Jan 14 15:47:10.099: INFO: Pod downwardapi-volume-60d8b82b-0800-4854-bc17-0e40623ff14f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:47:10.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2257" for this suite.

â€¢ [SLOW TEST:16.426 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":278,"completed":126,"skipped":1781,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:47:10.347: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5938
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:47:12.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5938" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":278,"completed":127,"skipped":1801,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:47:13.246: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9848
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jan 14 15:47:57.558: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W0114 15:47:57.558002      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 14 15:47:57.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9848" for this suite.

â€¢ [SLOW TEST:44.418 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":278,"completed":128,"skipped":1810,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:47:57.665: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4937
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 14 15:47:59.150: INFO: Waiting up to 5m0s for pod "downwardapi-volume-76793834-527b-4de0-bc36-2e8d5f5128ab" in namespace "projected-4937" to be "success or failure"
Jan 14 15:47:59.233: INFO: Pod "downwardapi-volume-76793834-527b-4de0-bc36-2e8d5f5128ab": Phase="Pending", Reason="", readiness=false. Elapsed: 83.179156ms
Jan 14 15:48:01.239: INFO: Pod "downwardapi-volume-76793834-527b-4de0-bc36-2e8d5f5128ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0893164s
Jan 14 15:48:03.365: INFO: Pod "downwardapi-volume-76793834-527b-4de0-bc36-2e8d5f5128ab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.215157312s
Jan 14 15:48:06.189: INFO: Pod "downwardapi-volume-76793834-527b-4de0-bc36-2e8d5f5128ab": Phase="Pending", Reason="", readiness=false. Elapsed: 7.038795106s
Jan 14 15:48:08.445: INFO: Pod "downwardapi-volume-76793834-527b-4de0-bc36-2e8d5f5128ab": Phase="Pending", Reason="", readiness=false. Elapsed: 9.295561854s
Jan 14 15:48:10.543: INFO: Pod "downwardapi-volume-76793834-527b-4de0-bc36-2e8d5f5128ab": Phase="Pending", Reason="", readiness=false. Elapsed: 11.39347874s
Jan 14 15:48:13.194: INFO: Pod "downwardapi-volume-76793834-527b-4de0-bc36-2e8d5f5128ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.043910359s
STEP: Saw pod success
Jan 14 15:48:13.194: INFO: Pod "downwardapi-volume-76793834-527b-4de0-bc36-2e8d5f5128ab" satisfied condition "success or failure"
Jan 14 15:48:13.217: INFO: Trying to get logs from node slave2 pod downwardapi-volume-76793834-527b-4de0-bc36-2e8d5f5128ab container client-container: <nil>
STEP: delete the pod
Jan 14 15:48:16.435: INFO: Waiting for pod downwardapi-volume-76793834-527b-4de0-bc36-2e8d5f5128ab to disappear
Jan 14 15:48:16.744: INFO: Pod downwardapi-volume-76793834-527b-4de0-bc36-2e8d5f5128ab no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:48:16.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4937" for this suite.

â€¢ [SLOW TEST:19.783 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":278,"completed":129,"skipped":1828,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:48:17.467: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1141
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 15:48:19.307: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:48:32.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1141" for this suite.

â€¢ [SLOW TEST:15.296 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":278,"completed":130,"skipped":1887,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:48:32.764: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-638
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-00da4b7d-5e69-4036-bedb-875b512eff29 in namespace container-probe-638
Jan 14 15:48:46.759: INFO: Started pod liveness-00da4b7d-5e69-4036-bedb-875b512eff29 in namespace container-probe-638
STEP: checking the pod's current state and verifying that restartCount is present
Jan 14 15:48:47.247: INFO: Initial restart count of pod liveness-00da4b7d-5e69-4036-bedb-875b512eff29 is 0
Jan 14 15:49:09.331: INFO: Restart count of pod container-probe-638/liveness-00da4b7d-5e69-4036-bedb-875b512eff29 is now 1 (22.083839492s elapsed)
Jan 14 15:49:29.550: INFO: Restart count of pod container-probe-638/liveness-00da4b7d-5e69-4036-bedb-875b512eff29 is now 2 (42.302903468s elapsed)
Jan 14 15:49:47.744: INFO: Restart count of pod container-probe-638/liveness-00da4b7d-5e69-4036-bedb-875b512eff29 is now 3 (1m0.496911157s elapsed)
Jan 14 15:50:10.859: INFO: Restart count of pod container-probe-638/liveness-00da4b7d-5e69-4036-bedb-875b512eff29 is now 4 (1m23.611147893s elapsed)
Jan 14 15:51:19.844: INFO: Restart count of pod container-probe-638/liveness-00da4b7d-5e69-4036-bedb-875b512eff29 is now 5 (2m32.596464122s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:51:20.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-638" for this suite.

â€¢ [SLOW TEST:168.144 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":278,"completed":131,"skipped":1936,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:51:20.910: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3287
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with configMap that has name projected-configmap-test-upd-f99d4b35-e999-40ec-b05b-3699c10e3bb4
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-f99d4b35-e999-40ec-b05b-3699c10e3bb4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:51:38.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3287" for this suite.

â€¢ [SLOW TEST:18.104 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":132,"skipped":1941,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:51:39.022: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-545
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-projected-6bkn
STEP: Creating a pod to test atomic-volume-subpath
Jan 14 15:51:41.143: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-6bkn" in namespace "subpath-545" to be "success or failure"
Jan 14 15:51:41.183: INFO: Pod "pod-subpath-test-projected-6bkn": Phase="Pending", Reason="", readiness=false. Elapsed: 39.767103ms
Jan 14 15:51:43.198: INFO: Pod "pod-subpath-test-projected-6bkn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054625127s
Jan 14 15:51:45.237: INFO: Pod "pod-subpath-test-projected-6bkn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0934724s
Jan 14 15:51:47.251: INFO: Pod "pod-subpath-test-projected-6bkn": Phase="Pending", Reason="", readiness=false. Elapsed: 6.107840062s
Jan 14 15:51:49.316: INFO: Pod "pod-subpath-test-projected-6bkn": Phase="Pending", Reason="", readiness=false. Elapsed: 8.172650128s
Jan 14 15:51:51.608: INFO: Pod "pod-subpath-test-projected-6bkn": Phase="Running", Reason="", readiness=true. Elapsed: 10.464879864s
Jan 14 15:51:53.694: INFO: Pod "pod-subpath-test-projected-6bkn": Phase="Running", Reason="", readiness=true. Elapsed: 12.550951345s
Jan 14 15:51:55.701: INFO: Pod "pod-subpath-test-projected-6bkn": Phase="Running", Reason="", readiness=true. Elapsed: 14.557973355s
Jan 14 15:51:57.707: INFO: Pod "pod-subpath-test-projected-6bkn": Phase="Running", Reason="", readiness=true. Elapsed: 16.563490716s
Jan 14 15:52:00.009: INFO: Pod "pod-subpath-test-projected-6bkn": Phase="Running", Reason="", readiness=true. Elapsed: 18.865673077s
Jan 14 15:52:02.045: INFO: Pod "pod-subpath-test-projected-6bkn": Phase="Running", Reason="", readiness=true. Elapsed: 20.902135653s
Jan 14 15:52:04.324: INFO: Pod "pod-subpath-test-projected-6bkn": Phase="Running", Reason="", readiness=true. Elapsed: 23.180691879s
Jan 14 15:52:06.331: INFO: Pod "pod-subpath-test-projected-6bkn": Phase="Running", Reason="", readiness=true. Elapsed: 25.18737718s
Jan 14 15:52:08.336: INFO: Pod "pod-subpath-test-projected-6bkn": Phase="Running", Reason="", readiness=true. Elapsed: 27.193093595s
Jan 14 15:52:10.345: INFO: Pod "pod-subpath-test-projected-6bkn": Phase="Running", Reason="", readiness=true. Elapsed: 29.201816309s
Jan 14 15:52:12.350: INFO: Pod "pod-subpath-test-projected-6bkn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 31.207003199s
STEP: Saw pod success
Jan 14 15:52:12.350: INFO: Pod "pod-subpath-test-projected-6bkn" satisfied condition "success or failure"
Jan 14 15:52:12.359: INFO: Trying to get logs from node slave2 pod pod-subpath-test-projected-6bkn container test-container-subpath-projected-6bkn: <nil>
STEP: delete the pod
Jan 14 15:52:12.570: INFO: Waiting for pod pod-subpath-test-projected-6bkn to disappear
Jan 14 15:52:12.697: INFO: Pod pod-subpath-test-projected-6bkn no longer exists
STEP: Deleting pod pod-subpath-test-projected-6bkn
Jan 14 15:52:12.697: INFO: Deleting pod "pod-subpath-test-projected-6bkn" in namespace "subpath-545"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:52:12.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-545" for this suite.

â€¢ [SLOW TEST:34.182 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":278,"completed":133,"skipped":1963,"failed":0}
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:52:13.205: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-1485
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 15:52:15.191: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1485
I0114 15:52:15.349483      23 runners.go:189] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1485, replica count: 1
I0114 15:52:16.400083      23 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 15:52:17.400329      23 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 15:52:18.400599      23 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 15:52:19.400844      23 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 15:52:20.401058      23 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 15:52:21.401359      23 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 15:52:22.401968      23 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 15:52:23.402236      23 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 15:52:24.402579      23 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 14 15:52:25.401: INFO: Created: latency-svc-j9tps
Jan 14 15:52:25.492: INFO: Got endpoints: latency-svc-j9tps [989.526776ms]
Jan 14 15:52:25.949: INFO: Created: latency-svc-79vxk
Jan 14 15:52:26.246: INFO: Got endpoints: latency-svc-79vxk [753.281933ms]
Jan 14 15:52:26.546: INFO: Created: latency-svc-74hr6
Jan 14 15:52:27.112: INFO: Created: latency-svc-w9tlb
Jan 14 15:52:27.138: INFO: Got endpoints: latency-svc-74hr6 [1.645095113s]
Jan 14 15:52:27.557: INFO: Got endpoints: latency-svc-w9tlb [2.063809912s]
Jan 14 15:52:27.790: INFO: Created: latency-svc-zphc5
Jan 14 15:52:27.835: INFO: Got endpoints: latency-svc-zphc5 [2.342617306s]
Jan 14 15:52:28.554: INFO: Created: latency-svc-frt4c
Jan 14 15:52:29.062: INFO: Got endpoints: latency-svc-frt4c [3.568863371s]
Jan 14 15:52:30.367: INFO: Created: latency-svc-zh7zp
Jan 14 15:52:30.743: INFO: Got endpoints: latency-svc-zh7zp [5.249285659s]
Jan 14 15:52:30.744: INFO: Created: latency-svc-jpdnt
Jan 14 15:52:30.858: INFO: Got endpoints: latency-svc-jpdnt [5.363845741s]
Jan 14 15:52:31.224: INFO: Created: latency-svc-fvst8
Jan 14 15:52:31.446: INFO: Got endpoints: latency-svc-fvst8 [5.95196914s]
Jan 14 15:52:31.883: INFO: Created: latency-svc-cpqlk
Jan 14 15:52:31.946: INFO: Got endpoints: latency-svc-cpqlk [6.452293509s]
Jan 14 15:52:31.949: INFO: Created: latency-svc-hdx9z
Jan 14 15:52:32.133: INFO: Created: latency-svc-2b6vx
Jan 14 15:52:32.515: INFO: Got endpoints: latency-svc-hdx9z [7.020757602s]
Jan 14 15:52:32.547: INFO: Created: latency-svc-x87kv
Jan 14 15:52:33.253: INFO: Got endpoints: latency-svc-2b6vx [7.758634682s]
Jan 14 15:52:33.256: INFO: Got endpoints: latency-svc-x87kv [7.761541692s]
Jan 14 15:52:33.839: INFO: Created: latency-svc-qtjtb
Jan 14 15:52:34.070: INFO: Got endpoints: latency-svc-qtjtb [8.575802891s]
Jan 14 15:52:34.951: INFO: Created: latency-svc-nb2bt
Jan 14 15:52:35.488: INFO: Got endpoints: latency-svc-nb2bt [9.993444247s]
Jan 14 15:52:36.346: INFO: Created: latency-svc-r28px
Jan 14 15:52:36.660: INFO: Got endpoints: latency-svc-r28px [11.165363201s]
Jan 14 15:52:37.238: INFO: Created: latency-svc-27228
Jan 14 15:52:37.675: INFO: Created: latency-svc-bvjgs
Jan 14 15:52:37.772: INFO: Got endpoints: latency-svc-bvjgs [10.633484495s]
Jan 14 15:52:37.772: INFO: Got endpoints: latency-svc-27228 [11.520257318s]
Jan 14 15:52:38.848: INFO: Created: latency-svc-nlnmf
Jan 14 15:52:39.457: INFO: Got endpoints: latency-svc-nlnmf [11.900086676s]
Jan 14 15:52:39.731: INFO: Created: latency-svc-zj9d4
Jan 14 15:52:40.019: INFO: Got endpoints: latency-svc-zj9d4 [12.183878285s]
Jan 14 15:52:40.253: INFO: Created: latency-svc-c6w56
Jan 14 15:52:40.717: INFO: Got endpoints: latency-svc-c6w56 [11.65465062s]
Jan 14 15:52:40.727: INFO: Created: latency-svc-xv9cx
Jan 14 15:52:41.202: INFO: Got endpoints: latency-svc-xv9cx [10.459541941s]
Jan 14 15:52:41.243: INFO: Created: latency-svc-tb99h
Jan 14 15:52:41.579: INFO: Got endpoints: latency-svc-tb99h [10.720919968s]
Jan 14 15:52:41.727: INFO: Created: latency-svc-67vzn
Jan 14 15:52:41.812: INFO: Created: latency-svc-bbgk8
Jan 14 15:52:41.976: INFO: Got endpoints: latency-svc-67vzn [10.530568333s]
Jan 14 15:52:41.976: INFO: Got endpoints: latency-svc-bbgk8 [10.029351798s]
Jan 14 15:52:42.190: INFO: Created: latency-svc-jtkjk
Jan 14 15:52:42.452: INFO: Got endpoints: latency-svc-jtkjk [9.937255264s]
Jan 14 15:52:43.244: INFO: Created: latency-svc-rqr6c
Jan 14 15:52:43.536: INFO: Created: latency-svc-vqwsh
Jan 14 15:52:43.906: INFO: Got endpoints: latency-svc-rqr6c [10.652954414s]
Jan 14 15:52:43.908: INFO: Got endpoints: latency-svc-vqwsh [10.651729891s]
Jan 14 15:52:44.443: INFO: Created: latency-svc-msfwl
Jan 14 15:52:44.640: INFO: Got endpoints: latency-svc-msfwl [10.569164085s]
Jan 14 15:52:45.039: INFO: Created: latency-svc-7h8j6
Jan 14 15:52:45.243: INFO: Got endpoints: latency-svc-7h8j6 [9.754821915s]
Jan 14 15:52:45.969: INFO: Created: latency-svc-jvvff
Jan 14 15:52:46.112: INFO: Created: latency-svc-ghfgr
Jan 14 15:52:46.131: INFO: Created: latency-svc-pspc9
Jan 14 15:52:46.296: INFO: Got endpoints: latency-svc-ghfgr [9.635624856s]
Jan 14 15:52:46.297: INFO: Got endpoints: latency-svc-jvvff [8.525746607s]
Jan 14 15:52:46.594: INFO: Got endpoints: latency-svc-pspc9 [8.82162389s]
Jan 14 15:52:46.807: INFO: Created: latency-svc-5x4br
Jan 14 15:52:47.058: INFO: Got endpoints: latency-svc-5x4br [7.600803669s]
Jan 14 15:52:47.157: INFO: Created: latency-svc-t7wcr
Jan 14 15:52:47.660: INFO: Got endpoints: latency-svc-t7wcr [7.637884515s]
Jan 14 15:52:47.665: INFO: Created: latency-svc-s8cpl
Jan 14 15:52:48.293: INFO: Got endpoints: latency-svc-s8cpl [7.576035413s]
Jan 14 15:52:48.294: INFO: Created: latency-svc-shxrz
Jan 14 15:52:49.361: INFO: Got endpoints: latency-svc-shxrz [8.158062461s]
Jan 14 15:52:49.362: INFO: Created: latency-svc-fx5w2
Jan 14 15:52:49.643: INFO: Got endpoints: latency-svc-fx5w2 [8.064595895s]
Jan 14 15:52:49.855: INFO: Created: latency-svc-75mms
Jan 14 15:52:50.063: INFO: Got endpoints: latency-svc-75mms [8.086569555s]
Jan 14 15:52:50.543: INFO: Created: latency-svc-qtr78
Jan 14 15:52:50.818: INFO: Got endpoints: latency-svc-qtr78 [8.827912133s]
Jan 14 15:52:51.378: INFO: Created: latency-svc-pl8vn
Jan 14 15:52:51.640: INFO: Got endpoints: latency-svc-pl8vn [9.18779008s]
Jan 14 15:52:52.399: INFO: Created: latency-svc-nqc4w
Jan 14 15:52:52.629: INFO: Got endpoints: latency-svc-nqc4w [8.722509861s]
Jan 14 15:52:52.950: INFO: Created: latency-svc-9szq8
Jan 14 15:52:53.382: INFO: Got endpoints: latency-svc-9szq8 [9.473831059s]
Jan 14 15:52:53.391: INFO: Created: latency-svc-hkbw7
Jan 14 15:52:53.958: INFO: Got endpoints: latency-svc-hkbw7 [9.317890599s]
Jan 14 15:52:54.086: INFO: Created: latency-svc-r2zth
Jan 14 15:52:54.090: INFO: Got endpoints: latency-svc-r2zth [8.847033499s]
Jan 14 15:52:54.532: INFO: Created: latency-svc-nflkp
Jan 14 15:52:54.872: INFO: Got endpoints: latency-svc-nflkp [8.574838499s]
Jan 14 15:52:55.006: INFO: Created: latency-svc-d6nsv
Jan 14 15:52:55.356: INFO: Got endpoints: latency-svc-d6nsv [9.059655623s]
Jan 14 15:52:55.578: INFO: Created: latency-svc-h6wg6
Jan 14 15:52:55.959: INFO: Got endpoints: latency-svc-h6wg6 [9.365282857s]
Jan 14 15:52:56.064: INFO: Created: latency-svc-xpzbl
Jan 14 15:52:56.193: INFO: Got endpoints: latency-svc-xpzbl [9.134870579s]
Jan 14 15:52:56.643: INFO: Created: latency-svc-prbgv
Jan 14 15:52:56.927: INFO: Got endpoints: latency-svc-prbgv [9.26694098s]
Jan 14 15:52:57.512: INFO: Created: latency-svc-cqbfk
Jan 14 15:52:57.537: INFO: Got endpoints: latency-svc-cqbfk [9.244212511s]
Jan 14 15:52:59.143: INFO: Created: latency-svc-c7cpc
Jan 14 15:52:59.283: INFO: Got endpoints: latency-svc-c7cpc [9.921870641s]
Jan 14 15:52:59.650: INFO: Created: latency-svc-t9fvj
Jan 14 15:52:59.678: INFO: Created: latency-svc-tftrd
Jan 14 15:53:00.029: INFO: Got endpoints: latency-svc-t9fvj [10.386009961s]
Jan 14 15:53:00.406: INFO: Got endpoints: latency-svc-tftrd [10.342781631s]
Jan 14 15:53:00.407: INFO: Created: latency-svc-4r6nv
Jan 14 15:53:00.898: INFO: Got endpoints: latency-svc-4r6nv [10.068219714s]
Jan 14 15:53:00.904: INFO: Created: latency-svc-hmlst
Jan 14 15:53:01.196: INFO: Got endpoints: latency-svc-hmlst [9.555907144s]
Jan 14 15:53:01.481: INFO: Created: latency-svc-t87qr
Jan 14 15:53:01.910: INFO: Got endpoints: latency-svc-t87qr [9.280803661s]
Jan 14 15:53:02.406: INFO: Created: latency-svc-z7h77
Jan 14 15:53:02.839: INFO: Created: latency-svc-tfnnb
Jan 14 15:53:03.154: INFO: Got endpoints: latency-svc-tfnnb [9.195312696s]
Jan 14 15:53:03.156: INFO: Got endpoints: latency-svc-z7h77 [9.77450551s]
Jan 14 15:53:03.447: INFO: Created: latency-svc-vw5rz
Jan 14 15:53:03.725: INFO: Got endpoints: latency-svc-vw5rz [9.634072997s]
Jan 14 15:53:04.259: INFO: Created: latency-svc-t9c75
Jan 14 15:53:04.718: INFO: Got endpoints: latency-svc-t9c75 [9.845473013s]
Jan 14 15:53:05.295: INFO: Created: latency-svc-sc2j5
Jan 14 15:53:05.498: INFO: Got endpoints: latency-svc-sc2j5 [10.142381991s]
Jan 14 15:53:06.781: INFO: Created: latency-svc-s8kmb
Jan 14 15:53:07.103: INFO: Got endpoints: latency-svc-s8kmb [11.143813362s]
Jan 14 15:53:07.765: INFO: Created: latency-svc-jx7hh
Jan 14 15:53:08.099: INFO: Got endpoints: latency-svc-jx7hh [11.905445096s]
Jan 14 15:53:08.743: INFO: Created: latency-svc-5vm4k
Jan 14 15:53:08.893: INFO: Created: latency-svc-js5gc
Jan 14 15:53:09.208: INFO: Got endpoints: latency-svc-5vm4k [12.281162823s]
Jan 14 15:53:09.303: INFO: Got endpoints: latency-svc-js5gc [11.765076264s]
Jan 14 15:53:10.144: INFO: Created: latency-svc-mfl8q
Jan 14 15:53:10.467: INFO: Got endpoints: latency-svc-mfl8q [11.18404248s]
Jan 14 15:53:10.558: INFO: Created: latency-svc-vdvpj
Jan 14 15:53:11.077: INFO: Got endpoints: latency-svc-vdvpj [11.047695113s]
Jan 14 15:53:11.881: INFO: Created: latency-svc-5zs2q
Jan 14 15:53:11.888: INFO: Got endpoints: latency-svc-5zs2q [11.48217875s]
Jan 14 15:53:12.419: INFO: Created: latency-svc-xjwjn
Jan 14 15:53:12.888: INFO: Got endpoints: latency-svc-xjwjn [11.990316532s]
Jan 14 15:53:13.330: INFO: Created: latency-svc-2nk25
Jan 14 15:53:13.386: INFO: Got endpoints: latency-svc-2nk25 [12.190123799s]
Jan 14 15:53:13.609: INFO: Created: latency-svc-nsvv5
Jan 14 15:53:13.978: INFO: Got endpoints: latency-svc-nsvv5 [12.067536864s]
Jan 14 15:53:13.982: INFO: Created: latency-svc-lp7f4
Jan 14 15:53:13.999: INFO: Got endpoints: latency-svc-lp7f4 [10.845167909s]
Jan 14 15:53:15.249: INFO: Created: latency-svc-55hwh
Jan 14 15:53:15.551: INFO: Created: latency-svc-2td4w
Jan 14 15:53:15.670: INFO: Got endpoints: latency-svc-55hwh [12.514103764s]
Jan 14 15:53:16.172: INFO: Got endpoints: latency-svc-2td4w [12.446809258s]
Jan 14 15:53:16.755: INFO: Created: latency-svc-gwzpw
Jan 14 15:53:16.755: INFO: Created: latency-svc-62q76
Jan 14 15:53:16.768: INFO: Created: latency-svc-4xks4
Jan 14 15:53:17.193: INFO: Got endpoints: latency-svc-62q76 [12.474244525s]
Jan 14 15:53:17.323: INFO: Got endpoints: latency-svc-gwzpw [11.824757437s]
Jan 14 15:53:17.324: INFO: Created: latency-svc-vl8nl
Jan 14 15:53:17.325: INFO: Got endpoints: latency-svc-4xks4 [10.221412342s]
Jan 14 15:53:17.403: INFO: Got endpoints: latency-svc-vl8nl [9.304876546s]
Jan 14 15:53:17.419: INFO: Created: latency-svc-l48ff
Jan 14 15:53:17.811: INFO: Got endpoints: latency-svc-l48ff [8.603004976s]
Jan 14 15:53:17.812: INFO: Created: latency-svc-nkkkx
Jan 14 15:53:17.831: INFO: Got endpoints: latency-svc-nkkkx [8.528709863s]
Jan 14 15:53:18.453: INFO: Created: latency-svc-znlx5
Jan 14 15:53:18.883: INFO: Got endpoints: latency-svc-znlx5 [8.416221542s]
Jan 14 15:53:19.181: INFO: Created: latency-svc-9rwqm
Jan 14 15:53:19.181: INFO: Got endpoints: latency-svc-9rwqm [8.103388878s]
Jan 14 15:53:20.060: INFO: Created: latency-svc-ctlmv
Jan 14 15:53:20.456: INFO: Got endpoints: latency-svc-ctlmv [8.567902041s]
Jan 14 15:53:20.464: INFO: Created: latency-svc-ls9rx
Jan 14 15:53:20.865: INFO: Created: latency-svc-qbrgs
Jan 14 15:53:20.865: INFO: Got endpoints: latency-svc-ls9rx [7.97699998s]
Jan 14 15:53:21.043: INFO: Got endpoints: latency-svc-qbrgs [7.656293563s]
Jan 14 15:53:21.290: INFO: Created: latency-svc-72j4m
Jan 14 15:53:21.293: INFO: Got endpoints: latency-svc-72j4m [7.315867796s]
Jan 14 15:53:21.600: INFO: Created: latency-svc-k67jx
Jan 14 15:53:21.876: INFO: Got endpoints: latency-svc-k67jx [7.876825562s]
Jan 14 15:53:22.196: INFO: Created: latency-svc-gkmhd
Jan 14 15:53:22.529: INFO: Got endpoints: latency-svc-gkmhd [6.858469906s]
Jan 14 15:53:22.540: INFO: Created: latency-svc-b2jc5
Jan 14 15:53:23.432: INFO: Created: latency-svc-hzd8c
Jan 14 15:53:23.433: INFO: Got endpoints: latency-svc-b2jc5 [7.261218848s]
Jan 14 15:53:23.986: INFO: Created: latency-svc-79hcz
Jan 14 15:53:23.987: INFO: Got endpoints: latency-svc-hzd8c [6.794782049s]
Jan 14 15:53:24.330: INFO: Got endpoints: latency-svc-79hcz [7.006240432s]
Jan 14 15:53:24.701: INFO: Created: latency-svc-854fm
Jan 14 15:53:24.806: INFO: Created: latency-svc-ph5q9
Jan 14 15:53:25.136: INFO: Got endpoints: latency-svc-ph5q9 [7.732033589s]
Jan 14 15:53:25.136: INFO: Got endpoints: latency-svc-854fm [7.811165518s]
Jan 14 15:53:25.464: INFO: Created: latency-svc-xzdp8
Jan 14 15:53:25.744: INFO: Created: latency-svc-4h4sv
Jan 14 15:53:26.018: INFO: Got endpoints: latency-svc-xzdp8 [8.206941672s]
Jan 14 15:53:26.391: INFO: Got endpoints: latency-svc-4h4sv [8.559670885s]
Jan 14 15:53:26.461: INFO: Created: latency-svc-rnjgm
Jan 14 15:53:26.485: INFO: Got endpoints: latency-svc-rnjgm [7.601022696s]
Jan 14 15:53:27.702: INFO: Created: latency-svc-crnfm
Jan 14 15:53:27.900: INFO: Got endpoints: latency-svc-crnfm [8.718843497s]
Jan 14 15:53:28.481: INFO: Created: latency-svc-x2hsr
Jan 14 15:53:28.679: INFO: Got endpoints: latency-svc-x2hsr [8.223364315s]
Jan 14 15:53:29.239: INFO: Created: latency-svc-qjxs4
Jan 14 15:53:29.506: INFO: Got endpoints: latency-svc-qjxs4 [8.639935065s]
Jan 14 15:53:30.250: INFO: Created: latency-svc-brj66
Jan 14 15:53:30.479: INFO: Created: latency-svc-4cvqx
Jan 14 15:53:30.907: INFO: Got endpoints: latency-svc-brj66 [9.864126031s]
Jan 14 15:53:30.951: INFO: Got endpoints: latency-svc-4cvqx [9.657304743s]
Jan 14 15:53:31.065: INFO: Created: latency-svc-tdbpl
Jan 14 15:53:31.445: INFO: Got endpoints: latency-svc-tdbpl [9.568472681s]
Jan 14 15:53:31.465: INFO: Created: latency-svc-zsxck
Jan 14 15:53:31.536: INFO: Got endpoints: latency-svc-zsxck [9.007060519s]
Jan 14 15:53:32.883: INFO: Created: latency-svc-m48jp
Jan 14 15:53:33.244: INFO: Got endpoints: latency-svc-m48jp [9.810291732s]
Jan 14 15:53:33.943: INFO: Created: latency-svc-p6g2z
Jan 14 15:53:33.971: INFO: Got endpoints: latency-svc-p6g2z [9.983283934s]
Jan 14 15:53:34.160: INFO: Created: latency-svc-r6wpg
Jan 14 15:53:34.515: INFO: Got endpoints: latency-svc-r6wpg [10.185344882s]
Jan 14 15:53:34.960: INFO: Created: latency-svc-28vqh
Jan 14 15:53:35.376: INFO: Got endpoints: latency-svc-28vqh [10.240214062s]
Jan 14 15:53:35.944: INFO: Created: latency-svc-mdpb4
Jan 14 15:53:36.241: INFO: Created: latency-svc-x98wv
Jan 14 15:53:36.691: INFO: Got endpoints: latency-svc-x98wv [10.671954991s]
Jan 14 15:53:36.691: INFO: Got endpoints: latency-svc-mdpb4 [11.554486069s]
Jan 14 15:53:36.994: INFO: Created: latency-svc-42j4k
Jan 14 15:53:37.317: INFO: Got endpoints: latency-svc-42j4k [10.925560116s]
Jan 14 15:53:37.931: INFO: Created: latency-svc-n6b78
Jan 14 15:53:38.147: INFO: Got endpoints: latency-svc-n6b78 [11.661555314s]
Jan 14 15:53:38.999: INFO: Created: latency-svc-2hwdv
Jan 14 15:53:39.790: INFO: Got endpoints: latency-svc-2hwdv [11.889589203s]
Jan 14 15:53:40.420: INFO: Created: latency-svc-mv4mz
Jan 14 15:53:40.723: INFO: Got endpoints: latency-svc-mv4mz [12.042778084s]
Jan 14 15:53:40.956: INFO: Created: latency-svc-jlsr4
Jan 14 15:53:40.988: INFO: Got endpoints: latency-svc-jlsr4 [11.482026176s]
Jan 14 15:53:41.165: INFO: Created: latency-svc-9hw9l
Jan 14 15:53:41.378: INFO: Got endpoints: latency-svc-9hw9l [10.470469965s]
Jan 14 15:53:41.594: INFO: Created: latency-svc-brx4q
Jan 14 15:53:41.683: INFO: Got endpoints: latency-svc-brx4q [10.732387677s]
Jan 14 15:53:42.611: INFO: Created: latency-svc-59r9w
Jan 14 15:53:43.062: INFO: Got endpoints: latency-svc-59r9w [11.616555897s]
Jan 14 15:53:43.347: INFO: Created: latency-svc-khmkg
Jan 14 15:53:43.726: INFO: Got endpoints: latency-svc-khmkg [12.189640382s]
Jan 14 15:53:44.748: INFO: Created: latency-svc-w9r2m
Jan 14 15:53:45.015: INFO: Created: latency-svc-76n8j
Jan 14 15:53:45.105: INFO: Got endpoints: latency-svc-w9r2m [11.85390096s]
Jan 14 15:53:45.341: INFO: Got endpoints: latency-svc-76n8j [11.370053348s]
Jan 14 15:53:45.771: INFO: Created: latency-svc-7cz67
Jan 14 15:53:46.241: INFO: Got endpoints: latency-svc-7cz67 [11.725433556s]
Jan 14 15:53:46.485: INFO: Created: latency-svc-kcgm6
Jan 14 15:53:47.161: INFO: Got endpoints: latency-svc-kcgm6 [11.782864796s]
Jan 14 15:53:47.448: INFO: Created: latency-svc-6gxs9
Jan 14 15:53:47.459: INFO: Got endpoints: latency-svc-6gxs9 [10.768401023s]
Jan 14 15:53:48.519: INFO: Created: latency-svc-zvdmr
Jan 14 15:53:48.948: INFO: Got endpoints: latency-svc-zvdmr [12.256252668s]
Jan 14 15:53:49.196: INFO: Created: latency-svc-k57jt
Jan 14 15:53:49.210: INFO: Got endpoints: latency-svc-k57jt [11.892818496s]
Jan 14 15:53:49.383: INFO: Created: latency-svc-j9b6w
Jan 14 15:53:49.560: INFO: Got endpoints: latency-svc-j9b6w [11.412813104s]
Jan 14 15:53:50.132: INFO: Created: latency-svc-b8wx6
Jan 14 15:53:50.433: INFO: Created: latency-svc-njvhw
Jan 14 15:53:50.445: INFO: Got endpoints: latency-svc-b8wx6 [10.655383529s]
Jan 14 15:53:50.499: INFO: Created: latency-svc-mh7kl
Jan 14 15:53:50.773: INFO: Got endpoints: latency-svc-mh7kl [9.78498911s]
Jan 14 15:53:50.774: INFO: Got endpoints: latency-svc-njvhw [10.050980001s]
Jan 14 15:53:51.022: INFO: Created: latency-svc-jwgf8
Jan 14 15:53:51.034: INFO: Got endpoints: latency-svc-jwgf8 [9.656205366s]
Jan 14 15:53:51.440: INFO: Created: latency-svc-zgcv6
Jan 14 15:53:51.755: INFO: Created: latency-svc-psgt8
Jan 14 15:53:52.426: INFO: Got endpoints: latency-svc-zgcv6 [10.742952078s]
Jan 14 15:53:52.427: INFO: Got endpoints: latency-svc-psgt8 [9.364859259s]
Jan 14 15:53:52.430: INFO: Created: latency-svc-zvpjg
Jan 14 15:53:52.831: INFO: Got endpoints: latency-svc-zvpjg [9.104962257s]
Jan 14 15:53:53.061: INFO: Created: latency-svc-flxdk
Jan 14 15:53:53.094: INFO: Got endpoints: latency-svc-flxdk [7.989293547s]
Jan 14 15:53:53.443: INFO: Created: latency-svc-8s7ph
Jan 14 15:53:53.451: INFO: Got endpoints: latency-svc-8s7ph [8.109915658s]
Jan 14 15:53:53.944: INFO: Created: latency-svc-zmhxq
Jan 14 15:53:54.102: INFO: Got endpoints: latency-svc-zmhxq [7.861508921s]
Jan 14 15:53:54.169: INFO: Created: latency-svc-8mjf2
Jan 14 15:53:54.420: INFO: Created: latency-svc-ss75l
Jan 14 15:53:54.948: INFO: Got endpoints: latency-svc-ss75l [7.488860053s]
Jan 14 15:53:54.949: INFO: Got endpoints: latency-svc-8mjf2 [7.787799665s]
Jan 14 15:53:54.996: INFO: Created: latency-svc-5tm99
Jan 14 15:53:55.313: INFO: Got endpoints: latency-svc-5tm99 [6.361896998s]
Jan 14 15:53:55.963: INFO: Created: latency-svc-dv6pv
Jan 14 15:53:56.062: INFO: Created: latency-svc-jfsd2
Jan 14 15:53:56.440: INFO: Got endpoints: latency-svc-jfsd2 [6.880036233s]
Jan 14 15:53:56.441: INFO: Got endpoints: latency-svc-dv6pv [7.230140801s]
Jan 14 15:53:56.720: INFO: Created: latency-svc-4znxg
Jan 14 15:53:56.813: INFO: Got endpoints: latency-svc-4znxg [6.367985668s]
Jan 14 15:53:57.919: INFO: Created: latency-svc-ddlss
Jan 14 15:53:57.930: INFO: Got endpoints: latency-svc-ddlss [7.157624216s]
Jan 14 15:53:58.250: INFO: Created: latency-svc-9blwl
Jan 14 15:53:58.464: INFO: Created: latency-svc-wrsht
Jan 14 15:53:58.482: INFO: Got endpoints: latency-svc-wrsht [7.447597721s]
Jan 14 15:53:58.483: INFO: Got endpoints: latency-svc-9blwl [7.70906326s]
Jan 14 15:53:59.355: INFO: Created: latency-svc-44zrc
Jan 14 15:53:59.501: INFO: Got endpoints: latency-svc-44zrc [7.074729714s]
Jan 14 15:54:00.241: INFO: Created: latency-svc-scjxv
Jan 14 15:54:00.426: INFO: Got endpoints: latency-svc-scjxv [7.999552526s]
Jan 14 15:54:00.743: INFO: Created: latency-svc-d6wgr
Jan 14 15:54:01.422: INFO: Got endpoints: latency-svc-d6wgr [8.591474899s]
Jan 14 15:54:01.425: INFO: Created: latency-svc-xj2xx
Jan 14 15:54:01.944: INFO: Created: latency-svc-ljflm
Jan 14 15:54:02.116: INFO: Got endpoints: latency-svc-xj2xx [9.022338539s]
Jan 14 15:54:02.247: INFO: Created: latency-svc-ch75f
Jan 14 15:54:02.248: INFO: Got endpoints: latency-svc-ljflm [8.796741456s]
Jan 14 15:54:02.498: INFO: Got endpoints: latency-svc-ch75f [8.395725161s]
Jan 14 15:54:03.216: INFO: Created: latency-svc-b4sjw
Jan 14 15:54:03.842: INFO: Created: latency-svc-znl7q
Jan 14 15:54:03.842: INFO: Got endpoints: latency-svc-b4sjw [8.89364468s]
Jan 14 15:54:04.316: INFO: Created: latency-svc-7wsxq
Jan 14 15:54:04.538: INFO: Got endpoints: latency-svc-7wsxq [9.224793924s]
Jan 14 15:54:04.539: INFO: Got endpoints: latency-svc-znl7q [9.58976504s]
Jan 14 15:54:05.847: INFO: Created: latency-svc-bjthl
Jan 14 15:54:06.289: INFO: Got endpoints: latency-svc-bjthl [9.848806953s]
Jan 14 15:54:06.404: INFO: Created: latency-svc-458l6
Jan 14 15:54:07.168: INFO: Got endpoints: latency-svc-458l6 [10.727679438s]
Jan 14 15:54:07.169: INFO: Created: latency-svc-vfpq6
Jan 14 15:54:07.177: INFO: Got endpoints: latency-svc-vfpq6 [10.362516683s]
Jan 14 15:54:08.040: INFO: Created: latency-svc-2lgm7
Jan 14 15:54:08.343: INFO: Got endpoints: latency-svc-2lgm7 [10.412456379s]
Jan 14 15:54:08.600: INFO: Created: latency-svc-4bps2
Jan 14 15:54:08.636: INFO: Got endpoints: latency-svc-4bps2 [10.153793234s]
Jan 14 15:54:09.039: INFO: Created: latency-svc-8hn85
Jan 14 15:54:09.111: INFO: Got endpoints: latency-svc-8hn85 [10.628786078s]
Jan 14 15:54:09.442: INFO: Created: latency-svc-4x8qg
Jan 14 15:54:09.844: INFO: Got endpoints: latency-svc-4x8qg [10.342609181s]
Jan 14 15:54:10.109: INFO: Created: latency-svc-cjjvj
Jan 14 15:54:10.409: INFO: Got endpoints: latency-svc-cjjvj [9.983495606s]
Jan 14 15:54:10.975: INFO: Created: latency-svc-bfgqv
Jan 14 15:54:11.200: INFO: Created: latency-svc-59szx
Jan 14 15:54:11.694: INFO: Got endpoints: latency-svc-bfgqv [10.271865451s]
Jan 14 15:54:11.947: INFO: Got endpoints: latency-svc-59szx [9.83051451s]
Jan 14 15:54:12.314: INFO: Created: latency-svc-5lv4d
Jan 14 15:54:12.372: INFO: Created: latency-svc-cf8qq
Jan 14 15:54:13.192: INFO: Got endpoints: latency-svc-cf8qq [10.693452424s]
Jan 14 15:54:13.192: INFO: Got endpoints: latency-svc-5lv4d [10.944470826s]
Jan 14 15:54:13.199: INFO: Created: latency-svc-q56b5
Jan 14 15:54:13.964: INFO: Created: latency-svc-wbdq7
Jan 14 15:54:13.965: INFO: Got endpoints: latency-svc-q56b5 [10.122674181s]
Jan 14 15:54:14.596: INFO: Got endpoints: latency-svc-wbdq7 [10.057522504s]
Jan 14 15:54:14.736: INFO: Created: latency-svc-sscfd
Jan 14 15:54:15.296: INFO: Got endpoints: latency-svc-sscfd [10.757498748s]
Jan 14 15:54:15.757: INFO: Created: latency-svc-vq5lp
Jan 14 15:54:16.510: INFO: Got endpoints: latency-svc-vq5lp [10.220596746s]
Jan 14 15:54:16.843: INFO: Created: latency-svc-srj47
Jan 14 15:54:17.242: INFO: Got endpoints: latency-svc-srj47 [10.073716552s]
Jan 14 15:54:18.808: INFO: Created: latency-svc-df8bc
Jan 14 15:54:18.808: INFO: Created: latency-svc-b9p82
Jan 14 15:54:19.233: INFO: Created: latency-svc-rcwcw
Jan 14 15:54:19.378: INFO: Got endpoints: latency-svc-df8bc [12.200074205s]
Jan 14 15:54:19.380: INFO: Got endpoints: latency-svc-b9p82 [11.036604993s]
Jan 14 15:54:19.833: INFO: Got endpoints: latency-svc-rcwcw [11.196593286s]
Jan 14 15:54:20.132: INFO: Created: latency-svc-4g7q8
Jan 14 15:54:20.156: INFO: Got endpoints: latency-svc-4g7q8 [11.0442007s]
Jan 14 15:54:20.454: INFO: Created: latency-svc-bjlvv
Jan 14 15:54:20.470: INFO: Got endpoints: latency-svc-bjlvv [10.625433294s]
Jan 14 15:54:21.412: INFO: Created: latency-svc-wlr8z
Jan 14 15:54:21.907: INFO: Got endpoints: latency-svc-wlr8z [11.497989103s]
Jan 14 15:54:22.035: INFO: Created: latency-svc-hb8tn
Jan 14 15:54:22.270: INFO: Got endpoints: latency-svc-hb8tn [10.576059198s]
Jan 14 15:54:22.955: INFO: Created: latency-svc-dkf4d
Jan 14 15:54:23.318: INFO: Got endpoints: latency-svc-dkf4d [11.362212115s]
Jan 14 15:54:23.572: INFO: Created: latency-svc-rl8qx
Jan 14 15:54:23.983: INFO: Got endpoints: latency-svc-rl8qx [10.791141253s]
Jan 14 15:54:24.060: INFO: Created: latency-svc-8qngz
Jan 14 15:54:24.072: INFO: Got endpoints: latency-svc-8qngz [10.879858458s]
Jan 14 15:54:24.670: INFO: Created: latency-svc-6dwqv
Jan 14 15:54:25.105: INFO: Got endpoints: latency-svc-6dwqv [11.140549051s]
Jan 14 15:54:25.601: INFO: Created: latency-svc-ws6v7
Jan 14 15:54:25.770: INFO: Got endpoints: latency-svc-ws6v7 [11.172451264s]
Jan 14 15:54:26.119: INFO: Created: latency-svc-q5qc4
Jan 14 15:54:26.303: INFO: Got endpoints: latency-svc-q5qc4 [11.007218686s]
Jan 14 15:54:26.595: INFO: Created: latency-svc-tg28f
Jan 14 15:54:26.595: INFO: Got endpoints: latency-svc-tg28f [10.085507737s]
Jan 14 15:54:27.205: INFO: Created: latency-svc-qbxm4
Jan 14 15:54:27.212: INFO: Got endpoints: latency-svc-qbxm4 [9.970220911s]
Jan 14 15:54:27.839: INFO: Created: latency-svc-bxqhz
Jan 14 15:54:28.424: INFO: Got endpoints: latency-svc-bxqhz [9.045997668s]
Jan 14 15:54:28.809: INFO: Created: latency-svc-45f6t
Jan 14 15:54:29.053: INFO: Got endpoints: latency-svc-45f6t [9.672843494s]
Jan 14 15:54:29.400: INFO: Created: latency-svc-brllt
Jan 14 15:54:29.888: INFO: Created: latency-svc-pb7j4
Jan 14 15:54:30.028: INFO: Got endpoints: latency-svc-brllt [10.195503093s]
Jan 14 15:54:30.310: INFO: Got endpoints: latency-svc-pb7j4 [10.15408458s]
Jan 14 15:54:31.397: INFO: Created: latency-svc-rzswb
Jan 14 15:54:31.877: INFO: Got endpoints: latency-svc-rzswb [11.402365442s]
Jan 14 15:54:31.978: INFO: Created: latency-svc-sml7s
Jan 14 15:54:32.259: INFO: Got endpoints: latency-svc-sml7s [10.352029407s]
Jan 14 15:54:32.923: INFO: Created: latency-svc-n7c46
Jan 14 15:54:33.438: INFO: Created: latency-svc-294vq
Jan 14 15:54:34.008: INFO: Created: latency-svc-r2djk
Jan 14 15:54:34.008: INFO: Got endpoints: latency-svc-294vq [10.689793272s]
Jan 14 15:54:34.008: INFO: Got endpoints: latency-svc-n7c46 [11.73740756s]
Jan 14 15:54:34.534: INFO: Created: latency-svc-wmbh7
Jan 14 15:54:34.799: INFO: Created: latency-svc-ft4m8
Jan 14 15:54:34.856: INFO: Got endpoints: latency-svc-r2djk [10.872981283s]
Jan 14 15:54:35.046: INFO: Got endpoints: latency-svc-wmbh7 [10.973392357s]
Jan 14 15:54:35.964: INFO: Created: latency-svc-8pf5z
Jan 14 15:54:35.964: INFO: Got endpoints: latency-svc-ft4m8 [10.858294867s]
Jan 14 15:54:36.215: INFO: Got endpoints: latency-svc-8pf5z [10.445663042s]
Jan 14 15:54:36.708: INFO: Created: latency-svc-42q67
Jan 14 15:54:37.801: INFO: Got endpoints: latency-svc-42q67 [11.494696624s]
Jan 14 15:54:37.828: INFO: Created: latency-svc-gv9lv
Jan 14 15:54:37.834: INFO: Got endpoints: latency-svc-gv9lv [11.238880477s]
Jan 14 15:54:37.838: INFO: Latencies: [753.281933ms 1.645095113s 2.063809912s 2.342617306s 3.568863371s 5.249285659s 5.363845741s 5.95196914s 6.361896998s 6.367985668s 6.452293509s 6.794782049s 6.858469906s 6.880036233s 7.006240432s 7.020757602s 7.074729714s 7.157624216s 7.230140801s 7.261218848s 7.315867796s 7.447597721s 7.488860053s 7.576035413s 7.600803669s 7.601022696s 7.637884515s 7.656293563s 7.70906326s 7.732033589s 7.758634682s 7.761541692s 7.787799665s 7.811165518s 7.861508921s 7.876825562s 7.97699998s 7.989293547s 7.999552526s 8.064595895s 8.086569555s 8.103388878s 8.109915658s 8.158062461s 8.206941672s 8.223364315s 8.395725161s 8.416221542s 8.525746607s 8.528709863s 8.559670885s 8.567902041s 8.574838499s 8.575802891s 8.591474899s 8.603004976s 8.639935065s 8.718843497s 8.722509861s 8.796741456s 8.82162389s 8.827912133s 8.847033499s 8.89364468s 9.007060519s 9.022338539s 9.045997668s 9.059655623s 9.104962257s 9.134870579s 9.18779008s 9.195312696s 9.224793924s 9.244212511s 9.26694098s 9.280803661s 9.304876546s 9.317890599s 9.364859259s 9.365282857s 9.473831059s 9.555907144s 9.568472681s 9.58976504s 9.634072997s 9.635624856s 9.656205366s 9.657304743s 9.672843494s 9.754821915s 9.77450551s 9.78498911s 9.810291732s 9.83051451s 9.845473013s 9.848806953s 9.864126031s 9.921870641s 9.937255264s 9.970220911s 9.983283934s 9.983495606s 9.993444247s 10.029351798s 10.050980001s 10.057522504s 10.068219714s 10.073716552s 10.085507737s 10.122674181s 10.142381991s 10.153793234s 10.15408458s 10.185344882s 10.195503093s 10.220596746s 10.221412342s 10.240214062s 10.271865451s 10.342609181s 10.342781631s 10.352029407s 10.362516683s 10.386009961s 10.412456379s 10.445663042s 10.459541941s 10.470469965s 10.530568333s 10.569164085s 10.576059198s 10.625433294s 10.628786078s 10.633484495s 10.651729891s 10.652954414s 10.655383529s 10.671954991s 10.689793272s 10.693452424s 10.720919968s 10.727679438s 10.732387677s 10.742952078s 10.757498748s 10.768401023s 10.791141253s 10.845167909s 10.858294867s 10.872981283s 10.879858458s 10.925560116s 10.944470826s 10.973392357s 11.007218686s 11.036604993s 11.0442007s 11.047695113s 11.140549051s 11.143813362s 11.165363201s 11.172451264s 11.18404248s 11.196593286s 11.238880477s 11.362212115s 11.370053348s 11.402365442s 11.412813104s 11.482026176s 11.48217875s 11.494696624s 11.497989103s 11.520257318s 11.554486069s 11.616555897s 11.65465062s 11.661555314s 11.725433556s 11.73740756s 11.765076264s 11.782864796s 11.824757437s 11.85390096s 11.889589203s 11.892818496s 11.900086676s 11.905445096s 11.990316532s 12.042778084s 12.067536864s 12.183878285s 12.189640382s 12.190123799s 12.200074205s 12.256252668s 12.281162823s 12.446809258s 12.474244525s 12.514103764s]
Jan 14 15:54:37.838: INFO: 50 %ile: 9.983283934s
Jan 14 15:54:37.838: INFO: 90 %ile: 11.765076264s
Jan 14 15:54:37.838: INFO: 99 %ile: 12.474244525s
Jan 14 15:54:37.838: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:54:37.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1485" for this suite.

â€¢ [SLOW TEST:145.121 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":278,"completed":134,"skipped":1963,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:54:38.327: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9972
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-3b96c5b5-0524-40d3-b2cd-b92a64676333
STEP: Creating secret with name s-test-opt-upd-6ad419a3-fe05-4ce6-ae5d-f3a84133de4b
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-3b96c5b5-0524-40d3-b2cd-b92a64676333
STEP: Updating secret s-test-opt-upd-6ad419a3-fe05-4ce6-ae5d-f3a84133de4b
STEP: Creating secret with name s-test-opt-create-84793854-7753-49e2-88cf-f5ec7480cf58
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:56:20.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9972" for this suite.

â€¢ [SLOW TEST:103.333 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":135,"skipped":1965,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:56:21.661: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6195
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1713
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan 14 15:56:23.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-6195'
Jan 14 15:56:32.574: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 14 15:56:32.574: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1718
Jan 14 15:56:37.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 delete deployment e2e-test-httpd-deployment --namespace=kubectl-6195'
Jan 14 15:56:38.886: INFO: stderr: ""
Jan 14 15:56:38.886: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:56:38.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6195" for this suite.

â€¢ [SLOW TEST:17.442 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run deployment should create a deployment from an image  [Conformance]","total":278,"completed":136,"skipped":1968,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:56:39.104: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8795
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jan 14 15:56:55.278: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8795 PodName:pod-sharedvolume-5ad35bbf-60a8-430b-a4ca-dc2081c3ad10 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 15:56:55.278: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 15:56:55.938: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:56:55.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8795" for this suite.

â€¢ [SLOW TEST:17.865 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":278,"completed":137,"skipped":1970,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:56:56.969: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8504
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on node default medium
Jan 14 15:56:59.263: INFO: Waiting up to 5m0s for pod "pod-19100c85-1fe0-49c1-ac9f-a4ea6e664fe4" in namespace "emptydir-8504" to be "success or failure"
Jan 14 15:57:00.084: INFO: Pod "pod-19100c85-1fe0-49c1-ac9f-a4ea6e664fe4": Phase="Pending", Reason="", readiness=false. Elapsed: 820.79316ms
Jan 14 15:57:02.145: INFO: Pod "pod-19100c85-1fe0-49c1-ac9f-a4ea6e664fe4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.881686344s
Jan 14 15:57:04.424: INFO: Pod "pod-19100c85-1fe0-49c1-ac9f-a4ea6e664fe4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.161503219s
Jan 14 15:57:07.227: INFO: Pod "pod-19100c85-1fe0-49c1-ac9f-a4ea6e664fe4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.96372721s
Jan 14 15:57:09.348: INFO: Pod "pod-19100c85-1fe0-49c1-ac9f-a4ea6e664fe4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.08528131s
Jan 14 15:57:12.028: INFO: Pod "pod-19100c85-1fe0-49c1-ac9f-a4ea6e664fe4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.765060022s
STEP: Saw pod success
Jan 14 15:57:12.028: INFO: Pod "pod-19100c85-1fe0-49c1-ac9f-a4ea6e664fe4" satisfied condition "success or failure"
Jan 14 15:57:12.635: INFO: Trying to get logs from node slave3 pod pod-19100c85-1fe0-49c1-ac9f-a4ea6e664fe4 container test-container: <nil>
STEP: delete the pod
Jan 14 15:57:13.491: INFO: Waiting for pod pod-19100c85-1fe0-49c1-ac9f-a4ea6e664fe4 to disappear
Jan 14 15:57:13.545: INFO: Pod pod-19100c85-1fe0-49c1-ac9f-a4ea6e664fe4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:57:13.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8504" for this suite.

â€¢ [SLOW TEST:17.563 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":138,"skipped":1980,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:57:14.534: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3947
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 15:57:17.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 version'
Jan 14 15:57:17.723: INFO: stderr: ""
Jan 14 15:57:17.723: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.0\", GitCommit:\"70132b0f130acc0bed193d9ba59dd186f0e634cf\", GitTreeState:\"clean\", BuildDate:\"2019-12-07T21:20:10Z\", GoVersion:\"go1.13.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.0\", GitCommit:\"70132b0f130acc0bed193d9ba59dd186f0e634cf\", GitTreeState:\"clean\", BuildDate:\"2019-12-07T21:12:17Z\", GoVersion:\"go1.13.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 15:57:17.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3947" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":278,"completed":139,"skipped":1996,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 15:57:18.489: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-5868
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jan 14 15:57:32.042: INFO: Pod name wrapped-volume-race-754eebfa-11f2-4a35-8db6-d5783f06cd82: Found 0 pods out of 5
Jan 14 15:57:37.120: INFO: Pod name wrapped-volume-race-754eebfa-11f2-4a35-8db6-d5783f06cd82: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-754eebfa-11f2-4a35-8db6-d5783f06cd82 in namespace emptydir-wrapper-5868, will wait for the garbage collector to delete the pods
Jan 14 15:58:12.151: INFO: Deleting ReplicationController wrapped-volume-race-754eebfa-11f2-4a35-8db6-d5783f06cd82 took: 458.072161ms
Jan 14 15:58:13.852: INFO: Terminating ReplicationController wrapped-volume-race-754eebfa-11f2-4a35-8db6-d5783f06cd82 pods took: 1.700383009s
STEP: Creating RC which spawns configmap-volume pods
Jan 14 15:58:34.470: INFO: Pod name wrapped-volume-race-d7854b57-c393-42ae-9514-3131e86e7a24: Found 0 pods out of 5
Jan 14 15:58:39.544: INFO: Pod name wrapped-volume-race-d7854b57-c393-42ae-9514-3131e86e7a24: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d7854b57-c393-42ae-9514-3131e86e7a24 in namespace emptydir-wrapper-5868, will wait for the garbage collector to delete the pods
Jan 14 15:59:18.543: INFO: Deleting ReplicationController wrapped-volume-race-d7854b57-c393-42ae-9514-3131e86e7a24 took: 168.642032ms
Jan 14 15:59:19.943: INFO: Terminating ReplicationController wrapped-volume-race-d7854b57-c393-42ae-9514-3131e86e7a24 pods took: 1.400203087s
STEP: Creating RC which spawns configmap-volume pods
Jan 14 15:59:51.052: INFO: Pod name wrapped-volume-race-3e272225-f780-488b-8028-e8ce411f58fc: Found 0 pods out of 5
Jan 14 15:59:56.146: INFO: Pod name wrapped-volume-race-3e272225-f780-488b-8028-e8ce411f58fc: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-3e272225-f780-488b-8028-e8ce411f58fc in namespace emptydir-wrapper-5868, will wait for the garbage collector to delete the pods
Jan 14 16:00:31.545: INFO: Deleting ReplicationController wrapped-volume-race-3e272225-f780-488b-8028-e8ce411f58fc took: 602.906333ms
Jan 14 16:00:33.746: INFO: Terminating ReplicationController wrapped-volume-race-3e272225-f780-488b-8028-e8ce411f58fc pods took: 2.200421133s
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:01:08.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5868" for this suite.

â€¢ [SLOW TEST:230.498 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":278,"completed":140,"skipped":2044,"failed":0}
S
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:01:08.987: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3117
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-3117/configmap-test-638a4dc1-e6a7-42c4-a84a-d61427a4d565
STEP: Creating a pod to test consume configMaps
Jan 14 16:01:11.942: INFO: Waiting up to 5m0s for pod "pod-configmaps-00b92fae-d99e-4559-a998-6d7c2b5d72fb" in namespace "configmap-3117" to be "success or failure"
Jan 14 16:01:12.261: INFO: Pod "pod-configmaps-00b92fae-d99e-4559-a998-6d7c2b5d72fb": Phase="Pending", Reason="", readiness=false. Elapsed: 319.185936ms
Jan 14 16:01:14.318: INFO: Pod "pod-configmaps-00b92fae-d99e-4559-a998-6d7c2b5d72fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.375515959s
Jan 14 16:01:16.461: INFO: Pod "pod-configmaps-00b92fae-d99e-4559-a998-6d7c2b5d72fb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.518603292s
Jan 14 16:01:18.637: INFO: Pod "pod-configmaps-00b92fae-d99e-4559-a998-6d7c2b5d72fb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.694742081s
Jan 14 16:01:20.819: INFO: Pod "pod-configmaps-00b92fae-d99e-4559-a998-6d7c2b5d72fb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.876846472s
Jan 14 16:01:23.047: INFO: Pod "pod-configmaps-00b92fae-d99e-4559-a998-6d7c2b5d72fb": Phase="Pending", Reason="", readiness=false. Elapsed: 11.104758214s
Jan 14 16:01:25.209: INFO: Pod "pod-configmaps-00b92fae-d99e-4559-a998-6d7c2b5d72fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.26718919s
STEP: Saw pod success
Jan 14 16:01:25.210: INFO: Pod "pod-configmaps-00b92fae-d99e-4559-a998-6d7c2b5d72fb" satisfied condition "success or failure"
Jan 14 16:01:25.427: INFO: Trying to get logs from node slave2 pod pod-configmaps-00b92fae-d99e-4559-a998-6d7c2b5d72fb container env-test: <nil>
STEP: delete the pod
Jan 14 16:01:26.419: INFO: Waiting for pod pod-configmaps-00b92fae-d99e-4559-a998-6d7c2b5d72fb to disappear
Jan 14 16:01:26.425: INFO: Pod pod-configmaps-00b92fae-d99e-4559-a998-6d7c2b5d72fb no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:01:26.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3117" for this suite.

â€¢ [SLOW TEST:17.964 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":278,"completed":141,"skipped":2045,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:01:26.952: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-9823
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:46
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:01:30.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-9823" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":278,"completed":142,"skipped":2052,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:01:31.117: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2844
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 14 16:01:35.214: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f6c84c4b-041e-41f4-86a8-a71364e54226" in namespace "downward-api-2844" to be "success or failure"
Jan 14 16:01:35.399: INFO: Pod "downwardapi-volume-f6c84c4b-041e-41f4-86a8-a71364e54226": Phase="Pending", Reason="", readiness=false. Elapsed: 184.824933ms
Jan 14 16:01:37.492: INFO: Pod "downwardapi-volume-f6c84c4b-041e-41f4-86a8-a71364e54226": Phase="Pending", Reason="", readiness=false. Elapsed: 2.277325071s
Jan 14 16:01:39.673: INFO: Pod "downwardapi-volume-f6c84c4b-041e-41f4-86a8-a71364e54226": Phase="Pending", Reason="", readiness=false. Elapsed: 4.458215268s
Jan 14 16:01:41.973: INFO: Pod "downwardapi-volume-f6c84c4b-041e-41f4-86a8-a71364e54226": Phase="Pending", Reason="", readiness=false. Elapsed: 6.758504652s
Jan 14 16:01:44.074: INFO: Pod "downwardapi-volume-f6c84c4b-041e-41f4-86a8-a71364e54226": Phase="Pending", Reason="", readiness=false. Elapsed: 8.85974656s
Jan 14 16:01:46.363: INFO: Pod "downwardapi-volume-f6c84c4b-041e-41f4-86a8-a71364e54226": Phase="Pending", Reason="", readiness=false. Elapsed: 11.148718574s
Jan 14 16:01:48.369: INFO: Pod "downwardapi-volume-f6c84c4b-041e-41f4-86a8-a71364e54226": Phase="Pending", Reason="", readiness=false. Elapsed: 13.154254844s
Jan 14 16:01:50.956: INFO: Pod "downwardapi-volume-f6c84c4b-041e-41f4-86a8-a71364e54226": Phase="Succeeded", Reason="", readiness=false. Elapsed: 15.741063041s
STEP: Saw pod success
Jan 14 16:01:50.956: INFO: Pod "downwardapi-volume-f6c84c4b-041e-41f4-86a8-a71364e54226" satisfied condition "success or failure"
Jan 14 16:01:50.961: INFO: Trying to get logs from node slave2 pod downwardapi-volume-f6c84c4b-041e-41f4-86a8-a71364e54226 container client-container: <nil>
STEP: delete the pod
Jan 14 16:01:53.307: INFO: Waiting for pod downwardapi-volume-f6c84c4b-041e-41f4-86a8-a71364e54226 to disappear
Jan 14 16:01:53.311: INFO: Pod downwardapi-volume-f6c84c4b-041e-41f4-86a8-a71364e54226 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:01:53.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2844" for this suite.

â€¢ [SLOW TEST:22.853 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":278,"completed":143,"skipped":2054,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:01:53.971: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2216
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-fqtx
STEP: Creating a pod to test atomic-volume-subpath
Jan 14 16:01:57.861: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-fqtx" in namespace "subpath-2216" to be "success or failure"
Jan 14 16:01:58.106: INFO: Pod "pod-subpath-test-configmap-fqtx": Phase="Pending", Reason="", readiness=false. Elapsed: 244.376416ms
Jan 14 16:02:00.536: INFO: Pod "pod-subpath-test-configmap-fqtx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.674641565s
Jan 14 16:02:02.557: INFO: Pod "pod-subpath-test-configmap-fqtx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.695749668s
Jan 14 16:02:04.967: INFO: Pod "pod-subpath-test-configmap-fqtx": Phase="Pending", Reason="", readiness=false. Elapsed: 7.105318168s
Jan 14 16:02:07.454: INFO: Pod "pod-subpath-test-configmap-fqtx": Phase="Pending", Reason="", readiness=false. Elapsed: 9.593014778s
Jan 14 16:02:09.550: INFO: Pod "pod-subpath-test-configmap-fqtx": Phase="Running", Reason="", readiness=true. Elapsed: 11.688728479s
Jan 14 16:02:12.034: INFO: Pod "pod-subpath-test-configmap-fqtx": Phase="Running", Reason="", readiness=true. Elapsed: 14.173296265s
Jan 14 16:02:14.041: INFO: Pod "pod-subpath-test-configmap-fqtx": Phase="Running", Reason="", readiness=true. Elapsed: 16.179437555s
Jan 14 16:02:16.048: INFO: Pod "pod-subpath-test-configmap-fqtx": Phase="Running", Reason="", readiness=true. Elapsed: 18.186771142s
Jan 14 16:02:18.053: INFO: Pod "pod-subpath-test-configmap-fqtx": Phase="Running", Reason="", readiness=true. Elapsed: 20.191574718s
Jan 14 16:02:20.058: INFO: Pod "pod-subpath-test-configmap-fqtx": Phase="Running", Reason="", readiness=true. Elapsed: 22.196592115s
Jan 14 16:02:22.221: INFO: Pod "pod-subpath-test-configmap-fqtx": Phase="Running", Reason="", readiness=true. Elapsed: 24.359696669s
Jan 14 16:02:24.226: INFO: Pod "pod-subpath-test-configmap-fqtx": Phase="Running", Reason="", readiness=true. Elapsed: 26.364978311s
Jan 14 16:02:26.950: INFO: Pod "pod-subpath-test-configmap-fqtx": Phase="Running", Reason="", readiness=true. Elapsed: 29.0883887s
Jan 14 16:02:29.010: INFO: Pod "pod-subpath-test-configmap-fqtx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 31.148913778s
STEP: Saw pod success
Jan 14 16:02:29.010: INFO: Pod "pod-subpath-test-configmap-fqtx" satisfied condition "success or failure"
Jan 14 16:02:29.072: INFO: Trying to get logs from node slave2 pod pod-subpath-test-configmap-fqtx container test-container-subpath-configmap-fqtx: <nil>
STEP: delete the pod
Jan 14 16:02:30.224: INFO: Waiting for pod pod-subpath-test-configmap-fqtx to disappear
Jan 14 16:02:30.977: INFO: Pod pod-subpath-test-configmap-fqtx no longer exists
STEP: Deleting pod pod-subpath-test-configmap-fqtx
Jan 14 16:02:30.977: INFO: Deleting pod "pod-subpath-test-configmap-fqtx" in namespace "subpath-2216"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:02:30.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2216" for this suite.

â€¢ [SLOW TEST:37.665 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":278,"completed":144,"skipped":2065,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:02:31.636: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1243
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 14 16:02:34.253: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ba97373a-5def-4dda-9162-92fad7e4a935" in namespace "projected-1243" to be "success or failure"
Jan 14 16:02:34.313: INFO: Pod "downwardapi-volume-ba97373a-5def-4dda-9162-92fad7e4a935": Phase="Pending", Reason="", readiness=false. Elapsed: 59.788268ms
Jan 14 16:02:36.468: INFO: Pod "downwardapi-volume-ba97373a-5def-4dda-9162-92fad7e4a935": Phase="Pending", Reason="", readiness=false. Elapsed: 2.21508474s
Jan 14 16:02:38.742: INFO: Pod "downwardapi-volume-ba97373a-5def-4dda-9162-92fad7e4a935": Phase="Pending", Reason="", readiness=false. Elapsed: 4.488619441s
Jan 14 16:02:41.300: INFO: Pod "downwardapi-volume-ba97373a-5def-4dda-9162-92fad7e4a935": Phase="Pending", Reason="", readiness=false. Elapsed: 7.046803839s
Jan 14 16:02:43.491: INFO: Pod "downwardapi-volume-ba97373a-5def-4dda-9162-92fad7e4a935": Phase="Pending", Reason="", readiness=false. Elapsed: 9.237846203s
Jan 14 16:02:45.836: INFO: Pod "downwardapi-volume-ba97373a-5def-4dda-9162-92fad7e4a935": Phase="Pending", Reason="", readiness=false. Elapsed: 11.583082518s
Jan 14 16:02:47.960: INFO: Pod "downwardapi-volume-ba97373a-5def-4dda-9162-92fad7e4a935": Phase="Pending", Reason="", readiness=false. Elapsed: 13.706476049s
Jan 14 16:02:49.967: INFO: Pod "downwardapi-volume-ba97373a-5def-4dda-9162-92fad7e4a935": Phase="Succeeded", Reason="", readiness=false. Elapsed: 15.713485067s
STEP: Saw pod success
Jan 14 16:02:49.967: INFO: Pod "downwardapi-volume-ba97373a-5def-4dda-9162-92fad7e4a935" satisfied condition "success or failure"
Jan 14 16:02:49.972: INFO: Trying to get logs from node slave2 pod downwardapi-volume-ba97373a-5def-4dda-9162-92fad7e4a935 container client-container: <nil>
STEP: delete the pod
Jan 14 16:02:50.335: INFO: Waiting for pod downwardapi-volume-ba97373a-5def-4dda-9162-92fad7e4a935 to disappear
Jan 14 16:02:50.634: INFO: Pod downwardapi-volume-ba97373a-5def-4dda-9162-92fad7e4a935 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:02:50.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1243" for this suite.

â€¢ [SLOW TEST:20.036 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":278,"completed":145,"skipped":2075,"failed":0}
SSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:02:51.673: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8350
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 14 16:03:07.022: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:03:07.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8350" for this suite.

â€¢ [SLOW TEST:16.147 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:131
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":278,"completed":146,"skipped":2080,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:03:07.822: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6316
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jan 14 16:03:10.882: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6316 /api/v1/namespaces/watch-6316/configmaps/e2e-watch-test-label-changed 7b6c611e-47e1-4fa4-bb56-4bc0e87c1b1c 116681 0 2020-01-14 16:03:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 14 16:03:10.882: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6316 /api/v1/namespaces/watch-6316/configmaps/e2e-watch-test-label-changed 7b6c611e-47e1-4fa4-bb56-4bc0e87c1b1c 116682 0 2020-01-14 16:03:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jan 14 16:03:10.882: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6316 /api/v1/namespaces/watch-6316/configmaps/e2e-watch-test-label-changed 7b6c611e-47e1-4fa4-bb56-4bc0e87c1b1c 116684 0 2020-01-14 16:03:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jan 14 16:03:21.958: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6316 /api/v1/namespaces/watch-6316/configmaps/e2e-watch-test-label-changed 7b6c611e-47e1-4fa4-bb56-4bc0e87c1b1c 116724 0 2020-01-14 16:03:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 14 16:03:21.958: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6316 /api/v1/namespaces/watch-6316/configmaps/e2e-watch-test-label-changed 7b6c611e-47e1-4fa4-bb56-4bc0e87c1b1c 116725 0 2020-01-14 16:03:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jan 14 16:03:21.958: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6316 /api/v1/namespaces/watch-6316/configmaps/e2e-watch-test-label-changed 7b6c611e-47e1-4fa4-bb56-4bc0e87c1b1c 116726 0 2020-01-14 16:03:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:03:21.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6316" for this suite.

â€¢ [SLOW TEST:14.345 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":278,"completed":147,"skipped":2107,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:03:22.167: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7720
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 14 16:03:23.717: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a07d1503-8cdc-45a1-b371-614bc48a754e" in namespace "downward-api-7720" to be "success or failure"
Jan 14 16:03:24.238: INFO: Pod "downwardapi-volume-a07d1503-8cdc-45a1-b371-614bc48a754e": Phase="Pending", Reason="", readiness=false. Elapsed: 520.642133ms
Jan 14 16:03:27.014: INFO: Pod "downwardapi-volume-a07d1503-8cdc-45a1-b371-614bc48a754e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.296431479s
Jan 14 16:03:29.054: INFO: Pod "downwardapi-volume-a07d1503-8cdc-45a1-b371-614bc48a754e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.336956988s
Jan 14 16:03:31.076: INFO: Pod "downwardapi-volume-a07d1503-8cdc-45a1-b371-614bc48a754e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.358427295s
Jan 14 16:03:33.583: INFO: Pod "downwardapi-volume-a07d1503-8cdc-45a1-b371-614bc48a754e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.865624206s
Jan 14 16:03:35.659: INFO: Pod "downwardapi-volume-a07d1503-8cdc-45a1-b371-614bc48a754e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.942075002s
STEP: Saw pod success
Jan 14 16:03:35.659: INFO: Pod "downwardapi-volume-a07d1503-8cdc-45a1-b371-614bc48a754e" satisfied condition "success or failure"
Jan 14 16:03:35.702: INFO: Trying to get logs from node slave2 pod downwardapi-volume-a07d1503-8cdc-45a1-b371-614bc48a754e container client-container: <nil>
STEP: delete the pod
Jan 14 16:03:36.144: INFO: Waiting for pod downwardapi-volume-a07d1503-8cdc-45a1-b371-614bc48a754e to disappear
Jan 14 16:03:36.149: INFO: Pod downwardapi-volume-a07d1503-8cdc-45a1-b371-614bc48a754e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:03:36.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7720" for this suite.

â€¢ [SLOW TEST:14.871 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":278,"completed":148,"skipped":2131,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:03:37.039: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1170
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: executing a command with run --rm and attach with stdin
Jan 14 16:03:39.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 --namespace=kubectl-1170 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jan 14 16:03:53.114: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jan 14 16:03:53.114: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:03:55.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1170" for this suite.

â€¢ [SLOW TEST:18.513 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1924
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run --rm job should create a job from an image, then delete the job  [Conformance]","total":278,"completed":149,"skipped":2155,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:03:55.553: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3632
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 14 16:04:11.011: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:04:11.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3632" for this suite.

â€¢ [SLOW TEST:15.734 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:131
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":278,"completed":150,"skipped":2175,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:04:11.288: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-256
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl run default
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1576
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan 14 16:04:12.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-256'
Jan 14 16:04:13.341: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 14 16:04:13.341: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1582
Jan 14 16:04:16.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 delete deployment e2e-test-httpd-deployment --namespace=kubectl-256'
Jan 14 16:04:16.824: INFO: stderr: ""
Jan 14 16:04:16.824: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:04:16.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-256" for this suite.

â€¢ [SLOW TEST:5.982 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1570
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run default should create an rc or deployment from an image  [Conformance]","total":278,"completed":151,"skipped":2198,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:04:17.271: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4349
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-004da611-75a8-4b2c-b91c-ff0d0d21780d
STEP: Creating a pod to test consume configMaps
Jan 14 16:04:21.579: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0db6572f-a60b-4d7e-b692-0e67fd17a561" in namespace "projected-4349" to be "success or failure"
Jan 14 16:04:21.948: INFO: Pod "pod-projected-configmaps-0db6572f-a60b-4d7e-b692-0e67fd17a561": Phase="Pending", Reason="", readiness=false. Elapsed: 369.280074ms
Jan 14 16:04:24.264: INFO: Pod "pod-projected-configmaps-0db6572f-a60b-4d7e-b692-0e67fd17a561": Phase="Pending", Reason="", readiness=false. Elapsed: 2.685496877s
Jan 14 16:04:26.754: INFO: Pod "pod-projected-configmaps-0db6572f-a60b-4d7e-b692-0e67fd17a561": Phase="Pending", Reason="", readiness=false. Elapsed: 5.175146352s
Jan 14 16:04:28.955: INFO: Pod "pod-projected-configmaps-0db6572f-a60b-4d7e-b692-0e67fd17a561": Phase="Pending", Reason="", readiness=false. Elapsed: 7.37626019s
Jan 14 16:04:31.017: INFO: Pod "pod-projected-configmaps-0db6572f-a60b-4d7e-b692-0e67fd17a561": Phase="Pending", Reason="", readiness=false. Elapsed: 9.438117412s
Jan 14 16:04:33.023: INFO: Pod "pod-projected-configmaps-0db6572f-a60b-4d7e-b692-0e67fd17a561": Phase="Pending", Reason="", readiness=false. Elapsed: 11.444436625s
Jan 14 16:04:35.031: INFO: Pod "pod-projected-configmaps-0db6572f-a60b-4d7e-b692-0e67fd17a561": Phase="Pending", Reason="", readiness=false. Elapsed: 13.451579087s
Jan 14 16:04:37.354: INFO: Pod "pod-projected-configmaps-0db6572f-a60b-4d7e-b692-0e67fd17a561": Phase="Succeeded", Reason="", readiness=false. Elapsed: 15.774822555s
STEP: Saw pod success
Jan 14 16:04:37.354: INFO: Pod "pod-projected-configmaps-0db6572f-a60b-4d7e-b692-0e67fd17a561" satisfied condition "success or failure"
Jan 14 16:04:37.361: INFO: Trying to get logs from node slave3 pod pod-projected-configmaps-0db6572f-a60b-4d7e-b692-0e67fd17a561 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 14 16:04:38.031: INFO: Waiting for pod pod-projected-configmaps-0db6572f-a60b-4d7e-b692-0e67fd17a561 to disappear
Jan 14 16:04:38.039: INFO: Pod pod-projected-configmaps-0db6572f-a60b-4d7e-b692-0e67fd17a561 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:04:38.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4349" for this suite.

â€¢ [SLOW TEST:21.397 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":152,"skipped":2207,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:04:38.688: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4842
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jan 14 16:04:41.176: INFO: Waiting up to 5m0s for pod "downward-api-1dc35ff4-2ff3-4f5f-9c56-f25f1eb39093" in namespace "downward-api-4842" to be "success or failure"
Jan 14 16:04:41.181: INFO: Pod "downward-api-1dc35ff4-2ff3-4f5f-9c56-f25f1eb39093": Phase="Pending", Reason="", readiness=false. Elapsed: 4.289221ms
Jan 14 16:04:43.186: INFO: Pod "downward-api-1dc35ff4-2ff3-4f5f-9c56-f25f1eb39093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009695501s
Jan 14 16:04:45.193: INFO: Pod "downward-api-1dc35ff4-2ff3-4f5f-9c56-f25f1eb39093": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016674288s
Jan 14 16:04:47.256: INFO: Pod "downward-api-1dc35ff4-2ff3-4f5f-9c56-f25f1eb39093": Phase="Pending", Reason="", readiness=false. Elapsed: 6.079856489s
Jan 14 16:04:49.300: INFO: Pod "downward-api-1dc35ff4-2ff3-4f5f-9c56-f25f1eb39093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.124057924s
STEP: Saw pod success
Jan 14 16:04:49.301: INFO: Pod "downward-api-1dc35ff4-2ff3-4f5f-9c56-f25f1eb39093" satisfied condition "success or failure"
Jan 14 16:04:49.419: INFO: Trying to get logs from node slave2 pod downward-api-1dc35ff4-2ff3-4f5f-9c56-f25f1eb39093 container dapi-container: <nil>
STEP: delete the pod
Jan 14 16:04:50.201: INFO: Waiting for pod downward-api-1dc35ff4-2ff3-4f5f-9c56-f25f1eb39093 to disappear
Jan 14 16:04:50.208: INFO: Pod downward-api-1dc35ff4-2ff3-4f5f-9c56-f25f1eb39093 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:04:50.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4842" for this suite.

â€¢ [SLOW TEST:12.904 seconds]
[sig-node] Downward API
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:33
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":278,"completed":153,"skipped":2250,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:04:51.590: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9374
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-fae30cd2-a10c-4da8-904a-cf99aab80aec
STEP: Creating a pod to test consume secrets
Jan 14 16:04:54.489: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4fb9a8af-ba95-4eb2-a70f-128c4a4f28f1" in namespace "projected-9374" to be "success or failure"
Jan 14 16:04:54.494: INFO: Pod "pod-projected-secrets-4fb9a8af-ba95-4eb2-a70f-128c4a4f28f1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.994252ms
Jan 14 16:04:56.555: INFO: Pod "pod-projected-secrets-4fb9a8af-ba95-4eb2-a70f-128c4a4f28f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066322558s
Jan 14 16:04:58.792: INFO: Pod "pod-projected-secrets-4fb9a8af-ba95-4eb2-a70f-128c4a4f28f1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.303567127s
Jan 14 16:05:01.468: INFO: Pod "pod-projected-secrets-4fb9a8af-ba95-4eb2-a70f-128c4a4f28f1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.979441232s
Jan 14 16:05:03.814: INFO: Pod "pod-projected-secrets-4fb9a8af-ba95-4eb2-a70f-128c4a4f28f1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.325105992s
Jan 14 16:05:06.141: INFO: Pod "pod-projected-secrets-4fb9a8af-ba95-4eb2-a70f-128c4a4f28f1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.652193537s
Jan 14 16:05:09.394: INFO: Pod "pod-projected-secrets-4fb9a8af-ba95-4eb2-a70f-128c4a4f28f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.905212232s
STEP: Saw pod success
Jan 14 16:05:09.394: INFO: Pod "pod-projected-secrets-4fb9a8af-ba95-4eb2-a70f-128c4a4f28f1" satisfied condition "success or failure"
Jan 14 16:05:09.894: INFO: Trying to get logs from node slave2 pod pod-projected-secrets-4fb9a8af-ba95-4eb2-a70f-128c4a4f28f1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 14 16:05:10.856: INFO: Waiting for pod pod-projected-secrets-4fb9a8af-ba95-4eb2-a70f-128c4a4f28f1 to disappear
Jan 14 16:05:10.867: INFO: Pod pod-projected-secrets-4fb9a8af-ba95-4eb2-a70f-128c4a4f28f1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:05:10.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9374" for this suite.

â€¢ [SLOW TEST:19.555 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":278,"completed":154,"skipped":2255,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:05:11.145: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-7464
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 16:05:13.879: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:05:21.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7464" for this suite.

â€¢ [SLOW TEST:11.321 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:47
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":278,"completed":155,"skipped":2258,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:05:22.472: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-300
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 14 16:05:25.809: INFO: Waiting up to 5m0s for pod "downwardapi-volume-695bbb54-e177-44aa-96fc-5c5afb0e113f" in namespace "projected-300" to be "success or failure"
Jan 14 16:05:25.815: INFO: Pod "downwardapi-volume-695bbb54-e177-44aa-96fc-5c5afb0e113f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.943991ms
Jan 14 16:05:28.435: INFO: Pod "downwardapi-volume-695bbb54-e177-44aa-96fc-5c5afb0e113f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.626174289s
Jan 14 16:05:30.582: INFO: Pod "downwardapi-volume-695bbb54-e177-44aa-96fc-5c5afb0e113f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.772808476s
Jan 14 16:05:32.755: INFO: Pod "downwardapi-volume-695bbb54-e177-44aa-96fc-5c5afb0e113f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.94538251s
Jan 14 16:05:35.156: INFO: Pod "downwardapi-volume-695bbb54-e177-44aa-96fc-5c5afb0e113f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.346420028s
Jan 14 16:05:37.516: INFO: Pod "downwardapi-volume-695bbb54-e177-44aa-96fc-5c5afb0e113f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.706794639s
Jan 14 16:05:39.521: INFO: Pod "downwardapi-volume-695bbb54-e177-44aa-96fc-5c5afb0e113f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.71132181s
STEP: Saw pod success
Jan 14 16:05:39.521: INFO: Pod "downwardapi-volume-695bbb54-e177-44aa-96fc-5c5afb0e113f" satisfied condition "success or failure"
Jan 14 16:05:39.526: INFO: Trying to get logs from node slave2 pod downwardapi-volume-695bbb54-e177-44aa-96fc-5c5afb0e113f container client-container: <nil>
STEP: delete the pod
Jan 14 16:05:40.230: INFO: Waiting for pod downwardapi-volume-695bbb54-e177-44aa-96fc-5c5afb0e113f to disappear
Jan 14 16:05:40.235: INFO: Pod downwardapi-volume-695bbb54-e177-44aa-96fc-5c5afb0e113f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:05:40.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-300" for this suite.

â€¢ [SLOW TEST:18.262 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":278,"completed":156,"skipped":2304,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:05:40.735: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-645
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:05:57.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-645" for this suite.

â€¢ [SLOW TEST:17.152 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":278,"completed":157,"skipped":2328,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:05:57.888: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1074
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jan 14 16:06:25.749: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 14 16:06:25.758: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 14 16:06:27.758: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 14 16:06:27.764: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 14 16:06:29.762: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 14 16:06:30.057: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 14 16:06:31.758: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 14 16:06:31.949: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 14 16:06:33.758: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 14 16:06:34.154: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:06:34.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1074" for this suite.

â€¢ [SLOW TEST:36.500 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":278,"completed":158,"skipped":2363,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:06:34.390: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7758
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:06:51.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7758" for this suite.

â€¢ [SLOW TEST:17.510 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":159,"skipped":2436,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:06:51.900: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5960
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 14 16:06:54.358: INFO: Number of nodes with available pods: 0
Jan 14 16:06:54.358: INFO: Node master1 is running more than one daemon pod
Jan 14 16:06:58.029: INFO: Number of nodes with available pods: 0
Jan 14 16:06:58.029: INFO: Node master1 is running more than one daemon pod
Jan 14 16:06:59.444: INFO: Number of nodes with available pods: 0
Jan 14 16:06:59.444: INFO: Node master1 is running more than one daemon pod
Jan 14 16:07:00.424: INFO: Number of nodes with available pods: 0
Jan 14 16:07:00.425: INFO: Node master1 is running more than one daemon pod
Jan 14 16:07:01.666: INFO: Number of nodes with available pods: 0
Jan 14 16:07:01.666: INFO: Node master1 is running more than one daemon pod
Jan 14 16:07:02.678: INFO: Number of nodes with available pods: 0
Jan 14 16:07:02.678: INFO: Node master1 is running more than one daemon pod
Jan 14 16:07:04.076: INFO: Number of nodes with available pods: 0
Jan 14 16:07:04.076: INFO: Node master1 is running more than one daemon pod
Jan 14 16:07:04.884: INFO: Number of nodes with available pods: 0
Jan 14 16:07:04.884: INFO: Node master1 is running more than one daemon pod
Jan 14 16:07:06.658: INFO: Number of nodes with available pods: 0
Jan 14 16:07:06.658: INFO: Node master1 is running more than one daemon pod
Jan 14 16:07:08.316: INFO: Number of nodes with available pods: 1
Jan 14 16:07:08.316: INFO: Node master1 is running more than one daemon pod
Jan 14 16:07:08.594: INFO: Number of nodes with available pods: 2
Jan 14 16:07:08.594: INFO: Node master1 is running more than one daemon pod
Jan 14 16:07:11.793: INFO: Number of nodes with available pods: 2
Jan 14 16:07:11.793: INFO: Node master1 is running more than one daemon pod
Jan 14 16:07:13.988: INFO: Number of nodes with available pods: 2
Jan 14 16:07:13.988: INFO: Node master1 is running more than one daemon pod
Jan 14 16:07:14.758: INFO: Number of nodes with available pods: 4
Jan 14 16:07:14.766: INFO: Node master2 is running more than one daemon pod
Jan 14 16:07:15.958: INFO: Number of nodes with available pods: 5
Jan 14 16:07:15.960: INFO: Node master3 is running more than one daemon pod
Jan 14 16:07:17.559: INFO: Number of nodes with available pods: 6
Jan 14 16:07:17.559: INFO: Node slave4 is running more than one daemon pod
Jan 14 16:07:18.597: INFO: Number of nodes with available pods: 7
Jan 14 16:07:18.597: INFO: Number of running nodes: 7, number of available pods: 7
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jan 14 16:07:18.958: INFO: Number of nodes with available pods: 7
Jan 14 16:07:18.958: INFO: Number of running nodes: 7, number of available pods: 7
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5960, will wait for the garbage collector to delete the pods
Jan 14 16:07:21.072: INFO: Deleting DaemonSet.extensions daemon-set took: 369.819321ms
Jan 14 16:07:22.372: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.300225338s
Jan 14 16:07:46.668: INFO: Number of nodes with available pods: 0
Jan 14 16:07:46.668: INFO: Number of running nodes: 0, number of available pods: 0
Jan 14 16:07:46.934: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5960/daemonsets","resourceVersion":"118000"},"items":null}

Jan 14 16:07:46.949: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5960/pods","resourceVersion":"118001"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:07:46.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5960" for this suite.

â€¢ [SLOW TEST:55.876 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":278,"completed":160,"skipped":2439,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:07:47.777: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5287
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 16:07:49.766: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jan 14 16:08:00.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 --namespace=crd-publish-openapi-5287 create -f -'
Jan 14 16:08:13.615: INFO: stderr: ""
Jan 14 16:08:13.615: INFO: stdout: "e2e-test-crd-publish-openapi-7082-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 14 16:08:13.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 --namespace=crd-publish-openapi-5287 delete e2e-test-crd-publish-openapi-7082-crds test-cr'
Jan 14 16:08:13.861: INFO: stderr: ""
Jan 14 16:08:13.861: INFO: stdout: "e2e-test-crd-publish-openapi-7082-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 14 16:08:13.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 --namespace=crd-publish-openapi-5287 apply -f -'
Jan 14 16:08:15.545: INFO: stderr: ""
Jan 14 16:08:15.545: INFO: stdout: "e2e-test-crd-publish-openapi-7082-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 14 16:08:15.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 --namespace=crd-publish-openapi-5287 delete e2e-test-crd-publish-openapi-7082-crds test-cr'
Jan 14 16:08:16.634: INFO: stderr: ""
Jan 14 16:08:16.634: INFO: stdout: "e2e-test-crd-publish-openapi-7082-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jan 14 16:08:16.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 explain e2e-test-crd-publish-openapi-7082-crds'
Jan 14 16:08:17.607: INFO: stderr: ""
Jan 14 16:08:17.610: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7082-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:08:22.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5287" for this suite.

â€¢ [SLOW TEST:35.594 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":278,"completed":161,"skipped":2446,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:08:23.380: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6746
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-projected-all-test-volume-fb07abfc-2bfb-470c-9a3c-6e7586a489ad
STEP: Creating secret with name secret-projected-all-test-volume-bd5125b5-dd4a-46f5-b210-12df50a2ba1c
STEP: Creating a pod to test Check all projections for projected volume plugin
Jan 14 16:08:27.984: INFO: Waiting up to 5m0s for pod "projected-volume-2428e009-2182-4987-99d6-add6bb82fb31" in namespace "projected-6746" to be "success or failure"
Jan 14 16:08:28.057: INFO: Pod "projected-volume-2428e009-2182-4987-99d6-add6bb82fb31": Phase="Pending", Reason="", readiness=false. Elapsed: 73.456307ms
Jan 14 16:08:30.122: INFO: Pod "projected-volume-2428e009-2182-4987-99d6-add6bb82fb31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.137750414s
Jan 14 16:08:32.572: INFO: Pod "projected-volume-2428e009-2182-4987-99d6-add6bb82fb31": Phase="Pending", Reason="", readiness=false. Elapsed: 4.587902381s
Jan 14 16:08:34.601: INFO: Pod "projected-volume-2428e009-2182-4987-99d6-add6bb82fb31": Phase="Pending", Reason="", readiness=false. Elapsed: 6.617170457s
Jan 14 16:08:37.166: INFO: Pod "projected-volume-2428e009-2182-4987-99d6-add6bb82fb31": Phase="Pending", Reason="", readiness=false. Elapsed: 9.182474955s
Jan 14 16:08:39.172: INFO: Pod "projected-volume-2428e009-2182-4987-99d6-add6bb82fb31": Phase="Pending", Reason="", readiness=false. Elapsed: 11.188081768s
Jan 14 16:08:41.440: INFO: Pod "projected-volume-2428e009-2182-4987-99d6-add6bb82fb31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.455837639s
STEP: Saw pod success
Jan 14 16:08:41.440: INFO: Pod "projected-volume-2428e009-2182-4987-99d6-add6bb82fb31" satisfied condition "success or failure"
Jan 14 16:08:41.451: INFO: Trying to get logs from node slave2 pod projected-volume-2428e009-2182-4987-99d6-add6bb82fb31 container projected-all-volume-test: <nil>
STEP: delete the pod
Jan 14 16:08:41.754: INFO: Waiting for pod projected-volume-2428e009-2182-4987-99d6-add6bb82fb31 to disappear
Jan 14 16:08:41.770: INFO: Pod projected-volume-2428e009-2182-4987-99d6-add6bb82fb31 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:08:41.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6746" for this suite.

â€¢ [SLOW TEST:18.967 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":278,"completed":162,"skipped":2454,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:08:42.348: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1729
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:08:48.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1729" for this suite.

â€¢ [SLOW TEST:7.253 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":278,"completed":163,"skipped":2462,"failed":0}
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:08:49.601: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7122
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jan 14 16:08:51.275: INFO: PodSpec: initContainers in spec.initContainers
Jan 14 16:09:56.568: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-77ec4af2-c3ea-4377-977c-f35424c62edc", GenerateName:"", Namespace:"init-container-7122", SelfLink:"/api/v1/namespaces/init-container-7122/pods/pod-init-77ec4af2-c3ea-4377-977c-f35424c62edc", UID:"25c1b9e6-7dc3-4413-8c6c-2a8439bc2002", ResourceVersion:"118489", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63714614931, loc:(*time.Location)(0x7d421e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"275466637"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-9sfxz", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc006650480), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9sfxz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9sfxz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9sfxz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0064ffca8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"slave2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002aa5860), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0064ffd60)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0064ffd80)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0064ffd88), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0064ffd8c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714614931, loc:(*time.Location)(0x7d421e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714614931, loc:(*time.Location)(0x7d421e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714614931, loc:(*time.Location)(0x7d421e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714614931, loc:(*time.Location)(0x7d421e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.0.142", PodIP:"10.151.49.224", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.151.49.224"}}, StartTime:(*v1.Time)(0xc002d9b660), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003062000)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003062070)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://51c37322626f1aa4e6290ca6817703fb187e7e322ca9648870af36c95cfc83b8", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002d9b6a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002d9b680), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc0064ffe44)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:09:56.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7122" for this suite.

â€¢ [SLOW TEST:68.468 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":278,"completed":164,"skipped":2467,"failed":0}
SSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:09:58.069: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-4965
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jan 14 16:10:18.399: INFO: &Pod{ObjectMeta:{send-events-158f56e7-f1f9-42ff-9907-498b73d89738  events-4965 /api/v1/namespaces/events-4965/pods/send-events-158f56e7-f1f9-42ff-9907-498b73d89738 07e6fe2d-8b13-46c4-aa5b-9f500c8453e7 118575 0 2020-01-14 16:10:00 +0000 UTC <nil> <nil> map[name:foo time:115242609] map[] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q2x4f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q2x4f,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q2x4f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 16:10:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 16:10:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 16:10:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 16:10:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.142,PodIP:10.151.49.225,StartTime:2020-01-14 16:10:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-14 16:10:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker://sha256:a450ec43af3738f7fc05227a9a778feb9f90e08bfee4362b6a59224bcbf172cf,ContainerID:docker://243db5da238805647bc378fb2b23f5df220ac4c051f58cd141a41732ed54785d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.151.49.225,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jan 14 16:10:20.450: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jan 14 16:10:22.472: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:10:22.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4965" for this suite.

â€¢ [SLOW TEST:25.648 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":278,"completed":165,"skipped":2473,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:10:23.718: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6382
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6382 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6382;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6382 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6382;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6382.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6382.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6382.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6382.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6382.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6382.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6382.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6382.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6382.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6382.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6382.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6382.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6382.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 162.181.150.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.150.181.162_udp@PTR;check="$$(dig +tcp +noall +answer +search 162.181.150.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.150.181.162_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6382 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6382;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6382 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6382;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6382.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6382.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6382.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6382.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6382.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6382.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6382.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6382.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6382.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6382.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6382.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6382.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6382.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 162.181.150.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.150.181.162_udp@PTR;check="$$(dig +tcp +noall +answer +search 162.181.150.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.150.181.162_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 14 16:10:47.072: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6382/dns-test-9d136f78-a57a-483c-b21a-3b899f19726d: the server could not find the requested resource (get pods dns-test-9d136f78-a57a-483c-b21a-3b899f19726d)
Jan 14 16:10:47.373: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6382/dns-test-9d136f78-a57a-483c-b21a-3b899f19726d: the server could not find the requested resource (get pods dns-test-9d136f78-a57a-483c-b21a-3b899f19726d)
Jan 14 16:10:47.478: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6382 from pod dns-6382/dns-test-9d136f78-a57a-483c-b21a-3b899f19726d: the server could not find the requested resource (get pods dns-test-9d136f78-a57a-483c-b21a-3b899f19726d)
Jan 14 16:10:47.484: INFO: Unable to read wheezy_udp@dns-test-service.dns-6382.svc from pod dns-6382/dns-test-9d136f78-a57a-483c-b21a-3b899f19726d: the server could not find the requested resource (get pods dns-test-9d136f78-a57a-483c-b21a-3b899f19726d)
Jan 14 16:10:47.489: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6382.svc from pod dns-6382/dns-test-9d136f78-a57a-483c-b21a-3b899f19726d: the server could not find the requested resource (get pods dns-test-9d136f78-a57a-483c-b21a-3b899f19726d)
Jan 14 16:10:47.500: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6382.svc from pod dns-6382/dns-test-9d136f78-a57a-483c-b21a-3b899f19726d: the server could not find the requested resource (get pods dns-test-9d136f78-a57a-483c-b21a-3b899f19726d)
Jan 14 16:10:47.535: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6382/dns-test-9d136f78-a57a-483c-b21a-3b899f19726d: the server could not find the requested resource (get pods dns-test-9d136f78-a57a-483c-b21a-3b899f19726d)
Jan 14 16:10:47.539: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6382/dns-test-9d136f78-a57a-483c-b21a-3b899f19726d: the server could not find the requested resource (get pods dns-test-9d136f78-a57a-483c-b21a-3b899f19726d)
Jan 14 16:10:47.543: INFO: Unable to read jessie_udp@dns-test-service.dns-6382 from pod dns-6382/dns-test-9d136f78-a57a-483c-b21a-3b899f19726d: the server could not find the requested resource (get pods dns-test-9d136f78-a57a-483c-b21a-3b899f19726d)
Jan 14 16:10:47.549: INFO: Unable to read jessie_tcp@dns-test-service.dns-6382 from pod dns-6382/dns-test-9d136f78-a57a-483c-b21a-3b899f19726d: the server could not find the requested resource (get pods dns-test-9d136f78-a57a-483c-b21a-3b899f19726d)
Jan 14 16:10:47.554: INFO: Unable to read jessie_udp@dns-test-service.dns-6382.svc from pod dns-6382/dns-test-9d136f78-a57a-483c-b21a-3b899f19726d: the server could not find the requested resource (get pods dns-test-9d136f78-a57a-483c-b21a-3b899f19726d)
Jan 14 16:10:47.558: INFO: Unable to read jessie_tcp@dns-test-service.dns-6382.svc from pod dns-6382/dns-test-9d136f78-a57a-483c-b21a-3b899f19726d: the server could not find the requested resource (get pods dns-test-9d136f78-a57a-483c-b21a-3b899f19726d)
Jan 14 16:10:47.569: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6382.svc from pod dns-6382/dns-test-9d136f78-a57a-483c-b21a-3b899f19726d: the server could not find the requested resource (get pods dns-test-9d136f78-a57a-483c-b21a-3b899f19726d)
Jan 14 16:10:47.628: INFO: Lookups using dns-6382/dns-test-9d136f78-a57a-483c-b21a-3b899f19726d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.dns-6382 wheezy_udp@dns-test-service.dns-6382.svc wheezy_tcp@dns-test-service.dns-6382.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6382.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6382 jessie_tcp@dns-test-service.dns-6382 jessie_udp@dns-test-service.dns-6382.svc jessie_tcp@dns-test-service.dns-6382.svc jessie_tcp@_http._tcp.dns-test-service.dns-6382.svc]

Jan 14 16:10:53.742: INFO: DNS probes using dns-6382/dns-test-9d136f78-a57a-483c-b21a-3b899f19726d succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:10:58.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6382" for this suite.

â€¢ [SLOW TEST:35.299 seconds]
[sig-network] DNS
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":278,"completed":166,"skipped":2486,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:10:59.018: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8128
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating secret secrets-8128/secret-test-fe6dc7e2-90ea-4903-b409-14b82b59bb21
STEP: Creating a pod to test consume secrets
Jan 14 16:11:04.342: INFO: Waiting up to 5m0s for pod "pod-configmaps-0727be5d-cf9e-426c-a5d2-39bd4a76455b" in namespace "secrets-8128" to be "success or failure"
Jan 14 16:11:04.350: INFO: Pod "pod-configmaps-0727be5d-cf9e-426c-a5d2-39bd4a76455b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.292606ms
Jan 14 16:11:06.781: INFO: Pod "pod-configmaps-0727be5d-cf9e-426c-a5d2-39bd4a76455b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.439461715s
Jan 14 16:11:08.910: INFO: Pod "pod-configmaps-0727be5d-cf9e-426c-a5d2-39bd4a76455b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.568753319s
Jan 14 16:11:11.072: INFO: Pod "pod-configmaps-0727be5d-cf9e-426c-a5d2-39bd4a76455b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.730803706s
Jan 14 16:11:13.368: INFO: Pod "pod-configmaps-0727be5d-cf9e-426c-a5d2-39bd4a76455b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.026709616s
Jan 14 16:11:15.448: INFO: Pod "pod-configmaps-0727be5d-cf9e-426c-a5d2-39bd4a76455b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.106235766s
STEP: Saw pod success
Jan 14 16:11:15.449: INFO: Pod "pod-configmaps-0727be5d-cf9e-426c-a5d2-39bd4a76455b" satisfied condition "success or failure"
Jan 14 16:11:15.814: INFO: Trying to get logs from node slave2 pod pod-configmaps-0727be5d-cf9e-426c-a5d2-39bd4a76455b container env-test: <nil>
STEP: delete the pod
Jan 14 16:11:18.689: INFO: Waiting for pod pod-configmaps-0727be5d-cf9e-426c-a5d2-39bd4a76455b to disappear
Jan 14 16:11:18.694: INFO: Pod pod-configmaps-0727be5d-cf9e-426c-a5d2-39bd4a76455b no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:11:18.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8128" for this suite.

â€¢ [SLOW TEST:20.029 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":278,"completed":167,"skipped":2511,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:11:19.047: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8675
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-3889
STEP: Creating secret with name secret-test-557153f8-9f8d-45d5-8caa-3c1cba983c71
STEP: Creating a pod to test consume secrets
Jan 14 16:11:26.036: INFO: Waiting up to 5m0s for pod "pod-secrets-519fb8b2-2e03-49d1-bb11-ac0ecbac69b5" in namespace "secrets-8675" to be "success or failure"
Jan 14 16:11:26.446: INFO: Pod "pod-secrets-519fb8b2-2e03-49d1-bb11-ac0ecbac69b5": Phase="Pending", Reason="", readiness=false. Elapsed: 409.815832ms
Jan 14 16:11:28.700: INFO: Pod "pod-secrets-519fb8b2-2e03-49d1-bb11-ac0ecbac69b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.663668099s
Jan 14 16:11:31.256: INFO: Pod "pod-secrets-519fb8b2-2e03-49d1-bb11-ac0ecbac69b5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.219345162s
Jan 14 16:11:33.991: INFO: Pod "pod-secrets-519fb8b2-2e03-49d1-bb11-ac0ecbac69b5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.955172705s
Jan 14 16:11:36.249: INFO: Pod "pod-secrets-519fb8b2-2e03-49d1-bb11-ac0ecbac69b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.213030531s
STEP: Saw pod success
Jan 14 16:11:36.249: INFO: Pod "pod-secrets-519fb8b2-2e03-49d1-bb11-ac0ecbac69b5" satisfied condition "success or failure"
Jan 14 16:11:36.257: INFO: Trying to get logs from node slave2 pod pod-secrets-519fb8b2-2e03-49d1-bb11-ac0ecbac69b5 container secret-volume-test: <nil>
STEP: delete the pod
Jan 14 16:11:36.959: INFO: Waiting for pod pod-secrets-519fb8b2-2e03-49d1-bb11-ac0ecbac69b5 to disappear
Jan 14 16:11:37.250: INFO: Pod pod-secrets-519fb8b2-2e03-49d1-bb11-ac0ecbac69b5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:11:37.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8675" for this suite.
STEP: Destroying namespace "secret-namespace-3889" for this suite.

â€¢ [SLOW TEST:21.173 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":278,"completed":168,"skipped":2546,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:11:40.221: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7959
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 16:11:43.077: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jan 14 16:11:47.103: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:11:48.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7959" for this suite.

â€¢ [SLOW TEST:8.577 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":278,"completed":169,"skipped":2571,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:11:48.799: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7576
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service nodeport-test with type=NodePort in namespace services-7576
STEP: creating replication controller nodeport-test in namespace services-7576
I0114 16:11:52.418676      23 runners.go:189] Created replication controller with name: nodeport-test, namespace: services-7576, replica count: 2
I0114 16:11:55.469044      23 runners.go:189] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 16:11:58.469350      23 runners.go:189] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 16:12:01.469792      23 runners.go:189] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 16:12:04.470046      23 runners.go:189] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 16:12:07.470288      23 runners.go:189] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 16:12:10.470954      23 runners.go:189] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 16:12:13.471296      23 runners.go:189] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 14 16:12:13.471: INFO: Creating new exec pod
Jan 14 16:12:24.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=services-7576 execpodntvg4 -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Jan 14 16:12:25.418: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 14 16:12:25.418: INFO: stdout: ""
Jan 14 16:12:25.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=services-7576 execpodntvg4 -- /bin/sh -x -c nc -zv -t -w 2 10.150.7.132 80'
Jan 14 16:12:26.374: INFO: stderr: "+ nc -zv -t -w 2 10.150.7.132 80\nConnection to 10.150.7.132 80 port [tcp/http] succeeded!\n"
Jan 14 16:12:26.374: INFO: stdout: ""
Jan 14 16:12:26.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=services-7576 execpodntvg4 -- /bin/sh -x -c nc -zv -t -w 2 192.168.0.132 30402'
Jan 14 16:12:27.284: INFO: stderr: "+ nc -zv -t -w 2 192.168.0.132 30402\nConnection to 192.168.0.132 30402 port [tcp/30402] succeeded!\n"
Jan 14 16:12:27.284: INFO: stdout: ""
Jan 14 16:12:27.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=services-7576 execpodntvg4 -- /bin/sh -x -c nc -zv -t -w 2 192.168.0.161 30402'
Jan 14 16:12:28.344: INFO: stderr: "+ nc -zv -t -w 2 192.168.0.161 30402\nConnection to 192.168.0.161 30402 port [tcp/30402] succeeded!\n"
Jan 14 16:12:28.344: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:12:28.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7576" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:39.799 seconds]
[sig-network] Services
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":278,"completed":170,"skipped":2572,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:12:28.597: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8200
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 16:12:44.177: INFO: Waiting up to 5m0s for pod "client-envvars-dca55637-3b38-42ea-9816-5613b4c9cbc8" in namespace "pods-8200" to be "success or failure"
Jan 14 16:12:44.263: INFO: Pod "client-envvars-dca55637-3b38-42ea-9816-5613b4c9cbc8": Phase="Pending", Reason="", readiness=false. Elapsed: 85.458733ms
Jan 14 16:12:46.296: INFO: Pod "client-envvars-dca55637-3b38-42ea-9816-5613b4c9cbc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.118307388s
Jan 14 16:12:48.300: INFO: Pod "client-envvars-dca55637-3b38-42ea-9816-5613b4c9cbc8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.123064635s
Jan 14 16:12:50.752: INFO: Pod "client-envvars-dca55637-3b38-42ea-9816-5613b4c9cbc8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.575242173s
Jan 14 16:12:53.113: INFO: Pod "client-envvars-dca55637-3b38-42ea-9816-5613b4c9cbc8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.936130774s
Jan 14 16:12:55.348: INFO: Pod "client-envvars-dca55637-3b38-42ea-9816-5613b4c9cbc8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.170386093s
Jan 14 16:12:57.376: INFO: Pod "client-envvars-dca55637-3b38-42ea-9816-5613b4c9cbc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.198609736s
STEP: Saw pod success
Jan 14 16:12:57.376: INFO: Pod "client-envvars-dca55637-3b38-42ea-9816-5613b4c9cbc8" satisfied condition "success or failure"
Jan 14 16:12:57.383: INFO: Trying to get logs from node slave3 pod client-envvars-dca55637-3b38-42ea-9816-5613b4c9cbc8 container env3cont: <nil>
STEP: delete the pod
Jan 14 16:12:58.673: INFO: Waiting for pod client-envvars-dca55637-3b38-42ea-9816-5613b4c9cbc8 to disappear
Jan 14 16:12:58.965: INFO: Pod client-envvars-dca55637-3b38-42ea-9816-5613b4c9cbc8 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:12:58.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8200" for this suite.

â€¢ [SLOW TEST:31.210 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":278,"completed":171,"skipped":2574,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:12:59.809: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6304
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jan 14 16:13:16.840: INFO: Successfully updated pod "labelsupdate518ecb67-f742-4319-a394-9bc07cbd136b"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:13:18.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6304" for this suite.

â€¢ [SLOW TEST:19.444 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":278,"completed":172,"skipped":2605,"failed":0}
SSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:13:19.253: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-7606
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:172
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating server pod server in namespace prestop-7606
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-7606
STEP: Deleting pre-stop pod
Jan 14 16:13:49.223: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:13:49.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-7606" for this suite.

â€¢ [SLOW TEST:30.886 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":278,"completed":173,"skipped":2612,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:13:50.139: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6782
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 14 16:13:57.306: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 14 16:13:59.679: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615238, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615238, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615238, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615236, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:14:01.792: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615238, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615238, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615238, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615236, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:14:03.881: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615238, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615238, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615238, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615236, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:14:05.689: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615238, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615238, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615238, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615236, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 14 16:14:09.391: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jan 14 16:14:09.501: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:14:09.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6782" for this suite.
STEP: Destroying namespace "webhook-6782-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:22.364 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":278,"completed":174,"skipped":2633,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:14:12.504: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8713
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-fptx
STEP: Creating a pod to test atomic-volume-subpath
Jan 14 16:14:17.128: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-fptx" in namespace "subpath-8713" to be "success or failure"
Jan 14 16:14:17.135: INFO: Pod "pod-subpath-test-configmap-fptx": Phase="Pending", Reason="", readiness=false. Elapsed: 7.583118ms
Jan 14 16:14:19.410: INFO: Pod "pod-subpath-test-configmap-fptx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.282120585s
Jan 14 16:14:22.070: INFO: Pod "pod-subpath-test-configmap-fptx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.942000958s
Jan 14 16:14:24.080: INFO: Pod "pod-subpath-test-configmap-fptx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.952048481s
Jan 14 16:14:26.339: INFO: Pod "pod-subpath-test-configmap-fptx": Phase="Running", Reason="", readiness=true. Elapsed: 9.211549125s
Jan 14 16:14:28.344: INFO: Pod "pod-subpath-test-configmap-fptx": Phase="Running", Reason="", readiness=true. Elapsed: 11.215962398s
Jan 14 16:14:30.360: INFO: Pod "pod-subpath-test-configmap-fptx": Phase="Running", Reason="", readiness=true. Elapsed: 13.232179565s
Jan 14 16:14:32.400: INFO: Pod "pod-subpath-test-configmap-fptx": Phase="Running", Reason="", readiness=true. Elapsed: 15.271954013s
Jan 14 16:14:34.480: INFO: Pod "pod-subpath-test-configmap-fptx": Phase="Running", Reason="", readiness=true. Elapsed: 17.351790847s
Jan 14 16:14:36.565: INFO: Pod "pod-subpath-test-configmap-fptx": Phase="Running", Reason="", readiness=true. Elapsed: 19.437204727s
Jan 14 16:14:38.571: INFO: Pod "pod-subpath-test-configmap-fptx": Phase="Running", Reason="", readiness=true. Elapsed: 21.442757052s
Jan 14 16:14:40.683: INFO: Pod "pod-subpath-test-configmap-fptx": Phase="Running", Reason="", readiness=true. Elapsed: 23.555284372s
Jan 14 16:14:42.688: INFO: Pod "pod-subpath-test-configmap-fptx": Phase="Running", Reason="", readiness=true. Elapsed: 25.560196395s
Jan 14 16:14:45.002: INFO: Pod "pod-subpath-test-configmap-fptx": Phase="Running", Reason="", readiness=true. Elapsed: 27.874457576s
Jan 14 16:14:47.025: INFO: Pod "pod-subpath-test-configmap-fptx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 29.897291316s
STEP: Saw pod success
Jan 14 16:14:47.028: INFO: Pod "pod-subpath-test-configmap-fptx" satisfied condition "success or failure"
Jan 14 16:14:47.312: INFO: Trying to get logs from node slave2 pod pod-subpath-test-configmap-fptx container test-container-subpath-configmap-fptx: <nil>
STEP: delete the pod
Jan 14 16:14:47.980: INFO: Waiting for pod pod-subpath-test-configmap-fptx to disappear
Jan 14 16:14:48.283: INFO: Pod pod-subpath-test-configmap-fptx no longer exists
STEP: Deleting pod pod-subpath-test-configmap-fptx
Jan 14 16:14:48.283: INFO: Deleting pod "pod-subpath-test-configmap-fptx" in namespace "subpath-8713"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:14:48.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8713" for this suite.

â€¢ [SLOW TEST:36.330 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":278,"completed":175,"skipped":2654,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:14:48.834: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4816
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:15:09.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4816" for this suite.

â€¢ [SLOW TEST:21.180 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":278,"completed":176,"skipped":2659,"failed":0}
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:15:10.014: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3188
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-8940895d-ec15-4f93-8446-34463a0786a9
STEP: Creating a pod to test consume configMaps
Jan 14 16:15:11.412: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-957ff070-6924-4577-a672-253c074b8af0" in namespace "projected-3188" to be "success or failure"
Jan 14 16:15:11.597: INFO: Pod "pod-projected-configmaps-957ff070-6924-4577-a672-253c074b8af0": Phase="Pending", Reason="", readiness=false. Elapsed: 184.774051ms
Jan 14 16:15:13.602: INFO: Pod "pod-projected-configmaps-957ff070-6924-4577-a672-253c074b8af0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.190127109s
Jan 14 16:15:15.895: INFO: Pod "pod-projected-configmaps-957ff070-6924-4577-a672-253c074b8af0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.483570874s
Jan 14 16:15:18.051: INFO: Pod "pod-projected-configmaps-957ff070-6924-4577-a672-253c074b8af0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.638860098s
Jan 14 16:15:20.090: INFO: Pod "pod-projected-configmaps-957ff070-6924-4577-a672-253c074b8af0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.678027243s
Jan 14 16:15:22.229: INFO: Pod "pod-projected-configmaps-957ff070-6924-4577-a672-253c074b8af0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.816671104s
Jan 14 16:15:24.346: INFO: Pod "pod-projected-configmaps-957ff070-6924-4577-a672-253c074b8af0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.933975772s
STEP: Saw pod success
Jan 14 16:15:24.346: INFO: Pod "pod-projected-configmaps-957ff070-6924-4577-a672-253c074b8af0" satisfied condition "success or failure"
Jan 14 16:15:24.365: INFO: Trying to get logs from node slave2 pod pod-projected-configmaps-957ff070-6924-4577-a672-253c074b8af0 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 14 16:15:24.420: INFO: Waiting for pod pod-projected-configmaps-957ff070-6924-4577-a672-253c074b8af0 to disappear
Jan 14 16:15:24.423: INFO: Pod pod-projected-configmaps-957ff070-6924-4577-a672-253c074b8af0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:15:24.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3188" for this suite.

â€¢ [SLOW TEST:14.723 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":278,"completed":177,"skipped":2659,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:15:24.738: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6216
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-6216
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-6216
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6216
Jan 14 16:15:27.614: INFO: Found 0 stateful pods, waiting for 1
Jan 14 16:15:38.405: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jan 14 16:15:38.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-6216 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 14 16:15:39.536: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 14 16:15:39.536: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 14 16:15:39.536: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 14 16:15:39.543: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 14 16:15:49.548: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 14 16:15:49.548: INFO: Waiting for statefulset status.replicas updated to 0
Jan 14 16:15:50.198: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999598s
Jan 14 16:15:51.205: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.447893426s
Jan 14 16:15:52.213: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.44081353s
Jan 14 16:15:53.278: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.432497568s
Jan 14 16:15:54.385: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.367519224s
Jan 14 16:15:55.595: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.260516423s
Jan 14 16:15:56.604: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.050040836s
Jan 14 16:15:57.611: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.041114425s
Jan 14 16:15:58.679: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.034822197s
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6216
Jan 14 16:15:59.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-6216 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 16:16:00.516: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 14 16:16:00.516: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 14 16:16:00.516: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 14 16:16:00.545: INFO: Found 1 stateful pods, waiting for 3
Jan 14 16:16:10.853: INFO: Found 2 stateful pods, waiting for 3
Jan 14 16:16:20.553: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 14 16:16:20.553: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 14 16:16:20.553: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Jan 14 16:16:31.127: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 14 16:16:31.127: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 14 16:16:31.127: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jan 14 16:16:31.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-6216 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 14 16:16:31.782: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 14 16:16:31.782: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 14 16:16:31.782: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 14 16:16:31.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-6216 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 14 16:16:32.730: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 14 16:16:32.730: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 14 16:16:32.730: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 14 16:16:32.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-6216 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 14 16:16:33.836: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 14 16:16:33.836: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 14 16:16:33.836: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 14 16:16:33.836: INFO: Waiting for statefulset status.replicas updated to 0
Jan 14 16:16:34.651: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan 14 16:16:44.832: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 14 16:16:44.832: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 14 16:16:44.832: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 14 16:16:44.870: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999702s
Jan 14 16:16:45.904: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.974402828s
Jan 14 16:16:46.909: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.940324377s
Jan 14 16:16:47.999: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.935323422s
Jan 14 16:16:49.056: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.845372217s
Jan 14 16:16:50.145: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.788617238s
Jan 14 16:16:51.285: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.698904342s
Jan 14 16:16:52.771: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.559494249s
Jan 14 16:16:53.776: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.073745111s
Jan 14 16:16:54.782: INFO: Verifying statefulset ss doesn't scale past 3 for another 68.606448ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6216
Jan 14 16:16:55.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-6216 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 16:16:56.788: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 14 16:16:56.788: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 14 16:16:56.788: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 14 16:16:56.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-6216 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 16:16:57.758: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 14 16:16:57.758: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 14 16:16:57.758: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 14 16:16:57.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-6216 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 16:16:58.310: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 14 16:16:58.310: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 14 16:16:58.310: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 14 16:16:58.310: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jan 14 16:17:59.001: INFO: Deleting all statefulset in ns statefulset-6216
Jan 14 16:17:59.006: INFO: Scaling statefulset ss to 0
Jan 14 16:17:59.027: INFO: Waiting for statefulset status.replicas updated to 0
Jan 14 16:17:59.035: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:17:59.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6216" for this suite.

â€¢ [SLOW TEST:154.354 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":278,"completed":178,"skipped":2675,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:17:59.096: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9762
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-acdd7422-cbb3-42c4-89d5-5d0f5b441e20
STEP: Creating a pod to test consume configMaps
Jan 14 16:18:01.013: INFO: Waiting up to 5m0s for pod "pod-configmaps-7b7d7625-2351-4e16-bf12-79f6c96fb16d" in namespace "configmap-9762" to be "success or failure"
Jan 14 16:18:01.185: INFO: Pod "pod-configmaps-7b7d7625-2351-4e16-bf12-79f6c96fb16d": Phase="Pending", Reason="", readiness=false. Elapsed: 171.606712ms
Jan 14 16:18:03.272: INFO: Pod "pod-configmaps-7b7d7625-2351-4e16-bf12-79f6c96fb16d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.258485304s
Jan 14 16:18:05.674: INFO: Pod "pod-configmaps-7b7d7625-2351-4e16-bf12-79f6c96fb16d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.660862736s
Jan 14 16:18:08.346: INFO: Pod "pod-configmaps-7b7d7625-2351-4e16-bf12-79f6c96fb16d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.332970536s
Jan 14 16:18:10.379: INFO: Pod "pod-configmaps-7b7d7625-2351-4e16-bf12-79f6c96fb16d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.365388702s
Jan 14 16:18:12.839: INFO: Pod "pod-configmaps-7b7d7625-2351-4e16-bf12-79f6c96fb16d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.825567144s
STEP: Saw pod success
Jan 14 16:18:12.839: INFO: Pod "pod-configmaps-7b7d7625-2351-4e16-bf12-79f6c96fb16d" satisfied condition "success or failure"
Jan 14 16:18:12.846: INFO: Trying to get logs from node slave2 pod pod-configmaps-7b7d7625-2351-4e16-bf12-79f6c96fb16d container configmap-volume-test: <nil>
STEP: delete the pod
Jan 14 16:18:14.437: INFO: Waiting for pod pod-configmaps-7b7d7625-2351-4e16-bf12-79f6c96fb16d to disappear
Jan 14 16:18:14.659: INFO: Pod pod-configmaps-7b7d7625-2351-4e16-bf12-79f6c96fb16d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:18:14.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9762" for this suite.

â€¢ [SLOW TEST:16.215 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":278,"completed":179,"skipped":2702,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:18:15.322: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-2462
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 16:18:19.203: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Creating first CR 
Jan 14 16:18:25.560: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-14T16:18:25Z generation:1 name:name1 resourceVersion:120771 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:84ff5a96-98df-4124-8d34-e52f5b3a2836] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jan 14 16:18:35.907: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-14T16:18:35Z generation:1 name:name2 resourceVersion:120802 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:4d9753a9-e0ab-43af-8a30-e80397d678b1] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jan 14 16:18:46.223: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-14T16:18:25Z generation:2 name:name1 resourceVersion:120831 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:84ff5a96-98df-4124-8d34-e52f5b3a2836] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jan 14 16:18:56.358: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-14T16:18:35Z generation:2 name:name2 resourceVersion:120857 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:4d9753a9-e0ab-43af-8a30-e80397d678b1] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jan 14 16:19:07.151: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-14T16:18:25Z generation:2 name:name1 resourceVersion:120885 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:84ff5a96-98df-4124-8d34-e52f5b3a2836] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jan 14 16:19:17.667: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-14T16:18:35Z generation:2 name:name2 resourceVersion:120913 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:4d9753a9-e0ab-43af-8a30-e80397d678b1] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:19:28.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-2462" for this suite.

â€¢ [SLOW TEST:73.770 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:41
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":278,"completed":180,"skipped":2742,"failed":0}
SSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:19:29.093: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-8782
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jan 14 16:20:03.446: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8782 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 16:20:03.446: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 16:20:04.748: INFO: Exec stderr: ""
Jan 14 16:20:04.748: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8782 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 16:20:04.748: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 16:20:05.471: INFO: Exec stderr: ""
Jan 14 16:20:05.471: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8782 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 16:20:05.471: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 16:20:05.920: INFO: Exec stderr: ""
Jan 14 16:20:05.920: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8782 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 16:20:05.920: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 16:20:06.383: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jan 14 16:20:06.383: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8782 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 16:20:06.383: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 16:20:06.698: INFO: Exec stderr: ""
Jan 14 16:20:06.698: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8782 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 16:20:06.698: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 16:20:07.017: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jan 14 16:20:07.018: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8782 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 16:20:07.018: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 16:20:07.539: INFO: Exec stderr: ""
Jan 14 16:20:07.539: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8782 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 16:20:07.540: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 16:20:08.316: INFO: Exec stderr: ""
Jan 14 16:20:08.316: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8782 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 16:20:08.316: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 16:20:09.278: INFO: Exec stderr: ""
Jan 14 16:20:09.278: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8782 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 16:20:09.278: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 16:20:09.974: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:20:09.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-8782" for this suite.

â€¢ [SLOW TEST:40.944 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":181,"skipped":2747,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:20:10.038: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6472
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:20:28.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6472" for this suite.

â€¢ [SLOW TEST:19.849 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":278,"completed":182,"skipped":2753,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:20:29.887: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1483
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-f06b9bfd-87a4-4776-ac96-05ebf1ba0882
STEP: Creating a pod to test consume secrets
Jan 14 16:20:31.904: INFO: Waiting up to 5m0s for pod "pod-secrets-110b7a85-0104-4d19-aba9-8c4f53e8a89b" in namespace "secrets-1483" to be "success or failure"
Jan 14 16:20:31.909: INFO: Pod "pod-secrets-110b7a85-0104-4d19-aba9-8c4f53e8a89b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.989676ms
Jan 14 16:20:33.987: INFO: Pod "pod-secrets-110b7a85-0104-4d19-aba9-8c4f53e8a89b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.082527422s
Jan 14 16:20:36.215: INFO: Pod "pod-secrets-110b7a85-0104-4d19-aba9-8c4f53e8a89b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.311254928s
Jan 14 16:20:38.234: INFO: Pod "pod-secrets-110b7a85-0104-4d19-aba9-8c4f53e8a89b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.329607766s
Jan 14 16:20:40.239: INFO: Pod "pod-secrets-110b7a85-0104-4d19-aba9-8c4f53e8a89b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.334416731s
Jan 14 16:20:42.251: INFO: Pod "pod-secrets-110b7a85-0104-4d19-aba9-8c4f53e8a89b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.347282165s
Jan 14 16:20:44.620: INFO: Pod "pod-secrets-110b7a85-0104-4d19-aba9-8c4f53e8a89b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.71572888s
STEP: Saw pod success
Jan 14 16:20:44.620: INFO: Pod "pod-secrets-110b7a85-0104-4d19-aba9-8c4f53e8a89b" satisfied condition "success or failure"
Jan 14 16:20:44.629: INFO: Trying to get logs from node slave1 pod pod-secrets-110b7a85-0104-4d19-aba9-8c4f53e8a89b container secret-volume-test: <nil>
STEP: delete the pod
Jan 14 16:20:45.773: INFO: Waiting for pod pod-secrets-110b7a85-0104-4d19-aba9-8c4f53e8a89b to disappear
Jan 14 16:20:45.786: INFO: Pod pod-secrets-110b7a85-0104-4d19-aba9-8c4f53e8a89b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:20:45.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1483" for this suite.

â€¢ [SLOW TEST:16.695 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":278,"completed":183,"skipped":2755,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:20:46.581: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-279
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1841
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan 14 16:20:49.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-279'
Jan 14 16:21:01.534: INFO: stderr: ""
Jan 14 16:21:01.534: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1846
Jan 14 16:21:02.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 delete pods e2e-test-httpd-pod --namespace=kubectl-279'
Jan 14 16:21:12.691: INFO: stderr: ""
Jan 14 16:21:12.691: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:21:12.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-279" for this suite.

â€¢ [SLOW TEST:26.377 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1837
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":278,"completed":184,"skipped":2758,"failed":0}
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:21:12.959: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4012
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 14 16:21:16.125: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3cfd4d88-c71a-465e-9d14-aaf51a521c65" in namespace "downward-api-4012" to be "success or failure"
Jan 14 16:21:16.592: INFO: Pod "downwardapi-volume-3cfd4d88-c71a-465e-9d14-aaf51a521c65": Phase="Pending", Reason="", readiness=false. Elapsed: 466.848239ms
Jan 14 16:21:18.673: INFO: Pod "downwardapi-volume-3cfd4d88-c71a-465e-9d14-aaf51a521c65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.547881361s
Jan 14 16:21:20.965: INFO: Pod "downwardapi-volume-3cfd4d88-c71a-465e-9d14-aaf51a521c65": Phase="Pending", Reason="", readiness=false. Elapsed: 4.839832249s
Jan 14 16:21:23.330: INFO: Pod "downwardapi-volume-3cfd4d88-c71a-465e-9d14-aaf51a521c65": Phase="Pending", Reason="", readiness=false. Elapsed: 7.204963671s
Jan 14 16:21:25.619: INFO: Pod "downwardapi-volume-3cfd4d88-c71a-465e-9d14-aaf51a521c65": Phase="Pending", Reason="", readiness=false. Elapsed: 9.49394974s
Jan 14 16:21:27.716: INFO: Pod "downwardapi-volume-3cfd4d88-c71a-465e-9d14-aaf51a521c65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.591420607s
STEP: Saw pod success
Jan 14 16:21:27.721: INFO: Pod "downwardapi-volume-3cfd4d88-c71a-465e-9d14-aaf51a521c65" satisfied condition "success or failure"
Jan 14 16:21:27.798: INFO: Trying to get logs from node slave2 pod downwardapi-volume-3cfd4d88-c71a-465e-9d14-aaf51a521c65 container client-container: <nil>
STEP: delete the pod
Jan 14 16:21:28.111: INFO: Waiting for pod downwardapi-volume-3cfd4d88-c71a-465e-9d14-aaf51a521c65 to disappear
Jan 14 16:21:28.114: INFO: Pod downwardapi-volume-3cfd4d88-c71a-465e-9d14-aaf51a521c65 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:21:28.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4012" for this suite.

â€¢ [SLOW TEST:16.821 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":185,"skipped":2758,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:21:29.780: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8800
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0114 16:21:37.904554      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 14 16:21:37.904: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:21:37.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8800" for this suite.

â€¢ [SLOW TEST:8.680 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":278,"completed":186,"skipped":2769,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:21:38.461: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-546
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:22:05.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-546" for this suite.

â€¢ [SLOW TEST:26.993 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":278,"completed":187,"skipped":2843,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:22:05.455: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8445
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating api versions
Jan 14 16:22:07.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 api-versions'
Jan 14 16:22:08.486: INFO: stderr: ""
Jan 14 16:22:08.486: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\nvolumesnapshot.external-storage.k8s.io/v1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:22:08.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8445" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":278,"completed":188,"skipped":2848,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:22:08.893: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1365
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jan 14 16:22:10.838: INFO: Waiting up to 5m0s for pod "downward-api-e08fb6aa-163c-42aa-84db-885170588d68" in namespace "downward-api-1365" to be "success or failure"
Jan 14 16:22:11.075: INFO: Pod "downward-api-e08fb6aa-163c-42aa-84db-885170588d68": Phase="Pending", Reason="", readiness=false. Elapsed: 236.933392ms
Jan 14 16:22:13.589: INFO: Pod "downward-api-e08fb6aa-163c-42aa-84db-885170588d68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.751179822s
Jan 14 16:22:16.293: INFO: Pod "downward-api-e08fb6aa-163c-42aa-84db-885170588d68": Phase="Pending", Reason="", readiness=false. Elapsed: 5.455026193s
Jan 14 16:22:18.392: INFO: Pod "downward-api-e08fb6aa-163c-42aa-84db-885170588d68": Phase="Pending", Reason="", readiness=false. Elapsed: 7.554222138s
Jan 14 16:22:20.556: INFO: Pod "downward-api-e08fb6aa-163c-42aa-84db-885170588d68": Phase="Pending", Reason="", readiness=false. Elapsed: 9.718766491s
Jan 14 16:22:22.755: INFO: Pod "downward-api-e08fb6aa-163c-42aa-84db-885170588d68": Phase="Pending", Reason="", readiness=false. Elapsed: 11.917628342s
Jan 14 16:22:24.892: INFO: Pod "downward-api-e08fb6aa-163c-42aa-84db-885170588d68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.053945615s
STEP: Saw pod success
Jan 14 16:22:24.892: INFO: Pod "downward-api-e08fb6aa-163c-42aa-84db-885170588d68" satisfied condition "success or failure"
Jan 14 16:22:24.896: INFO: Trying to get logs from node slave2 pod downward-api-e08fb6aa-163c-42aa-84db-885170588d68 container dapi-container: <nil>
STEP: delete the pod
Jan 14 16:22:25.493: INFO: Waiting for pod downward-api-e08fb6aa-163c-42aa-84db-885170588d68 to disappear
Jan 14 16:22:25.625: INFO: Pod downward-api-e08fb6aa-163c-42aa-84db-885170588d68 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:22:25.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1365" for this suite.

â€¢ [SLOW TEST:17.331 seconds]
[sig-node] Downward API
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:33
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":278,"completed":189,"skipped":2856,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:22:26.225: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9373
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 14 16:22:32.776: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 14 16:22:36.182: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615753, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615753, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615754, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615752, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:22:38.191: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615753, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615753, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615754, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615752, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:22:40.461: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615753, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615753, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615754, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615752, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:22:42.337: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615753, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615753, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615754, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615752, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:22:44.187: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615753, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615753, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615754, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714615752, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 14 16:22:48.287: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 16:22:48.445: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-374-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:22:58.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9373" for this suite.
STEP: Destroying namespace "webhook-9373-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:35.567 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":278,"completed":190,"skipped":2879,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:23:01.792: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-4933
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jan 14 16:23:17.941: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:23:18.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4933" for this suite.

â€¢ [SLOW TEST:19.082 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":278,"completed":191,"skipped":2893,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:23:20.874: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1020
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:23:25.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1020" for this suite.

â€¢ [SLOW TEST:5.063 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":278,"completed":192,"skipped":2915,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:23:25.938: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-1230
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 16:23:27.794: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-e42d1ee3-f9a3-4d15-913b-bb51bc1c5c28" in namespace "security-context-test-1230" to be "success or failure"
Jan 14 16:23:28.006: INFO: Pod "alpine-nnp-false-e42d1ee3-f9a3-4d15-913b-bb51bc1c5c28": Phase="Pending", Reason="", readiness=false. Elapsed: 211.626827ms
Jan 14 16:23:30.796: INFO: Pod "alpine-nnp-false-e42d1ee3-f9a3-4d15-913b-bb51bc1c5c28": Phase="Pending", Reason="", readiness=false. Elapsed: 3.001780474s
Jan 14 16:23:32.956: INFO: Pod "alpine-nnp-false-e42d1ee3-f9a3-4d15-913b-bb51bc1c5c28": Phase="Pending", Reason="", readiness=false. Elapsed: 5.16231936s
Jan 14 16:23:35.669: INFO: Pod "alpine-nnp-false-e42d1ee3-f9a3-4d15-913b-bb51bc1c5c28": Phase="Pending", Reason="", readiness=false. Elapsed: 7.874561009s
Jan 14 16:23:37.804: INFO: Pod "alpine-nnp-false-e42d1ee3-f9a3-4d15-913b-bb51bc1c5c28": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009661036s
Jan 14 16:23:40.163: INFO: Pod "alpine-nnp-false-e42d1ee3-f9a3-4d15-913b-bb51bc1c5c28": Phase="Pending", Reason="", readiness=false. Elapsed: 12.368759496s
Jan 14 16:23:42.180: INFO: Pod "alpine-nnp-false-e42d1ee3-f9a3-4d15-913b-bb51bc1c5c28": Phase="Pending", Reason="", readiness=false. Elapsed: 14.38644208s
Jan 14 16:23:44.237: INFO: Pod "alpine-nnp-false-e42d1ee3-f9a3-4d15-913b-bb51bc1c5c28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.443250619s
Jan 14 16:23:44.237: INFO: Pod "alpine-nnp-false-e42d1ee3-f9a3-4d15-913b-bb51bc1c5c28" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:23:44.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1230" for this suite.

â€¢ [SLOW TEST:19.029 seconds]
[k8s.io] Security Context
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:289
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":193,"skipped":2927,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:23:44.972: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7221
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-downwardapi-cb9f
STEP: Creating a pod to test atomic-volume-subpath
Jan 14 16:23:48.559: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-cb9f" in namespace "subpath-7221" to be "success or failure"
Jan 14 16:23:48.569: INFO: Pod "pod-subpath-test-downwardapi-cb9f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.783167ms
Jan 14 16:23:50.820: INFO: Pod "pod-subpath-test-downwardapi-cb9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.260843023s
Jan 14 16:23:52.831: INFO: Pod "pod-subpath-test-downwardapi-cb9f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.272118978s
Jan 14 16:23:54.975: INFO: Pod "pod-subpath-test-downwardapi-cb9f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.415545814s
Jan 14 16:23:57.298: INFO: Pod "pod-subpath-test-downwardapi-cb9f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.738679668s
Jan 14 16:23:59.303: INFO: Pod "pod-subpath-test-downwardapi-cb9f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.743895691s
Jan 14 16:24:01.343: INFO: Pod "pod-subpath-test-downwardapi-cb9f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.783536124s
Jan 14 16:24:03.394: INFO: Pod "pod-subpath-test-downwardapi-cb9f": Phase="Running", Reason="", readiness=true. Elapsed: 14.834958013s
Jan 14 16:24:05.403: INFO: Pod "pod-subpath-test-downwardapi-cb9f": Phase="Running", Reason="", readiness=true. Elapsed: 16.844168623s
Jan 14 16:24:07.538: INFO: Pod "pod-subpath-test-downwardapi-cb9f": Phase="Running", Reason="", readiness=true. Elapsed: 18.979230291s
Jan 14 16:24:09.549: INFO: Pod "pod-subpath-test-downwardapi-cb9f": Phase="Running", Reason="", readiness=true. Elapsed: 20.990270909s
Jan 14 16:24:11.554: INFO: Pod "pod-subpath-test-downwardapi-cb9f": Phase="Running", Reason="", readiness=true. Elapsed: 22.99490127s
Jan 14 16:24:13.561: INFO: Pod "pod-subpath-test-downwardapi-cb9f": Phase="Running", Reason="", readiness=true. Elapsed: 25.001413544s
Jan 14 16:24:15.570: INFO: Pod "pod-subpath-test-downwardapi-cb9f": Phase="Running", Reason="", readiness=true. Elapsed: 27.011293705s
Jan 14 16:24:17.649: INFO: Pod "pod-subpath-test-downwardapi-cb9f": Phase="Running", Reason="", readiness=true. Elapsed: 29.089913775s
Jan 14 16:24:19.665: INFO: Pod "pod-subpath-test-downwardapi-cb9f": Phase="Running", Reason="", readiness=true. Elapsed: 31.105629076s
Jan 14 16:24:21.974: INFO: Pod "pod-subpath-test-downwardapi-cb9f": Phase="Running", Reason="", readiness=true. Elapsed: 33.41497365s
Jan 14 16:24:23.980: INFO: Pod "pod-subpath-test-downwardapi-cb9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 35.420541121s
STEP: Saw pod success
Jan 14 16:24:23.980: INFO: Pod "pod-subpath-test-downwardapi-cb9f" satisfied condition "success or failure"
Jan 14 16:24:23.986: INFO: Trying to get logs from node slave2 pod pod-subpath-test-downwardapi-cb9f container test-container-subpath-downwardapi-cb9f: <nil>
STEP: delete the pod
Jan 14 16:24:24.518: INFO: Waiting for pod pod-subpath-test-downwardapi-cb9f to disappear
Jan 14 16:24:24.595: INFO: Pod pod-subpath-test-downwardapi-cb9f no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-cb9f
Jan 14 16:24:24.595: INFO: Deleting pod "pod-subpath-test-downwardapi-cb9f" in namespace "subpath-7221"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:24:24.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7221" for this suite.

â€¢ [SLOW TEST:39.938 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":278,"completed":194,"skipped":2961,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:24:24.911: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6021
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 14 16:24:27.694: INFO: Waiting up to 5m0s for pod "pod-fb9ed943-0551-4203-90ee-9e4735564993" in namespace "emptydir-6021" to be "success or failure"
Jan 14 16:24:27.700: INFO: Pod "pod-fb9ed943-0551-4203-90ee-9e4735564993": Phase="Pending", Reason="", readiness=false. Elapsed: 5.66538ms
Jan 14 16:24:29.764: INFO: Pod "pod-fb9ed943-0551-4203-90ee-9e4735564993": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069427635s
Jan 14 16:24:32.186: INFO: Pod "pod-fb9ed943-0551-4203-90ee-9e4735564993": Phase="Pending", Reason="", readiness=false. Elapsed: 4.491341944s
Jan 14 16:24:34.660: INFO: Pod "pod-fb9ed943-0551-4203-90ee-9e4735564993": Phase="Pending", Reason="", readiness=false. Elapsed: 6.966070744s
Jan 14 16:24:36.744: INFO: Pod "pod-fb9ed943-0551-4203-90ee-9e4735564993": Phase="Pending", Reason="", readiness=false. Elapsed: 9.050019671s
Jan 14 16:24:38.754: INFO: Pod "pod-fb9ed943-0551-4203-90ee-9e4735564993": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.059278075s
STEP: Saw pod success
Jan 14 16:24:38.754: INFO: Pod "pod-fb9ed943-0551-4203-90ee-9e4735564993" satisfied condition "success or failure"
Jan 14 16:24:38.757: INFO: Trying to get logs from node slave2 pod pod-fb9ed943-0551-4203-90ee-9e4735564993 container test-container: <nil>
STEP: delete the pod
Jan 14 16:24:39.075: INFO: Waiting for pod pod-fb9ed943-0551-4203-90ee-9e4735564993 to disappear
Jan 14 16:24:39.080: INFO: Pod pod-fb9ed943-0551-4203-90ee-9e4735564993 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:24:39.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6021" for this suite.

â€¢ [SLOW TEST:14.949 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":195,"skipped":2993,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:24:39.860: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8352
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-0e8fd1da-98ff-445d-bff7-d0b83adaae94
STEP: Creating configMap with name cm-test-opt-upd-e67ba101-5f4a-4f61-a9cc-07d7f59b40b3
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-0e8fd1da-98ff-445d-bff7-d0b83adaae94
STEP: Updating configmap cm-test-opt-upd-e67ba101-5f4a-4f61-a9cc-07d7f59b40b3
STEP: Creating configMap with name cm-test-opt-create-fb25dedd-7c4b-4a86-a48f-204a6b9b6fb8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:26:04.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8352" for this suite.

â€¢ [SLOW TEST:84.732 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":196,"skipped":2999,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:26:04.592: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2078
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:26:20.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2078" for this suite.

â€¢ [SLOW TEST:16.874 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when scheduling a busybox command in a pod
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":278,"completed":197,"skipped":3014,"failed":0}
SSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:26:21.466: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2283
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-2283/configmap-test-4cac2750-dc6e-40ce-9308-bfa08667476b
STEP: Creating a pod to test consume configMaps
Jan 14 16:26:23.286: INFO: Waiting up to 5m0s for pod "pod-configmaps-df633b28-dd71-43fc-87ed-7ce257544595" in namespace "configmap-2283" to be "success or failure"
Jan 14 16:26:23.398: INFO: Pod "pod-configmaps-df633b28-dd71-43fc-87ed-7ce257544595": Phase="Pending", Reason="", readiness=false. Elapsed: 111.830697ms
Jan 14 16:26:25.425: INFO: Pod "pod-configmaps-df633b28-dd71-43fc-87ed-7ce257544595": Phase="Pending", Reason="", readiness=false. Elapsed: 2.139216154s
Jan 14 16:26:27.445: INFO: Pod "pod-configmaps-df633b28-dd71-43fc-87ed-7ce257544595": Phase="Pending", Reason="", readiness=false. Elapsed: 4.159269302s
Jan 14 16:26:29.496: INFO: Pod "pod-configmaps-df633b28-dd71-43fc-87ed-7ce257544595": Phase="Pending", Reason="", readiness=false. Elapsed: 6.209930928s
Jan 14 16:26:31.794: INFO: Pod "pod-configmaps-df633b28-dd71-43fc-87ed-7ce257544595": Phase="Pending", Reason="", readiness=false. Elapsed: 8.508067611s
Jan 14 16:26:34.209: INFO: Pod "pod-configmaps-df633b28-dd71-43fc-87ed-7ce257544595": Phase="Pending", Reason="", readiness=false. Elapsed: 10.922498458s
Jan 14 16:26:36.547: INFO: Pod "pod-configmaps-df633b28-dd71-43fc-87ed-7ce257544595": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.261239585s
STEP: Saw pod success
Jan 14 16:26:36.553: INFO: Pod "pod-configmaps-df633b28-dd71-43fc-87ed-7ce257544595" satisfied condition "success or failure"
Jan 14 16:26:36.557: INFO: Trying to get logs from node slave2 pod pod-configmaps-df633b28-dd71-43fc-87ed-7ce257544595 container env-test: <nil>
STEP: delete the pod
Jan 14 16:26:37.485: INFO: Waiting for pod pod-configmaps-df633b28-dd71-43fc-87ed-7ce257544595 to disappear
Jan 14 16:26:37.721: INFO: Pod pod-configmaps-df633b28-dd71-43fc-87ed-7ce257544595 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:26:37.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2283" for this suite.

â€¢ [SLOW TEST:16.430 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":278,"completed":198,"skipped":3019,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:26:37.899: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-92
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating replication controller my-hostname-basic-9526fc61-b4e4-4eab-ae1d-e502aa0809d8
Jan 14 16:26:39.310: INFO: Pod name my-hostname-basic-9526fc61-b4e4-4eab-ae1d-e502aa0809d8: Found 0 pods out of 1
Jan 14 16:26:44.315: INFO: Pod name my-hostname-basic-9526fc61-b4e4-4eab-ae1d-e502aa0809d8: Found 1 pods out of 1
Jan 14 16:26:44.315: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-9526fc61-b4e4-4eab-ae1d-e502aa0809d8" are running
Jan 14 16:26:50.819: INFO: Pod "my-hostname-basic-9526fc61-b4e4-4eab-ae1d-e502aa0809d8-qwbpx" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-14 16:26:39 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-14 16:26:39 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-9526fc61-b4e4-4eab-ae1d-e502aa0809d8]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-14 16:26:39 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-9526fc61-b4e4-4eab-ae1d-e502aa0809d8]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-14 16:26:39 +0000 UTC Reason: Message:}])
Jan 14 16:26:50.819: INFO: Trying to dial the pod
Jan 14 16:26:56.298: INFO: Controller my-hostname-basic-9526fc61-b4e4-4eab-ae1d-e502aa0809d8: Got expected result from replica 1 [my-hostname-basic-9526fc61-b4e4-4eab-ae1d-e502aa0809d8-qwbpx]: "my-hostname-basic-9526fc61-b4e4-4eab-ae1d-e502aa0809d8-qwbpx", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:26:56.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-92" for this suite.

â€¢ [SLOW TEST:19.457 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":278,"completed":199,"skipped":3037,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:26:57.356: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-5686
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jan 14 16:26:59.896: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the sample API server.
Jan 14 16:27:02.109: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jan 14 16:27:06.802: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616022, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616022, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616023, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616022, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:27:08.999: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616022, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616022, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616023, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616022, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:27:11.098: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616022, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616022, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616023, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616022, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:27:13.600: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616022, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616022, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616023, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616022, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:27:15.073: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616022, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616022, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616023, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616022, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:27:16.955: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616022, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616022, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616023, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616022, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:27:18.808: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616022, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616022, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616023, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616022, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:27:23.418: INFO: Waited 2.229614904s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:27:29.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5686" for this suite.

â€¢ [SLOW TEST:32.152 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]","total":278,"completed":200,"skipped":3051,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:27:29.510: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9424
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-fcb0e510-dd80-43a7-a9dc-8b951f646258
STEP: Creating a pod to test consume secrets
Jan 14 16:27:31.491: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6a2c5d1c-7957-4149-8e6d-433cbdb3b737" in namespace "projected-9424" to be "success or failure"
Jan 14 16:27:31.504: INFO: Pod "pod-projected-secrets-6a2c5d1c-7957-4149-8e6d-433cbdb3b737": Phase="Pending", Reason="", readiness=false. Elapsed: 13.247695ms
Jan 14 16:27:33.510: INFO: Pod "pod-projected-secrets-6a2c5d1c-7957-4149-8e6d-433cbdb3b737": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019335723s
Jan 14 16:27:35.600: INFO: Pod "pod-projected-secrets-6a2c5d1c-7957-4149-8e6d-433cbdb3b737": Phase="Pending", Reason="", readiness=false. Elapsed: 4.109012773s
Jan 14 16:27:37.781: INFO: Pod "pod-projected-secrets-6a2c5d1c-7957-4149-8e6d-433cbdb3b737": Phase="Pending", Reason="", readiness=false. Elapsed: 6.290550558s
Jan 14 16:27:39.839: INFO: Pod "pod-projected-secrets-6a2c5d1c-7957-4149-8e6d-433cbdb3b737": Phase="Pending", Reason="", readiness=false. Elapsed: 8.348448479s
Jan 14 16:27:41.871: INFO: Pod "pod-projected-secrets-6a2c5d1c-7957-4149-8e6d-433cbdb3b737": Phase="Pending", Reason="", readiness=false. Elapsed: 10.379969159s
Jan 14 16:27:44.093: INFO: Pod "pod-projected-secrets-6a2c5d1c-7957-4149-8e6d-433cbdb3b737": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.602058894s
STEP: Saw pod success
Jan 14 16:27:44.093: INFO: Pod "pod-projected-secrets-6a2c5d1c-7957-4149-8e6d-433cbdb3b737" satisfied condition "success or failure"
Jan 14 16:27:44.097: INFO: Trying to get logs from node slave2 pod pod-projected-secrets-6a2c5d1c-7957-4149-8e6d-433cbdb3b737 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 14 16:27:44.576: INFO: Waiting for pod pod-projected-secrets-6a2c5d1c-7957-4149-8e6d-433cbdb3b737 to disappear
Jan 14 16:27:44.581: INFO: Pod pod-projected-secrets-6a2c5d1c-7957-4149-8e6d-433cbdb3b737 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:27:44.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9424" for this suite.

â€¢ [SLOW TEST:15.287 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":201,"skipped":3053,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:27:44.799: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2690
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-2690
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 14 16:27:46.506: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 14 16:28:35.303: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.49.253:8080/dial?request=hostname&protocol=http&host=10.151.161.29&port=8080&tries=1'] Namespace:pod-network-test-2690 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 16:28:35.303: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 16:28:35.716: INFO: Waiting for responses: map[]
Jan 14 16:28:35.798: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.49.253:8080/dial?request=hostname&protocol=http&host=10.151.208.25&port=8080&tries=1'] Namespace:pod-network-test-2690 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 16:28:35.798: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 16:28:36.109: INFO: Waiting for responses: map[]
Jan 14 16:28:36.473: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.49.253:8080/dial?request=hostname&protocol=http&host=10.151.32.28&port=8080&tries=1'] Namespace:pod-network-test-2690 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 16:28:36.473: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 16:28:37.063: INFO: Waiting for responses: map[]
Jan 14 16:28:37.068: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.49.253:8080/dial?request=hostname&protocol=http&host=10.151.51.59&port=8080&tries=1'] Namespace:pod-network-test-2690 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 16:28:37.068: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 16:28:37.654: INFO: Waiting for responses: map[]
Jan 14 16:28:37.726: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.49.253:8080/dial?request=hostname&protocol=http&host=10.151.49.252&port=8080&tries=1'] Namespace:pod-network-test-2690 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 16:28:37.726: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 16:28:38.122: INFO: Waiting for responses: map[]
Jan 14 16:28:38.126: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.49.253:8080/dial?request=hostname&protocol=http&host=10.151.194.108&port=8080&tries=1'] Namespace:pod-network-test-2690 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 16:28:38.126: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 16:28:38.561: INFO: Waiting for responses: map[]
Jan 14 16:28:38.567: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.49.253:8080/dial?request=hostname&protocol=http&host=10.151.26.68&port=8080&tries=1'] Namespace:pod-network-test-2690 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 16:28:38.567: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 16:28:38.897: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:28:38.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2690" for this suite.

â€¢ [SLOW TEST:54.335 seconds]
[sig-network] Networking
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":202,"skipped":3070,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:28:39.134: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-32
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 14 16:28:42.112: INFO: Waiting up to 5m0s for pod "pod-b938fdf2-e492-4281-8123-eac2314f0518" in namespace "emptydir-32" to be "success or failure"
Jan 14 16:28:42.251: INFO: Pod "pod-b938fdf2-e492-4281-8123-eac2314f0518": Phase="Pending", Reason="", readiness=false. Elapsed: 138.994318ms
Jan 14 16:28:44.256: INFO: Pod "pod-b938fdf2-e492-4281-8123-eac2314f0518": Phase="Pending", Reason="", readiness=false. Elapsed: 2.144014094s
Jan 14 16:28:46.315: INFO: Pod "pod-b938fdf2-e492-4281-8123-eac2314f0518": Phase="Pending", Reason="", readiness=false. Elapsed: 4.202661218s
Jan 14 16:28:49.524: INFO: Pod "pod-b938fdf2-e492-4281-8123-eac2314f0518": Phase="Pending", Reason="", readiness=false. Elapsed: 7.411887108s
Jan 14 16:28:52.296: INFO: Pod "pod-b938fdf2-e492-4281-8123-eac2314f0518": Phase="Pending", Reason="", readiness=false. Elapsed: 10.184063442s
Jan 14 16:28:54.321: INFO: Pod "pod-b938fdf2-e492-4281-8123-eac2314f0518": Phase="Pending", Reason="", readiness=false. Elapsed: 12.20901713s
Jan 14 16:28:56.702: INFO: Pod "pod-b938fdf2-e492-4281-8123-eac2314f0518": Phase="Pending", Reason="", readiness=false. Elapsed: 14.589580621s
Jan 14 16:28:58.910: INFO: Pod "pod-b938fdf2-e492-4281-8123-eac2314f0518": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.79796678s
STEP: Saw pod success
Jan 14 16:28:58.910: INFO: Pod "pod-b938fdf2-e492-4281-8123-eac2314f0518" satisfied condition "success or failure"
Jan 14 16:28:58.915: INFO: Trying to get logs from node slave3 pod pod-b938fdf2-e492-4281-8123-eac2314f0518 container test-container: <nil>
STEP: delete the pod
Jan 14 16:28:59.794: INFO: Waiting for pod pod-b938fdf2-e492-4281-8123-eac2314f0518 to disappear
Jan 14 16:28:59.800: INFO: Pod pod-b938fdf2-e492-4281-8123-eac2314f0518 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:28:59.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-32" for this suite.

â€¢ [SLOW TEST:21.082 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":203,"skipped":3083,"failed":0}
SSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:29:00.217: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2032
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-af18824e-55ef-43c8-8e99-9877fbab0bdf in namespace container-probe-2032
Jan 14 16:29:13.199: INFO: Started pod liveness-af18824e-55ef-43c8-8e99-9877fbab0bdf in namespace container-probe-2032
STEP: checking the pod's current state and verifying that restartCount is present
Jan 14 16:29:13.204: INFO: Initial restart count of pod liveness-af18824e-55ef-43c8-8e99-9877fbab0bdf is 0
Jan 14 16:29:32.693: INFO: Restart count of pod container-probe-2032/liveness-af18824e-55ef-43c8-8e99-9877fbab0bdf is now 1 (19.489588412s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:29:33.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2032" for this suite.

â€¢ [SLOW TEST:33.119 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":278,"completed":204,"skipped":3088,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:29:33.336: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8957
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 14 16:29:35.192: INFO: Waiting up to 5m0s for pod "downwardapi-volume-23c580ca-db58-426f-9f0f-f630502e9251" in namespace "downward-api-8957" to be "success or failure"
Jan 14 16:29:35.318: INFO: Pod "downwardapi-volume-23c580ca-db58-426f-9f0f-f630502e9251": Phase="Pending", Reason="", readiness=false. Elapsed: 126.276914ms
Jan 14 16:29:37.609: INFO: Pod "downwardapi-volume-23c580ca-db58-426f-9f0f-f630502e9251": Phase="Pending", Reason="", readiness=false. Elapsed: 2.417315214s
Jan 14 16:29:39.648: INFO: Pod "downwardapi-volume-23c580ca-db58-426f-9f0f-f630502e9251": Phase="Pending", Reason="", readiness=false. Elapsed: 4.45627262s
Jan 14 16:29:42.131: INFO: Pod "downwardapi-volume-23c580ca-db58-426f-9f0f-f630502e9251": Phase="Pending", Reason="", readiness=false. Elapsed: 6.93957712s
Jan 14 16:29:44.136: INFO: Pod "downwardapi-volume-23c580ca-db58-426f-9f0f-f630502e9251": Phase="Pending", Reason="", readiness=false. Elapsed: 8.944554393s
Jan 14 16:29:46.276: INFO: Pod "downwardapi-volume-23c580ca-db58-426f-9f0f-f630502e9251": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.083831884s
STEP: Saw pod success
Jan 14 16:29:46.276: INFO: Pod "downwardapi-volume-23c580ca-db58-426f-9f0f-f630502e9251" satisfied condition "success or failure"
Jan 14 16:29:46.287: INFO: Trying to get logs from node slave2 pod downwardapi-volume-23c580ca-db58-426f-9f0f-f630502e9251 container client-container: <nil>
STEP: delete the pod
Jan 14 16:29:47.072: INFO: Waiting for pod downwardapi-volume-23c580ca-db58-426f-9f0f-f630502e9251 to disappear
Jan 14 16:29:47.076: INFO: Pod downwardapi-volume-23c580ca-db58-426f-9f0f-f630502e9251 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:29:47.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8957" for this suite.

â€¢ [SLOW TEST:14.152 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":278,"completed":205,"skipped":3093,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:29:47.488: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4799
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 16:29:49.336: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jan 14 16:29:50.066: INFO: Number of nodes with available pods: 0
Jan 14 16:29:50.066: INFO: Node master1 is running more than one daemon pod
Jan 14 16:29:52.100: INFO: Number of nodes with available pods: 0
Jan 14 16:29:52.100: INFO: Node master1 is running more than one daemon pod
Jan 14 16:29:55.291: INFO: Number of nodes with available pods: 0
Jan 14 16:29:55.291: INFO: Node master1 is running more than one daemon pod
Jan 14 16:29:56.657: INFO: Number of nodes with available pods: 0
Jan 14 16:29:56.663: INFO: Node master1 is running more than one daemon pod
Jan 14 16:29:57.798: INFO: Number of nodes with available pods: 0
Jan 14 16:29:57.798: INFO: Node master1 is running more than one daemon pod
Jan 14 16:29:58.354: INFO: Number of nodes with available pods: 0
Jan 14 16:29:58.354: INFO: Node master1 is running more than one daemon pod
Jan 14 16:30:00.304: INFO: Number of nodes with available pods: 0
Jan 14 16:30:00.304: INFO: Node master1 is running more than one daemon pod
Jan 14 16:30:02.480: INFO: Number of nodes with available pods: 1
Jan 14 16:30:02.480: INFO: Node master1 is running more than one daemon pod
Jan 14 16:30:04.000: INFO: Number of nodes with available pods: 2
Jan 14 16:30:04.000: INFO: Node master1 is running more than one daemon pod
Jan 14 16:30:07.341: INFO: Number of nodes with available pods: 2
Jan 14 16:30:07.342: INFO: Node master1 is running more than one daemon pod
Jan 14 16:30:09.501: INFO: Number of nodes with available pods: 3
Jan 14 16:30:09.502: INFO: Node master1 is running more than one daemon pod
Jan 14 16:30:11.769: INFO: Number of nodes with available pods: 5
Jan 14 16:30:11.769: INFO: Node master3 is running more than one daemon pod
Jan 14 16:30:12.169: INFO: Number of nodes with available pods: 5
Jan 14 16:30:12.169: INFO: Node master3 is running more than one daemon pod
Jan 14 16:30:13.520: INFO: Number of nodes with available pods: 6
Jan 14 16:30:13.520: INFO: Node slave4 is running more than one daemon pod
Jan 14 16:30:14.535: INFO: Number of nodes with available pods: 6
Jan 14 16:30:14.536: INFO: Node slave4 is running more than one daemon pod
Jan 14 16:30:15.125: INFO: Number of nodes with available pods: 6
Jan 14 16:30:15.126: INFO: Node slave4 is running more than one daemon pod
Jan 14 16:30:16.898: INFO: Number of nodes with available pods: 6
Jan 14 16:30:16.899: INFO: Node slave4 is running more than one daemon pod
Jan 14 16:30:17.500: INFO: Number of nodes with available pods: 7
Jan 14 16:30:17.500: INFO: Number of running nodes: 7, number of available pods: 7
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jan 14 16:30:18.676: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:18.676: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:18.676: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:18.676: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:18.676: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:18.676: INFO: Wrong image for pod: daemon-set-n7qxs. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:18.676: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:19.698: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:19.698: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:19.698: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:19.699: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:19.699: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:19.699: INFO: Wrong image for pod: daemon-set-n7qxs. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:19.699: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:20.989: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:20.989: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:20.989: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:20.989: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:20.989: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:20.989: INFO: Wrong image for pod: daemon-set-n7qxs. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:20.989: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:22.102: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:22.103: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:22.105: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:22.105: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:22.105: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:22.105: INFO: Wrong image for pod: daemon-set-n7qxs. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:22.105: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:22.704: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:22.704: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:22.704: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:22.704: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:22.704: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:22.704: INFO: Wrong image for pod: daemon-set-n7qxs. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:22.704: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:23.699: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:23.699: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:23.699: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:23.699: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:23.699: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:23.699: INFO: Wrong image for pod: daemon-set-n7qxs. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:23.699: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:24.991: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:24.992: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:24.992: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:24.992: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:24.992: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:24.992: INFO: Wrong image for pod: daemon-set-n7qxs. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:24.992: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:25.924: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:25.924: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:25.924: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:25.924: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:25.924: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:25.924: INFO: Wrong image for pod: daemon-set-n7qxs. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:25.924: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:27.172: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:27.172: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:27.172: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:27.172: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:27.172: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:27.172: INFO: Wrong image for pod: daemon-set-n7qxs. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:27.172: INFO: Pod daemon-set-n7qxs is not available
Jan 14 16:30:27.172: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:28.211: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:28.211: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:28.211: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:28.211: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:28.211: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:28.211: INFO: Pod daemon-set-vhrzp is not available
Jan 14 16:30:28.211: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:29.809: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:29.809: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:29.809: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:29.809: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:29.809: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:29.809: INFO: Pod daemon-set-vhrzp is not available
Jan 14 16:30:29.809: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:30.741: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:30.741: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:30.741: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:30.741: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:30.741: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:30.741: INFO: Pod daemon-set-vhrzp is not available
Jan 14 16:30:30.741: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:31.700: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:31.700: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:31.700: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:31.700: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:31.700: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:31.700: INFO: Pod daemon-set-vhrzp is not available
Jan 14 16:30:31.700: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:32.704: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:32.704: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:32.704: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:32.704: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:32.704: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:32.704: INFO: Pod daemon-set-vhrzp is not available
Jan 14 16:30:32.704: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:34.238: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:34.238: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:34.238: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:34.238: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:34.238: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:34.238: INFO: Pod daemon-set-vhrzp is not available
Jan 14 16:30:34.238: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:34.806: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:34.806: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:34.806: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:34.806: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:34.806: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:34.806: INFO: Pod daemon-set-vhrzp is not available
Jan 14 16:30:34.806: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:36.180: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:36.180: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:36.180: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:36.181: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:36.181: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:36.181: INFO: Pod daemon-set-vhrzp is not available
Jan 14 16:30:36.181: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:37.772: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:37.772: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:37.772: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:37.772: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:37.772: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:37.772: INFO: Pod daemon-set-vhrzp is not available
Jan 14 16:30:37.772: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:38.794: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:38.794: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:38.794: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:38.794: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:38.794: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:38.794: INFO: Pod daemon-set-vhrzp is not available
Jan 14 16:30:38.794: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:39.801: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:39.801: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:39.801: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:39.801: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:39.801: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:39.801: INFO: Pod daemon-set-vhrzp is not available
Jan 14 16:30:39.801: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:41.003: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:41.003: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:41.003: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:41.003: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:41.003: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:41.003: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:42.052: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:42.052: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:42.052: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:42.052: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:42.052: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:42.052: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:42.718: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:42.718: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:42.718: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:42.718: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:42.718: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:42.718: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:43.925: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:43.925: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:43.925: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:43.925: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:43.925: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:43.925: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:44.700: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:44.705: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:44.705: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:44.705: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:44.705: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:44.705: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:45.702: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:45.703: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:45.703: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:45.703: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:45.703: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:45.703: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:47.020: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:47.020: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:47.020: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:47.020: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:47.020: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:47.020: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:47.849: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:47.849: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:47.849: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:47.849: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:47.849: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:47.849: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:48.705: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:48.705: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:48.705: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:48.705: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:48.705: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:48.705: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:48.705: INFO: Pod daemon-set-wltzm is not available
Jan 14 16:30:49.702: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:49.702: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:49.702: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:49.702: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:49.702: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:49.702: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:49.702: INFO: Pod daemon-set-wltzm is not available
Jan 14 16:30:50.699: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:50.699: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:50.699: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:50.699: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:50.699: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:50.699: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:50.699: INFO: Pod daemon-set-wltzm is not available
Jan 14 16:30:52.200: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:52.200: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:52.200: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:52.200: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:52.200: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:52.200: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:52.200: INFO: Pod daemon-set-wltzm is not available
Jan 14 16:30:52.699: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:52.699: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:52.699: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:52.699: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:52.699: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:52.699: INFO: Wrong image for pod: daemon-set-wltzm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:52.699: INFO: Pod daemon-set-wltzm is not available
Jan 14 16:30:53.699: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:53.699: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:53.699: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:53.699: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:53.699: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:54.992: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:54.992: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:54.992: INFO: Pod daemon-set-brxhp is not available
Jan 14 16:30:54.992: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:54.992: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:54.992: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:55.885: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:55.886: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:55.886: INFO: Pod daemon-set-brxhp is not available
Jan 14 16:30:55.886: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:55.886: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:55.886: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:56.748: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:56.748: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:56.748: INFO: Pod daemon-set-brxhp is not available
Jan 14 16:30:56.748: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:56.748: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:56.748: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:58.102: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:58.102: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:58.102: INFO: Pod daemon-set-brxhp is not available
Jan 14 16:30:58.102: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:58.102: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:58.102: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:58.698: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:58.698: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:58.698: INFO: Pod daemon-set-brxhp is not available
Jan 14 16:30:58.698: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:58.698: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:58.698: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:59.700: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:59.700: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:59.700: INFO: Pod daemon-set-brxhp is not available
Jan 14 16:30:59.700: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:59.700: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:30:59.700: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:00.760: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:00.760: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:00.760: INFO: Pod daemon-set-brxhp is not available
Jan 14 16:31:00.760: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:00.760: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:00.760: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:01.700: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:01.700: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:01.700: INFO: Pod daemon-set-brxhp is not available
Jan 14 16:31:01.700: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:01.700: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:01.700: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:02.803: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:02.806: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:02.807: INFO: Pod daemon-set-brxhp is not available
Jan 14 16:31:02.807: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:02.807: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:02.807: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:03.778: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:03.778: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:03.778: INFO: Pod daemon-set-brxhp is not available
Jan 14 16:31:03.778: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:03.778: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:03.778: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:04.911: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:04.911: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:04.911: INFO: Pod daemon-set-brxhp is not available
Jan 14 16:31:04.911: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:04.911: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:04.911: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:05.699: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:05.699: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:05.699: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:05.699: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:05.699: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:06.779: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:06.779: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:06.779: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:06.779: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:06.779: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:07.824: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:07.824: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:07.824: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:07.824: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:07.824: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:08.716: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:08.716: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:08.716: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:08.716: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:08.716: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:09.980: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:09.980: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:09.980: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:09.980: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:09.980: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:10.785: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:10.785: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:10.785: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:10.785: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:10.785: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:11.965: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:11.965: INFO: Pod daemon-set-4wjrv is not available
Jan 14 16:31:11.965: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:11.965: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:11.965: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:11.965: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:12.880: INFO: Wrong image for pod: daemon-set-4wjrv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:12.880: INFO: Pod daemon-set-4wjrv is not available
Jan 14 16:31:12.880: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:12.880: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:12.880: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:12.880: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:13.995: INFO: Pod daemon-set-2nk2v is not available
Jan 14 16:31:13.995: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:13.995: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:13.995: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:13.995: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:14.802: INFO: Pod daemon-set-2nk2v is not available
Jan 14 16:31:14.802: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:14.802: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:14.802: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:14.803: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:15.698: INFO: Pod daemon-set-2nk2v is not available
Jan 14 16:31:15.699: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:15.699: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:15.699: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:15.699: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:16.731: INFO: Pod daemon-set-2nk2v is not available
Jan 14 16:31:16.731: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:16.731: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:16.731: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:16.731: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:17.895: INFO: Pod daemon-set-2nk2v is not available
Jan 14 16:31:17.895: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:17.895: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:17.895: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:17.895: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:19.002: INFO: Pod daemon-set-2nk2v is not available
Jan 14 16:31:19.002: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:19.002: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:19.002: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:19.002: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:19.808: INFO: Pod daemon-set-2nk2v is not available
Jan 14 16:31:19.808: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:19.808: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:19.808: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:19.808: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:20.705: INFO: Pod daemon-set-2nk2v is not available
Jan 14 16:31:20.705: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:20.705: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:20.705: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:20.705: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:21.698: INFO: Pod daemon-set-2nk2v is not available
Jan 14 16:31:21.698: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:21.698: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:21.698: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:21.698: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:22.829: INFO: Pod daemon-set-2nk2v is not available
Jan 14 16:31:22.829: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:22.829: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:22.829: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:22.829: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:23.707: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:23.707: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:23.707: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:23.707: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:24.923: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:24.923: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:24.925: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:24.925: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:25.937: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:25.937: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:25.937: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:25.937: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:26.825: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:26.825: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:26.825: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:26.825: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:27.822: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:27.822: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:27.822: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:27.822: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:28.701: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:28.701: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:28.701: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:28.701: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:29.703: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:29.703: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:29.703: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:29.703: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:30.756: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:30.756: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:30.756: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:30.756: INFO: Wrong image for pod: daemon-set-lc6r4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:30.756: INFO: Pod daemon-set-lc6r4 is not available
Jan 14 16:31:31.699: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:31.699: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:31.699: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:33.015: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:33.015: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:33.015: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:33.015: INFO: Pod daemon-set-vxm7f is not available
Jan 14 16:31:33.950: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:33.950: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:33.950: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:33.950: INFO: Pod daemon-set-vxm7f is not available
Jan 14 16:31:34.916: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:34.916: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:34.916: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:34.916: INFO: Pod daemon-set-vxm7f is not available
Jan 14 16:31:35.854: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:35.855: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:35.855: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:35.855: INFO: Pod daemon-set-vxm7f is not available
Jan 14 16:31:36.804: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:36.804: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:36.804: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:36.804: INFO: Pod daemon-set-vxm7f is not available
Jan 14 16:31:37.761: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:37.761: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:37.761: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:37.761: INFO: Pod daemon-set-vxm7f is not available
Jan 14 16:31:38.793: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:38.793: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:38.793: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:38.793: INFO: Pod daemon-set-vxm7f is not available
Jan 14 16:31:39.901: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:39.901: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:39.901: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:39.901: INFO: Pod daemon-set-vxm7f is not available
Jan 14 16:31:40.827: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:40.827: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:40.827: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:40.827: INFO: Pod daemon-set-vxm7f is not available
Jan 14 16:31:41.700: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:41.700: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:41.700: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:41.700: INFO: Pod daemon-set-vxm7f is not available
Jan 14 16:31:42.741: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:42.741: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:42.741: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:42.742: INFO: Pod daemon-set-vxm7f is not available
Jan 14 16:31:43.702: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:43.702: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:43.702: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:44.850: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:44.850: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:44.850: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:45.704: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:45.704: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:45.704: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:46.875: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:46.875: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:46.875: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:47.742: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:47.742: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:47.742: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:48.752: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:48.753: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:48.753: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:49.699: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:49.699: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:49.699: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:49.699: INFO: Pod daemon-set-hgbhp is not available
Jan 14 16:31:50.801: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:50.801: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:50.801: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:50.801: INFO: Pod daemon-set-hgbhp is not available
Jan 14 16:31:51.771: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:51.771: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:51.771: INFO: Wrong image for pod: daemon-set-hgbhp. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:51.771: INFO: Pod daemon-set-hgbhp is not available
Jan 14 16:31:53.003: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:53.003: INFO: Pod daemon-set-bclqh is not available
Jan 14 16:31:53.003: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:53.901: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:53.901: INFO: Pod daemon-set-bclqh is not available
Jan 14 16:31:53.901: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:54.946: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:54.946: INFO: Pod daemon-set-bclqh is not available
Jan 14 16:31:54.946: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:55.699: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:55.699: INFO: Pod daemon-set-bclqh is not available
Jan 14 16:31:55.699: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:56.768: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:56.768: INFO: Pod daemon-set-bclqh is not available
Jan 14 16:31:56.768: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:57.749: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:57.749: INFO: Pod daemon-set-bclqh is not available
Jan 14 16:31:57.749: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:58.698: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:58.699: INFO: Pod daemon-set-bclqh is not available
Jan 14 16:31:58.699: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:59.902: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:31:59.902: INFO: Pod daemon-set-bclqh is not available
Jan 14 16:31:59.902: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:00.747: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:00.747: INFO: Pod daemon-set-bclqh is not available
Jan 14 16:32:00.747: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:01.802: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:01.802: INFO: Pod daemon-set-bclqh is not available
Jan 14 16:32:01.802: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:03.020: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:03.020: INFO: Pod daemon-set-bclqh is not available
Jan 14 16:32:03.020: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:03.801: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:03.801: INFO: Pod daemon-set-bclqh is not available
Jan 14 16:32:03.801: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:04.704: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:04.704: INFO: Pod daemon-set-bclqh is not available
Jan 14 16:32:04.704: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:05.709: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:05.709: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:06.772: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:06.772: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:07.715: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:07.715: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:08.705: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:08.705: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:09.828: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:09.828: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:10.728: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:10.728: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:11.739: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:11.739: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:11.739: INFO: Pod daemon-set-c7rnk is not available
Jan 14 16:32:12.757: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:12.757: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:12.757: INFO: Pod daemon-set-c7rnk is not available
Jan 14 16:32:13.701: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:13.701: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:13.701: INFO: Pod daemon-set-c7rnk is not available
Jan 14 16:32:14.708: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:14.708: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:14.708: INFO: Pod daemon-set-c7rnk is not available
Jan 14 16:32:15.698: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:15.698: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:15.698: INFO: Pod daemon-set-c7rnk is not available
Jan 14 16:32:16.853: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:16.853: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:16.853: INFO: Pod daemon-set-c7rnk is not available
Jan 14 16:32:17.918: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:17.918: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:17.918: INFO: Pod daemon-set-c7rnk is not available
Jan 14 16:32:18.699: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:18.699: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:18.699: INFO: Pod daemon-set-c7rnk is not available
Jan 14 16:32:19.801: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:19.801: INFO: Wrong image for pod: daemon-set-c7rnk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:19.801: INFO: Pod daemon-set-c7rnk is not available
Jan 14 16:32:20.867: INFO: Pod daemon-set-6p9gk is not available
Jan 14 16:32:20.867: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:21.699: INFO: Pod daemon-set-6p9gk is not available
Jan 14 16:32:21.699: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:22.867: INFO: Pod daemon-set-6p9gk is not available
Jan 14 16:32:22.867: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:23.827: INFO: Pod daemon-set-6p9gk is not available
Jan 14 16:32:23.827: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:24.700: INFO: Pod daemon-set-6p9gk is not available
Jan 14 16:32:24.700: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:25.759: INFO: Pod daemon-set-6p9gk is not available
Jan 14 16:32:25.759: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:26.698: INFO: Pod daemon-set-6p9gk is not available
Jan 14 16:32:26.698: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:27.699: INFO: Pod daemon-set-6p9gk is not available
Jan 14 16:32:27.701: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:29.002: INFO: Pod daemon-set-6p9gk is not available
Jan 14 16:32:29.002: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:30.649: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:31.701: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:32.765: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:34.201: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:34.858: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:36.621: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:36.621: INFO: Pod daemon-set-8xwwk is not available
Jan 14 16:32:37.710: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:37.710: INFO: Pod daemon-set-8xwwk is not available
Jan 14 16:32:38.699: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:38.699: INFO: Pod daemon-set-8xwwk is not available
Jan 14 16:32:39.699: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:39.699: INFO: Pod daemon-set-8xwwk is not available
Jan 14 16:32:40.698: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:40.698: INFO: Pod daemon-set-8xwwk is not available
Jan 14 16:32:41.699: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:41.699: INFO: Pod daemon-set-8xwwk is not available
Jan 14 16:32:42.784: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:42.784: INFO: Pod daemon-set-8xwwk is not available
Jan 14 16:32:43.711: INFO: Wrong image for pod: daemon-set-8xwwk. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 14 16:32:43.711: INFO: Pod daemon-set-8xwwk is not available
Jan 14 16:32:45.457: INFO: Pod daemon-set-bpbjs is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jan 14 16:32:45.499: INFO: Number of nodes with available pods: 6
Jan 14 16:32:45.499: INFO: Node master3 is running more than one daemon pod
Jan 14 16:32:46.778: INFO: Number of nodes with available pods: 6
Jan 14 16:32:46.778: INFO: Node master3 is running more than one daemon pod
Jan 14 16:32:47.857: INFO: Number of nodes with available pods: 6
Jan 14 16:32:47.857: INFO: Node master3 is running more than one daemon pod
Jan 14 16:32:48.736: INFO: Number of nodes with available pods: 6
Jan 14 16:32:48.736: INFO: Node master3 is running more than one daemon pod
Jan 14 16:32:49.636: INFO: Number of nodes with available pods: 6
Jan 14 16:32:49.636: INFO: Node master3 is running more than one daemon pod
Jan 14 16:32:50.709: INFO: Number of nodes with available pods: 6
Jan 14 16:32:50.709: INFO: Node master3 is running more than one daemon pod
Jan 14 16:32:52.206: INFO: Number of nodes with available pods: 6
Jan 14 16:32:52.206: INFO: Node master3 is running more than one daemon pod
Jan 14 16:32:52.571: INFO: Number of nodes with available pods: 6
Jan 14 16:32:52.571: INFO: Node master3 is running more than one daemon pod
Jan 14 16:32:53.814: INFO: Number of nodes with available pods: 6
Jan 14 16:32:53.814: INFO: Node master3 is running more than one daemon pod
Jan 14 16:32:54.702: INFO: Number of nodes with available pods: 6
Jan 14 16:32:54.702: INFO: Node master3 is running more than one daemon pod
Jan 14 16:32:55.646: INFO: Number of nodes with available pods: 6
Jan 14 16:32:55.646: INFO: Node master3 is running more than one daemon pod
Jan 14 16:32:56.581: INFO: Number of nodes with available pods: 7
Jan 14 16:32:56.581: INFO: Number of running nodes: 7, number of available pods: 7
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4799, will wait for the garbage collector to delete the pods
Jan 14 16:32:57.669: INFO: Deleting DaemonSet.extensions daemon-set took: 146.736254ms
Jan 14 16:32:58.669: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.000371198s
Jan 14 16:33:16.566: INFO: Number of nodes with available pods: 0
Jan 14 16:33:16.566: INFO: Number of running nodes: 0, number of available pods: 0
Jan 14 16:33:16.601: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4799/daemonsets","resourceVersion":"124624"},"items":null}

Jan 14 16:33:16.608: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4799/pods","resourceVersion":"124625"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:33:16.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4799" for this suite.

â€¢ [SLOW TEST:210.482 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":278,"completed":206,"skipped":3112,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:33:17.970: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6640
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jan 14 16:33:20.363: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 16:33:29.371: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:33:57.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6640" for this suite.

â€¢ [SLOW TEST:39.395 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":278,"completed":207,"skipped":3114,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:33:57.367: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6298
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 14 16:34:10.123: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:34:10.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6298" for this suite.

â€¢ [SLOW TEST:13.634 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:131
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":278,"completed":208,"skipped":3131,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:34:11.002: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7453
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Update Demo
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:329
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Jan 14 16:34:13.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 create -f - --namespace=kubectl-7453'
Jan 14 16:34:20.265: INFO: stderr: ""
Jan 14 16:34:20.265: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 14 16:34:20.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7453'
Jan 14 16:34:20.598: INFO: stderr: ""
Jan 14 16:34:20.598: INFO: stdout: ""
STEP: Replicas for name=update-demo: expected=2 actual=0
Jan 14 16:34:25.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7453'
Jan 14 16:34:26.323: INFO: stderr: ""
Jan 14 16:34:26.323: INFO: stdout: "update-demo-nautilus-27fk5 update-demo-nautilus-tpx2l "
Jan 14 16:34:26.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-nautilus-27fk5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Jan 14 16:34:26.731: INFO: stderr: ""
Jan 14 16:34:26.731: INFO: stdout: ""
Jan 14 16:34:26.731: INFO: update-demo-nautilus-27fk5 is created but not running
Jan 14 16:34:31.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7453'
Jan 14 16:34:31.874: INFO: stderr: ""
Jan 14 16:34:31.902: INFO: stdout: "update-demo-nautilus-27fk5 update-demo-nautilus-tpx2l "
Jan 14 16:34:31.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-nautilus-27fk5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Jan 14 16:34:32.119: INFO: stderr: ""
Jan 14 16:34:32.119: INFO: stdout: "true"
Jan 14 16:34:32.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-nautilus-27fk5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Jan 14 16:34:32.350: INFO: stderr: ""
Jan 14 16:34:32.351: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 14 16:34:32.351: INFO: validating pod update-demo-nautilus-27fk5
Jan 14 16:34:32.404: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 14 16:34:32.404: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 14 16:34:32.404: INFO: update-demo-nautilus-27fk5 is verified up and running
Jan 14 16:34:32.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-nautilus-tpx2l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Jan 14 16:34:32.559: INFO: stderr: ""
Jan 14 16:34:32.559: INFO: stdout: "true"
Jan 14 16:34:32.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods update-demo-nautilus-tpx2l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Jan 14 16:34:32.757: INFO: stderr: ""
Jan 14 16:34:32.757: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 14 16:34:32.757: INFO: validating pod update-demo-nautilus-tpx2l
Jan 14 16:34:32.772: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 14 16:34:32.772: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 14 16:34:32.772: INFO: update-demo-nautilus-tpx2l is verified up and running
STEP: using delete to clean up resources
Jan 14 16:34:32.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 delete --grace-period=0 --force -f - --namespace=kubectl-7453'
Jan 14 16:34:33.001: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 14 16:34:33.001: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 14 16:34:33.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7453'
Jan 14 16:34:33.229: INFO: stderr: "No resources found in kubectl-7453 namespace.\n"
Jan 14 16:34:33.229: INFO: stdout: ""
Jan 14 16:34:33.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods -l name=update-demo --namespace=kubectl-7453 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 14 16:34:33.512: INFO: stderr: ""
Jan 14 16:34:33.512: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:34:33.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7453" for this suite.

â€¢ [SLOW TEST:22.797 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:327
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":278,"completed":209,"skipped":3136,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:34:33.818: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-3384
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jan 14 16:34:40.116: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jan 14 16:34:42.361: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616480, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616480, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616480, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616479, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:34:44.531: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616480, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616480, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616480, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616479, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:34:46.487: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616480, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616480, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616480, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616479, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:34:48.371: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616480, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616480, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616480, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616479, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 14 16:34:51.496: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 16:34:51.544: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:34:59.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3384" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

â€¢ [SLOW TEST:28.798 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":278,"completed":210,"skipped":3151,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:35:02.617: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7729
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 14 16:35:05.415: INFO: Waiting up to 5m0s for pod "downwardapi-volume-39c33882-1681-4ec9-8fef-32d4575e97dd" in namespace "projected-7729" to be "success or failure"
Jan 14 16:35:06.962: INFO: Pod "downwardapi-volume-39c33882-1681-4ec9-8fef-32d4575e97dd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.546421561s
Jan 14 16:35:09.203: INFO: Pod "downwardapi-volume-39c33882-1681-4ec9-8fef-32d4575e97dd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.786943833s
Jan 14 16:35:11.226: INFO: Pod "downwardapi-volume-39c33882-1681-4ec9-8fef-32d4575e97dd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.810342813s
Jan 14 16:35:13.350: INFO: Pod "downwardapi-volume-39c33882-1681-4ec9-8fef-32d4575e97dd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.934177877s
Jan 14 16:35:15.469: INFO: Pod "downwardapi-volume-39c33882-1681-4ec9-8fef-32d4575e97dd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.052928501s
Jan 14 16:35:17.586: INFO: Pod "downwardapi-volume-39c33882-1681-4ec9-8fef-32d4575e97dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.170169867s
STEP: Saw pod success
Jan 14 16:35:17.586: INFO: Pod "downwardapi-volume-39c33882-1681-4ec9-8fef-32d4575e97dd" satisfied condition "success or failure"
Jan 14 16:35:17.598: INFO: Trying to get logs from node slave2 pod downwardapi-volume-39c33882-1681-4ec9-8fef-32d4575e97dd container client-container: <nil>
STEP: delete the pod
Jan 14 16:35:18.913: INFO: Waiting for pod downwardapi-volume-39c33882-1681-4ec9-8fef-32d4575e97dd to disappear
Jan 14 16:35:18.921: INFO: Pod downwardapi-volume-39c33882-1681-4ec9-8fef-32d4575e97dd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:35:18.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7729" for this suite.

â€¢ [SLOW TEST:16.910 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":278,"completed":211,"skipped":3195,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:35:19.528: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6611
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 16:35:21.025: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 14 16:35:22.273: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 14 16:35:32.527: INFO: Creating deployment "test-rolling-update-deployment"
Jan 14 16:35:32.535: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 14 16:35:32.543: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 14 16:35:34.642: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 14 16:35:35.048: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616533, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616533, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616534, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616532, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67cf4f6444\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:35:37.062: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616533, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616533, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616534, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616532, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67cf4f6444\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:35:39.052: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616533, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616533, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616534, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616532, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67cf4f6444\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:35:41.053: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616533, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616533, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616534, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616532, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67cf4f6444\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:35:43.245: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616533, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616533, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616542, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616532, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67cf4f6444\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:35:45.204: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jan 14 16:35:45.649: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6611 /apis/apps/v1/namespaces/deployment-6611/deployments/test-rolling-update-deployment 6282d21c-deb5-4d7f-ba55-69d52870b7a1 125404 1 2020-01-14 16:35:32 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002cafb28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-01-14 16:35:33 +0000 UTC,LastTransitionTime:2020-01-14 16:35:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67cf4f6444" has successfully progressed.,LastUpdateTime:2020-01-14 16:35:44 +0000 UTC,LastTransitionTime:2020-01-14 16:35:32 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 14 16:35:45.716: INFO: New ReplicaSet "test-rolling-update-deployment-67cf4f6444" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67cf4f6444  deployment-6611 /apis/apps/v1/namespaces/deployment-6611/replicasets/test-rolling-update-deployment-67cf4f6444 067c3ac5-7f4f-4d6a-a583-b011a086d4ac 125388 1 2020-01-14 16:35:32 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 6282d21c-deb5-4d7f-ba55-69d52870b7a1 0xc002caffd7 0xc002caffd8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67cf4f6444,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003b7e048 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 14 16:35:45.716: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 14 16:35:45.717: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6611 /apis/apps/v1/namespaces/deployment-6611/replicasets/test-rolling-update-controller d7e86fa6-27f0-4f8a-8d94-b1a9471a7d93 125401 2 2020-01-14 16:35:20 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 6282d21c-deb5-4d7f-ba55-69d52870b7a1 0xc002caff07 0xc002caff08}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002caff68 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 14 16:35:45.864: INFO: Pod "test-rolling-update-deployment-67cf4f6444-ktj8s" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67cf4f6444-ktj8s test-rolling-update-deployment-67cf4f6444- deployment-6611 /api/v1/namespaces/deployment-6611/pods/test-rolling-update-deployment-67cf4f6444-ktj8s dbbae403-436f-40f9-b2a3-bb17f79f20dd 125387 0 2020-01-14 16:35:32 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-67cf4f6444 067c3ac5-7f4f-4d6a-a583-b011a086d4ac 0xc003b7e4b7 0xc003b7e4b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4xfv7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4xfv7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4xfv7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 16:35:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 16:35:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 16:35:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 16:35:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.142,PodIP:10.151.49.9,StartTime:2020-01-14 16:35:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-14 16:35:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker://sha256:a450ec43af3738f7fc05227a9a778feb9f90e08bfee4362b6a59224bcbf172cf,ContainerID:docker://56e1882a9a3e2e4310d24946fd6ddf3aa047ff13e6b15eb0a9a87b2bebcb5b3a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.151.49.9,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:35:45.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6611" for this suite.

â€¢ [SLOW TEST:26.757 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":278,"completed":212,"skipped":3214,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:35:46.299: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4935
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4935
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-4935
I0114 16:35:49.849387      23 runners.go:189] Created replication controller with name: externalname-service, namespace: services-4935, replica count: 2
I0114 16:35:52.899877      23 runners.go:189] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 16:35:55.900151      23 runners.go:189] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 16:35:58.901237      23 runners.go:189] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 16:36:01.901575      23 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 14 16:36:01.902: INFO: Creating new exec pod
Jan 14 16:36:15.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=services-4935 execpodcnhzr -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jan 14 16:36:16.525: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 14 16:36:16.532: INFO: stdout: ""
Jan 14 16:36:16.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=services-4935 execpodcnhzr -- /bin/sh -x -c nc -zv -t -w 2 10.150.200.234 80'
Jan 14 16:36:17.553: INFO: stderr: "+ nc -zv -t -w 2 10.150.200.234 80\nConnection to 10.150.200.234 80 port [tcp/http] succeeded!\n"
Jan 14 16:36:17.553: INFO: stdout: ""
Jan 14 16:36:17.553: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:36:18.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4935" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:32.730 seconds]
[sig-network] Services
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":278,"completed":213,"skipped":3232,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:36:19.028: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3422
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jan 14 16:36:20.980: INFO: Waiting up to 5m0s for pod "downward-api-987c7fec-2f48-43de-8942-63b6680e4572" in namespace "downward-api-3422" to be "success or failure"
Jan 14 16:36:20.984: INFO: Pod "downward-api-987c7fec-2f48-43de-8942-63b6680e4572": Phase="Pending", Reason="", readiness=false. Elapsed: 4.545856ms
Jan 14 16:36:22.999: INFO: Pod "downward-api-987c7fec-2f48-43de-8942-63b6680e4572": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019048436s
Jan 14 16:36:25.651: INFO: Pod "downward-api-987c7fec-2f48-43de-8942-63b6680e4572": Phase="Pending", Reason="", readiness=false. Elapsed: 4.671390634s
Jan 14 16:36:27.835: INFO: Pod "downward-api-987c7fec-2f48-43de-8942-63b6680e4572": Phase="Pending", Reason="", readiness=false. Elapsed: 6.85501773s
Jan 14 16:36:29.910: INFO: Pod "downward-api-987c7fec-2f48-43de-8942-63b6680e4572": Phase="Pending", Reason="", readiness=false. Elapsed: 8.930052981s
Jan 14 16:36:32.379: INFO: Pod "downward-api-987c7fec-2f48-43de-8942-63b6680e4572": Phase="Pending", Reason="", readiness=false. Elapsed: 11.399489282s
Jan 14 16:36:34.506: INFO: Pod "downward-api-987c7fec-2f48-43de-8942-63b6680e4572": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.526601991s
STEP: Saw pod success
Jan 14 16:36:34.514: INFO: Pod "downward-api-987c7fec-2f48-43de-8942-63b6680e4572" satisfied condition "success or failure"
Jan 14 16:36:35.335: INFO: Trying to get logs from node master3 pod downward-api-987c7fec-2f48-43de-8942-63b6680e4572 container dapi-container: <nil>
STEP: delete the pod
Jan 14 16:36:37.684: INFO: Waiting for pod downward-api-987c7fec-2f48-43de-8942-63b6680e4572 to disappear
Jan 14 16:36:37.917: INFO: Pod downward-api-987c7fec-2f48-43de-8942-63b6680e4572 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:36:37.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3422" for this suite.

â€¢ [SLOW TEST:19.024 seconds]
[sig-node] Downward API
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:33
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":278,"completed":214,"skipped":3233,"failed":0}
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:36:38.052: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2201
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-ef11fda2-d56d-42f0-9ca1-7d10315670cf
STEP: Creating a pod to test consume configMaps
Jan 14 16:36:40.767: INFO: Waiting up to 5m0s for pod "pod-configmaps-99cd5bae-0e5e-476a-94e6-6971434f647c" in namespace "configmap-2201" to be "success or failure"
Jan 14 16:36:41.404: INFO: Pod "pod-configmaps-99cd5bae-0e5e-476a-94e6-6971434f647c": Phase="Pending", Reason="", readiness=false. Elapsed: 637.075706ms
Jan 14 16:36:43.660: INFO: Pod "pod-configmaps-99cd5bae-0e5e-476a-94e6-6971434f647c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.892543535s
Jan 14 16:36:45.811: INFO: Pod "pod-configmaps-99cd5bae-0e5e-476a-94e6-6971434f647c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.044120633s
Jan 14 16:36:48.030: INFO: Pod "pod-configmaps-99cd5bae-0e5e-476a-94e6-6971434f647c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.262228357s
Jan 14 16:36:50.102: INFO: Pod "pod-configmaps-99cd5bae-0e5e-476a-94e6-6971434f647c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.334618964s
Jan 14 16:36:52.194: INFO: Pod "pod-configmaps-99cd5bae-0e5e-476a-94e6-6971434f647c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.427105636s
STEP: Saw pod success
Jan 14 16:36:52.194: INFO: Pod "pod-configmaps-99cd5bae-0e5e-476a-94e6-6971434f647c" satisfied condition "success or failure"
Jan 14 16:36:52.388: INFO: Trying to get logs from node slave2 pod pod-configmaps-99cd5bae-0e5e-476a-94e6-6971434f647c container configmap-volume-test: <nil>
STEP: delete the pod
Jan 14 16:36:52.761: INFO: Waiting for pod pod-configmaps-99cd5bae-0e5e-476a-94e6-6971434f647c to disappear
Jan 14 16:36:52.778: INFO: Pod pod-configmaps-99cd5bae-0e5e-476a-94e6-6971434f647c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:36:52.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2201" for this suite.

â€¢ [SLOW TEST:15.410 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":215,"skipped":3233,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:36:53.462: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2244
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 14 16:36:57.874: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 14 16:37:00.475: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616618, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616618, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616619, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616617, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:37:02.691: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616618, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616618, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616619, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616617, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:37:05.304: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616618, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616618, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616619, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616617, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:37:07.119: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616618, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616618, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616619, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616617, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:37:08.494: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616618, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616618, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616619, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616617, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:37:10.696: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616618, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616618, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616619, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616617, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 14 16:37:14.091: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:37:29.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2244" for this suite.
STEP: Destroying namespace "webhook-2244-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:40.231 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":278,"completed":216,"skipped":3259,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:37:33.694: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6867
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jan 14 16:37:49.858: INFO: Successfully updated pod "annotationupdatea2a80250-7666-4c9a-98e4-ab81d491c997"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:37:51.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6867" for this suite.

â€¢ [SLOW TEST:18.302 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":278,"completed":217,"skipped":3265,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:37:51.996: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8826
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8826
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8826
STEP: creating replication controller externalsvc in namespace services-8826
I0114 16:37:54.554082      23 runners.go:189] Created replication controller with name: externalsvc, namespace: services-8826, replica count: 2
I0114 16:37:57.604619      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 16:38:00.604928      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 16:38:03.605320      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 16:38:06.605572      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jan 14 16:38:07.689: INFO: Creating new exec pod
Jan 14 16:38:18.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=services-8826 execpod8ghls -- /bin/sh -x -c nslookup nodeport-service'
Jan 14 16:38:19.603: INFO: stderr: "+ nslookup nodeport-service\n"
Jan 14 16:38:19.604: INFO: stdout: "Server:    10.150.0.3\nAddress 1: 10.150.0.3 coredns.kube-system.svc.cluster.local\n\nName:      nodeport-service\nAddress 1: 10.150.164.114 externalsvc.services-8826.svc.cluster.local\n"
STEP: deleting ReplicationController externalsvc in namespace services-8826, will wait for the garbage collector to delete the pods
Jan 14 16:38:19.904: INFO: Deleting ReplicationController externalsvc took: 245.770393ms
Jan 14 16:38:20.704: INFO: Terminating ReplicationController externalsvc pods took: 800.279019ms
Jan 14 16:38:40.400: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:38:40.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8826" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:49.130 seconds]
[sig-network] Services
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":278,"completed":218,"skipped":3280,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:38:41.126: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-6305
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 16:38:44.609: INFO: Waiting up to 5m0s for pod "busybox-user-65534-8eee0e81-5394-4247-8091-907a34fbd31c" in namespace "security-context-test-6305" to be "success or failure"
Jan 14 16:38:45.004: INFO: Pod "busybox-user-65534-8eee0e81-5394-4247-8091-907a34fbd31c": Phase="Pending", Reason="", readiness=false. Elapsed: 392.125581ms
Jan 14 16:38:47.009: INFO: Pod "busybox-user-65534-8eee0e81-5394-4247-8091-907a34fbd31c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.396912476s
Jan 14 16:38:49.227: INFO: Pod "busybox-user-65534-8eee0e81-5394-4247-8091-907a34fbd31c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.614265777s
Jan 14 16:38:51.235: INFO: Pod "busybox-user-65534-8eee0e81-5394-4247-8091-907a34fbd31c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.62263501s
Jan 14 16:38:53.240: INFO: Pod "busybox-user-65534-8eee0e81-5394-4247-8091-907a34fbd31c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.627361669s
Jan 14 16:38:55.524: INFO: Pod "busybox-user-65534-8eee0e81-5394-4247-8091-907a34fbd31c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.911259989s
Jan 14 16:38:55.524: INFO: Pod "busybox-user-65534-8eee0e81-5394-4247-8091-907a34fbd31c" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:38:55.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6305" for this suite.

â€¢ [SLOW TEST:14.493 seconds]
[k8s.io] Security Context
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  When creating a container with runAsUser
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:43
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":219,"skipped":3311,"failed":0}
SSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:38:55.620: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-2986
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 16:38:57.169: INFO: (0) /api/v1/nodes/slave4/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 61.139521ms)
Jan 14 16:38:57.177: INFO: (1) /api/v1/nodes/slave4/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.77144ms)
Jan 14 16:38:57.184: INFO: (2) /api/v1/nodes/slave4/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.625199ms)
Jan 14 16:38:57.191: INFO: (3) /api/v1/nodes/slave4/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.75205ms)
Jan 14 16:38:57.201: INFO: (4) /api/v1/nodes/slave4/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.102763ms)
Jan 14 16:38:57.237: INFO: (5) /api/v1/nodes/slave4/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 36.76044ms)
Jan 14 16:38:57.244: INFO: (6) /api/v1/nodes/slave4/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.756366ms)
Jan 14 16:38:57.250: INFO: (7) /api/v1/nodes/slave4/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.231324ms)
Jan 14 16:38:57.257: INFO: (8) /api/v1/nodes/slave4/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.108796ms)
Jan 14 16:38:57.262: INFO: (9) /api/v1/nodes/slave4/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.740023ms)
Jan 14 16:38:57.276: INFO: (10) /api/v1/nodes/slave4/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 13.492352ms)
Jan 14 16:38:57.283: INFO: (11) /api/v1/nodes/slave4/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.238163ms)
Jan 14 16:38:57.295: INFO: (12) /api/v1/nodes/slave4/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 11.670445ms)
Jan 14 16:38:57.310: INFO: (13) /api/v1/nodes/slave4/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 15.313825ms)
Jan 14 16:38:57.317: INFO: (14) /api/v1/nodes/slave4/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.553963ms)
Jan 14 16:38:57.327: INFO: (15) /api/v1/nodes/slave4/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.912551ms)
Jan 14 16:38:57.332: INFO: (16) /api/v1/nodes/slave4/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.88395ms)
Jan 14 16:38:57.352: INFO: (17) /api/v1/nodes/slave4/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 19.558334ms)
Jan 14 16:38:57.357: INFO: (18) /api/v1/nodes/slave4/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.373412ms)
Jan 14 16:38:57.361: INFO: (19) /api/v1/nodes/slave4/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.341487ms)
[AfterEach] version v1
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:38:57.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2986" for this suite.
â€¢{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":278,"completed":220,"skipped":3315,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:38:57.480: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5484
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jan 14 16:38:58.996: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:39:18.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5484" for this suite.

â€¢ [SLOW TEST:20.929 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":278,"completed":221,"skipped":3327,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:39:18.409: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9108
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's command
Jan 14 16:39:20.772: INFO: Waiting up to 5m0s for pod "var-expansion-c792f29c-34ea-47f6-8a3e-1d8173f34e08" in namespace "var-expansion-9108" to be "success or failure"
Jan 14 16:39:21.186: INFO: Pod "var-expansion-c792f29c-34ea-47f6-8a3e-1d8173f34e08": Phase="Pending", Reason="", readiness=false. Elapsed: 414.051416ms
Jan 14 16:39:23.207: INFO: Pod "var-expansion-c792f29c-34ea-47f6-8a3e-1d8173f34e08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.434503808s
Jan 14 16:39:25.433: INFO: Pod "var-expansion-c792f29c-34ea-47f6-8a3e-1d8173f34e08": Phase="Pending", Reason="", readiness=false. Elapsed: 4.661123402s
Jan 14 16:39:27.505: INFO: Pod "var-expansion-c792f29c-34ea-47f6-8a3e-1d8173f34e08": Phase="Pending", Reason="", readiness=false. Elapsed: 6.732826473s
Jan 14 16:39:29.510: INFO: Pod "var-expansion-c792f29c-34ea-47f6-8a3e-1d8173f34e08": Phase="Pending", Reason="", readiness=false. Elapsed: 8.738235874s
Jan 14 16:39:31.592: INFO: Pod "var-expansion-c792f29c-34ea-47f6-8a3e-1d8173f34e08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.820009889s
STEP: Saw pod success
Jan 14 16:39:31.592: INFO: Pod "var-expansion-c792f29c-34ea-47f6-8a3e-1d8173f34e08" satisfied condition "success or failure"
Jan 14 16:39:31.666: INFO: Trying to get logs from node slave2 pod var-expansion-c792f29c-34ea-47f6-8a3e-1d8173f34e08 container dapi-container: <nil>
STEP: delete the pod
Jan 14 16:39:32.675: INFO: Waiting for pod var-expansion-c792f29c-34ea-47f6-8a3e-1d8173f34e08 to disappear
Jan 14 16:39:32.804: INFO: Pod var-expansion-c792f29c-34ea-47f6-8a3e-1d8173f34e08 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:39:32.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9108" for this suite.

â€¢ [SLOW TEST:15.210 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":278,"completed":222,"skipped":3334,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:39:33.620: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8167
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8167
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8167
STEP: creating replication controller externalsvc in namespace services-8167
I0114 16:39:37.076717      23 runners.go:189] Created replication controller with name: externalsvc, namespace: services-8167, replica count: 2
I0114 16:39:40.128452      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 16:39:43.128781      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 16:39:46.131200      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 16:39:49.131498      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 16:39:52.131892      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jan 14 16:39:52.532: INFO: Creating new exec pod
Jan 14 16:40:03.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=services-8167 execpod46rlk -- /bin/sh -x -c nslookup clusterip-service'
Jan 14 16:40:05.877: INFO: stderr: "+ nslookup clusterip-service\n"
Jan 14 16:40:05.877: INFO: stdout: "Server:    10.150.0.3\nAddress 1: 10.150.0.3 coredns.kube-system.svc.cluster.local\n\nName:      clusterip-service\nAddress 1: 10.150.12.249 externalsvc.services-8167.svc.cluster.local\n"
STEP: deleting ReplicationController externalsvc in namespace services-8167, will wait for the garbage collector to delete the pods
Jan 14 16:40:06.438: INFO: Deleting ReplicationController externalsvc took: 363.897101ms
Jan 14 16:40:08.138: INFO: Terminating ReplicationController externalsvc pods took: 1.700395884s
Jan 14 16:40:30.644: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:40:31.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8167" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:57.547 seconds]
[sig-network] Services
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":278,"completed":223,"skipped":3342,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:40:31.167: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-678
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 14 16:40:37.160: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 14 16:40:39.991: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616837, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616837, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616838, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616837, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:40:42.106: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616837, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616837, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616838, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616837, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:40:44.043: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616837, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616837, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616838, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616837, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:40:46.442: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616837, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616837, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616838, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616837, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:40:48.153: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616837, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616837, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616838, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616837, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 14 16:40:51.548: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:40:53.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-678" for this suite.
STEP: Destroying namespace "webhook-678-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:23.686 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":278,"completed":224,"skipped":3362,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:40:54.855: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3233
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-3233
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 14 16:40:58.226: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 14 16:41:48.366: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.151.161.32:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3233 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 16:41:48.366: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 16:41:48.750: INFO: Found all expected endpoints: [netserver-0]
Jan 14 16:41:48.758: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.151.208.28:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3233 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 16:41:48.758: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 16:41:49.181: INFO: Found all expected endpoints: [netserver-1]
Jan 14 16:41:49.212: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.151.32.32:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3233 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 16:41:49.212: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 16:41:49.638: INFO: Found all expected endpoints: [netserver-2]
Jan 14 16:41:49.643: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.151.51.62:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3233 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 16:41:49.643: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 16:41:49.994: INFO: Found all expected endpoints: [netserver-3]
Jan 14 16:41:50.217: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.151.49.20:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3233 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 16:41:50.217: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 16:41:50.827: INFO: Found all expected endpoints: [netserver-4]
Jan 14 16:41:50.857: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.151.194.118:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3233 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 16:41:50.857: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 16:41:51.138: INFO: Found all expected endpoints: [netserver-5]
Jan 14 16:41:51.170: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.151.26.71:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3233 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 16:41:51.170: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 16:41:51.481: INFO: Found all expected endpoints: [netserver-6]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:41:51.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3233" for this suite.

â€¢ [SLOW TEST:56.653 seconds]
[sig-network] Networking
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":225,"skipped":3388,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:41:51.511: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-533
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 14 16:41:53.487: INFO: Waiting up to 5m0s for pod "downwardapi-volume-949e93c1-0839-4615-b417-8c15d7cb02fc" in namespace "downward-api-533" to be "success or failure"
Jan 14 16:41:53.492: INFO: Pod "downwardapi-volume-949e93c1-0839-4615-b417-8c15d7cb02fc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.629541ms
Jan 14 16:41:55.606: INFO: Pod "downwardapi-volume-949e93c1-0839-4615-b417-8c15d7cb02fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.119086035s
Jan 14 16:41:58.094: INFO: Pod "downwardapi-volume-949e93c1-0839-4615-b417-8c15d7cb02fc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.607343386s
Jan 14 16:42:00.500: INFO: Pod "downwardapi-volume-949e93c1-0839-4615-b417-8c15d7cb02fc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.013380356s
Jan 14 16:42:02.505: INFO: Pod "downwardapi-volume-949e93c1-0839-4615-b417-8c15d7cb02fc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.018483242s
Jan 14 16:42:04.793: INFO: Pod "downwardapi-volume-949e93c1-0839-4615-b417-8c15d7cb02fc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.306874379s
Jan 14 16:42:06.856: INFO: Pod "downwardapi-volume-949e93c1-0839-4615-b417-8c15d7cb02fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.369037564s
STEP: Saw pod success
Jan 14 16:42:06.856: INFO: Pod "downwardapi-volume-949e93c1-0839-4615-b417-8c15d7cb02fc" satisfied condition "success or failure"
Jan 14 16:42:06.866: INFO: Trying to get logs from node slave3 pod downwardapi-volume-949e93c1-0839-4615-b417-8c15d7cb02fc container client-container: <nil>
STEP: delete the pod
Jan 14 16:42:07.307: INFO: Waiting for pod downwardapi-volume-949e93c1-0839-4615-b417-8c15d7cb02fc to disappear
Jan 14 16:42:07.312: INFO: Pod downwardapi-volume-949e93c1-0839-4615-b417-8c15d7cb02fc no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:42:07.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-533" for this suite.

â€¢ [SLOW TEST:17.158 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":278,"completed":226,"skipped":3403,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:42:08.670: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6805
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-0badd645-1c52-4164-8b33-a4b6f5a339eb
STEP: Creating a pod to test consume configMaps
Jan 14 16:42:12.808: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-75ae353d-e99e-4890-995e-fa38ef2c3166" in namespace "projected-6805" to be "success or failure"
Jan 14 16:42:12.905: INFO: Pod "pod-projected-configmaps-75ae353d-e99e-4890-995e-fa38ef2c3166": Phase="Pending", Reason="", readiness=false. Elapsed: 97.654378ms
Jan 14 16:42:15.073: INFO: Pod "pod-projected-configmaps-75ae353d-e99e-4890-995e-fa38ef2c3166": Phase="Pending", Reason="", readiness=false. Elapsed: 2.26538356s
Jan 14 16:42:17.613: INFO: Pod "pod-projected-configmaps-75ae353d-e99e-4890-995e-fa38ef2c3166": Phase="Pending", Reason="", readiness=false. Elapsed: 4.805458183s
Jan 14 16:42:20.206: INFO: Pod "pod-projected-configmaps-75ae353d-e99e-4890-995e-fa38ef2c3166": Phase="Pending", Reason="", readiness=false. Elapsed: 7.398162098s
Jan 14 16:42:22.743: INFO: Pod "pod-projected-configmaps-75ae353d-e99e-4890-995e-fa38ef2c3166": Phase="Pending", Reason="", readiness=false. Elapsed: 9.93564346s
Jan 14 16:42:24.748: INFO: Pod "pod-projected-configmaps-75ae353d-e99e-4890-995e-fa38ef2c3166": Phase="Pending", Reason="", readiness=false. Elapsed: 11.940061692s
Jan 14 16:42:26.840: INFO: Pod "pod-projected-configmaps-75ae353d-e99e-4890-995e-fa38ef2c3166": Phase="Pending", Reason="", readiness=false. Elapsed: 14.032620314s
Jan 14 16:42:28.909: INFO: Pod "pod-projected-configmaps-75ae353d-e99e-4890-995e-fa38ef2c3166": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.101023381s
STEP: Saw pod success
Jan 14 16:42:28.909: INFO: Pod "pod-projected-configmaps-75ae353d-e99e-4890-995e-fa38ef2c3166" satisfied condition "success or failure"
Jan 14 16:42:28.944: INFO: Trying to get logs from node slave2 pod pod-projected-configmaps-75ae353d-e99e-4890-995e-fa38ef2c3166 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 14 16:42:29.002: INFO: Waiting for pod pod-projected-configmaps-75ae353d-e99e-4890-995e-fa38ef2c3166 to disappear
Jan 14 16:42:29.005: INFO: Pod pod-projected-configmaps-75ae353d-e99e-4890-995e-fa38ef2c3166 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:42:29.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6805" for this suite.

â€¢ [SLOW TEST:20.355 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":278,"completed":227,"skipped":3449,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:42:29.029: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2412
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 16:42:31.918: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 14 16:42:42.393: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 14 16:42:44.397: INFO: Creating deployment "test-rollover-deployment"
Jan 14 16:42:44.562: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 14 16:42:48.606: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 14 16:42:48.832: INFO: Ensure that both replica sets have 1 created replica
Jan 14 16:42:49.048: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 14 16:42:49.064: INFO: Updating deployment test-rollover-deployment
Jan 14 16:42:49.065: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 14 16:42:51.971: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 14 16:42:52.319: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 14 16:42:52.341: INFO: all replica sets need to contain the pod-template-hash label
Jan 14 16:42:52.343: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616965, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616965, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616972, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616964, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:42:54.757: INFO: all replica sets need to contain the pod-template-hash label
Jan 14 16:42:54.758: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616965, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616965, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616972, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616964, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:42:56.506: INFO: all replica sets need to contain the pod-template-hash label
Jan 14 16:42:56.506: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616965, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616965, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616972, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616964, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:42:58.353: INFO: all replica sets need to contain the pod-template-hash label
Jan 14 16:42:58.353: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616965, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616965, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616972, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616964, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:43:00.459: INFO: all replica sets need to contain the pod-template-hash label
Jan 14 16:43:00.459: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616965, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616965, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616972, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616964, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:43:02.360: INFO: all replica sets need to contain the pod-template-hash label
Jan 14 16:43:02.360: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616965, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616965, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616980, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616964, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:43:04.601: INFO: all replica sets need to contain the pod-template-hash label
Jan 14 16:43:04.601: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616965, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616965, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616980, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616964, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:43:06.414: INFO: all replica sets need to contain the pod-template-hash label
Jan 14 16:43:06.414: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616965, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616965, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616980, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616964, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:43:08.356: INFO: all replica sets need to contain the pod-template-hash label
Jan 14 16:43:08.356: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616965, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616965, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616980, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616964, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:43:11.092: INFO: 
Jan 14 16:43:11.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616965, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616965, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616980, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616964, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:43:12.755: INFO: 
Jan 14 16:43:12.755: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616965, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616965, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616990, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616964, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:43:14.571: INFO: 
Jan 14 16:43:14.571: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jan 14 16:43:14.591: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-2412 /apis/apps/v1/namespaces/deployment-2412/deployments/test-rollover-deployment c08677e9-fd04-4677-85a7-652bb241ace0 127633 2 2020-01-14 16:42:44 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005ec8c18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-01-14 16:42:45 +0000 UTC,LastTransitionTime:2020-01-14 16:42:45 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-574d6dfbff" has successfully progressed.,LastUpdateTime:2020-01-14 16:43:12 +0000 UTC,LastTransitionTime:2020-01-14 16:42:44 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 14 16:43:14.594: INFO: New ReplicaSet "test-rollover-deployment-574d6dfbff" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-574d6dfbff  deployment-2412 /apis/apps/v1/namespaces/deployment-2412/replicasets/test-rollover-deployment-574d6dfbff f7e5a5c3-ccea-42e1-b786-d5e75705eac8 127618 2 2020-01-14 16:42:49 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment c08677e9-fd04-4677-85a7-652bb241ace0 0xc005ec90c7 0xc005ec90c8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 574d6dfbff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005ec9138 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 14 16:43:14.594: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 14 16:43:14.594: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2412 /apis/apps/v1/namespaces/deployment-2412/replicasets/test-rollover-controller 60aca8c6-4dfc-4903-aad3-1f5bee902751 127630 2 2020-01-14 16:42:31 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment c08677e9-fd04-4677-85a7-652bb241ace0 0xc005ec8ff7 0xc005ec8ff8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005ec9058 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 14 16:43:14.595: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-2412 /apis/apps/v1/namespaces/deployment-2412/replicasets/test-rollover-deployment-f6c94f66c cace5508-6aaf-477f-bcff-bac140a5a662 127550 2 2020-01-14 16:42:44 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment c08677e9-fd04-4677-85a7-652bb241ace0 0xc005ec91a0 0xc005ec91a1}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005ec9218 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 14 16:43:14.748: INFO: Pod "test-rollover-deployment-574d6dfbff-qkcbj" is available:
&Pod{ObjectMeta:{test-rollover-deployment-574d6dfbff-qkcbj test-rollover-deployment-574d6dfbff- deployment-2412 /api/v1/namespaces/deployment-2412/pods/test-rollover-deployment-574d6dfbff-qkcbj d72c88e3-20f3-430c-b2e4-16ffa0bcf0f8 127589 0 2020-01-14 16:42:49 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[] [{apps/v1 ReplicaSet test-rollover-deployment-574d6dfbff f7e5a5c3-ccea-42e1-b786-d5e75705eac8 0xc005f00ee7 0xc005f00ee8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lprk6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lprk6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lprk6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 16:42:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 16:42:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 16:42:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 16:42:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.132,PodIP:10.151.194.120,StartTime:2020-01-14 16:42:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-14 16:42:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker://sha256:f50f6e57bf4f12cf2cba3b5046260d9ea4e01525b7132011df6f7f694f3b1817,ContainerID:docker://e707af4b60c034f55fc5ff9246148af55df7192d7eedbe7e3a5a60083cee9feb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.151.194.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:43:14.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2412" for this suite.

â€¢ [SLOW TEST:46.139 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":278,"completed":228,"skipped":3460,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:43:15.169: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5416
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 14 16:43:20.083: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 14 16:43:22.511: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617000, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617000, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617002, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616999, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:43:24.884: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617000, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617000, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617002, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616999, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:43:26.522: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617000, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617000, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617002, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616999, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:43:28.603: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617000, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617000, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617002, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714616999, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 14 16:43:32.688: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:43:33.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5416" for this suite.
STEP: Destroying namespace "webhook-5416-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:21.183 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":278,"completed":229,"skipped":3469,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:43:36.354: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7480
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating pod
Jan 14 16:43:49.389: INFO: Pod pod-hostip-ecb11d62-8fc6-49b3-8fd3-af260b3ffcb9 has hostIP: 192.168.0.142
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:43:49.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7480" for this suite.

â€¢ [SLOW TEST:13.225 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":278,"completed":230,"skipped":3479,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:43:49.582: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4045
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jan 14 16:43:51.193: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4045 /api/v1/namespaces/watch-4045/configmaps/e2e-watch-test-watch-closed d80b2e7c-dd56-4563-aa55-0549e8c820fb 127877 0 2020-01-14 16:43:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 14 16:43:51.194: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4045 /api/v1/namespaces/watch-4045/configmaps/e2e-watch-test-watch-closed d80b2e7c-dd56-4563-aa55-0549e8c820fb 127878 0 2020-01-14 16:43:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jan 14 16:43:51.442: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4045 /api/v1/namespaces/watch-4045/configmaps/e2e-watch-test-watch-closed d80b2e7c-dd56-4563-aa55-0549e8c820fb 127879 0 2020-01-14 16:43:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 14 16:43:51.442: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4045 /api/v1/namespaces/watch-4045/configmaps/e2e-watch-test-watch-closed d80b2e7c-dd56-4563-aa55-0549e8c820fb 127880 0 2020-01-14 16:43:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:43:51.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4045" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":278,"completed":231,"skipped":3498,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:43:51.524: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3134
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jan 14 16:43:53.658: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 14 16:43:58.664: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:44:00.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3134" for this suite.

â€¢ [SLOW TEST:9.377 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":278,"completed":232,"skipped":3500,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:44:00.907: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6894
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap that has name configmap-test-emptyKey-0d3d7eb5-6dbd-4618-8e69-d98cea0a9bfd
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:44:03.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6894" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":278,"completed":233,"skipped":3536,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:44:03.383: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4484
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-05272597-1e9b-460d-a43e-fdc761312b1e
STEP: Creating a pod to test consume secrets
Jan 14 16:44:05.363: INFO: Waiting up to 5m0s for pod "pod-secrets-74a0bcaa-3bf0-48c1-a287-d552910283db" in namespace "secrets-4484" to be "success or failure"
Jan 14 16:44:05.556: INFO: Pod "pod-secrets-74a0bcaa-3bf0-48c1-a287-d552910283db": Phase="Pending", Reason="", readiness=false. Elapsed: 192.567101ms
Jan 14 16:44:07.562: INFO: Pod "pod-secrets-74a0bcaa-3bf0-48c1-a287-d552910283db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.19809605s
Jan 14 16:44:09.623: INFO: Pod "pod-secrets-74a0bcaa-3bf0-48c1-a287-d552910283db": Phase="Pending", Reason="", readiness=false. Elapsed: 4.25944374s
Jan 14 16:44:12.005: INFO: Pod "pod-secrets-74a0bcaa-3bf0-48c1-a287-d552910283db": Phase="Pending", Reason="", readiness=false. Elapsed: 6.641667614s
Jan 14 16:44:14.401: INFO: Pod "pod-secrets-74a0bcaa-3bf0-48c1-a287-d552910283db": Phase="Pending", Reason="", readiness=false. Elapsed: 9.037029856s
Jan 14 16:44:16.852: INFO: Pod "pod-secrets-74a0bcaa-3bf0-48c1-a287-d552910283db": Phase="Pending", Reason="", readiness=false. Elapsed: 11.488816236s
Jan 14 16:44:19.304: INFO: Pod "pod-secrets-74a0bcaa-3bf0-48c1-a287-d552910283db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.940683072s
STEP: Saw pod success
Jan 14 16:44:19.304: INFO: Pod "pod-secrets-74a0bcaa-3bf0-48c1-a287-d552910283db" satisfied condition "success or failure"
Jan 14 16:44:19.339: INFO: Trying to get logs from node slave3 pod pod-secrets-74a0bcaa-3bf0-48c1-a287-d552910283db container secret-volume-test: <nil>
STEP: delete the pod
Jan 14 16:44:20.177: INFO: Waiting for pod pod-secrets-74a0bcaa-3bf0-48c1-a287-d552910283db to disappear
Jan 14 16:44:20.191: INFO: Pod pod-secrets-74a0bcaa-3bf0-48c1-a287-d552910283db no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:44:20.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4484" for this suite.

â€¢ [SLOW TEST:17.220 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":234,"skipped":3544,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:44:20.603: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6996
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 14 16:44:24.598: INFO: Waiting up to 5m0s for pod "downwardapi-volume-14f8ea95-28f2-4237-85b7-1a5caa0826b3" in namespace "downward-api-6996" to be "success or failure"
Jan 14 16:44:24.611: INFO: Pod "downwardapi-volume-14f8ea95-28f2-4237-85b7-1a5caa0826b3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.070125ms
Jan 14 16:44:26.735: INFO: Pod "downwardapi-volume-14f8ea95-28f2-4237-85b7-1a5caa0826b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.137288546s
Jan 14 16:44:28.810: INFO: Pod "downwardapi-volume-14f8ea95-28f2-4237-85b7-1a5caa0826b3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.211868399s
Jan 14 16:44:30.908: INFO: Pod "downwardapi-volume-14f8ea95-28f2-4237-85b7-1a5caa0826b3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.310175807s
Jan 14 16:44:32.913: INFO: Pod "downwardapi-volume-14f8ea95-28f2-4237-85b7-1a5caa0826b3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.314515698s
Jan 14 16:44:34.917: INFO: Pod "downwardapi-volume-14f8ea95-28f2-4237-85b7-1a5caa0826b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.318990548s
STEP: Saw pod success
Jan 14 16:44:34.917: INFO: Pod "downwardapi-volume-14f8ea95-28f2-4237-85b7-1a5caa0826b3" satisfied condition "success or failure"
Jan 14 16:44:34.923: INFO: Trying to get logs from node slave2 pod downwardapi-volume-14f8ea95-28f2-4237-85b7-1a5caa0826b3 container client-container: <nil>
STEP: delete the pod
Jan 14 16:44:36.087: INFO: Waiting for pod downwardapi-volume-14f8ea95-28f2-4237-85b7-1a5caa0826b3 to disappear
Jan 14 16:44:36.092: INFO: Pod downwardapi-volume-14f8ea95-28f2-4237-85b7-1a5caa0826b3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:44:36.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6996" for this suite.

â€¢ [SLOW TEST:16.203 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":278,"completed":235,"skipped":3547,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:44:36.807: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5597
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 14 16:44:40.945: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 14 16:44:43.014: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617080, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617080, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617081, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617080, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:44:45.180: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617080, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617080, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617081, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617080, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:44:47.111: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617080, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617080, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617081, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617080, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:44:49.134: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617080, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617080, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617081, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617080, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:44:51.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617080, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617080, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617081, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617080, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 14 16:44:54.820: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:45:04.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5597" for this suite.
STEP: Destroying namespace "webhook-5597-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:33.896 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":278,"completed":236,"skipped":3555,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:45:10.703: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4657
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-df0431b5-7f0a-422f-9727-5a2beca68cfe
STEP: Creating a pod to test consume configMaps
Jan 14 16:45:14.420: INFO: Waiting up to 5m0s for pod "pod-configmaps-94349344-24a7-47e5-a898-ffb364bd51c5" in namespace "configmap-4657" to be "success or failure"
Jan 14 16:45:14.546: INFO: Pod "pod-configmaps-94349344-24a7-47e5-a898-ffb364bd51c5": Phase="Pending", Reason="", readiness=false. Elapsed: 126.161993ms
Jan 14 16:45:16.679: INFO: Pod "pod-configmaps-94349344-24a7-47e5-a898-ffb364bd51c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.258665507s
Jan 14 16:45:18.836: INFO: Pod "pod-configmaps-94349344-24a7-47e5-a898-ffb364bd51c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.415878479s
Jan 14 16:45:20.862: INFO: Pod "pod-configmaps-94349344-24a7-47e5-a898-ffb364bd51c5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.442413563s
Jan 14 16:45:22.931: INFO: Pod "pod-configmaps-94349344-24a7-47e5-a898-ffb364bd51c5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.510932622s
Jan 14 16:45:25.023: INFO: Pod "pod-configmaps-94349344-24a7-47e5-a898-ffb364bd51c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.602831789s
STEP: Saw pod success
Jan 14 16:45:25.023: INFO: Pod "pod-configmaps-94349344-24a7-47e5-a898-ffb364bd51c5" satisfied condition "success or failure"
Jan 14 16:45:25.080: INFO: Trying to get logs from node slave2 pod pod-configmaps-94349344-24a7-47e5-a898-ffb364bd51c5 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 14 16:45:25.122: INFO: Waiting for pod pod-configmaps-94349344-24a7-47e5-a898-ffb364bd51c5 to disappear
Jan 14 16:45:25.126: INFO: Pod pod-configmaps-94349344-24a7-47e5-a898-ffb364bd51c5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:45:25.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4657" for this suite.

â€¢ [SLOW TEST:14.826 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":278,"completed":237,"skipped":3562,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:45:25.533: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5964
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-0f5650f3-50ce-4cb0-b35f-6b04c9d8d8a6
STEP: Creating secret with name s-test-opt-upd-45a1e3f4-e7ad-4ef2-9457-abfc47bf36fd
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-0f5650f3-50ce-4cb0-b35f-6b04c9d8d8a6
STEP: Updating secret s-test-opt-upd-45a1e3f4-e7ad-4ef2-9457-abfc47bf36fd
STEP: Creating secret with name s-test-opt-create-ebb218aa-0901-4c2c-8633-16effbf1b358
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:47:14.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5964" for this suite.

â€¢ [SLOW TEST:109.223 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":238,"skipped":3606,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:47:14.756: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5878
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 16:47:16.479: INFO: Creating deployment "test-recreate-deployment"
Jan 14 16:47:16.621: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 14 16:47:17.421: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jan 14 16:47:19.803: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 14 16:47:20.487: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617238, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617238, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617239, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617236, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-799c574856\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:47:23.064: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617238, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617238, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617239, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617236, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-799c574856\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:47:24.717: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617238, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617238, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617239, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617236, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-799c574856\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:47:26.943: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617238, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617238, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617239, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617236, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-799c574856\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:47:28.492: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617238, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617238, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617239, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617236, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-799c574856\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:47:30.895: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 14 16:47:31.493: INFO: Updating deployment test-recreate-deployment
Jan 14 16:47:31.494: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jan 14 16:47:37.684: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5878 /apis/apps/v1/namespaces/deployment-5878/deployments/test-recreate-deployment 5116855f-eb80-498a-968a-71c21663a9f9 128856 2 2020-01-14 16:47:16 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0056ca378 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-01-14 16:47:34 +0000 UTC,LastTransitionTime:2020-01-14 16:47:34 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-01-14 16:47:36 +0000 UTC,LastTransitionTime:2020-01-14 16:47:16 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 14 16:47:37.720: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-5878 /apis/apps/v1/namespaces/deployment-5878/replicasets/test-recreate-deployment-5f94c574ff eb2b2275-e0cf-48d4-b55d-3b7a9c1baeca 128846 1 2020-01-14 16:47:33 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 5116855f-eb80-498a-968a-71c21663a9f9 0xc0056ca747 0xc0056ca748}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0056ca7a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 14 16:47:37.720: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 14 16:47:37.720: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-799c574856  deployment-5878 /apis/apps/v1/namespaces/deployment-5878/replicasets/test-recreate-deployment-799c574856 99ba9e2c-67a7-49d0-a29f-fef9f56adbe2 128834 2 2020-01-14 16:47:16 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 5116855f-eb80-498a-968a-71c21663a9f9 0xc0056ca817 0xc0056ca818}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 799c574856,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0056ca888 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 14 16:47:38.094: INFO: Pod "test-recreate-deployment-5f94c574ff-bv767" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-bv767 test-recreate-deployment-5f94c574ff- deployment-5878 /api/v1/namespaces/deployment-5878/pods/test-recreate-deployment-5f94c574ff-bv767 6a8fa00e-8797-41e6-a16e-0ccebc8b157c 128849 0 2020-01-14 16:47:33 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff eb2b2275-e0cf-48d4-b55d-3b7a9c1baeca 0xc0028d7787 0xc0028d7788}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cxj4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cxj4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cxj4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 16:47:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 16:47:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 16:47:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-14 16:47:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.132,PodIP:,StartTime:2020-01-14 16:47:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:47:38.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5878" for this suite.

â€¢ [SLOW TEST:23.354 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":278,"completed":239,"skipped":3619,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:47:38.111: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-6854
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jan 14 16:47:43.096: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jan 14 16:47:45.429: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617263, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617263, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617264, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617262, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:47:47.858: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617263, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617263, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617264, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617262, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:47:49.434: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617263, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617263, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617264, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617262, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:47:51.463: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617263, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617263, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617264, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617262, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:47:53.440: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617263, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617263, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617264, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617262, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 16:47:55.995: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617263, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617263, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617264, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714617262, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 14 16:48:00.202: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 16:48:00.206: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:48:12.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6854" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

â€¢ [SLOW TEST:37.143 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":278,"completed":240,"skipped":3648,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:48:15.255: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7826
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jan 14 16:48:16.899: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 14 16:48:17.245: INFO: Waiting for terminating namespaces to be deleted...
Jan 14 16:48:17.253: INFO: 
Logging pods the kubelet thinks is on node master1 before test
Jan 14 16:48:17.268: INFO: daemon-set-gg2wn from daemonsets-9852 started at 2020-01-14 13:36:55 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.269: INFO: 	Container app ready: true, restart count 0
Jan 14 16:48:17.272: INFO: resource-reserver-master1 from kube-system started at 2020-01-14 11:40:33 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.274: INFO: 	Container sleep-forever ready: true, restart count 0
Jan 14 16:48:17.274: INFO: oss-provisioner-7bb5d4c769-vhrt6 from kube-system started at 2020-01-14 05:31:38 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.277: INFO: 	Container oss-provisioner ready: true, restart count 1
Jan 14 16:48:17.278: INFO: kube-controller-manager-master1 from kube-system started at 2020-01-14 11:40:33 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.282: INFO: 	Container kube-controller-manager ready: true, restart count 3
Jan 14 16:48:17.282: INFO: coredns-52ppc from kube-system started at 2020-01-14 12:58:20 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.282: INFO: 	Container coredns ready: true, restart count 0
Jan 14 16:48:17.282: INFO: kube-scheduler-master1 from kube-system started at 2020-01-14 11:40:33 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.282: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan 14 16:48:17.282: INFO: cinder-provisioner-xxxjh from kube-system started at 2020-01-14 05:31:24 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.282: INFO: 	Container cinder-provisioner ready: true, restart count 1
Jan 14 16:48:17.282: INFO: nginx-proxy-master1 from kube-system started at 2020-01-14 11:40:33 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.282: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 14 16:48:17.283: INFO: calico-node-c275v from kube-system started at 2020-01-14 05:30:06 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.283: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 16:48:17.283: INFO: kube-state-metrics-8988b595-gq7dc from monitoring started at 2020-01-14 12:57:51 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.283: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 14 16:48:17.283: INFO: kube-apiserver-master1 from kube-system started at 2020-01-14 10:55:20 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.283: INFO: 	Container kube-apiserver ready: true, restart count 1
Jan 14 16:48:17.283: INFO: kube-proxy-master1 from kube-system started at 2020-01-14 11:40:33 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.283: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 16:48:17.283: INFO: 
Logging pods the kubelet thinks is on node master2 before test
Jan 14 16:48:17.304: INFO: calico-node-vhmj5 from kube-system started at 2020-01-14 05:30:04 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.304: INFO: 	Container calico-node ready: true, restart count 2
Jan 14 16:48:17.304: INFO: kube-scheduler-master2 from kube-system started at 2020-01-14 08:24:38 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.304: INFO: 	Container kube-scheduler ready: true, restart count 2
Jan 14 16:48:17.304: INFO: daemon-set-5jq8c from daemonsets-9852 started at 2020-01-14 13:36:55 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.304: INFO: 	Container app ready: true, restart count 0
Jan 14 16:48:17.304: INFO: cinder-snapshot-58545f46f8-d2m7m from kube-system started at 2020-01-14 05:31:52 +0000 UTC (2 container statuses recorded)
Jan 14 16:48:17.304: INFO: 	Container cinder-snapshot-controller ready: true, restart count 2
Jan 14 16:48:17.304: INFO: 	Container cinder-snapshot-provisioner ready: true, restart count 2
Jan 14 16:48:17.304: INFO: kube-controller-manager-master2 from kube-system started at 2020-01-14 08:23:38 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.304: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jan 14 16:48:17.304: INFO: kube-proxy-master2 from kube-system started at 2020-01-14 08:24:11 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.304: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 16:48:17.304: INFO: nginx-proxy-master2 from kube-system started at 2020-01-14 05:27:39 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.304: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 14 16:48:17.304: INFO: coredns-cnrrm from kube-system started at 2020-01-14 12:58:20 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.304: INFO: 	Container coredns ready: true, restart count 0
Jan 14 16:48:17.304: INFO: cinder-provisioner-9fk6f from kube-system started at 2020-01-14 05:31:23 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.304: INFO: 	Container cinder-provisioner ready: true, restart count 2
Jan 14 16:48:17.304: INFO: kube-apiserver-master2 from kube-system started at 2020-01-14 08:26:26 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.304: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 14 16:48:17.304: INFO: resource-reserver-master2 from kube-system started at 2020-01-14 05:27:40 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.304: INFO: 	Container sleep-forever ready: true, restart count 0
Jan 14 16:48:17.304: INFO: 
Logging pods the kubelet thinks is on node master3 before test
Jan 14 16:48:17.566: INFO: kube-scheduler-master3 from kube-system started at 2020-01-14 08:31:06 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.566: INFO: 	Container kube-scheduler ready: true, restart count 1
Jan 14 16:48:17.566: INFO: daemon-set-247jg from daemonsets-9852 started at 2020-01-14 13:36:56 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.566: INFO: 	Container app ready: true, restart count 0
Jan 14 16:48:17.566: INFO: localpv-provisioner-6c5467f94-q9cvw from kube-system started at 2020-01-14 05:31:32 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.566: INFO: 	Container localpv-provisioner ready: true, restart count 1
Jan 14 16:48:17.566: INFO: coredns-f24kh from kube-system started at 2020-01-14 12:58:20 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.566: INFO: 	Container coredns ready: true, restart count 0
Jan 14 16:48:17.566: INFO: kube-proxy-master3 from kube-system started at 2020-01-14 08:30:38 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.566: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 16:48:17.566: INFO: calico-node-p7t5k from kube-system started at 2020-01-14 05:30:03 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.566: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 16:48:17.566: INFO: nginx-proxy-master3 from kube-system started at 2020-01-14 05:27:40 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.566: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 14 16:48:17.566: INFO: resource-reserver-master3 from kube-system started at 2020-01-14 08:31:44 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.566: INFO: 	Container sleep-forever ready: true, restart count 0
Jan 14 16:48:17.566: INFO: cinder-provisioner-wbhvj from kube-system started at 2020-01-14 05:31:22 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.566: INFO: 	Container cinder-provisioner ready: true, restart count 1
Jan 14 16:48:17.566: INFO: kube-apiserver-master3 from kube-system started at 2020-01-14 08:29:42 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.566: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 14 16:48:17.566: INFO: kube-controller-manager-master3 from kube-system started at 2020-01-14 08:30:08 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.566: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jan 14 16:48:17.566: INFO: 
Logging pods the kubelet thinks is on node slave1 before test
Jan 14 16:48:17.647: INFO: kube-proxy-slave1 from kube-system started at 2020-01-14 08:17:36 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.647: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 16:48:17.647: INFO: calico-node-wzpws from kube-system started at 2020-01-14 05:30:06 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.647: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 16:48:17.647: INFO: calico-kube-controllers-6ff9f48ccd-phjqn from kube-system started at 2020-01-14 05:30:37 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.647: INFO: 	Container calico-kube-controllers ready: true, restart count 1
Jan 14 16:48:17.647: INFO: tiller-deploy-77d97f584c-l4s9r from kube-system started at 2020-01-14 05:32:44 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.647: INFO: 	Container tiller ready: true, restart count 1
Jan 14 16:48:17.648: INFO: daemon-set-d6b8h from daemonsets-9852 started at 2020-01-14 13:36:56 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.648: INFO: 	Container app ready: true, restart count 0
Jan 14 16:48:17.648: INFO: dns-test-b39ed90a-d7ff-4205-b2f7-ffe423eff36b from dns-1195 started at 2020-01-14 12:33:00 +0000 UTC (3 container statuses recorded)
Jan 14 16:48:17.648: INFO: 	Container jessie-querier ready: true, restart count 20
Jan 14 16:48:17.648: INFO: 	Container querier ready: true, restart count 22
Jan 14 16:48:17.648: INFO: 	Container webserver ready: true, restart count 0
Jan 14 16:48:17.648: INFO: 
Logging pods the kubelet thinks is on node slave2 before test
Jan 14 16:48:17.721: INFO: kube-proxy-slave2 from kube-system started at 2020-01-14 08:17:48 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.721: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 16:48:17.721: INFO: cirros-26408-6bb4b5b58b-kfs9n from default started at 2020-01-14 11:01:57 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.721: INFO: 	Container cirros-26408 ready: true, restart count 0
Jan 14 16:48:17.721: INFO: agnhost-deployment-54964f567d-498qd from default started at 2020-01-14 11:05:35 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.721: INFO: 	Container agnhost ready: true, restart count 0
Jan 14 16:48:17.721: INFO: annotationupdate8b560796-65d3-444d-b2a8-95487af080c1 from downward-api-9309 started at 2020-01-14 12:08:41 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.721: INFO: 	Container client-container ready: false, restart count 0
Jan 14 16:48:17.721: INFO: daemon-set-zww4m from daemonsets-9852 started at 2020-01-14 13:36:55 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.721: INFO: 	Container app ready: true, restart count 0
Jan 14 16:48:17.721: INFO: calico-node-gwd6p from kube-system started at 2020-01-14 05:30:03 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.721: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 16:48:17.721: INFO: 
Logging pods the kubelet thinks is on node slave3 before test
Jan 14 16:48:17.738: INFO: daemon-set-gn9px from daemonsets-9852 started at 2020-01-14 13:36:55 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.738: INFO: 	Container app ready: true, restart count 0
Jan 14 16:48:17.738: INFO: sonobuoy from sonobuoy started at 2020-01-14 14:39:37 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.738: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 14 16:48:17.738: INFO: sonobuoy-e2e-job-5f91618b21714006 from sonobuoy started at 2020-01-14 14:39:48 +0000 UTC (2 container statuses recorded)
Jan 14 16:48:17.738: INFO: 	Container e2e ready: true, restart count 0
Jan 14 16:48:17.738: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 14 16:48:17.739: INFO: calico-node-zprlr from kube-system started at 2020-01-14 05:30:06 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.739: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 16:48:17.739: INFO: kube-proxy-slave3 from kube-system started at 2020-01-14 08:14:15 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.739: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 16:48:17.739: INFO: 
Logging pods the kubelet thinks is on node slave4 before test
Jan 14 16:48:17.765: INFO: kube-proxy-slave4 from kube-system started at 2020-01-14 08:13:30 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.765: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 16:48:17.765: INFO: cirros-15453-7f8dd5d978-gjlvv from default started at 2020-01-14 11:03:42 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.765: INFO: 	Container cirros-15453 ready: true, restart count 0
Jan 14 16:48:17.765: INFO: calico-node-hc57z from kube-system started at 2020-01-14 05:30:06 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.765: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 16:48:17.765: INFO: cirros-19353-6f67878d75-p7mck from default started at 2020-01-14 11:01:24 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.765: INFO: 	Container cirros-19353 ready: true, restart count 0
Jan 14 16:48:17.765: INFO: dns-test-53c7521e-6d65-4cd3-b40e-b0e2058d5107 from dns-5986 started at 2020-01-14 12:27:30 +0000 UTC (3 container statuses recorded)
Jan 14 16:48:17.765: INFO: 	Container jessie-querier ready: true, restart count 19
Jan 14 16:48:17.765: INFO: 	Container querier ready: true, restart count 21
Jan 14 16:48:17.765: INFO: 	Container webserver ready: true, restart count 0
Jan 14 16:48:17.765: INFO: daemon-set-b8jkm from daemonsets-9852 started at 2020-01-14 13:36:54 +0000 UTC (1 container statuses recorded)
Jan 14 16:48:17.765: INFO: 	Container app ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-0daf2cdd-8c86-4648-a187-1445b85af16b 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-0daf2cdd-8c86-4648-a187-1445b85af16b off the node slave2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-0daf2cdd-8c86-4648-a187-1445b85af16b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:49:09.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7826" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

â€¢ [SLOW TEST:54.559 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":278,"completed":241,"skipped":3657,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:49:09.815: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6781
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service endpoint-test2 in namespace services-6781
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6781 to expose endpoints map[]
Jan 14 16:49:14.393: INFO: Get endpoints failed (233.156091ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Jan 14 16:49:15.510: INFO: successfully validated that service endpoint-test2 in namespace services-6781 exposes endpoints map[] (1.349562219s elapsed)
STEP: Creating pod pod1 in namespace services-6781
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6781 to expose endpoints map[pod1:[80]]
Jan 14 16:49:23.711: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (6.16622849s elapsed, will retry)
Jan 14 16:49:29.696: INFO: successfully validated that service endpoint-test2 in namespace services-6781 exposes endpoints map[pod1:[80]] (12.151650315s elapsed)
STEP: Creating pod pod2 in namespace services-6781
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6781 to expose endpoints map[pod1:[80] pod2:[80]]
Jan 14 16:49:35.204: INFO: Unexpected endpoints: found map[5f92f73d-2217-477f-9a56-9b2b6758eb3a:[80]], expected map[pod1:[80] pod2:[80]] (5.497385022s elapsed, will retry)
Jan 14 16:49:41.406: INFO: successfully validated that service endpoint-test2 in namespace services-6781 exposes endpoints map[pod1:[80] pod2:[80]] (11.699143313s elapsed)
STEP: Deleting pod pod1 in namespace services-6781
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6781 to expose endpoints map[pod2:[80]]
Jan 14 16:49:43.205: INFO: successfully validated that service endpoint-test2 in namespace services-6781 exposes endpoints map[pod2:[80]] (1.459503285s elapsed)
STEP: Deleting pod pod2 in namespace services-6781
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6781 to expose endpoints map[]
Jan 14 16:49:43.826: INFO: successfully validated that service endpoint-test2 in namespace services-6781 exposes endpoints map[] (343.496853ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:49:44.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6781" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:34.748 seconds]
[sig-network] Services
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":278,"completed":242,"skipped":3692,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:49:44.567: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2144
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 16:49:48.905: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:49:54.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2144" for this suite.

â€¢ [SLOW TEST:11.959 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:47
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":278,"completed":243,"skipped":3717,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:49:56.528: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9602
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-9602
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating statefulset ss in namespace statefulset-9602
Jan 14 16:49:59.099: INFO: Found 0 stateful pods, waiting for 1
Jan 14 16:50:09.579: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Jan 14 16:50:19.329: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jan 14 16:50:20.047: INFO: Deleting all statefulset in ns statefulset-9602
Jan 14 16:50:20.447: INFO: Scaling statefulset ss to 0
Jan 14 16:50:40.956: INFO: Waiting for statefulset status.replicas updated to 0
Jan 14 16:50:40.964: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:50:41.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9602" for this suite.

â€¢ [SLOW TEST:45.905 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":278,"completed":244,"skipped":3731,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:50:42.444: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9802
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jan 14 16:50:44.799: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 14 16:50:45.445: INFO: Waiting for terminating namespaces to be deleted...
Jan 14 16:50:45.630: INFO: 
Logging pods the kubelet thinks is on node master1 before test
Jan 14 16:50:45.705: INFO: oss-provisioner-7bb5d4c769-vhrt6 from kube-system started at 2020-01-14 05:31:38 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.705: INFO: 	Container oss-provisioner ready: true, restart count 1
Jan 14 16:50:45.705: INFO: daemon-set-gg2wn from daemonsets-9852 started at 2020-01-14 13:36:55 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.705: INFO: 	Container app ready: true, restart count 0
Jan 14 16:50:45.705: INFO: resource-reserver-master1 from kube-system started at 2020-01-14 11:40:33 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.705: INFO: 	Container sleep-forever ready: true, restart count 0
Jan 14 16:50:45.705: INFO: kube-controller-manager-master1 from kube-system started at 2020-01-14 11:40:33 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.706: INFO: 	Container kube-controller-manager ready: true, restart count 3
Jan 14 16:50:45.706: INFO: cinder-provisioner-xxxjh from kube-system started at 2020-01-14 05:31:24 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.706: INFO: 	Container cinder-provisioner ready: true, restart count 1
Jan 14 16:50:45.706: INFO: coredns-52ppc from kube-system started at 2020-01-14 12:58:20 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.706: INFO: 	Container coredns ready: true, restart count 0
Jan 14 16:50:45.706: INFO: kube-scheduler-master1 from kube-system started at 2020-01-14 11:40:33 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.706: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan 14 16:50:45.707: INFO: kube-proxy-master1 from kube-system started at 2020-01-14 11:40:33 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.719: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 16:50:45.719: INFO: nginx-proxy-master1 from kube-system started at 2020-01-14 11:40:33 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.719: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 14 16:50:45.719: INFO: calico-node-c275v from kube-system started at 2020-01-14 05:30:06 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.720: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 16:50:45.720: INFO: kube-state-metrics-8988b595-gq7dc from monitoring started at 2020-01-14 12:57:51 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.720: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 14 16:50:45.720: INFO: kube-apiserver-master1 from kube-system started at 2020-01-14 10:55:20 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.720: INFO: 	Container kube-apiserver ready: true, restart count 1
Jan 14 16:50:45.720: INFO: 
Logging pods the kubelet thinks is on node master2 before test
Jan 14 16:50:45.749: INFO: calico-node-vhmj5 from kube-system started at 2020-01-14 05:30:04 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.749: INFO: 	Container calico-node ready: true, restart count 2
Jan 14 16:50:45.749: INFO: kube-scheduler-master2 from kube-system started at 2020-01-14 08:24:38 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.749: INFO: 	Container kube-scheduler ready: true, restart count 2
Jan 14 16:50:45.749: INFO: daemon-set-5jq8c from daemonsets-9852 started at 2020-01-14 13:36:55 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.749: INFO: 	Container app ready: true, restart count 0
Jan 14 16:50:45.749: INFO: cinder-snapshot-58545f46f8-d2m7m from kube-system started at 2020-01-14 05:31:52 +0000 UTC (2 container statuses recorded)
Jan 14 16:50:45.749: INFO: 	Container cinder-snapshot-controller ready: true, restart count 2
Jan 14 16:50:45.749: INFO: 	Container cinder-snapshot-provisioner ready: true, restart count 2
Jan 14 16:50:45.750: INFO: kube-controller-manager-master2 from kube-system started at 2020-01-14 08:23:38 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.750: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jan 14 16:50:45.750: INFO: kube-proxy-master2 from kube-system started at 2020-01-14 08:24:11 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.750: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 16:50:45.750: INFO: cinder-provisioner-9fk6f from kube-system started at 2020-01-14 05:31:23 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.750: INFO: 	Container cinder-provisioner ready: true, restart count 2
Jan 14 16:50:45.750: INFO: kube-apiserver-master2 from kube-system started at 2020-01-14 08:26:26 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.750: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 14 16:50:45.750: INFO: resource-reserver-master2 from kube-system started at 2020-01-14 05:27:40 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.750: INFO: 	Container sleep-forever ready: true, restart count 0
Jan 14 16:50:45.750: INFO: nginx-proxy-master2 from kube-system started at 2020-01-14 05:27:39 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.750: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 14 16:50:45.750: INFO: coredns-cnrrm from kube-system started at 2020-01-14 12:58:20 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.750: INFO: 	Container coredns ready: true, restart count 0
Jan 14 16:50:45.750: INFO: 
Logging pods the kubelet thinks is on node master3 before test
Jan 14 16:50:45.779: INFO: daemon-set-247jg from daemonsets-9852 started at 2020-01-14 13:36:56 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.779: INFO: 	Container app ready: true, restart count 0
Jan 14 16:50:45.779: INFO: kube-scheduler-master3 from kube-system started at 2020-01-14 08:31:06 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.779: INFO: 	Container kube-scheduler ready: true, restart count 1
Jan 14 16:50:45.779: INFO: calico-node-p7t5k from kube-system started at 2020-01-14 05:30:03 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.779: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 16:50:45.779: INFO: localpv-provisioner-6c5467f94-q9cvw from kube-system started at 2020-01-14 05:31:32 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.779: INFO: 	Container localpv-provisioner ready: true, restart count 1
Jan 14 16:50:45.779: INFO: coredns-f24kh from kube-system started at 2020-01-14 12:58:20 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.779: INFO: 	Container coredns ready: true, restart count 0
Jan 14 16:50:45.779: INFO: kube-proxy-master3 from kube-system started at 2020-01-14 08:30:38 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.779: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 16:50:45.779: INFO: nginx-proxy-master3 from kube-system started at 2020-01-14 05:27:40 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.779: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 14 16:50:45.779: INFO: kube-controller-manager-master3 from kube-system started at 2020-01-14 08:30:08 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.779: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jan 14 16:50:45.779: INFO: resource-reserver-master3 from kube-system started at 2020-01-14 08:31:44 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.779: INFO: 	Container sleep-forever ready: true, restart count 0
Jan 14 16:50:45.779: INFO: cinder-provisioner-wbhvj from kube-system started at 2020-01-14 05:31:22 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.779: INFO: 	Container cinder-provisioner ready: true, restart count 1
Jan 14 16:50:45.779: INFO: kube-apiserver-master3 from kube-system started at 2020-01-14 08:29:42 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.779: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 14 16:50:45.779: INFO: 
Logging pods the kubelet thinks is on node slave1 before test
Jan 14 16:50:45.793: INFO: tiller-deploy-77d97f584c-l4s9r from kube-system started at 2020-01-14 05:32:44 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.794: INFO: 	Container tiller ready: true, restart count 1
Jan 14 16:50:45.794: INFO: daemon-set-d6b8h from daemonsets-9852 started at 2020-01-14 13:36:56 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.794: INFO: 	Container app ready: true, restart count 0
Jan 14 16:50:45.794: INFO: dns-test-b39ed90a-d7ff-4205-b2f7-ffe423eff36b from dns-1195 started at 2020-01-14 12:33:00 +0000 UTC (3 container statuses recorded)
Jan 14 16:50:45.794: INFO: 	Container jessie-querier ready: true, restart count 20
Jan 14 16:50:45.794: INFO: 	Container querier ready: true, restart count 22
Jan 14 16:50:45.794: INFO: 	Container webserver ready: true, restart count 0
Jan 14 16:50:45.794: INFO: kube-proxy-slave1 from kube-system started at 2020-01-14 08:17:36 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.794: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 16:50:45.794: INFO: calico-node-wzpws from kube-system started at 2020-01-14 05:30:06 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.794: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 16:50:45.794: INFO: calico-kube-controllers-6ff9f48ccd-phjqn from kube-system started at 2020-01-14 05:30:37 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.794: INFO: 	Container calico-kube-controllers ready: true, restart count 1
Jan 14 16:50:45.794: INFO: 
Logging pods the kubelet thinks is on node slave2 before test
Jan 14 16:50:45.883: INFO: calico-node-gwd6p from kube-system started at 2020-01-14 05:30:03 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.883: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 16:50:45.883: INFO: kube-proxy-slave2 from kube-system started at 2020-01-14 08:17:48 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.883: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 16:50:45.883: INFO: cirros-26408-6bb4b5b58b-kfs9n from default started at 2020-01-14 11:01:57 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.883: INFO: 	Container cirros-26408 ready: true, restart count 0
Jan 14 16:50:45.883: INFO: agnhost-deployment-54964f567d-498qd from default started at 2020-01-14 11:05:35 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.883: INFO: 	Container agnhost ready: true, restart count 0
Jan 14 16:50:45.883: INFO: annotationupdate8b560796-65d3-444d-b2a8-95487af080c1 from downward-api-9309 started at 2020-01-14 12:08:41 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.884: INFO: 	Container client-container ready: false, restart count 0
Jan 14 16:50:45.884: INFO: daemon-set-zww4m from daemonsets-9852 started at 2020-01-14 13:36:55 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.884: INFO: 	Container app ready: true, restart count 0
Jan 14 16:50:45.884: INFO: 
Logging pods the kubelet thinks is on node slave3 before test
Jan 14 16:50:45.920: INFO: calico-node-zprlr from kube-system started at 2020-01-14 05:30:06 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.920: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 16:50:45.920: INFO: kube-proxy-slave3 from kube-system started at 2020-01-14 08:14:15 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.920: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 16:50:45.920: INFO: daemon-set-gn9px from daemonsets-9852 started at 2020-01-14 13:36:55 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.920: INFO: 	Container app ready: true, restart count 0
Jan 14 16:50:45.920: INFO: sonobuoy from sonobuoy started at 2020-01-14 14:39:37 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:45.920: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 14 16:50:45.920: INFO: sonobuoy-e2e-job-5f91618b21714006 from sonobuoy started at 2020-01-14 14:39:48 +0000 UTC (2 container statuses recorded)
Jan 14 16:50:45.920: INFO: 	Container e2e ready: true, restart count 0
Jan 14 16:50:45.920: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 14 16:50:45.920: INFO: 
Logging pods the kubelet thinks is on node slave4 before test
Jan 14 16:50:46.440: INFO: cirros-19353-6f67878d75-p7mck from default started at 2020-01-14 11:01:24 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:46.441: INFO: 	Container cirros-19353 ready: true, restart count 0
Jan 14 16:50:46.441: INFO: dns-test-53c7521e-6d65-4cd3-b40e-b0e2058d5107 from dns-5986 started at 2020-01-14 12:27:30 +0000 UTC (3 container statuses recorded)
Jan 14 16:50:46.441: INFO: 	Container jessie-querier ready: true, restart count 19
Jan 14 16:50:46.441: INFO: 	Container querier ready: true, restart count 21
Jan 14 16:50:46.441: INFO: 	Container webserver ready: true, restart count 0
Jan 14 16:50:46.441: INFO: daemon-set-b8jkm from daemonsets-9852 started at 2020-01-14 13:36:54 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:46.441: INFO: 	Container app ready: true, restart count 0
Jan 14 16:50:46.441: INFO: calico-node-hc57z from kube-system started at 2020-01-14 05:30:06 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:46.441: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 16:50:46.441: INFO: cirros-15453-7f8dd5d978-gjlvv from default started at 2020-01-14 11:03:42 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:46.441: INFO: 	Container cirros-15453 ready: true, restart count 0
Jan 14 16:50:46.441: INFO: kube-proxy-slave4 from kube-system started at 2020-01-14 08:13:30 +0000 UTC (1 container statuses recorded)
Jan 14 16:50:46.441: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-58ec8ca0-55e4-4b55-9ed7-cfa5512b4ab1 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-58ec8ca0-55e4-4b55-9ed7-cfa5512b4ab1 off the node slave2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-58ec8ca0-55e4-4b55-9ed7-cfa5512b4ab1
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:56:14.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9802" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

â€¢ [SLOW TEST:332.943 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":278,"completed":245,"skipped":3817,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:56:15.388: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8211
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 14 16:56:16.726: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9f4dee4d-1aa1-4279-b1d8-5644a92e3dce" in namespace "projected-8211" to be "success or failure"
Jan 14 16:56:17.093: INFO: Pod "downwardapi-volume-9f4dee4d-1aa1-4279-b1d8-5644a92e3dce": Phase="Pending", Reason="", readiness=false. Elapsed: 367.151205ms
Jan 14 16:56:19.102: INFO: Pod "downwardapi-volume-9f4dee4d-1aa1-4279-b1d8-5644a92e3dce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.375727884s
Jan 14 16:56:21.136: INFO: Pod "downwardapi-volume-9f4dee4d-1aa1-4279-b1d8-5644a92e3dce": Phase="Pending", Reason="", readiness=false. Elapsed: 4.409807555s
Jan 14 16:56:24.311: INFO: Pod "downwardapi-volume-9f4dee4d-1aa1-4279-b1d8-5644a92e3dce": Phase="Pending", Reason="", readiness=false. Elapsed: 7.584839846s
Jan 14 16:56:26.574: INFO: Pod "downwardapi-volume-9f4dee4d-1aa1-4279-b1d8-5644a92e3dce": Phase="Pending", Reason="", readiness=false. Elapsed: 9.847588832s
Jan 14 16:56:28.674: INFO: Pod "downwardapi-volume-9f4dee4d-1aa1-4279-b1d8-5644a92e3dce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.94807069s
STEP: Saw pod success
Jan 14 16:56:28.674: INFO: Pod "downwardapi-volume-9f4dee4d-1aa1-4279-b1d8-5644a92e3dce" satisfied condition "success or failure"
Jan 14 16:56:28.679: INFO: Trying to get logs from node slave2 pod downwardapi-volume-9f4dee4d-1aa1-4279-b1d8-5644a92e3dce container client-container: <nil>
STEP: delete the pod
Jan 14 16:56:29.628: INFO: Waiting for pod downwardapi-volume-9f4dee4d-1aa1-4279-b1d8-5644a92e3dce to disappear
Jan 14 16:56:29.633: INFO: Pod downwardapi-volume-9f4dee4d-1aa1-4279-b1d8-5644a92e3dce no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:56:29.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8211" for this suite.

â€¢ [SLOW TEST:14.740 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":246,"skipped":3821,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:56:30.128: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1129
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jan 14 16:56:32.433: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 16:56:43.118: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:57:09.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1129" for this suite.

â€¢ [SLOW TEST:39.918 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":278,"completed":247,"skipped":3827,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:57:10.046: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4715
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 14 16:57:12.996: INFO: Waiting up to 5m0s for pod "pod-41ad4de9-0706-44fe-87d5-f2d50bec5687" in namespace "emptydir-4715" to be "success or failure"
Jan 14 16:57:13.445: INFO: Pod "pod-41ad4de9-0706-44fe-87d5-f2d50bec5687": Phase="Pending", Reason="", readiness=false. Elapsed: 449.022386ms
Jan 14 16:57:15.616: INFO: Pod "pod-41ad4de9-0706-44fe-87d5-f2d50bec5687": Phase="Pending", Reason="", readiness=false. Elapsed: 2.62003779s
Jan 14 16:57:17.922: INFO: Pod "pod-41ad4de9-0706-44fe-87d5-f2d50bec5687": Phase="Pending", Reason="", readiness=false. Elapsed: 4.92561245s
Jan 14 16:57:20.265: INFO: Pod "pod-41ad4de9-0706-44fe-87d5-f2d50bec5687": Phase="Pending", Reason="", readiness=false. Elapsed: 7.268591421s
Jan 14 16:57:22.349: INFO: Pod "pod-41ad4de9-0706-44fe-87d5-f2d50bec5687": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.353099465s
STEP: Saw pod success
Jan 14 16:57:22.349: INFO: Pod "pod-41ad4de9-0706-44fe-87d5-f2d50bec5687" satisfied condition "success or failure"
Jan 14 16:57:22.379: INFO: Trying to get logs from node slave2 pod pod-41ad4de9-0706-44fe-87d5-f2d50bec5687 container test-container: <nil>
STEP: delete the pod
Jan 14 16:57:22.672: INFO: Waiting for pod pod-41ad4de9-0706-44fe-87d5-f2d50bec5687 to disappear
Jan 14 16:57:22.681: INFO: Pod pod-41ad4de9-0706-44fe-87d5-f2d50bec5687 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:57:22.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4715" for this suite.

â€¢ [SLOW TEST:13.429 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":248,"skipped":3830,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:57:23.477: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3430
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 16:57:27.072: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"fff7a886-51d8-4e56-bc79-743bd19ef307", Controller:(*bool)(0xc00486bc12), BlockOwnerDeletion:(*bool)(0xc00486bc13)}}
Jan 14 16:57:27.363: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"d2794b2f-ed5b-488d-a923-a79c079b6c35", Controller:(*bool)(0xc0046ff9a2), BlockOwnerDeletion:(*bool)(0xc0046ff9a3)}}
Jan 14 16:57:27.596: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"f701181b-d6d5-4eb4-bb02-0b861a93456b", Controller:(*bool)(0xc0046ffc6a), BlockOwnerDeletion:(*bool)(0xc0046ffc6b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:57:34.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3430" for this suite.

â€¢ [SLOW TEST:11.242 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":278,"completed":249,"skipped":3862,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:57:34.722: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4419
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service multi-endpoint-test in namespace services-4419
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4419 to expose endpoints map[]
Jan 14 16:57:38.405: INFO: Get endpoints failed (364.369963ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Jan 14 16:57:39.681: INFO: successfully validated that service multi-endpoint-test in namespace services-4419 exposes endpoints map[] (1.640529358s elapsed)
STEP: Creating pod pod1 in namespace services-4419
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4419 to expose endpoints map[pod1:[100]]
Jan 14 16:57:45.280: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]] (5.105889204s elapsed, will retry)
Jan 14 16:57:48.634: INFO: successfully validated that service multi-endpoint-test in namespace services-4419 exposes endpoints map[pod1:[100]] (8.459995152s elapsed)
STEP: Creating pod pod2 in namespace services-4419
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4419 to expose endpoints map[pod1:[100] pod2:[101]]
Jan 14 16:57:53.381: INFO: Unexpected endpoints: found map[77ec2fed-90ee-43b1-af1d-373165c96014:[100]], expected map[pod1:[100] pod2:[101]] (4.612069545s elapsed, will retry)
Jan 14 16:57:59.266: INFO: Unexpected endpoints: found map[77ec2fed-90ee-43b1-af1d-373165c96014:[100]], expected map[pod1:[100] pod2:[101]] (10.497495295s elapsed, will retry)
Jan 14 16:58:00.281: INFO: successfully validated that service multi-endpoint-test in namespace services-4419 exposes endpoints map[pod1:[100] pod2:[101]] (11.512361199s elapsed)
STEP: Deleting pod pod1 in namespace services-4419
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4419 to expose endpoints map[pod2:[101]]
Jan 14 16:58:01.412: INFO: successfully validated that service multi-endpoint-test in namespace services-4419 exposes endpoints map[pod2:[101]] (1.092878714s elapsed)
STEP: Deleting pod pod2 in namespace services-4419
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4419 to expose endpoints map[]
Jan 14 16:58:01.889: INFO: successfully validated that service multi-endpoint-test in namespace services-4419 exposes endpoints map[] (298.945918ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:58:02.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4419" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:28.335 seconds]
[sig-network] Services
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":278,"completed":250,"skipped":3865,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:58:03.058: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3262
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-5370c3b6-cc7b-4f16-bcf1-4346f2c67c8d
STEP: Creating a pod to test consume secrets
Jan 14 16:58:06.407: INFO: Waiting up to 5m0s for pod "pod-secrets-f9e240c4-2a99-42f7-b0a3-b5a07d631271" in namespace "secrets-3262" to be "success or failure"
Jan 14 16:58:07.035: INFO: Pod "pod-secrets-f9e240c4-2a99-42f7-b0a3-b5a07d631271": Phase="Pending", Reason="", readiness=false. Elapsed: 628.935329ms
Jan 14 16:58:09.225: INFO: Pod "pod-secrets-f9e240c4-2a99-42f7-b0a3-b5a07d631271": Phase="Pending", Reason="", readiness=false. Elapsed: 2.818304819s
Jan 14 16:58:11.241: INFO: Pod "pod-secrets-f9e240c4-2a99-42f7-b0a3-b5a07d631271": Phase="Pending", Reason="", readiness=false. Elapsed: 4.834003877s
Jan 14 16:58:13.437: INFO: Pod "pod-secrets-f9e240c4-2a99-42f7-b0a3-b5a07d631271": Phase="Pending", Reason="", readiness=false. Elapsed: 7.030916065s
Jan 14 16:58:15.506: INFO: Pod "pod-secrets-f9e240c4-2a99-42f7-b0a3-b5a07d631271": Phase="Pending", Reason="", readiness=false. Elapsed: 9.099416566s
Jan 14 16:58:17.521: INFO: Pod "pod-secrets-f9e240c4-2a99-42f7-b0a3-b5a07d631271": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.114031702s
STEP: Saw pod success
Jan 14 16:58:17.521: INFO: Pod "pod-secrets-f9e240c4-2a99-42f7-b0a3-b5a07d631271" satisfied condition "success or failure"
Jan 14 16:58:17.525: INFO: Trying to get logs from node slave2 pod pod-secrets-f9e240c4-2a99-42f7-b0a3-b5a07d631271 container secret-volume-test: <nil>
STEP: delete the pod
Jan 14 16:58:17.858: INFO: Waiting for pod pod-secrets-f9e240c4-2a99-42f7-b0a3-b5a07d631271 to disappear
Jan 14 16:58:17.865: INFO: Pod pod-secrets-f9e240c4-2a99-42f7-b0a3-b5a07d631271 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:58:17.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3262" for this suite.

â€¢ [SLOW TEST:16.749 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":251,"skipped":3868,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:58:19.808: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9405
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Jan 14 16:58:20.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 create -f - --namespace=kubectl-9405'
Jan 14 16:58:27.656: INFO: stderr: ""
Jan 14 16:58:27.657: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Jan 14 16:58:29.706: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 16:58:29.707: INFO: Found 0 / 1
Jan 14 16:58:30.852: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 16:58:30.852: INFO: Found 0 / 1
Jan 14 16:58:31.662: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 16:58:31.662: INFO: Found 0 / 1
Jan 14 16:58:32.751: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 16:58:32.752: INFO: Found 0 / 1
Jan 14 16:58:33.714: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 16:58:33.714: INFO: Found 0 / 1
Jan 14 16:58:34.789: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 16:58:34.789: INFO: Found 0 / 1
Jan 14 16:58:35.662: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 16:58:35.662: INFO: Found 0 / 1
Jan 14 16:58:36.871: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 16:58:36.871: INFO: Found 1 / 1
Jan 14 16:58:36.871: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jan 14 16:58:36.895: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 16:58:36.895: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 14 16:58:36.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 patch pod agnhost-master-5jc2b --namespace=kubectl-9405 -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 14 16:58:38.097: INFO: stderr: ""
Jan 14 16:58:38.097: INFO: stdout: "pod/agnhost-master-5jc2b patched\n"
STEP: checking annotations
Jan 14 16:58:38.246: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 14 16:58:38.246: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:58:38.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9405" for this suite.

â€¢ [SLOW TEST:18.770 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1519
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":278,"completed":252,"skipped":3881,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:58:38.578: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6940
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:58:53.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6940" for this suite.

â€¢ [SLOW TEST:16.350 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":278,"completed":253,"skipped":3930,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:58:54.929: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9558
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 16:58:56.670: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 16:59:07.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9558" for this suite.

â€¢ [SLOW TEST:13.191 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":278,"completed":254,"skipped":3943,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 16:59:08.122: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4972
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-4972
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 14 16:59:09.320: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 14 17:00:01.865: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.151.161.33 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4972 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 17:00:01.865: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 17:00:04.114: INFO: Found all expected endpoints: [netserver-0]
Jan 14 17:00:04.192: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.151.208.29 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4972 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 17:00:04.192: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 17:00:05.972: INFO: Found all expected endpoints: [netserver-1]
Jan 14 17:00:05.981: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.151.32.33 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4972 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 17:00:05.981: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 17:00:07.398: INFO: Found all expected endpoints: [netserver-2]
Jan 14 17:00:08.000: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.151.51.64 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4972 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 17:00:08.000: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 17:00:09.979: INFO: Found all expected endpoints: [netserver-3]
Jan 14 17:00:09.990: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.151.49.49 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4972 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 17:00:09.990: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 17:00:11.533: INFO: Found all expected endpoints: [netserver-4]
Jan 14 17:00:11.840: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.151.194.128 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4972 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 17:00:11.840: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 17:00:13.617: INFO: Found all expected endpoints: [netserver-5]
Jan 14 17:00:13.627: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.151.26.72 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4972 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 14 17:00:13.627: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
Jan 14 17:00:15.005: INFO: Found all expected endpoints: [netserver-6]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 17:00:15.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4972" for this suite.

â€¢ [SLOW TEST:67.061 seconds]
[sig-network] Networking
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":255,"skipped":4036,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 17:00:15.183: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8961
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 14 17:00:17.508: INFO: Waiting up to 5m0s for pod "pod-329812a2-016f-41eb-9b2e-664e96b1b078" in namespace "emptydir-8961" to be "success or failure"
Jan 14 17:00:18.110: INFO: Pod "pod-329812a2-016f-41eb-9b2e-664e96b1b078": Phase="Pending", Reason="", readiness=false. Elapsed: 601.149619ms
Jan 14 17:00:20.296: INFO: Pod "pod-329812a2-016f-41eb-9b2e-664e96b1b078": Phase="Pending", Reason="", readiness=false. Elapsed: 2.787780342s
Jan 14 17:00:22.537: INFO: Pod "pod-329812a2-016f-41eb-9b2e-664e96b1b078": Phase="Pending", Reason="", readiness=false. Elapsed: 5.02878783s
Jan 14 17:00:24.807: INFO: Pod "pod-329812a2-016f-41eb-9b2e-664e96b1b078": Phase="Pending", Reason="", readiness=false. Elapsed: 7.298689423s
Jan 14 17:00:27.009: INFO: Pod "pod-329812a2-016f-41eb-9b2e-664e96b1b078": Phase="Pending", Reason="", readiness=false. Elapsed: 9.500346543s
Jan 14 17:00:29.963: INFO: Pod "pod-329812a2-016f-41eb-9b2e-664e96b1b078": Phase="Pending", Reason="", readiness=false. Elapsed: 12.454452495s
Jan 14 17:00:32.635: INFO: Pod "pod-329812a2-016f-41eb-9b2e-664e96b1b078": Phase="Pending", Reason="", readiness=false. Elapsed: 15.126440512s
Jan 14 17:00:34.721: INFO: Pod "pod-329812a2-016f-41eb-9b2e-664e96b1b078": Phase="Succeeded", Reason="", readiness=false. Elapsed: 17.212552323s
STEP: Saw pod success
Jan 14 17:00:34.721: INFO: Pod "pod-329812a2-016f-41eb-9b2e-664e96b1b078" satisfied condition "success or failure"
Jan 14 17:00:35.225: INFO: Trying to get logs from node slave2 pod pod-329812a2-016f-41eb-9b2e-664e96b1b078 container test-container: <nil>
STEP: delete the pod
Jan 14 17:00:36.167: INFO: Waiting for pod pod-329812a2-016f-41eb-9b2e-664e96b1b078 to disappear
Jan 14 17:00:36.213: INFO: Pod pod-329812a2-016f-41eb-9b2e-664e96b1b078 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 17:00:36.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8961" for this suite.

â€¢ [SLOW TEST:22.950 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":256,"skipped":4071,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 17:00:38.134: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3396
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl run job
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1768
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan 14 17:00:40.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-3396'
Jan 14 17:00:40.636: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 14 17:00:40.659: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1773
Jan 14 17:00:40.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 delete jobs e2e-test-httpd-job --namespace=kubectl-3396'
Jan 14 17:00:43.019: INFO: stderr: ""
Jan 14 17:00:43.019: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 17:00:43.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3396" for this suite.

â€¢ [SLOW TEST:5.686 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1764
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run job should create a job from an image when restart is OnFailure  [Conformance]","total":278,"completed":257,"skipped":4077,"failed":0}
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 17:00:43.821: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3771
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override arguments
Jan 14 17:00:46.753: INFO: Waiting up to 5m0s for pod "client-containers-5e556bb4-f383-4472-a86c-bb63e60e10a7" in namespace "containers-3771" to be "success or failure"
Jan 14 17:00:47.011: INFO: Pod "client-containers-5e556bb4-f383-4472-a86c-bb63e60e10a7": Phase="Pending", Reason="", readiness=false. Elapsed: 257.74757ms
Jan 14 17:00:50.051: INFO: Pod "client-containers-5e556bb4-f383-4472-a86c-bb63e60e10a7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.297328545s
Jan 14 17:00:52.212: INFO: Pod "client-containers-5e556bb4-f383-4472-a86c-bb63e60e10a7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.458591043s
Jan 14 17:00:54.412: INFO: Pod "client-containers-5e556bb4-f383-4472-a86c-bb63e60e10a7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.65820667s
Jan 14 17:00:56.597: INFO: Pod "client-containers-5e556bb4-f383-4472-a86c-bb63e60e10a7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.843153687s
Jan 14 17:00:58.790: INFO: Pod "client-containers-5e556bb4-f383-4472-a86c-bb63e60e10a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.036194802s
STEP: Saw pod success
Jan 14 17:00:58.790: INFO: Pod "client-containers-5e556bb4-f383-4472-a86c-bb63e60e10a7" satisfied condition "success or failure"
Jan 14 17:00:59.060: INFO: Trying to get logs from node slave2 pod client-containers-5e556bb4-f383-4472-a86c-bb63e60e10a7 container test-container: <nil>
STEP: delete the pod
Jan 14 17:01:00.218: INFO: Waiting for pod client-containers-5e556bb4-f383-4472-a86c-bb63e60e10a7 to disappear
Jan 14 17:01:00.224: INFO: Pod client-containers-5e556bb4-f383-4472-a86c-bb63e60e10a7 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 17:01:00.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3771" for this suite.

â€¢ [SLOW TEST:17.301 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":278,"completed":258,"skipped":4078,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 17:01:01.122: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2474
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-2474
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating stateful set ss in namespace statefulset-2474
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2474
Jan 14 17:01:04.493: INFO: Found 0 stateful pods, waiting for 1
Jan 14 17:01:14.511: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Jan 14 17:01:24.605: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jan 14 17:01:24.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 14 17:01:25.270: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 14 17:01:25.270: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 14 17:01:25.270: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 14 17:01:25.339: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 14 17:01:36.013: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 14 17:01:36.013: INFO: Waiting for statefulset status.replicas updated to 0
Jan 14 17:01:36.711: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jan 14 17:01:36.711: INFO: ss-0  slave2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:05 +0000 UTC  }]
Jan 14 17:01:36.711: INFO: 
Jan 14 17:01:36.711: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 14 17:01:37.977: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jan 14 17:01:37.977: INFO: ss-0  slave2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:05 +0000 UTC  }]
Jan 14 17:01:37.977: INFO: ss-1  slave3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:37 +0000 UTC  }]
Jan 14 17:01:37.977: INFO: 
Jan 14 17:01:37.977: INFO: StatefulSet ss has not reached scale 3, at 2
Jan 14 17:01:38.984: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.641093032s
Jan 14 17:01:40.037: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.633649951s
Jan 14 17:01:41.137: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.58086233s
Jan 14 17:01:42.150: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.480765788s
Jan 14 17:01:43.667: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.468011345s
Jan 14 17:01:44.880: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.94630953s
Jan 14 17:01:46.167: INFO: Verifying statefulset ss doesn't scale past 3 for another 738.453223ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2474
Jan 14 17:01:47.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:01:48.809: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 14 17:01:48.809: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 14 17:01:48.809: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 14 17:01:48.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:01:49.693: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 14 17:01:49.693: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 14 17:01:49.693: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 14 17:01:49.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:01:50.722: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 14 17:01:50.722: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 14 17:01:50.722: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 14 17:01:50.905: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 14 17:01:50.905: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 14 17:01:50.905: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jan 14 17:01:50.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 14 17:01:51.520: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 14 17:01:51.520: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 14 17:01:51.520: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 14 17:01:51.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 14 17:01:52.819: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 14 17:01:52.819: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 14 17:01:52.819: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 14 17:01:52.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 14 17:01:53.412: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 14 17:01:53.412: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 14 17:01:53.412: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 14 17:01:53.412: INFO: Waiting for statefulset status.replicas updated to 0
Jan 14 17:01:53.426: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan 14 17:02:03.435: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 14 17:02:03.435: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 14 17:02:03.435: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 14 17:02:03.457: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jan 14 17:02:03.457: INFO: ss-0  slave2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:05 +0000 UTC  }]
Jan 14 17:02:03.457: INFO: ss-1  slave3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:37 +0000 UTC  }]
Jan 14 17:02:03.457: INFO: ss-2  slave1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:37 +0000 UTC  }]
Jan 14 17:02:03.457: INFO: 
Jan 14 17:02:03.457: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 14 17:02:04.711: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jan 14 17:02:04.711: INFO: ss-0  slave2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:05 +0000 UTC  }]
Jan 14 17:02:04.711: INFO: ss-1  slave3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:37 +0000 UTC  }]
Jan 14 17:02:04.712: INFO: ss-2  slave1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:37 +0000 UTC  }]
Jan 14 17:02:04.712: INFO: 
Jan 14 17:02:04.712: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 14 17:02:06.525: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jan 14 17:02:06.525: INFO: ss-0  slave2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:05 +0000 UTC  }]
Jan 14 17:02:06.525: INFO: ss-1  slave3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:37 +0000 UTC  }]
Jan 14 17:02:06.525: INFO: ss-2  slave1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:37 +0000 UTC  }]
Jan 14 17:02:06.525: INFO: 
Jan 14 17:02:06.525: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 14 17:02:07.711: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jan 14 17:02:07.711: INFO: ss-0  slave2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:05 +0000 UTC  }]
Jan 14 17:02:07.711: INFO: ss-1  slave3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:37 +0000 UTC  }]
Jan 14 17:02:07.711: INFO: ss-2  slave1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:37 +0000 UTC  }]
Jan 14 17:02:07.711: INFO: 
Jan 14 17:02:07.711: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 14 17:02:08.810: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jan 14 17:02:08.814: INFO: ss-0  slave2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:05 +0000 UTC  }]
Jan 14 17:02:08.814: INFO: ss-1  slave3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:37 +0000 UTC  }]
Jan 14 17:02:08.814: INFO: ss-2  slave1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:37 +0000 UTC  }]
Jan 14 17:02:08.814: INFO: 
Jan 14 17:02:08.814: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 14 17:02:09.929: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jan 14 17:02:09.929: INFO: ss-0  slave2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:05 +0000 UTC  }]
Jan 14 17:02:09.929: INFO: ss-1  slave3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:37 +0000 UTC  }]
Jan 14 17:02:09.929: INFO: ss-2  slave1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:37 +0000 UTC  }]
Jan 14 17:02:09.929: INFO: 
Jan 14 17:02:09.929: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 14 17:02:10.967: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jan 14 17:02:10.967: INFO: ss-0  slave2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:05 +0000 UTC  }]
Jan 14 17:02:10.967: INFO: ss-1  slave3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:37 +0000 UTC  }]
Jan 14 17:02:10.967: INFO: ss-2  slave1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:37 +0000 UTC  }]
Jan 14 17:02:10.967: INFO: 
Jan 14 17:02:10.967: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 14 17:02:11.994: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jan 14 17:02:11.994: INFO: ss-0  slave2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:05 +0000 UTC  }]
Jan 14 17:02:11.994: INFO: ss-1  slave3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:37 +0000 UTC  }]
Jan 14 17:02:11.994: INFO: ss-2  slave1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:37 +0000 UTC  }]
Jan 14 17:02:11.994: INFO: 
Jan 14 17:02:11.994: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 14 17:02:13.326: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jan 14 17:02:13.327: INFO: ss-0  slave2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:05 +0000 UTC  }]
Jan 14 17:02:13.327: INFO: ss-1  slave3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:37 +0000 UTC  }]
Jan 14 17:02:13.327: INFO: ss-2  slave1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-14 17:01:37 +0000 UTC  }]
Jan 14 17:02:13.327: INFO: 
Jan 14 17:02:13.327: INFO: StatefulSet ss has not reached scale 0, at 3
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2474
Jan 14 17:02:14.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:02:15.520: INFO: rc: 1
Jan 14 17:02:15.520: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Jan 14 17:02:25.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:02:25.874: INFO: rc: 1
Jan 14 17:02:25.874: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jan 14 17:02:35.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:02:36.285: INFO: rc: 1
Jan 14 17:02:36.286: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jan 14 17:02:46.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:02:47.603: INFO: rc: 1
Jan 14 17:02:47.603: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jan 14 17:02:57.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:02:57.949: INFO: rc: 1
Jan 14 17:02:57.949: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jan 14 17:03:07.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:03:08.233: INFO: rc: 1
Jan 14 17:03:08.233: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jan 14 17:03:18.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:03:18.573: INFO: rc: 1
Jan 14 17:03:18.573: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jan 14 17:03:28.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:03:29.008: INFO: rc: 1
Jan 14 17:03:29.008: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jan 14 17:03:39.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:03:39.701: INFO: rc: 1
Jan 14 17:03:39.701: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jan 14 17:03:49.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:03:49.986: INFO: rc: 1
Jan 14 17:03:49.986: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jan 14 17:03:59.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:04:00.901: INFO: rc: 1
Jan 14 17:04:00.903: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jan 14 17:04:10.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:04:11.319: INFO: rc: 1
Jan 14 17:04:11.319: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jan 14 17:04:21.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:04:21.875: INFO: rc: 1
Jan 14 17:04:21.876: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jan 14 17:04:31.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:04:32.141: INFO: rc: 1
Jan 14 17:04:32.141: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jan 14 17:04:42.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:04:42.362: INFO: rc: 1
Jan 14 17:04:42.362: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jan 14 17:04:52.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:04:52.922: INFO: rc: 1
Jan 14 17:04:52.923: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jan 14 17:05:02.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:05:03.252: INFO: rc: 1
Jan 14 17:05:03.252: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jan 14 17:05:13.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:05:13.423: INFO: rc: 1
Jan 14 17:05:13.423: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jan 14 17:05:23.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:05:23.933: INFO: rc: 1
Jan 14 17:05:23.933: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jan 14 17:05:33.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:05:34.440: INFO: rc: 1
Jan 14 17:05:34.441: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jan 14 17:05:44.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:05:44.722: INFO: rc: 1
Jan 14 17:05:44.722: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jan 14 17:05:54.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:05:55.022: INFO: rc: 1
Jan 14 17:05:55.022: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jan 14 17:06:05.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:06:05.133: INFO: rc: 1
Jan 14 17:06:05.133: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jan 14 17:06:15.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:06:15.315: INFO: rc: 1
Jan 14 17:06:15.315: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jan 14 17:06:25.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:06:26.115: INFO: rc: 1
Jan 14 17:06:26.115: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jan 14 17:06:36.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:06:36.922: INFO: rc: 1
Jan 14 17:06:36.922: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jan 14 17:06:46.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:06:47.336: INFO: rc: 1
Jan 14 17:06:47.336: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jan 14 17:06:57.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:06:58.300: INFO: rc: 1
Jan 14 17:06:58.300: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jan 14 17:07:08.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:07:08.765: INFO: rc: 1
Jan 14 17:07:08.765: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jan 14 17:07:18.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=statefulset-2474 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 14 17:07:19.739: INFO: rc: 1
Jan 14 17:07:19.739: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
Jan 14 17:07:19.739: INFO: Scaling statefulset ss to 0
Jan 14 17:07:19.808: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jan 14 17:07:19.816: INFO: Deleting all statefulset in ns statefulset-2474
Jan 14 17:07:19.820: INFO: Scaling statefulset ss to 0
Jan 14 17:07:19.832: INFO: Waiting for statefulset status.replicas updated to 0
Jan 14 17:07:19.836: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 17:07:20.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2474" for this suite.

â€¢ [SLOW TEST:378.976 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":278,"completed":259,"skipped":4086,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 17:07:20.100: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8574
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 14 17:07:26.767: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 14 17:07:30.560: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714618447, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714618447, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714618447, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714618446, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 17:07:32.818: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714618447, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714618447, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714618447, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714618446, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 17:07:35.021: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714618447, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714618447, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714618447, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714618446, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 17:07:36.900: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714618447, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714618447, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714618447, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714618446, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 14 17:07:38.984: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714618447, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714618447, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714618447, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714618446, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 14 17:07:43.034: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 17:07:43.231: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 17:07:53.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8574" for this suite.
STEP: Destroying namespace "webhook-8574-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:36.631 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":278,"completed":260,"skipped":4100,"failed":0}
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 17:07:56.734: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8689
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jan 14 17:08:24.550: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 14 17:08:24.616: INFO: Pod pod-with-prestop-http-hook still exists
Jan 14 17:08:26.616: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 14 17:08:26.821: INFO: Pod pod-with-prestop-http-hook still exists
Jan 14 17:08:28.616: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 14 17:08:28.800: INFO: Pod pod-with-prestop-http-hook still exists
Jan 14 17:08:30.618: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 14 17:08:31.013: INFO: Pod pod-with-prestop-http-hook still exists
Jan 14 17:08:32.619: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 14 17:08:32.825: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 17:08:32.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8689" for this suite.

â€¢ [SLOW TEST:36.369 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":278,"completed":261,"skipped":4101,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 17:08:33.103: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8832
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 14 17:08:37.912: INFO: Waiting up to 5m0s for pod "downwardapi-volume-402b3c5b-d176-4859-a98c-4fc6e94d19c3" in namespace "downward-api-8832" to be "success or failure"
Jan 14 17:08:38.298: INFO: Pod "downwardapi-volume-402b3c5b-d176-4859-a98c-4fc6e94d19c3": Phase="Pending", Reason="", readiness=false. Elapsed: 386.355246ms
Jan 14 17:08:40.313: INFO: Pod "downwardapi-volume-402b3c5b-d176-4859-a98c-4fc6e94d19c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.400868896s
Jan 14 17:08:43.112: INFO: Pod "downwardapi-volume-402b3c5b-d176-4859-a98c-4fc6e94d19c3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.200396502s
Jan 14 17:08:45.324: INFO: Pod "downwardapi-volume-402b3c5b-d176-4859-a98c-4fc6e94d19c3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.412327847s
Jan 14 17:08:47.647: INFO: Pod "downwardapi-volume-402b3c5b-d176-4859-a98c-4fc6e94d19c3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.735156127s
Jan 14 17:08:49.693: INFO: Pod "downwardapi-volume-402b3c5b-d176-4859-a98c-4fc6e94d19c3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.78105889s
Jan 14 17:08:51.798: INFO: Pod "downwardapi-volume-402b3c5b-d176-4859-a98c-4fc6e94d19c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.886350518s
STEP: Saw pod success
Jan 14 17:08:51.799: INFO: Pod "downwardapi-volume-402b3c5b-d176-4859-a98c-4fc6e94d19c3" satisfied condition "success or failure"
Jan 14 17:08:51.804: INFO: Trying to get logs from node slave2 pod downwardapi-volume-402b3c5b-d176-4859-a98c-4fc6e94d19c3 container client-container: <nil>
STEP: delete the pod
Jan 14 17:08:52.339: INFO: Waiting for pod downwardapi-volume-402b3c5b-d176-4859-a98c-4fc6e94d19c3 to disappear
Jan 14 17:08:52.347: INFO: Pod downwardapi-volume-402b3c5b-d176-4859-a98c-4fc6e94d19c3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 17:08:52.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8832" for this suite.

â€¢ [SLOW TEST:19.421 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":262,"skipped":4115,"failed":0}
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 17:08:52.525: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-2803
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 14 17:08:55.297: INFO: (0) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 457.751778ms)
Jan 14 17:08:55.304: INFO: (1) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.929151ms)
Jan 14 17:08:55.308: INFO: (2) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.870165ms)
Jan 14 17:08:55.315: INFO: (3) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.024863ms)
Jan 14 17:08:55.320: INFO: (4) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.357646ms)
Jan 14 17:08:55.323: INFO: (5) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.07775ms)
Jan 14 17:08:55.327: INFO: (6) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.14504ms)
Jan 14 17:08:55.331: INFO: (7) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.752819ms)
Jan 14 17:08:55.335: INFO: (8) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.030287ms)
Jan 14 17:08:55.341: INFO: (9) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.454248ms)
Jan 14 17:08:55.345: INFO: (10) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.184026ms)
Jan 14 17:08:55.350: INFO: (11) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.237794ms)
Jan 14 17:08:55.355: INFO: (12) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.057789ms)
Jan 14 17:08:55.359: INFO: (13) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.689674ms)
Jan 14 17:08:55.363: INFO: (14) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.66328ms)
Jan 14 17:08:55.367: INFO: (15) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.148432ms)
Jan 14 17:08:55.371: INFO: (16) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.677661ms)
Jan 14 17:08:55.379: INFO: (17) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.401757ms)
Jan 14 17:08:55.382: INFO: (18) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.318402ms)
Jan 14 17:08:55.401: INFO: (19) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 18.79995ms)
[AfterEach] version v1
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 17:08:55.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2803" for this suite.
â€¢{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":278,"completed":263,"skipped":4115,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 17:08:55.546: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-127
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl label
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1362
STEP: creating the pod
Jan 14 17:08:58.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 create -f - --namespace=kubectl-127'
Jan 14 17:09:07.036: INFO: stderr: ""
Jan 14 17:09:07.036: INFO: stdout: "pod/pause created\n"
Jan 14 17:09:07.036: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 14 17:09:07.036: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-127" to be "running and ready"
Jan 14 17:09:07.591: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 555.078927ms
Jan 14 17:09:09.597: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.560825647s
Jan 14 17:09:12.115: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.078717185s
Jan 14 17:09:14.319: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.282998743s
Jan 14 17:09:16.554: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 9.517274407s
Jan 14 17:09:18.558: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 11.521778512s
Jan 14 17:09:18.558: INFO: Pod "pause" satisfied condition "running and ready"
Jan 14 17:09:18.558: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: adding the label testing-label with value testing-label-value to a pod
Jan 14 17:09:18.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 label pods pause testing-label=testing-label-value --namespace=kubectl-127'
Jan 14 17:09:19.416: INFO: stderr: ""
Jan 14 17:09:19.416: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jan 14 17:09:19.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pod pause -L testing-label --namespace=kubectl-127'
Jan 14 17:09:19.770: INFO: stderr: ""
Jan 14 17:09:19.770: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          13s   testing-label-value\n"
STEP: removing the label testing-label of a pod
Jan 14 17:09:19.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 label pods pause testing-label- --namespace=kubectl-127'
Jan 14 17:09:20.470: INFO: stderr: ""
Jan 14 17:09:20.470: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jan 14 17:09:20.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pod pause -L testing-label --namespace=kubectl-127'
Jan 14 17:09:20.899: INFO: stderr: ""
Jan 14 17:09:20.900: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          14s   \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1369
STEP: using delete to clean up resources
Jan 14 17:09:20.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 delete --grace-period=0 --force -f - --namespace=kubectl-127'
Jan 14 17:09:21.393: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 14 17:09:21.393: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 14 17:09:21.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get rc,svc -l name=pause --no-headers --namespace=kubectl-127'
Jan 14 17:09:21.613: INFO: stderr: "No resources found in kubectl-127 namespace.\n"
Jan 14 17:09:21.617: INFO: stdout: ""
Jan 14 17:09:21.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 get pods -l name=pause --namespace=kubectl-127 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 14 17:09:21.979: INFO: stderr: ""
Jan 14 17:09:21.979: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 17:09:21.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-127" for this suite.

â€¢ [SLOW TEST:27.236 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":278,"completed":264,"skipped":4172,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 17:09:22.783: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2054
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 14 17:09:25.074: INFO: Waiting up to 5m0s for pod "pod-461969e0-2f9d-4977-a1b4-d356f89a20cf" in namespace "emptydir-2054" to be "success or failure"
Jan 14 17:09:25.193: INFO: Pod "pod-461969e0-2f9d-4977-a1b4-d356f89a20cf": Phase="Pending", Reason="", readiness=false. Elapsed: 119.053071ms
Jan 14 17:09:27.202: INFO: Pod "pod-461969e0-2f9d-4977-a1b4-d356f89a20cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.12871858s
Jan 14 17:09:29.386: INFO: Pod "pod-461969e0-2f9d-4977-a1b4-d356f89a20cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.312000588s
Jan 14 17:09:31.546: INFO: Pod "pod-461969e0-2f9d-4977-a1b4-d356f89a20cf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.472307127s
Jan 14 17:09:33.551: INFO: Pod "pod-461969e0-2f9d-4977-a1b4-d356f89a20cf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.476875893s
Jan 14 17:09:36.057: INFO: Pod "pod-461969e0-2f9d-4977-a1b4-d356f89a20cf": Phase="Pending", Reason="", readiness=false. Elapsed: 10.983199113s
Jan 14 17:09:38.181: INFO: Pod "pod-461969e0-2f9d-4977-a1b4-d356f89a20cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.106993748s
STEP: Saw pod success
Jan 14 17:09:38.181: INFO: Pod "pod-461969e0-2f9d-4977-a1b4-d356f89a20cf" satisfied condition "success or failure"
Jan 14 17:09:38.186: INFO: Trying to get logs from node slave2 pod pod-461969e0-2f9d-4977-a1b4-d356f89a20cf container test-container: <nil>
STEP: delete the pod
Jan 14 17:09:38.893: INFO: Waiting for pod pod-461969e0-2f9d-4977-a1b4-d356f89a20cf to disappear
Jan 14 17:09:38.899: INFO: Pod pod-461969e0-2f9d-4977-a1b4-d356f89a20cf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 17:09:38.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2054" for this suite.

â€¢ [SLOW TEST:16.601 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":265,"skipped":4175,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 17:09:39.383: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6776
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-ffbe0c73-bd42-4f42-968e-5b6180b05863
STEP: Creating a pod to test consume configMaps
Jan 14 17:09:42.615: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-94b46703-1d3c-4b92-bdfc-3e9177ae674b" in namespace "projected-6776" to be "success or failure"
Jan 14 17:09:42.967: INFO: Pod "pod-projected-configmaps-94b46703-1d3c-4b92-bdfc-3e9177ae674b": Phase="Pending", Reason="", readiness=false. Elapsed: 352.564149ms
Jan 14 17:09:45.260: INFO: Pod "pod-projected-configmaps-94b46703-1d3c-4b92-bdfc-3e9177ae674b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.645557953s
Jan 14 17:09:48.628: INFO: Pod "pod-projected-configmaps-94b46703-1d3c-4b92-bdfc-3e9177ae674b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013062436s
Jan 14 17:09:50.633: INFO: Pod "pod-projected-configmaps-94b46703-1d3c-4b92-bdfc-3e9177ae674b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018636994s
Jan 14 17:09:53.192: INFO: Pod "pod-projected-configmaps-94b46703-1d3c-4b92-bdfc-3e9177ae674b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.577642356s
Jan 14 17:09:55.361: INFO: Pod "pod-projected-configmaps-94b46703-1d3c-4b92-bdfc-3e9177ae674b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.746388329s
STEP: Saw pod success
Jan 14 17:09:55.361: INFO: Pod "pod-projected-configmaps-94b46703-1d3c-4b92-bdfc-3e9177ae674b" satisfied condition "success or failure"
Jan 14 17:09:55.418: INFO: Trying to get logs from node slave2 pod pod-projected-configmaps-94b46703-1d3c-4b92-bdfc-3e9177ae674b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 14 17:09:56.313: INFO: Waiting for pod pod-projected-configmaps-94b46703-1d3c-4b92-bdfc-3e9177ae674b to disappear
Jan 14 17:09:56.645: INFO: Pod pod-projected-configmaps-94b46703-1d3c-4b92-bdfc-3e9177ae674b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 17:09:56.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6776" for this suite.

â€¢ [SLOW TEST:18.430 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":278,"completed":266,"skipped":4201,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 17:09:57.813: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3314
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name projected-secret-test-c4f698aa-e648-4aa4-81cb-fb98d29da8cc
STEP: Creating a pod to test consume secrets
Jan 14 17:10:00.989: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f83eab62-d25e-448d-b15d-901ed7158fe9" in namespace "projected-3314" to be "success or failure"
Jan 14 17:10:00.994: INFO: Pod "pod-projected-secrets-f83eab62-d25e-448d-b15d-901ed7158fe9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.497844ms
Jan 14 17:10:03.117: INFO: Pod "pod-projected-secrets-f83eab62-d25e-448d-b15d-901ed7158fe9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.128125842s
Jan 14 17:10:05.907: INFO: Pod "pod-projected-secrets-f83eab62-d25e-448d-b15d-901ed7158fe9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.918075105s
Jan 14 17:10:08.215: INFO: Pod "pod-projected-secrets-f83eab62-d25e-448d-b15d-901ed7158fe9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.22649179s
Jan 14 17:10:10.624: INFO: Pod "pod-projected-secrets-f83eab62-d25e-448d-b15d-901ed7158fe9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.635530873s
Jan 14 17:10:12.822: INFO: Pod "pod-projected-secrets-f83eab62-d25e-448d-b15d-901ed7158fe9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.832726828s
Jan 14 17:10:14.888: INFO: Pod "pod-projected-secrets-f83eab62-d25e-448d-b15d-901ed7158fe9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.899039465s
Jan 14 17:10:17.417: INFO: Pod "pod-projected-secrets-f83eab62-d25e-448d-b15d-901ed7158fe9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.427983908s
STEP: Saw pod success
Jan 14 17:10:17.417: INFO: Pod "pod-projected-secrets-f83eab62-d25e-448d-b15d-901ed7158fe9" satisfied condition "success or failure"
Jan 14 17:10:18.121: INFO: Trying to get logs from node slave2 pod pod-projected-secrets-f83eab62-d25e-448d-b15d-901ed7158fe9 container secret-volume-test: <nil>
STEP: delete the pod
Jan 14 17:10:19.269: INFO: Waiting for pod pod-projected-secrets-f83eab62-d25e-448d-b15d-901ed7158fe9 to disappear
Jan 14 17:10:19.274: INFO: Pod pod-projected-secrets-f83eab62-d25e-448d-b15d-901ed7158fe9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 17:10:19.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3314" for this suite.

â€¢ [SLOW TEST:21.944 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":278,"completed":267,"skipped":4208,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 17:10:19.758: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1858
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 17:10:36.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1858" for this suite.

â€¢ [SLOW TEST:17.565 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":278,"completed":268,"skipped":4241,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 17:10:37.323: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6604
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-a2e92608-f675-4dbb-9e0b-4260b9c5319c
STEP: Creating a pod to test consume configMaps
Jan 14 17:10:40.307: INFO: Waiting up to 5m0s for pod "pod-configmaps-cddd9b49-ede1-4bc7-b927-3303e8e712bd" in namespace "configmap-6604" to be "success or failure"
Jan 14 17:10:41.092: INFO: Pod "pod-configmaps-cddd9b49-ede1-4bc7-b927-3303e8e712bd": Phase="Pending", Reason="", readiness=false. Elapsed: 784.765356ms
Jan 14 17:10:43.303: INFO: Pod "pod-configmaps-cddd9b49-ede1-4bc7-b927-3303e8e712bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.996210211s
Jan 14 17:10:45.486: INFO: Pod "pod-configmaps-cddd9b49-ede1-4bc7-b927-3303e8e712bd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.178452217s
Jan 14 17:10:47.996: INFO: Pod "pod-configmaps-cddd9b49-ede1-4bc7-b927-3303e8e712bd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.689241351s
Jan 14 17:10:50.017: INFO: Pod "pod-configmaps-cddd9b49-ede1-4bc7-b927-3303e8e712bd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.710138202s
Jan 14 17:10:52.337: INFO: Pod "pod-configmaps-cddd9b49-ede1-4bc7-b927-3303e8e712bd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.029762084s
Jan 14 17:10:54.619: INFO: Pod "pod-configmaps-cddd9b49-ede1-4bc7-b927-3303e8e712bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.312195984s
STEP: Saw pod success
Jan 14 17:10:54.619: INFO: Pod "pod-configmaps-cddd9b49-ede1-4bc7-b927-3303e8e712bd" satisfied condition "success or failure"
Jan 14 17:10:55.213: INFO: Trying to get logs from node slave2 pod pod-configmaps-cddd9b49-ede1-4bc7-b927-3303e8e712bd container configmap-volume-test: <nil>
STEP: delete the pod
Jan 14 17:10:56.644: INFO: Waiting for pod pod-configmaps-cddd9b49-ede1-4bc7-b927-3303e8e712bd to disappear
Jan 14 17:10:57.187: INFO: Pod pod-configmaps-cddd9b49-ede1-4bc7-b927-3303e8e712bd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 17:10:57.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6604" for this suite.

â€¢ [SLOW TEST:20.363 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":269,"skipped":4262,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 17:10:57.689: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2809
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-b1ec0389-42b8-4405-aff4-cb18bb8294fd
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-b1ec0389-42b8-4405-aff4-cb18bb8294fd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 17:12:31.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2809" for this suite.

â€¢ [SLOW TEST:93.446 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":270,"skipped":4324,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 17:12:31.135: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1895
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod test-webserver-5c96027a-326c-4f5b-b29d-5d432caffd73 in namespace container-probe-1895
Jan 14 17:12:43.813: INFO: Started pod test-webserver-5c96027a-326c-4f5b-b29d-5d432caffd73 in namespace container-probe-1895
STEP: checking the pod's current state and verifying that restartCount is present
Jan 14 17:12:43.818: INFO: Initial restart count of pod test-webserver-5c96027a-326c-4f5b-b29d-5d432caffd73 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 17:16:44.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1895" for this suite.

â€¢ [SLOW TEST:254.090 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":278,"completed":271,"skipped":4326,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 17:16:45.233: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7307
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7307
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-7307
I0114 17:16:49.180353      23 runners.go:189] Created replication controller with name: externalname-service, namespace: services-7307, replica count: 2
I0114 17:16:52.233387      23 runners.go:189] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 17:16:55.256550      23 runners.go:189] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 17:16:58.257709      23 runners.go:189] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0114 17:17:01.258100      23 runners.go:189] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 14 17:17:04.259: INFO: Creating new exec pod
I0114 17:17:04.259825      23 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 14 17:17:14.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=services-7307 execpodt67b7 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jan 14 17:17:15.705: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 14 17:17:15.705: INFO: stdout: ""
Jan 14 17:17:15.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=services-7307 execpodt67b7 -- /bin/sh -x -c nc -zv -t -w 2 10.150.136.251 80'
Jan 14 17:17:16.316: INFO: stderr: "+ nc -zv -t -w 2 10.150.136.251 80\nConnection to 10.150.136.251 80 port [tcp/http] succeeded!\n"
Jan 14 17:17:16.316: INFO: stdout: ""
Jan 14 17:17:16.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=services-7307 execpodt67b7 -- /bin/sh -x -c nc -zv -t -w 2 192.168.0.132 32474'
Jan 14 17:17:17.317: INFO: stderr: "+ nc -zv -t -w 2 192.168.0.132 32474\nConnection to 192.168.0.132 32474 port [tcp/32474] succeeded!\n"
Jan 14 17:17:17.317: INFO: stdout: ""
Jan 14 17:17:17.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 exec --namespace=services-7307 execpodt67b7 -- /bin/sh -x -c nc -zv -t -w 2 192.168.0.161 32474'
Jan 14 17:17:18.062: INFO: stderr: "+ nc -zv -t -w 2 192.168.0.161 32474\nConnection to 192.168.0.161 32474 port [tcp/32474] succeeded!\n"
Jan 14 17:17:18.062: INFO: stdout: ""
Jan 14 17:17:18.062: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 17:17:18.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7307" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:33.025 seconds]
[sig-network] Services
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":278,"completed":272,"skipped":4361,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 17:17:18.259: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5244
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-18c6b1c1-2f63-4b2d-a4fc-775bdda0f6ab
STEP: Creating configMap with name cm-test-opt-upd-f99a8b71-f2bd-458e-8435-229a9969bd28
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-18c6b1c1-2f63-4b2d-a4fc-775bdda0f6ab
STEP: Updating configmap cm-test-opt-upd-f99a8b71-f2bd-458e-8435-229a9969bd28
STEP: Creating configMap with name cm-test-opt-create-29920cb2-e400-4565-9140-8d21e4609817
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 17:19:00.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5244" for this suite.

â€¢ [SLOW TEST:102.321 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":273,"skipped":4394,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 17:19:00.580: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-5246
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 14 17:19:27.024: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 14 17:19:27.413: INFO: Pod pod-with-poststart-http-hook still exists
Jan 14 17:19:29.413: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 14 17:19:29.418: INFO: Pod pod-with-poststart-http-hook still exists
Jan 14 17:19:31.413: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 14 17:19:31.583: INFO: Pod pod-with-poststart-http-hook still exists
Jan 14 17:19:33.413: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 14 17:19:33.418: INFO: Pod pod-with-poststart-http-hook still exists
Jan 14 17:19:35.413: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 14 17:19:35.850: INFO: Pod pod-with-poststart-http-hook still exists
Jan 14 17:19:37.413: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 14 17:19:37.480: INFO: Pod pod-with-poststart-http-hook still exists
Jan 14 17:19:39.413: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 14 17:19:39.664: INFO: Pod pod-with-poststart-http-hook still exists
Jan 14 17:19:41.413: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 14 17:19:41.497: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 17:19:41.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5246" for this suite.

â€¢ [SLOW TEST:40.934 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":278,"completed":274,"skipped":4406,"failed":0}
S
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 17:19:41.514: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5160
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 17:19:42.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5160" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
â€¢{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":278,"completed":275,"skipped":4407,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 17:19:43.012: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1783
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jan 14 17:19:45.130: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 14 17:19:45.216: INFO: Waiting for terminating namespaces to be deleted...
Jan 14 17:19:45.320: INFO: 
Logging pods the kubelet thinks is on node master1 before test
Jan 14 17:19:45.399: INFO: resource-reserver-master1 from kube-system started at 2020-01-14 11:40:33 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.399: INFO: 	Container sleep-forever ready: true, restart count 0
Jan 14 17:19:45.399: INFO: oss-provisioner-7bb5d4c769-vhrt6 from kube-system started at 2020-01-14 05:31:38 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.399: INFO: 	Container oss-provisioner ready: true, restart count 1
Jan 14 17:19:45.399: INFO: daemon-set-gg2wn from daemonsets-9852 started at 2020-01-14 13:36:55 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.399: INFO: 	Container app ready: true, restart count 0
Jan 14 17:19:45.399: INFO: kube-controller-manager-master1 from kube-system started at 2020-01-14 11:40:33 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.399: INFO: 	Container kube-controller-manager ready: true, restart count 3
Jan 14 17:19:45.399: INFO: kube-scheduler-master1 from kube-system started at 2020-01-14 11:40:33 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.399: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan 14 17:19:45.399: INFO: cinder-provisioner-xxxjh from kube-system started at 2020-01-14 05:31:24 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.399: INFO: 	Container cinder-provisioner ready: true, restart count 1
Jan 14 17:19:45.399: INFO: coredns-52ppc from kube-system started at 2020-01-14 12:58:20 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.399: INFO: 	Container coredns ready: true, restart count 0
Jan 14 17:19:45.399: INFO: kube-apiserver-master1 from kube-system started at 2020-01-14 10:55:20 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.399: INFO: 	Container kube-apiserver ready: true, restart count 1
Jan 14 17:19:45.399: INFO: kube-proxy-master1 from kube-system started at 2020-01-14 11:40:33 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.399: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 17:19:45.399: INFO: nginx-proxy-master1 from kube-system started at 2020-01-14 11:40:33 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.399: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 14 17:19:45.399: INFO: calico-node-c275v from kube-system started at 2020-01-14 05:30:06 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.399: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 17:19:45.399: INFO: kube-state-metrics-8988b595-gq7dc from monitoring started at 2020-01-14 12:57:51 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.399: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 14 17:19:45.399: INFO: 
Logging pods the kubelet thinks is on node master2 before test
Jan 14 17:19:45.626: INFO: cinder-snapshot-58545f46f8-d2m7m from kube-system started at 2020-01-14 05:31:52 +0000 UTC (2 container statuses recorded)
Jan 14 17:19:45.626: INFO: 	Container cinder-snapshot-controller ready: true, restart count 2
Jan 14 17:19:45.626: INFO: 	Container cinder-snapshot-provisioner ready: true, restart count 2
Jan 14 17:19:45.626: INFO: kube-controller-manager-master2 from kube-system started at 2020-01-14 08:23:38 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.626: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jan 14 17:19:45.626: INFO: kube-proxy-master2 from kube-system started at 2020-01-14 08:24:11 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.626: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 17:19:45.626: INFO: coredns-cnrrm from kube-system started at 2020-01-14 12:58:20 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.626: INFO: 	Container coredns ready: true, restart count 0
Jan 14 17:19:45.626: INFO: cinder-provisioner-9fk6f from kube-system started at 2020-01-14 05:31:23 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.626: INFO: 	Container cinder-provisioner ready: true, restart count 2
Jan 14 17:19:45.626: INFO: kube-apiserver-master2 from kube-system started at 2020-01-14 08:26:26 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.626: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 14 17:19:45.626: INFO: resource-reserver-master2 from kube-system started at 2020-01-14 05:27:40 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.626: INFO: 	Container sleep-forever ready: true, restart count 0
Jan 14 17:19:45.626: INFO: nginx-proxy-master2 from kube-system started at 2020-01-14 05:27:39 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.626: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 14 17:19:45.626: INFO: calico-node-vhmj5 from kube-system started at 2020-01-14 05:30:04 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.626: INFO: 	Container calico-node ready: true, restart count 2
Jan 14 17:19:45.626: INFO: kube-scheduler-master2 from kube-system started at 2020-01-14 08:24:38 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.626: INFO: 	Container kube-scheduler ready: true, restart count 2
Jan 14 17:19:45.626: INFO: daemon-set-5jq8c from daemonsets-9852 started at 2020-01-14 13:36:55 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.626: INFO: 	Container app ready: true, restart count 0
Jan 14 17:19:45.626: INFO: 
Logging pods the kubelet thinks is on node master3 before test
Jan 14 17:19:45.644: INFO: nginx-proxy-master3 from kube-system started at 2020-01-14 05:27:40 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.644: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 14 17:19:45.644: INFO: kube-apiserver-master3 from kube-system started at 2020-01-14 08:29:42 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.645: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 14 17:19:45.645: INFO: kube-controller-manager-master3 from kube-system started at 2020-01-14 08:30:08 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.645: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jan 14 17:19:45.645: INFO: resource-reserver-master3 from kube-system started at 2020-01-14 08:31:44 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.645: INFO: 	Container sleep-forever ready: true, restart count 0
Jan 14 17:19:45.645: INFO: cinder-provisioner-wbhvj from kube-system started at 2020-01-14 05:31:22 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.645: INFO: 	Container cinder-provisioner ready: true, restart count 1
Jan 14 17:19:45.645: INFO: kube-scheduler-master3 from kube-system started at 2020-01-14 08:31:06 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.645: INFO: 	Container kube-scheduler ready: true, restart count 1
Jan 14 17:19:45.645: INFO: daemon-set-247jg from daemonsets-9852 started at 2020-01-14 13:36:56 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.645: INFO: 	Container app ready: true, restart count 0
Jan 14 17:19:45.645: INFO: kube-proxy-master3 from kube-system started at 2020-01-14 08:30:38 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.645: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 17:19:45.645: INFO: calico-node-p7t5k from kube-system started at 2020-01-14 05:30:03 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.645: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 17:19:45.645: INFO: localpv-provisioner-6c5467f94-q9cvw from kube-system started at 2020-01-14 05:31:32 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.645: INFO: 	Container localpv-provisioner ready: true, restart count 1
Jan 14 17:19:45.645: INFO: coredns-f24kh from kube-system started at 2020-01-14 12:58:20 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.645: INFO: 	Container coredns ready: true, restart count 0
Jan 14 17:19:45.645: INFO: 
Logging pods the kubelet thinks is on node slave1 before test
Jan 14 17:19:45.726: INFO: kube-proxy-slave1 from kube-system started at 2020-01-14 08:17:36 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.727: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 17:19:45.734: INFO: calico-node-wzpws from kube-system started at 2020-01-14 05:30:06 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.734: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 17:19:45.734: INFO: calico-kube-controllers-6ff9f48ccd-phjqn from kube-system started at 2020-01-14 05:30:37 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.734: INFO: 	Container calico-kube-controllers ready: true, restart count 1
Jan 14 17:19:45.734: INFO: tiller-deploy-77d97f584c-l4s9r from kube-system started at 2020-01-14 05:32:44 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.734: INFO: 	Container tiller ready: true, restart count 1
Jan 14 17:19:45.734: INFO: daemon-set-d6b8h from daemonsets-9852 started at 2020-01-14 13:36:56 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.734: INFO: 	Container app ready: true, restart count 0
Jan 14 17:19:45.734: INFO: dns-test-b39ed90a-d7ff-4205-b2f7-ffe423eff36b from dns-1195 started at 2020-01-14 12:33:00 +0000 UTC (3 container statuses recorded)
Jan 14 17:19:45.734: INFO: 	Container jessie-querier ready: true, restart count 22
Jan 14 17:19:45.734: INFO: 	Container querier ready: true, restart count 25
Jan 14 17:19:45.734: INFO: 	Container webserver ready: true, restart count 0
Jan 14 17:19:45.734: INFO: 
Logging pods the kubelet thinks is on node slave2 before test
Jan 14 17:19:45.776: INFO: calico-node-gwd6p from kube-system started at 2020-01-14 05:30:03 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.776: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 17:19:45.776: INFO: agnhost-deployment-54964f567d-498qd from default started at 2020-01-14 11:05:35 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.776: INFO: 	Container agnhost ready: true, restart count 0
Jan 14 17:19:45.776: INFO: annotationupdate8b560796-65d3-444d-b2a8-95487af080c1 from downward-api-9309 started at 2020-01-14 12:08:41 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.776: INFO: 	Container client-container ready: false, restart count 0
Jan 14 17:19:45.776: INFO: daemon-set-zww4m from daemonsets-9852 started at 2020-01-14 13:36:55 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.776: INFO: 	Container app ready: true, restart count 0
Jan 14 17:19:45.776: INFO: pod-handle-http-request from container-lifecycle-hook-5246 started at 2020-01-14 17:19:05 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.776: INFO: 	Container pod-handle-http-request ready: true, restart count 0
Jan 14 17:19:45.776: INFO: kube-proxy-slave2 from kube-system started at 2020-01-14 08:17:48 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.776: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 17:19:45.776: INFO: cirros-26408-6bb4b5b58b-kfs9n from default started at 2020-01-14 11:01:57 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.776: INFO: 	Container cirros-26408 ready: true, restart count 0
Jan 14 17:19:45.776: INFO: 
Logging pods the kubelet thinks is on node slave3 before test
Jan 14 17:19:45.792: INFO: calico-node-zprlr from kube-system started at 2020-01-14 05:30:06 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.792: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 17:19:45.792: INFO: kube-proxy-slave3 from kube-system started at 2020-01-14 08:14:15 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.792: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 17:19:45.792: INFO: sonobuoy from sonobuoy started at 2020-01-14 14:39:37 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.792: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 14 17:19:45.792: INFO: sonobuoy-e2e-job-5f91618b21714006 from sonobuoy started at 2020-01-14 14:39:48 +0000 UTC (2 container statuses recorded)
Jan 14 17:19:45.792: INFO: 	Container e2e ready: true, restart count 0
Jan 14 17:19:45.792: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 14 17:19:45.792: INFO: daemon-set-gn9px from daemonsets-9852 started at 2020-01-14 13:36:55 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.792: INFO: 	Container app ready: true, restart count 0
Jan 14 17:19:45.792: INFO: 
Logging pods the kubelet thinks is on node slave4 before test
Jan 14 17:19:45.816: INFO: kube-proxy-slave4 from kube-system started at 2020-01-14 08:13:30 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.816: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 14 17:19:45.816: INFO: cirros-15453-7f8dd5d978-gjlvv from default started at 2020-01-14 11:03:42 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.816: INFO: 	Container cirros-15453 ready: true, restart count 0
Jan 14 17:19:45.816: INFO: calico-node-hc57z from kube-system started at 2020-01-14 05:30:06 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.816: INFO: 	Container calico-node ready: true, restart count 1
Jan 14 17:19:45.816: INFO: cirros-19353-6f67878d75-p7mck from default started at 2020-01-14 11:01:24 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.816: INFO: 	Container cirros-19353 ready: true, restart count 0
Jan 14 17:19:45.816: INFO: dns-test-53c7521e-6d65-4cd3-b40e-b0e2058d5107 from dns-5986 started at 2020-01-14 12:27:30 +0000 UTC (3 container statuses recorded)
Jan 14 17:19:45.816: INFO: 	Container jessie-querier ready: true, restart count 21
Jan 14 17:19:45.816: INFO: 	Container querier ready: true, restart count 24
Jan 14 17:19:45.816: INFO: 	Container webserver ready: true, restart count 0
Jan 14 17:19:45.816: INFO: daemon-set-b8jkm from daemonsets-9852 started at 2020-01-14 13:36:54 +0000 UTC (1 container statuses recorded)
Jan 14 17:19:45.816: INFO: 	Container app ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15e9d07eaca88aa4], Reason = [FailedScheduling], Message = [0/7 nodes are available: 7 node(s) didn't match node selector.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15e9d07eae8852fb], Reason = [FailedScheduling], Message = [0/7 nodes are available: 7 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 17:19:48.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1783" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

â€¢ [SLOW TEST:5.482 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":278,"completed":276,"skipped":4409,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 17:19:48.495: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4619
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 17:20:05.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4619" for this suite.

â€¢ [SLOW TEST:17.145 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":278,"completed":277,"skipped":4485,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 14 17:20:05.640: INFO: >>> kubeConfig: /tmp/kubeconfig-321483740
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8280
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1612
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan 14 17:20:09.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-8280'
Jan 14 17:20:18.483: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 14 17:20:18.483: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Jan 14 17:20:19.240: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-t6tn2]
Jan 14 17:20:19.240: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-t6tn2" in namespace "kubectl-8280" to be "running and ready"
Jan 14 17:20:19.484: INFO: Pod "e2e-test-httpd-rc-t6tn2": Phase="Pending", Reason="", readiness=false. Elapsed: 244.368912ms
Jan 14 17:20:21.489: INFO: Pod "e2e-test-httpd-rc-t6tn2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.248667027s
Jan 14 17:20:23.895: INFO: Pod "e2e-test-httpd-rc-t6tn2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.654680946s
Jan 14 17:20:26.042: INFO: Pod "e2e-test-httpd-rc-t6tn2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.801777405s
Jan 14 17:20:28.048: INFO: Pod "e2e-test-httpd-rc-t6tn2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.808563398s
Jan 14 17:20:30.080: INFO: Pod "e2e-test-httpd-rc-t6tn2": Phase="Running", Reason="", readiness=true. Elapsed: 10.839683467s
Jan 14 17:20:30.080: INFO: Pod "e2e-test-httpd-rc-t6tn2" satisfied condition "running and ready"
Jan 14 17:20:30.080: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-t6tn2]
Jan 14 17:20:30.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 logs rc/e2e-test-httpd-rc --namespace=kubectl-8280'
Jan 14 17:20:30.493: INFO: stderr: ""
Jan 14 17:20:30.493: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.151.49.70. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.151.49.70. Set the 'ServerName' directive globally to suppress this message\n[Tue Jan 14 17:20:27.085704 2020] [mpm_event:notice] [pid 1:tid 139927700274024] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Tue Jan 14 17:20:27.085771 2020] [core:notice] [pid 1:tid 139927700274024] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1617
Jan 14 17:20:30.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321483740 delete rc e2e-test-httpd-rc --namespace=kubectl-8280'
Jan 14 17:20:31.159: INFO: stderr: ""
Jan 14 17:20:31.159: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 14 17:20:31.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8280" for this suite.

â€¢ [SLOW TEST:25.724 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1608
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run rc should create an rc from an image  [Conformance]","total":278,"completed":278,"skipped":4524,"failed":0}
SSSSSSSSSSSSJan 14 17:20:31.365: INFO: Running AfterSuite actions on all nodes
Jan 14 17:20:31.366: INFO: Running AfterSuite actions on node 1
Jan 14 17:20:31.366: INFO: Skipping dumping logs from cluster
{"msg":"Test Suite completed","total":278,"completed":278,"skipped":4536,"failed":0}

Ran 278 of 4814 Specs in 9631.029 seconds
SUCCESS! -- 278 Passed | 0 Failed | 0 Pending | 4536 Skipped
PASS

Ginkgo ran 1 suite in 2h40m34.031145748s
Test Suite Passed
